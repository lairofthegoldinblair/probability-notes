\documentclass{amsbook}

\usepackage{dsfont}
\usepackage{tikz}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\card}{card}
\DeclareMathOperator*{\RePart}{Re}
\DeclareMathOperator*{\ImPart}{Im}
\DeclareMathOperator*{\median}{Med}
\DeclareMathOperator*{\diag}{Diag}
\DeclareMathOperator*{\interior}{int}
\DeclareMathOperator*{\diam}{diam}
\DeclareMathOperator*{\sgn}{sgn}
\DeclareMathOperator*{\IdentityMatrix}{Id}
\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator*{\essinf}{ess\,inf}
\DeclareMathOperator*{\bplim}{bp-lim}

\newtheorem{thm}{Theorem}[chapter]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{alg}[thm]{Algorithm}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{ex}[thm]{Exercise}
\newtheorem{xca}{Exercise}
\newtheorem{examp}[thm]{Example}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{clm}{Claim}[thm]

\newcommand{\Independent}{\perp \! \! \! \perp}
\newcommand{\cindependent}[3]{#1 \Independent_{#3} #2}
\newcommand{\expectation}[1]{\textbf{E}\left[#1\right]} 
\newcommand{\expectationop}{\textbf{E}}
\newcommand{\sexpectation}[2]{\textbf{E}_{#2}[#1]} 
\newcommand{\cexpectationop}[1]{\textbf{E}^#1} 
\newcommand{\cexpectation}[2]{\textbf{E}^{#1} #2} 
\newcommand{\cexpectationlong}[2]{\textbf{E}\left[ #2 \mid #1 \right]} 
\newcommand{\csexpectationlong}[3]{\textbf{E}_{#3}\left[ #2 \mid #1 \right]} 
\newcommand{\variance}[1]{\textbf{Var} \left (#1 \right )} 
\newcommand{\covariance}[1]{\textbf{Cov} \left (#1 \right )} 
\newcommand{\scovariance}[2]{\textbf{Cov} \left (#1, #2 \right )} 
\newcommand{\probabilityop}{\textbf{P}} 
\newcommand{\probability}[1]{\textbf{P}\{#1\}} 
\newcommand{\sprobability}[2]{\textbf{P}_{#2}\{#1\}} 
\newcommand{\sprobabilityop}[1]{\textbf{P}_{#1}}
\newcommand{\cprobability}[2]{\textbf{P}\{#2 \mid #1\}} 
\newcommand{\csprobability}[3]{\textbf{P}_{#3}\{#2 \mid #1\}} 
\newcommand{\characteristic}[1]{\textbf{1}_{#1}} 
\newcommand{\pushforward}[2]{#2 \circ #1^{-1}} 
\newcommand{\complexes}{\mathbb{C}} 
\newcommand{\reals}{\mathbb{R}} 
\newcommand{\rationals}{\mathbb{Q}} 
\newcommand{\naturals}{\mathbb{N}} 
\newcommand{\integers}{\mathbb{Z}} 
\newcommand{\distribution}[1]{\textbf{P}^{#1}}
\newcommand{\borel}[1]{\mathcal{B}(#1)} 
\newcommand{\abs}[1]{\left \vert #1 \right \vert}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\toprob}{\overset{P}\to}
\newcommand{\toas}{\overset{a.s.}\to}
\newcommand{\todist}{\overset{d}\to}
\newcommand{\toweak}{\overset{w}\to}
\newcommand{\tolp}[1]{\overset{L^#1}\to}
\newcommand{\eqfdd}{\overset{f.d.d.}=}
\newcommand{\eqdist}{\overset{d}=}
\newcommand{\kldiv}[2]{D\left( #1 \mid\mid #2 \right)}
\newcommand{\sample}[1]{\boldsymbol{#1}}
\newcommand{\range}[1]{\Re(#1)}
\newcommand{\domain}[1]{\mathcal{D}(#1)}
\newcommand{\resolventset}[1]{\rho(#1)}
\newcommand{\law}[1]{\mathcal{L}(#1)}
\newcommand{\andop}{\wedge}
\newcommand{\orop}{\vee}

\begin{document}


\frontmatter
\title{Probability Theory}
\author{David Blair}
\email{dblair@akamai.com}
\maketitle

\mainmatter

\tableofcontents

\include{RealAnalysis}

\include{MeasureTheory}

\include{Probability}

\include{Independence}

\include{ConvergenceOfRandomVariables}

\include {Lindeberg}

\include{CharacteristicFunctions}

\include{Conditioning}

\include{Martingales}

\include{Concentration}

\include{Likelihood}

\include{BrownianMotion}

\include{MarkovProcesses}

\include{StochasticIntegration}

\include{RealAnalysis2}

\include{SkorohodSpace}

\include{FellerProcesses}

\include{StochasticApproximation}

\section{Exercises}
\begin{xca}Let $f : \reals \to \reals$ be a right continuous function,
  show that $f$ is Borel measurable.
\end{xca}
\begin{proof}
It suffices to show that $f^{-1}(t, \infty)$ is Borel measurable for
all $t \in \reals$.  Let $x \in f^{-1}(t, \infty)$ then by right
continuity there exists some $y_x$ with $x < y_x$ such that $[x,y_x) \subset
f^{-1}(t, \infty)$.  Clearly we may write $f^{-1}(t, \infty) = \cup_{x
  \in f^{-1}(t, \infty)} [x, y_x)$.  We now show that we can make this
a countable union of intervals.  For a fixed $q \in \rationals$
consider the set $A_q = \cup_{\substack{x \in f^{-1}(t, \infty) \\ q
    \in [x, y_x)}} [x, y_x)$.  It is easy to see that $A_q$ is either
empty or an
interval (either open or half open) by taking least upper bounds and
greatest lower bounds of the intervals in the union.  Thus each $A_q$
is measurable.  More over each $[x,y_x)$ contains a rational number so
it follows that $[x,y_x) \subset A_q$ for some $q \in rationals$.
From this it follows that $f^{-1}(t, \infty) = \cup_{q \in
  \rationals}A_q$ which is a countable union of measurable sets and
therefore measurable.
\end{proof}

\begin{xca}Let $f(x)$ be a Lebesgue integrable function on $\reals$.
  Show that there exists a measurable $a(x)$ with $\lim_{x \to \infty}
  a(x) = \infty$ such that $a(x)f(x)$ remains integrable.
\end{xca}
\begin{proof}
It suffices to assume that $f(x) \geq 0$ and $\int f(x) \, dx = 1$.
We know from Fundamental Theorem of Calculus that $g(y) = \int_{-\infty}^y
f(x) \, dx$ is almost everywhere differentiable (and montone) and
$g^\prime(y) = f(y)$.   By
definition $\lim_{y \to \infty} g(y) = 1$.  Now define $h(z) = 1 -
\sqrt{1 -z}$ and note that by the Chain Rule (TODO: Show that the
Chain Rule is still valid for functions that are merely absolutely continuous)
\begin{align*}
\frac{d}{dy} h(g(y)) &= \frac{f(y)}{2 \sqrt{1 - g(y)}}
\end{align*}
Now by the Fundamental Theorem of Calculus again, if we define $a(x) =
\frac{1}{2 \sqrt{1 - g(x)}}$ 
then 
\begin{align*}
\int a(x) f(x) \, dx &= \lim_{y \to \infty} h(g(y)) = h(1) = 1
\end{align*}
but 
\begin{align*}
\lim_{x \to \infty} a(x) &= \lim_{x \to \infty} \frac{1}{2 \sqrt{1 -
    g(x)}} = \infty
\end{align*}
\end{proof}
\begin{xca}Let $\xi$ be a random variable, show that for all $\lambda > 0$,
\begin{align*}
\min_k \expectation{\xi^q} \lambda^{-k} \leq \inf_{s>0} \expectation{e^{s(\xi-\lambda)}}
\end{align*}
Note that this shows that the best moment bound for a tail
probability is always better than the best Chernoff bound.
\end{xca}
\begin{proof}
Let $q = \argmin_k \expectation{\xi^k} \lambda^{-k}$.  Now expand as a
series
\begin{align*}
\expectation{e^{s(\xi-\lambda)}} &= e^{-s\lambda} \sum_{k=0}^\infty
\frac{s^k \expectation{\xi^k}}{k!} \\
& \geq e^{-s\lambda} \expectation{\xi^q} \lambda^{-q} \sum_{k=0}^\infty
\frac{s^k \lambda^k}{k!} = \expectation{\xi^q} \lambda^{-q}
\end{align*}
\end{proof}
\begin{xca}Let $\xi$ be a nonnegative integer valued random variable.
  Show $\probability{\xi \neq 0} \leq \expectation{\xi}$ and 
\begin{align*}
\probability{\xi = 0} \leq \frac{\variance{\xi}}{\variance{\xi} + \left(\expectation{\xi}\right)^2}
\end{align*}
\end{xca}
\begin{proof}
For the first inequality,
\begin{align*}
\probability{\xi \neq 0} = \sum_{k=1}^\infty \probability{\xi = k} \leq
\sum_{k=1}^\infty k\probability{\xi = k} = \expectation{\xi}
\end{align*}
For the second inequality, use Cauchy-Schwartz
\begin{align*}
\left(\expectation{\xi}\right)^2 &\leq
\left(\expectation{\characteristic{\xi > 0}\xi}\right)^2 \\
&\leq \expectation{\xi^2} \probability{\xi > 0}
\end{align*}
Now use $\probability{\xi > 0} = 1 - \probability{\xi=0}$ and
$\variance{\xi} = \expectation{\xi^2} -
\left(\expectation{\xi}\right)^2$ and rearrangement of terms to get
the result.
\end{proof}

\begin{xca}Let $f : S \to T$ be function.  If $\mathcal{T}$ is a
  $\sigma$-algebra on $T$ then $\mathcal{T} \subset f_*
  f^{-1}(\mathcal{T})$.  If $\mathcal{S}$ is a $\sigma$-algebra on
  $S$, then $f^{-1}f_*(\mathcal{S}) \subset \mathcal{S}$.  Find examples where the inclusions are strict.
\end{xca}
\begin{proof}
To see the inclusions just unwind the definitions.  For the first inclusion
\begin{align*}
f_* f^{-1}(\mathcal{T}) &= \lbrace A \subset T \mid f^{-1}(A) \in
f^{-1}(\mathcal{T}) \rbrace \\
&= \lbrace A \subset T \mid f^{-1}(A) =
f^{-1}(B) \text { for some } B \in \mathcal{T} \rbrace \\
&\supset \mathcal{T}
\end{align*}
and for the second
\begin{align*}
f^{-1} f_* (\mathcal{S}) &= \lbrace f^{-1}(A) \mid A \in  f_*
(\mathcal{S}) \rbrace \\
&= \lbrace f^{-1}(A) \mid A \subset T \text{ and } f^{-1}(A) \in
\mathcal{S} \rbrace \\
&\subset \mathcal{S}
\end{align*}

TODO: Find the examples of strict inclusion.
\end{proof}
\begin{xca}Let $f : S \to T$ be a set function and let $\mathcal{C}
  \subset 2^T$ then $f^{-1}(\sigma(\mathcal{C})) = \sigma(f^{-1}(\mathcal{C}))$.
\end{xca}
\begin{proof}
We know that $f^{-1}(\sigma(\mathcal{C}))$ is a $\sigma$-algebra and
clearly $f^{-1}(\mathcal{C}) \subset f^{-1}(\sigma(\mathcal{C}))$
therefore showing $\sigma(f^{-1}(\mathcal{C})) \subset
f^{-1}(\sigma(\mathcal{C}))$.  

To see the reverse inclusion we know that 
\begin{align*}
f_* (\sigma(f^{-1}(\mathcal{C}))) &= \lbrace A \subset T \mid
f^{-1}(A) \in \sigma(f^{-1}(\mathcal{C})) \rbrace
\end{align*}
is a $\sigma$-algebra and clearly $\mathcal{C} \subset f_*
(\sigma(f^{-1}(\mathcal{C})))$.  This implies $\sigma(\mathcal{C}) \subset f_*
(\sigma(f^{-1}(\mathcal{C})))$ and thus by the result of the previous
exercise
\begin{align*}
f^{-1}(\sigma(\mathcal{C})) &\subset 
f^{-1} (f_*(\sigma(f^{-1}(\mathcal{C})))) \subset \sigma(f^{-1}(\mathcal{C}))
\end{align*}
\end{proof}
\begin{xca}Let $f(x) = \abs{x}$.  Show that $f_*(\mathcal{B}(\reals))$
  is a strict $\sigma$-subalgebra of $\mathcal{B}(\reals)$.
\end{xca}
\begin{xca}Let $f : S \to T$ be a function, $\mathcal{C} \in
  2^S$ and define $f_*(\mathcal{C}) =  \{A \subset T \mid
    f^{-1}(A) \in \mathcal{C} \}$.  Show by counterexample that
    $\sigma(f_*(\mathcal{C})) \neq f_*(\sigma(\mathcal{C}))$.
\end{xca}

\begin{xca}Let $A_n$ be a sequence of events. Show that 
\begin{align*}
\probability{A_n \text{ i.o.}} \geq \limsup_{n \to \infty} \probability{A_n}
\end{align*}
\end{xca}
\begin{proof}
Note that we know that for every $k\geq n$, $A_k \subset
\cup_{k=n}^\infty A_k$ and therefore monotonicity of measure implies $\probability{A_k} \leq
\probability{\cup_{k=n}^\infty A_k}$ for $k\geq n$.  Therefore we know
$\sup_{k\geq n} \probability{A_k} \leq
\probability{\cup_{k=n}^\infty A_k}$.

By definition and continuity of measure and applying the above,
\begin{align*}
\probability{A_n \text{ i.o.}} &= \probability{\cap_{n=1}^\infty
  \cup_{k=n}^\infty A_k} \\
&= \lim_{n \to \infty} \probability{\cup_{k=n}^\infty A_k} \\
&\geq \lim_{n \to \infty} \sup_{k \geq n} \probability{ A_k} =
\limsup_{n \to \infty} \probability{ A_n} 
\end{align*}
\end{proof}

\begin{xca}Suppose we toss a coin repeatedly and the probability of
  heads is $0 < p < 1$ (i.e. the coin may be unfair but not
  pathological).  Without using the Strong Law of Large Numbers show
  that the probability of flipping only a finite number heads is $0$.
\end{xca}
\begin{proof}
Let $A_n = \lbrace \text{heads is flipped on the }n^{th}\text{
  toss}\rbrace$.  We know that $\probability{A_n} = p >0$, therefore
$\sum_{n=1}^\infty \probability{A_n} = \infty$.  We also know that
$A_n$ are independent events, therefore the converse of the
Borel-Cantelli Theorem (Theorem \ref{BorelCantelli}) tells us that $\probability{ A_n \text{ i.o.}}
= 1$.  The probability of tossing only a finite number of
heads is $1 - \probability{ A_n \text{ i.o.}} = 0$.
\end{proof}

\begin{xca}A sequence of random variables $\xi_1, \xi_2, \dots$ is said
  to be \emph{completely convergent} to $\xi$ if for every $\epsilon > 0$,
\begin{align*}
\sum_{n=1}^\infty \probability{\abs{\xi_n - \xi} > \epsilon} < \infty
\end{align*}
Show that if $\xi_n$ are independent then complete convergence is
equivalent to almost sure convergence.
\end{xca}
\begin{proof}
First assume that $\xi=0$.  

We first assume complete convergence.  If for a given $\epsilon > 0$, we know $\sum_{n=1}^\infty
\probability{\abs{\xi_n} > \epsilon} < \infty$ then we can apply
Borel Cantelli to conclude that $\probability{\xi_n > \epsilon \text{ i.o.}} =
0$.  Thus there exists a set $A_\epsilon$ of measure zero such that
for all $\omega \notin A_\epsilon$, we can find $N>0$ such that
$\xi_n(\omega) \leq \epsilon$.  Define $A = \cup_{m=1}^\infty A_\frac{1}{m}$,
note that $\probability{A} = 0$ and that for every $\omega \notin A$,
and every $\epsilon >0$ we can pick $\frac{1}{m} < \epsilon$  and then
we know  $N>0$ such that $\xi_n(\omega) \leq \frac{1}{m} \leq \epsilon$

Then if $\xi_n \toas 0$, then there
exists an event $A$ with $\probability{A} = 1$ and such that for any
$\omega \in A$, $\epsilon>0$ we can find $N > 0$ such that
$\abs{\xi_n} < \epsilon$, thus $\probability{\abs{\xi_n} > \epsilon
  \text{ i.o.}} \leq 1 - \probability{A} = 0$.  By independence of
$\xi_n$ and Borel Cantelli we conclude that $\sum_{n=1}^\infty
\probability{\abs{\xi_n} > \epsilon} < \infty$.

Now in the case in which $\xi \neq 0$ we can reduce to the case in
which $\xi = 0$.  Note that by Corollary
\ref{ConstantLimitOfIndependent} to the Kolmogorov 0-1 Theorem, we
know that $\xi$ is almost surely a constant $c$.  Then we can define $\xi_n
- c$ and note that $\xi_n - c$ are independent by Lemma \ref{IndependenceComposition}.
\end{proof}

\begin{xca}Suppose $\eta, \xi_1, \xi_2, \dots$ are random variables with
  $\abs{\xi_n} \leq \eta$ a.s. for all $n > 0$.  Show that $\sup_n
  \abs{\xi_n} \leq \eta$ a.s.
\end{xca}
\begin{proof}
Let $A_n = \lbrace \xi_n \leq \eta \rbrace$ and $A = \cup_n A_n$.
By assumption,
$\probability{A_n} =0$ and therefore by countable subadditivity of measure,
$\probability{A} = 0$.  For all $\omega \notin A$, we know for all
$n>0$, $\xi_n(\omega) \leq \eta(\omega)$ and therefore
$\sup_n\xi_n(\omega) \leq \eta(\omega)$.
\end{proof}

\begin{xca}\label{ExConvProb1}Let $\xi, \xi_n$ be random elements in a metric space $S$
  such that $\xi_n \toprob \xi$, let $A_n$ be events such that
  $\probability{A_n} = 1$ and let $\eta_n$ be random elements in
  $S$ such that $\eta_n = \xi_n$ on $A_n$, show that $\eta_n \toprob \xi$.
\end{xca}
\begin{proof}
Fix $\epsilon > 0$ and note that
\begin{align*}
\lim_{n \to \infty} \probability{d(\eta_n, \xi) > \epsilon} &=
\lim_{n \to \infty} \probability{d(\eta_n, \xi) > \epsilon ; A_n } +
\lim_{n \to \infty} \probability{d(\eta_n, \xi) > \epsilon;A_n^c}
\leq \lim_{n \to \infty} \probability{d(\xi_n, \xi) > \epsilon } +
\lim_{n \to \infty} \probability{A^c_n} = 0
\end{align*}
\end{proof}

\begin{xca}Suppose $\xi, \xi_1, \xi_2, \dots$ are random variables with
  $\xi_n \toas \xi$ and $\xi < \infty$ a.s.  Let $\eta = \sup_n
  \abs{\xi_n}$ and show that $\eta < \infty$ a.s.
\end{xca}
\begin{proof}
TODO
\end{proof}

\begin{xca}[Kallenberg Ex 3.6]Let $\mathcal{F}_{t,n}$ with $t \in T$ and $n \in \naturals$
  be $\sigma$-algebras such that for a fixed $t$ they are
  nondecreasing in $n$ and for a fixed $n$ they are independent in
  $t$.  Show that the $\sigma$-algebras $\bigvee_n \mathcal{F}_{t,n}$
  are independent.
\end{xca}
\begin{proof}
Because for fixed $t \in T$, we have $\mathcal{F}_{t,0} \subset
\mathcal{F}_{t,1} \subset \cdots$ we can see that $\bigcup_n
\mathcal{F}_{t,n}$ is a $\pi$-system.  Since by definition $\bigcup_n
\mathcal{F}_{t,n}$ generates $\bigvee_n
\mathcal{F}_{t,n}$ by Lemma \ref{IndependencePiSystem} it suffices to
show that $\bigcup_n \mathcal{F}_{t,n}$ are independent.

Pick $A_{t_1} \in \mathcal{F}_{t_1,n_1}, \dotsc, A_{t_m} \in
\mathcal{F}_{t_m,n_m}$. Let $n = n_1 \orop \dotsb \orop n_m$ and use
the nondecreasing property of $\mathcal{F}_{t,n}$ to observe that $A_{t_1} \in \mathcal{F}_{t_1,n}, \dotsc, A_{t_m} \in
\mathcal{F}_{t_m,n}$.  By the assumption that each of $\mathcal{F}_{t_j,n}$ is independent
therefore $\probability {A_1 \cup \dotsb \cup A_m} = \probability{A_1}
\dotsm \probability{A_m}$ and we are done.
\end{proof}

\begin{xca}[Kallenberg Ex 3.7]Let $T$ be an arbitrary index set and let
  $(S_t, \mathcal{B}(S_t))$ be metric spaces with Borel
  $\sigma$-algebras.  For each $t\in T$ suppose have random elements
  random elements $\xi^t, \xi^t_n \in S_t$  for $n \in \naturals$ such
  that $\xi^t_n \toas \xi^t$.  If for each fixed $n \in \naturals$ the
  $\xi^t_n$ are independent show that $\xi^t$ are independent.
\end{xca}
\begin{proof}
Pick a finite subset $\lbrace t_1, \dotsc, t_m \rbrace \subset T$ and
assume we are given bounded continuous functions $f_j : S_{t_j} \to
\reals$ for $j=1, \dotsc, m$.
By Lemma \ref{IndependenceExpectations} and the independence of the
$\xi^{t_j}_n$ we have $\expectation{f_1(\xi^{t_1}_n) \dotsm
  f(\xi^{t_m}_n)} = \expectation{f_1(\xi^{t_1}_n)}\dotsm
 \expectation{ f(\xi^{t_m}_n)}$ for each $n \in \naturals$.  But now
 we can use the boundedness and continuity of the $f_j$ 
\begin{align*}
&\expectation{f_1(\xi^{t_1}) \dotsm f_m(\xi^{t_m})} \\
&=
\expectation{\lim_{n \to \infty} f_1(\xi_n^{t_1}) \dotsm
  f_m(\xi_n^{t_m})} & & \text{by continuity} \\
&= \lim_{n \to \infty} \expectation{f_1(\xi_n^{t_1}) \dotsm
  f_m(\xi_n^{t_m})} & & \text{boundedness of $f_j$ and Dominated
  Convergence}  \\
&= \lim_{n \to \infty} \expectation{f_1(\xi_n^{t_1})} \dotsm
 \expectation{ f_m(\xi_n^{t_m})} & & \text{independence}  \\
&= \expectation{f_1(\xi^{t_1})} \dotsm
 \expectation{ f_m(\xi^{t_m})} & &
 \text{continuity and Dominated Convergence} \\
\end{align*}

We now prove a slight extension of Lemma
\ref{IndependenceExpectations} that shows this is sufficient to see
that $\xi^t$ are independent.  Let $(S,d)$ be a metric space and let
$U \subset S$ be open.  We show how to approximate the indicator
function $\characteristic{U}$ be bounded continuous functions.  Let
$d(x, U^c) = \inf \lbrace d(x,y) \mid y \not U \rbrace$.  Note that
$d(x, U^c)$ is continuous (see proof Lemma
\ref{DistanceToSetLipschitz}).  Let $f_n(x) = 1 \wedge n d(x, U^c)$
and observe that $f_n \uparrow \characteristic{U}$.  Now suppose
$U_{j} \subset S_{t_j}$ are open sets for $j=1, \dotsc,m$ and use the
construction just presented to create bounded continuous functions
$f^j_n \uparrow \characteristic{U_j}$.  Then it is also true that
$f^1_n \dotsm f^m_n \uparrow \characteristic{U_1} \dotsm
\characteristic{U_m}$ and so we can apply Montone convergence to see 
\begin{align*}
\probability{\xi^{t_1} \in U_1 \cap \dotsb \cap \xi^{t_m} \in U_m} &=
\lim_{n \to \infty} \expectation{f^1_n(\xi^{t_1}) \dotsm
f^m_n(\xi^{t_m})} \\
&= \lim_{n \to \infty} \expectation{f^1_n(\xi^{t_1}) } \dotsm
\expectation{f^m_n(\xi^{t_m})} \\
&= \probability{\xi^{t_1} \in U_1} \dotsm\probability{\xi^{t_m} \in
  U_m} 
\end{align*}
Now it suffices to note that the open sets  in a metric space are a
$\pi$-system that generates all of the Borel sets so by Lemma
\ref{IndependencePiSystem} it suffices to
show independence on open sets.
\end{proof}

A simpler subcase of the above
\begin{xca}Let $\xi, \xi_n$ be random elements in a metric space $S$
  such that $\xi_n \toprob \xi$ and each $\xi_n$ is
  $\mathcal{F}_n$-measurable.  Furthermore suppose $\mathcal{G}$ is a
  $\sigma$-algebra such that $\mathcal{F}_n \Independent \mathcal{G}$
  for all $n \in \naturals$, then show $\xi$ is independent of
  $\mathcal{G}$.
TODO: In the proof we mention that $\mathcal{F}_1 \subset
\mathcal{F}_2 \subset \cdots$.  Is that really required?  If not
provide a counter example.
\end{xca}
\begin{proof}
Since $\xi_n \toprob \xi$ we know there is a subsequence  that
converges almost surely.  Note that all of the hypotheses restrict
cleanly to the subsequence so we might as well assume that $\xi_n
\toas \xi$.  By the $\mathcal{F}_n$ measurability of $\xi_n$ we see
that each $\xi_n$ is $\bigvee_n \mathcal{F}_n$-measurable and
therefore $\xi$ is almost surely equal to a $\bigvee_n
\mathcal{F}_n$-measurable function.  It therefore suffices to show
that $\bigvee_n
\mathcal{F}_n \Independent \mathcal{G}$ (TODO: show this simple fact; if $\xi
= \eta$ a.s. and $\xi \Independent \mathcal{G}$ then $\eta \Independent
\mathcal{G}$).  This follows from the fact
that the nestedness of the $\mathcal{F}_n$ implies $\bigcup_n
\mathcal{F}_n$ is a $\pi$-system.  Since by definition it generates $\bigvee_n
\mathcal{F}_n$ we get the result from Lemma \ref{IndependencePiSystem}.
\end{proof}

\begin{xca}Let $\xi_1, \xi_2, \dots$ be independent random variables with values in
  $[0,1]$.  Show that $\expectation{\prod_{n =1}^\infty \xi_n} =
  \prod_{n=1}^\infty \expectation{\xi_n}$.  In particular, for
  independent events $A_n$ we have $\probability{\cup_{n=1}^\infty
    A_n} = \prod_{n=1}^\infty \probability{A_n}$.
\end{xca}
\begin{proof}
Note that because $\xi_n$ have values in $[0,1]$, the partial products
$\prod_{k=1}^n \xi_k \leq 1$ and therefore by Dominated Convergence
and Lemma \ref{IndependenceExpectations}, we have
\begin{align*}
\expectation{\prod_{k =1}^\infty \xi_k} &= \lim_{n \to \infty}
\expectation{\prod_{k=1}^n \xi_k} = \lim_{n \to \infty}
\prod_{k=1}^n\expectation{ \xi_k} = \prod_{k=1}^\infty \expectation{ \xi_k} 
\end{align*}
\end{proof}

\begin{xca}Provide an example of uncorrelated but non-independent
  random variables.
\end{xca}
\begin{proof}See Example \ref{UncorrelatedNotIndependent}.
\end{proof}

\begin{xca}Let $\xi_1, \xi_2, \dots$ be random variables.  Show that
  there exist constants $c_1 > 0, c_2 >0, \cdots$ such that
  $\sum_{n=1}^\infty c_n \xi_n$ converges almost surely.
\end{xca}
\begin{proof}First note that we can make a few assumptions about $\xi_n$ without
loss of generality.  First, we can assume that $\xi_n \geq 0$ for all
$n$; knowing that that will show absolute convergence for all series.  Next,
note that by a comparison test argument, we may further assume that $\xi_n >
0$ for all $n$ (e.g. for a random variable $\xi$ that takes $0$ as a value we can
always create the modification $\xi + \characteristic{\xi^{-1}(0)}$
which is nonzero and dominates $\xi$).  

The idea here is to leverage freshman calculus and use the ratio
test.  We first verify the following almost sure version of the ratio
test: Let $\xi_n$ be positive random variables such that there exists
a $0 < C < 1$ such that
$\sum_{n=1}^\infty \probability{\frac{\abs{\xi_{n+1}}}{\abs{\xi_n}} >
  C} < \infty$, then $\sum_{n=1}^\infty \xi_n$ converges almost
surely.

To verify the claim, we apply Borel Cantelli to conclude that $\probability{\frac{\abs{\xi_{n+1}}}{\abs{\xi_n}} >
  C \text{ i.o.}} = 0$.  Unwinding the definitions in this statement, we see that for
almost every $\omega \in \Omega$, there exists an $N>0$ such that
$\frac{\abs{\xi_{n+1}(\omega)}}{\abs{\xi_n(\omega)}} \leq C$
for all $n > N$.  The ratio test tells us $\sum_{n=1}^\infty
\xi_n(\omega)$  converges and the almost sure convergence is verified.

Now we apply the claim in our case by choosing $C=\frac{1}{2}$ and inductively defining $c_n$ so
that we guarantee $\probability{\frac{c_{n+1} \xi_{n+1}}{c_n \xi_n} >
  \frac{1}{2}} < \frac{1}{n^2}$.  To see that this is possible,
suppose we've defined $c_n$ and note that because $\xi_n > 0$, we know
that $0 < \frac{\xi_{n+1}}{c_n \xi_n} < \infty$.  This tells us that
$\lim_{N \to \infty} \probability{\frac{\xi_{n+1}}{c_n \xi_n} > N} =0$
and therefore we can find $M>0$ such that
$\probability{\frac{\xi_{n+1}}{c_n \xi_n} > N} < \frac{1}{n^2}$ for
all $N\geq M$.  Pick $c_{n+1} = \frac{1}{2M}$ and we are done.

Here is some things that I tried that proved to be a dead end.  Is there
a learning opportunity in looking at this?  Note that almost sure
convergence of $\sum_{n=1}^\infty c_n \xi_n$ is equivalent to
$\probability{\abs{\sum_{n=1}^\infty c_n \xi_n} \geq N \text{
    i.o.}}$.  The idea was to try to find $c_n$ so that we could
provide bounds on $\probability{c_n\abs{\xi_n} \geq N}$ and leverage
those to show bounds on the series.  The problem I had with this
approach is that to go from a bound on $c_n\abs{\xi_n}$ to convergence
of the series meant that $c_n\abs{\xi_n}$ had to decay fast enough to
get convergence.  If we assume a finite moment then Markov could
provide a rate of decay but in the absence of that one has to deal
with the fact that tails of $\xi_n$ can decay increasingly slowly.
I tried a truncation argument but fact that $\xi_n$ are not related
meant that I couldn't figure out how to control the residuals of the
truncations.  Maybe this line of reasoning could be made to work but I
got stuck.

Guolong asks a good follow on question: either prove this or (more
likely) provide a
counterexample on general (non-finite) measure spaces (e.g. Lebesgue measure on $\reals$).
\end{proof}

\begin{xca}Let $\xi_1, \xi_2, \dots$ be positive independent random
  variables, then $\sum_{n=1}^\infty \xi_n$ converges almost surely if
  and only if $\sum_{n=1}^\infty \expectation{\xi_n \wedge 1} <
  \infty$.
TODO: Provide hints
\end{xca}
\begin{proof}One direction is easy and doesn't require the assumption
  of independence; namely assume that $\sum_{n=1}^\infty
  \expectation{\xi_n \wedge 1} < \infty$.
Apply Tonelli's Theorem (Corollary \ref{TonelliIntegralSum}) to
conclude $\expectation{\sum_{n=1}^\infty \xi_n \wedge 1} < \infty $
which implies that $\sum_{n=1}^\infty \xi_n \wedge 1 < \infty$ almost
surely.  For any $\omega \in \Omega$ such that $\sum_{n=1}^\infty
\xi_n(\omega) \wedge 1 < \infty$ this implies $lim_{n \to \infty}
\xi_n(\omega) \wedge 1 = 0$ so there exists an $N_\omega > 0$ such
that $\xi_n(\omega) \wedge 1 = \xi_n(\omega)$  for all $n>N_\omega$ 
and therefore $\sum_{n=1}^\infty\xi_n(\omega) < \infty$ as
well.

Now lets assume $\sum_{n=1}^\infty \xi_n < \infty$.  Since $\xi_n
\wedge 1 \leq \xi_n$ we know that $\sum_{n=1}^\infty \xi_n < \infty$, 
so without loss of generality we
can assume $0 \leq \xi_n \leq 1$.

\begin{align*}
0 &< \expectation{e^{-\sum_{n=1}^\infty \xi_n}} 
&=\expectation{\prod_{n=1}^\infty e^{-\xi_n}} 
&= \prod_{n=1}^\infty \expectation{ e^{-\xi_n}} \\
&\leq \prod_{n=1}^\infty \left ( 1 - a\expectation{\xi_n} \right ) &
&\text{ where $a=1-e^{-1}$ by Lemma
  \ref{BasicExponentialInequalities}} \\
&\leq \prod_{n=1}^\infty e^{- a\expectation{\xi_n} } & & \text{since
  $1+x\leq e^x$ by Lemma \ref{BasicExponentialInequalities}} \\
&= e^{-a \sum_{n=1}^\infty \expectation{\xi_n} }
\end{align*}
which shows that $\sum_{n=1}^\infty \expectation{\xi_n} < \infty$.
\end{proof}

\begin{xca}\label{MeasurabilityKernelExtraParameter}Let $\mu : S \times \mathcal{T} to [0,1]$ be a probability kernel and let $f : U \times T \to \reals$ be measurable then $\int f(u, t) \, \mu(s, dt)$ is $\mathcal{U} \times \mathcal{S}$ measurable.
\end{xca}
\begin{proof}
Assume first that $f$ is the characteristic function of a set $A \times B \in \mathcal{U} \otimes \mathcal{T}$.  Then 
\begin{align*}
\int \characteristic{ A\times B} (u, t) \, \mu(s, dt) &= \characteristic{A}(u) \mu(s, B)
\end{align*}
which is clearly measurable since $\mu$ is a kernel.  We know that sets $A \times B$ are a $\pi$-system generating $\mathcal{U} \otimes \mathcal{T}$ so can argue with monotone classes to extend to general characteristic functions.  To be specific, let 
\begin{align*}
\mathcal{C} &= \lbrace C \in \mathcal{U} \otimes \mathcal{T}  \mid \int \characteristic{C}(u,t) \, \mu(s,dt) \text{ is measurable} \rbrace
\end{align*}
If $A \in \mathcal{C}$ and $B \in \mathcal{C}$ with $A \subset B$ then 
$\int \characteristic{ B \setminus A} (u, t) \, \mu(s, dt) = \int \characteristic{ B } (u, t) \, \mu(s, dt) - \int \characteristic{A} (u, t) \, \mu(s, dt)$ is measurable so that $B \setminus A \in \mathcal{C}$.  Similarly if $A_1 \subset A_2 \subset \dotsb$ with $A_n \in \mathcal{C}$ then defining $A = \cup_{n=1}^\infty A_n$ we have by Monotone Convergence 
$\int \characteristic{A}(u,t) \, \mu(s, dt) = \lim_{n \to \infty} \int \characteristic{A_n}(u,t) \, \mu(s, dt)$ is a limit of measurable function hence is measurable.  This shows that $\mathcal{C}$ is a $\lambda$-system and therefore by the $\pi$-$\lambda$ Theorem \ref{MonotoneClassTheorem} we know that $\mathcal{C} \subset \mathcal{U}\otimes \mathcal{T}$.  By linearity it follows that $\int f(u,t) \, \mu(s, dt)$ is measurable for all simple functions.

Now given an arbitrary non-negative measurable $f : U \times T \to [0,\infty)$ we find an increasing sequence of simple functions $f_n \uparrow f$, note that for each fixed $u \in U$ it remains true that the sections $f_n(u, \cdot) \uparrow f(u, \cdot)$ and thus we can use Monotone Convergence to see that $\int f(u,t) \, \mu(s,dt) = \lim_{n \to \infty} \int f_n(u,t) \, \mu(s,dt)$ for every $(u,s) \in U \times S$ so that $\int f(u,t) \, \mu(s,dt)$ is measurable.  For an arbitrary measurable function $f$ just write $f = f_+ - f_-$ and use the result for non-negative measurable functions.
\end{proof}

\begin{xca}Suppose $\xi$ is a random variable, let $\mathcal{F}$ be
  a $\sigma$-algebra and let $A$ be a measurable set.  Show that
  $\cexpectationlong{\mathcal{F},A}{\xi} =
  \frac{\cexpectationlong{\mathcal{F}}{\xi ;
      A}}{\cprobability{\mathcal{F}}{A}}$ on $A$.
\end{xca}
\begin{proof}
Note by Localization we know that $\characteristic{A}
\cexpectationlong{\mathcal{F},A}{\xi}  =
\cexpectationlong{\mathcal{F},A}{\xi;A}$, therefore we may assume that
$\xi = \characteristic{A} \xi$ and show
$\cexpectationlong{\mathcal{F},A}{\xi} = \characteristic{A}\frac{\cexpectationlong{\mathcal{F}}{\xi}}{\cprobability{\mathcal{F}}{A}}$ almost surely.

Pick $F \in \mathcal{F}$ and calculate
\begin{align*}
\expectation{\characteristic{A} \frac{\cexpectationlong{\mathcal{F}}
{\xi}}
{\cprobability{\mathcal{F}}{A}}
; A \cap F} 
&= \expectation{\cexpectationlong{\mathcal{F}}{\frac{\xi ; F}
{\cprobability{\mathcal{F}}{A}}} 
; A } & & \text{by pushout}\\
&= \expectation{\cexpectationlong{\mathcal{F}}{\frac{\xi ; F}
{\cprobability{\mathcal{F}}{A}}} 
\cprobability{\mathcal{F}}{A} } \\
&= \expectation{\cexpectationlong{\mathcal{F}}{\xi ; F}} & & \text{by
  pushout} \\
&= \expectation{\xi ; F} = 
\expectation{\xi ; A \cap F} & & \text{by tower property}
\end{align*}
and trivially
\begin{align*}
\expectation{\characteristic{A} \frac{\cexpectationlong{\mathcal{F}}
{\xi}}
{\cprobability{\mathcal{F}}{A}}
; A^c \cap F}  &= 0 = \expectation{\xi ; A^c \cap F}
\end{align*}
Since sets of the form $A \cap F$, $A^c \cap F$ and $F$ for $F \in
\mathcal{F}$ form a $\pi$-system that generate $\sigma(A,
\mathcal{F})$ we have shown the result.
\end{proof}

\begin{xca}Let $A_1, A_2, \cdots$ be a disjoint partition of $\Omega$
  and let $\mathcal{F} = \sigma(A_1, A_2, \dots)$.  Show that for
  every integrable random variable $\xi$ we have
$\cexpectationlong{\mathcal{F}}{\xi} = \sum_{\probability{A_n} \neq 0}
\frac{\expectation{\xi ; A_n}}{\probability{A_n}} \characteristic{A_n}$ almost surely.
\end{xca}
\begin{proof}First note that it is trivial that $\sum_{\probability{A_n} \neq 0}
\frac{\expectation{\xi ; A_n}}{\probability{A_n}}\characteristic{A_n}$ is
$\mathcal{F}$-measurable.  Because the $A_n$ are a disjoint partition,
  they are a $\pi$-system and the it will suffice to show the
  averaging property for the sets $A_n$.
Pick an $A_m$ such that $\probability{A_m} \neq 0$, they by
disjointness of the $A_n$ we get
\begin{align*}
\expectation{\sum_{\probability{A_n} \neq 0}
\frac{\expectation{\xi ; A_n}}{\probability{A_n}}
\characteristic{A_n}; A_m} &= \expectation{\frac{\expectation{\xi ;
    A_m}}{\probability{A_m}} \characteristic{A_m}} = \expectation{\xi ; A_m}
\end{align*}
For any $A_m$ with $\probability{A_m} = 0$ and again applying the
disjointness of the $A_n$ we get
disjointness of the $A_n$ that 
\begin{align*}
0 &= \expectation{\sum_{\probability{A_n} \neq 0}
\frac{\expectation{\xi ; A_n}}{\probability{A_n}}
\characteristic{A_n}; A_m}  = \expectation{\xi ; A_m}
\end{align*}
\end{proof}

\begin{xca}Suppose $\xi$ is a random element in $S$ such that
  $\cprobability{\mathcal{F}}{\xi \in \cdot}$ has a regular version
  $\nu$.  Let $f : S \to T$ be measurable.  Show that
  $\cprobability{\mathcal{F}}{f(\xi) \in \cdot}$ has a regular version
  given by $\pushforward{f} {\nu} (\omega, A) = \nu(\omega, f^{-1}(A))$.
\end{xca}
\begin{proof}
Our hypothesis is that for every $A$, $\cprobability{\mathcal{F}}{\xi
  \in A}(\omega) = \mu(\omega, A)$.  We calculate 
\begin{align*}
\cprobability{\mathcal{F}}{f(\xi) \in A}(\omega) &=
\cexpectationlong{\mathcal{F}}{\characteristic{f^{-1}(A)}(\xi)} \\
&=\int \characteristic{f^{-1}(A)}(s) \, d\mu(\omega, s) & & \text{by
  Theorem \ref{Disintegration}} \\
&=\mu(\omega, f^{-1}(A))
\end{align*}
and we are done.
\end{proof}

\begin{xca}Let $\xi$ be a random element in $S$.  Show that $\xi$ is
  $\mathcal{F}$-measurable if and only if $\delta_\xi$ is a regular
  version of 
  $\cprobability{\mathcal{F}}{\xi \in \cdot}$.

TODO: Refine this statement to include almost sureness...
\end{xca}
\begin{proof}
$\mathcal{F}$-measurability of $\xi$ is equivalent to
$\mathcal{F}$-measurability of $\characteristic{A}(\xi)$ for all $A$
which is equivalent to $\cprobability{\mathcal{F}}{\xi \in A} =
\characteristic{A}(\xi)$ almost surely for all $A$.  Evaluating the
last equality at $\omega$ we see that 
\begin{align*}
\cprobability{\mathcal{F}}{\xi \in A}(\omega) &= \begin{cases}
1 & \text{if $\xi(\omega) \in A$} \\
0 & \text{if $\xi(\omega) \notin A$} 
\end{cases}\\
&= \delta_{\xi(\omega)}(A)
\end{align*}

The fact that $\delta_\xi$ is a probability kernel is simple.  It is
trivial that for fixed $\omega$, $\delta_\xi(\omega)$ is a probability
measure.  If we fix $A$ then $\delta_\xi(\omega)(A)$ is clearly seen
to be measurable since it is just the characteristic function of the
measurable set $A$.
\end{proof}

\begin{xca}Let $\xi$ be an integrable random variable for which
  $\cexpectationlong{\mathcal{F}}{\xi} \eqdist \xi$.  Show that in
  fact $\cexpectationlong{\mathcal{F}}{\xi} = \xi$ a.s.
\end{xca}
\begin{proof}
Here is a simple and conceptual proof in the case that
$\cexpectationlong{\mathcal{F}}{\xi}$ (and therefore $\xi$) take
finitely many values/are simple functions.  Let $y_1 < \cdots < y_n$ be the values of
$\xi$ such that $\probability{\xi = y_i} \neq 0$.   Consider $A_1 = \lbrace \cexpectationlong{\mathcal{F}}{\xi} =
y_1 \rbrace$.  By definition of conditional expectation
$\expectation{\xi ; A_1} =
\expectation{\cexpectationlong{\mathcal{F}}{\xi} ; A_1} = y_1
\probability{A_1}$.  Because $y_1$ is the minimum value of $\xi$ it
follows that we must have $\xi = y_1$ identically on $A_1$.  Since $\xi \eqdist
\cexpectationlong{\mathcal{F}}{\xi} $, we know that $\probability{\xi
  = y_1} = \probability{A_1}$ and therefore $\xi \geq y_2$ almost
surely off of $A_1$.  Now induct.

If we want to apply standard machinery to go from the
simple function case.  Then we could approximate $\xi$ by an
increasing family of simple functions of the form $f_n(\xi)$ but then
we know that $f_n(\xi) \eqdist
f_n(\cexpectationlong{\mathcal{F}}{\xi})$ but not necessarily that $f_n(\xi) \eqdist
\cexpectationlong{\mathcal{F}}{f_n(\xi)}$ which is what we would need
in order to use the simple function case.  All roads seem to lead to a
need to show that $\cexpectationlong{\mathcal{F}}{f(\xi)}$ and
$f(\cexpectationlong{\mathcal{F}}{\xi})$ are equal in some sense
(either a.s. or in distribution).

The idea is to use Jensen's inequality.  First note that
  we can find a strictly convex function $f$ such that $0 \leq f(x)
  \leq \abs{x}$.  Therefore we know that $\expectation{f(\xi)} <
  \infty$.  

Moreover, by Theorem \ref{ExistenceConditionalDistribution} be have a
regular version $\nu$ for $\cprobability{\mathcal{F}}{\xi \in A}$.  By
Theorem \ref{Disintegration} we know that
$\cexpectationlong{\mathcal{F}}{f(\xi)} = \int s \, d\mu(s)$.

Because $\xi \eqdist \cexpectationlong{\mathcal{F}}{\xi}$ we also know
that $f(\xi) \eqdist f(\cexpectationlong{\mathcal{F}}{\xi})$ which
shows us that ... 

TODO: I am aiming to show that $\pushforward{f}{\mu}$ is a regular version
for
$\cprobability{\mathcal{F}}{f(\cexpectationlong{\mathcal{F}}{\xi}) \in
\cdot}$.
If we could get that then we could calculate
\begin{align*}
f(\cexpectationlong{\mathcal{F}}{\xi}) &=
\cexpectationlong{\mathcal{F}}
{f(\cexpectationlong{\mathcal{F}}{\xi})} \\
&= \int s d \pushforward{f}{\mu} (s) & & \text{by Theorem \ref{Disintegration}}\\
&= \int f(s) \, d\mu(s)  & & \text{by Expectation Rule} \\
&= \cexpectationlong{\mathcal{F}}{f(\xi)} & & \text{by Theorem \ref{Disintegration}}
\end{align*}
Now apply the strictly convex case of Jensen's Inequality to conclude
the result.

If we assume finite second moments then there should be a proof of
this by showing that the conditional variance is $0$.  TODO: Define
conditional variance and show the result.
\end{proof}

\begin{xca}Prove or disprove the following statement.  Suppose $\xi \eqdist \eta$, show that for every $A$, 
  $\cprobability{\mathcal{F}}{\xi \in A} =
  \cprobability{\mathcal{F}}{\eta \in A}$ a.s.
\end{xca}
\begin{proof}
This is false.  Let $\Omega = \lbrace 0,1 \rbrace$ with uniform distribution and
power set $\sigma$-algebra.  Let
$\xi(x) = x$ and let $\eta(x) = 1 - x$.  Note that $\xi \eqdist
\eta$ (both have a uniform distribution on $\lbrace 0,1 \rbrace$). Now take $\mathcal{F} = \mathcal{A}$ so that
$\cprobability{\mathcal{F}}{\xi \in A} = \characteristic{\xi \in A}$
and $\cprobability{\mathcal{F}}{\eta \in A} = \characteristic{\eta \in
  A}$ and take $A = \lbrace 0 \rbrace$ or $A = \lbrace 1 \rbrace$.
\end{proof}

\begin{xca}Find $\xi, \eta, \mathcal{F}$ such that $\xi \eqdist \eta$
  but $\cexpectationlong{\mathcal{F}}{\xi} \neq
  \cexpectationlong{\mathcal{F}}{\eta}$ a.s.
\end{xca}
\begin{proof}
Pick sets $A,B,C$ such that $\probability{A} = \probability{B}$ but
$\probability{A \cap C} \neq \probability{B \cap C}$.  Even more
trivially, take $\mathcal{F} = \mathcal{A}$ so that
$\cexpectationlong{\mathcal{F}}{\xi} = \xi$ and similarly with
$\eta$.  Now the statement is equivalent to show two random elements
that not almost surely equal but have the same distribution.
\end{proof}

\begin{xca}Suppose $\xi, \tilde{\xi}$ are integrable random variables
  and $\eta, \tilde{\eta}$ are random elements in $(T, \mathcal{T})$
  such that $(\xi,\eta) \eqdist (\tilde{\xi}, \tilde{\eta})$.  Show
  that $\cexpectationlong{\eta}{\xi} \eqdist \cexpectationlong{\tilde{\eta}}{\tilde{\xi}}$.
\end{xca}
\begin{proof}
First, note the intuition behind the statement.  As a result of
$(\xi,\eta) \eqdist (\tilde{\xi}, \tilde{\eta})$ we can also conclude
that $\xi \eqdist \tilde{\xi}$ and $\eta \eqdist \tilde{\eta}$.
However, we also expect that the conditional distributions on $T$ are
equal (thinking heuristically of a formula like $\cprobability{B}{A} =
\probability{A \cap B} / \probability{B}$).  The first order of
business is to formulate this intuition precisely and prove it.

By Theorem \ref{ExistenceConditionalDistribution} there are
probability kernels $\mu$ and $\tilde{\mu}$ such that
$\cprobability{\eta}{\xi \in A} = \mu(\eta, A)$ and
$\cprobability{\tilde{\eta}}{\tilde{\xi} \in A} =
\tilde{\mu}(\tilde{\eta}, A)$ for all Borel sets $A$.  Our first claim
is that $\mu = \tilde{\mu}$ almost surely with respect to
$\mathcal{L}{\eta}$.

Pick a Borel set $A$ and let $B = \lbrace t \in T \mid \mu(t, A) >
\tilde{\mu}(t, A) \rbrace$. 
\begin{align*}
0 &= \probability{\xi \in A; \eta \in B} - \probability{\tilde{\xi}
  \in A; \tilde{\eta} \in B} & \text{by hypothesis}\\
&=\expectation{\int \characteristic{A \times B}(s, \eta) \,
  d\mu(\eta, s) - \int \characteristic{A \times B}(s, \tilde{\eta}) \,
  d\tilde{\mu}(\eta, s)} & \text{by Theorem \ref{Disintegration}}\\
&=\expectation{\characteristic{B}(\eta) \mu(\eta, A) -
  \characteristic{B}(\tilde{\eta}) \tilde{\mu}(\tilde{\eta}, A)} \\
&=\int \characteristic{B}(t) \mu(t, A) -
  \characteristic{B}(t) \tilde{\mu}(t, A) \, d\mathcal{L}(\eta)(t) &
  \text{by Lemma \ref{ChangeOfVariables} and $\mathcal{L}(\eta) = \mathcal{L}(\tilde{\eta})$.}
\end{align*}
which by choice of $B$ shows that $\mu(t, A) =\tilde{\mu}(t, A)$
almost surely $\mathcal{L}(\eta)$.  We can show this almost surely for all $A =
(-\infty, r]$ with $r \in \rationals$ by taking the union of a
countable number of null sets.  This shows that $\mu = \tilde{\mu}$
a.s.

Having shown equality of the conditional distributions it follows from
Theorem \ref{Disintegration} that if we define $f(t) = \int s \, d\mu(t,
s)$ then we have $\cexpectationlong{\eta}{\xi} = f(\eta)$ and
$\cexpectationlong{\tilde{\eta}}{\tilde{\xi}} = f(\tilde{\eta})$.
Since $\eta \eqdist \tilde{\eta}$ it follows that $f(\eta) \eqdist
f(\tilde{\eta})$ and the result is proven.
\end{proof}

\begin{xca}Suppose $\xi$ is a random element in a Borel space $(S, \mathcal{S})$, let
  $\mathcal{F}$ be a $\sigma$-algebra and let $\eta =
  \cprobability{\mathcal{F}}{\xi \in \cdot}$, show $\cindependent{\xi}{\mathcal{F}}{\eta}$.
\end{xca}
\begin{proof}
First it is worth clarifying the question.  Since we have assume $S$
is Borel then by Theorem \ref{ExistenceConditionalDistribution} be may
assume that $\eta$ is an $\mathcal{F}$-measurable random measure on
$S$.  We are asked to show conditional independence of $\xi$ and
$\mathcal{F}$ relative to this random measure.  Conceptually, the
conditional distribution captures all of the dependence between a
random element and a $\sigma$-algebra (think of the case $\mathcal{F}
= \sigma(\zeta)$ for a random element $\zeta$ to make this even more concrete).  

By Lemma \ref{ConditionalIndependenceDoob} it will suffice to show for
every $A \in \mathcal{S}$, 
\begin{align*}
\cexpectationlong{\eta}{\xi \in  A} 
&= \cexpectationlong{\eta,\mathcal{F}}{\xi \in  A} 
= \cexpectationlong{\mathcal{F}}{\xi \in  A} 
\end{align*}
where the last equality follows from the $\mathcal{F}$-measurability
of $\eta$.  However this is easily verified since the $\sigma$-algebra
on the space of probability measures $\mathcal{P}(S)$ is the smallest $\sigma$-algebra that makes
evaluation maps $ev_B(\mu) = \mu(B)$ measurable (here $B \in \mathcal{S}$).  Thus we have by
definition of $\eta$, $\cexpectationlong{\mathcal{F}}{\xi \in  A} =
ev_A(\eta)$ which shows that $\cexpectationlong{\mathcal{F}}{\xi \in
  A}$ is in fact $\eta$-measurable.
\end{proof}

\begin{xca}Suppose $\cindependent{\xi}{\zeta}{\eta}$ and
  $\cindependent{\gamma}{(\xi,\eta, \zeta)}{}$, show that
  $\cindependent{\xi}{\zeta}{\eta,\gamma}$ and $\cindependent{\xi}{(\zeta,\gamma)}{\eta}$.
\end{xca}
\begin{proof}
By Lemma \ref{ConditionalIndependenceChainRule},
$\cindependent{\xi}{(\zeta,\gamma)}{\eta}$ is equivalent to
$\cindependent{\xi}{\zeta}{\eta}$ and
$\cindependent{\xi}{\gamma}{\eta, \zeta}$.  The fact that
$\cindependent{\xi}{\zeta}{\eta}$ is a hypothesis whereas
$\cindependent{\xi}{\gamma}{\eta, \zeta}$ follows from another
application of Lemma \ref{ConditionalIndependenceChainRule} to show
that $\cindependent{\gamma}{(\xi,\eta, \zeta)}{}$ is equivalent to
$\cindependent{\gamma}{\zeta}{}$ and
$\cindependent{\gamma}{\eta}{\zeta}$ and
$\cindependent{\gamma}{\xi}{\zeta, \eta}$

Now by Lemma \ref{ConditionalIndependenceChainRule} 
we know $\cindependent{\xi}{(\gamma, \zeta)}{\eta}$ is equivalent to
$\cindependent{\xi}{\gamma}{\eta}$ and
$\cindependent{\xi}{\zeta}{\eta, \gamma}$
hence implies $\cindependent{\xi}{\zeta}{\eta, \gamma}$.
\end{proof}

\begin{xca}Suppose we have $\sigma$-algebras $\mathcal{F}$,
  $\mathcal{G}_1$, $\mathcal{G}_2$, $\mathcal{H}$ with $\mathcal{G}_1
  \subset \mathcal{G}_2$.  If
  $\cindependent{\mathcal{F}}{\mathcal{H}}{\mathcal{G}_1}$
is it true that $\cindependent{\mathcal{F}}{\mathcal{H}}{\mathcal{G}_2}$?
Prove or give a counterexample.
\end{xca}
\begin{proof}
Here is a counterexample in which $\mathcal{G}_1$ is the trivial
$\sigma$-algebra.  Perform two independent Bernoulli trials with rate
$1/2$.  Thus we have sample space $\Omega = \lbrace HH, HT, TT, TH
\rbrace$ with the uniform distribution.  Let $A = \lbrace HH, HT
\rbrace$ (and let $\mathcal{F} = \lbrace \emptyset, \Omega, A, A^c
\rbrace$) and let $B = \lbrace HT, TT \rbrace$ (and let $\mathcal{H} =
\lbrace \emptyset , \Omega, B, B^c \rbrace$).  Note that $A$ and $B$
are independent.  Now let $C = \lbrace HH, TT \rbrace$ (and let
$\mathcal{G}_2 = \lbrace \emptyset, \Omega, C, C^c \rbrace$ and note
that $A$ and $B$ are not conditionally independent given $C$ because
$\cprobability{C}{A \cap B} = 0$ whereas $\cprobability{C}{A} = 1/2$
and $\cprobability{C}{B} = 1/2$.

Note
that primary conceptual point here is that given two independent
events (here ``first toss is heads'' and ``second toss is tails'') one
can condition that there is a relationship between them (here ``first toss
equals the second toss'') and destroy independence.  
\end{proof}

\begin{xca}Suppose $\mathcal{F}$ is independent of $\mathcal{G}$ and
  $\mathcal{H}$, is it true that $\mathcal{F}$ is independent of
  $\sigma(\mathcal{G}, \mathcal{H})$?  Prove or give a counterexample.
\end{xca}
\begin{proof}
Note that $\mathcal{F}$ is independent of
  $\sigma(\mathcal{G}, \mathcal{H})$ if and only if
  $\cindependent{\mathcal{F}}{\mathcal{G}}{}$
and $\cindependent{\mathcal{F}}{\mathcal{H}}{\mathcal{G}}$.  Because
of this equivalence the previous exercise is a counterexample here as
well.  Using the
notation of the previous exercise, let $\mathcal{F} = \sigma(A)$ and
let $\mathcal{G} = \sigma(C)$ and note that $A$ and $C$ are
independent
by direct calculation (this is also intuitively clear).  We also saw
in the previous exercise that $A$ and $B$ are independent and that $A$
is not conditionally independent of $B$ given $C$; hence $A$ is not
indepndent of $\sigma(B,C)$.

Note that we can also show this directly without using the Lemma. A
little work shows that $\sigma(B,C) = 2^\Omega$; it suffices to
note that $B \cap C = \lbrace TT \rbrace$,  $B^c \cap C^c = \lbrace TH
\rbrace$, $B \cap C^c = \lbrace HT \rbrace$ and $B^c \cap C = \lbrace HH
\rbrace$.  Given this fact
it is easy to see that $A$ is not independent of $\sigma(B,C)$ by noting that, because
$P(A) = 1/2$, it is not independent of itself.

Note also that the the key to the failure here is the fact that $A$, $B$
and $C$ are not jointly independent (they are pairwise independent),
otherwise we could appeal to Lemma \ref{IndependenceGrouping}.
To see the lack of joint independence consider $\probability {A \cap B
\cap C} = 0$.
\end{proof}

\begin{xca}Suppose we are given random elements such that $(\xi, \eta,
  \zeta) \eqdist (\tilde{\xi}, \tilde{\eta}, \tilde{\zeta})$, then
  $\cindependent{\xi}{\zeta}{\eta}$ if and only if $\cindependent{\tilde{\xi}}{\tilde{\zeta}}{\tilde{\eta}}$.
\end{xca}
\begin{proof}
First we 
\begin{align*}
\
\end{align*}
\end{proof}

\begin{xca}Suppose $\tau$ and $\sigma$ are discrete optional
  times with respect the filtration $\mathcal{F}_0 \subset
  \mathcal{F}_1 \subset \cdots$. Then $\sigma \wedge \tau$ and
  $\sigma$ and $\sigma \vee \tau$ are optional times.  In addition, 
\begin{align*}
\mathcal{F}_{\tau \wedge
    \sigma} &\subset \mathcal{F}_\sigma \subset \mathcal{F}_{\tau \vee
    \sigma}
\end{align*}
\end{xca}
\begin{proof}
First we show that $\tau \wedge \sigma$ and $\tau \vee \sigma$ are
actually optional times.  This is simple by noting
\begin{align*}
\lbrace \tau \wedge \sigma \leq n \rbrace = \lbrace \tau \leq n
\rbrace \cup \lbrace \sigma \leq n \rbrace \in \mathcal{F}_n
\end{align*}
and
\begin{align*}
\lbrace \tau \vee \sigma \leq n \rbrace = \lbrace \tau \leq n
\rbrace \cap \lbrace \sigma \leq n \rbrace \in \mathcal{F}_n
\end{align*}
If we are given $A \in \mathcal{F}_\sigma$ the by definition for all
$n$, $A \cap \lbrace \sigma \leq n \rbrace \in \mathcal{F}_n$.
Therefore since by definition of optional time we also have $\lbrace
\tau \leq n \rbrace \in \mathcal{F}_n$ we have
\begin{align*}
A \cap \lbrace \tau \vee \sigma \leq n \rbrace &= (A \cap \lbrace \sigma \leq n
\rbrace) \cap \lbrace \tau \leq n \rbrace \in \mathcal{F}_n
\end{align*}
which shows $A \in \mathcal{F}_{\sigma \vee \tau}$.

Now if we assume that $A \in \mathcal{F}_{\sigma \wedge \tau}$, then
for all $n$ we have
\begin{align*}
A \cap \lbrace \tau \wedge \sigma \leq n \rbrace &= A \cap \lbrace \tau \leq n
\rbrace \cup A \cap \lbrace \sigma \leq n \rbrace \in \mathcal{F}_n
\end{align*}
Since we have $\lbrace \sigma \leq n \rbrace, \lbrace \tau \leq n
\rbrace \in \mathcal{F}_n$, then we know that $\lbrace \tau \leq n
\rbrace \setminus \lbrace \sigma \leq n\rbrace \in \mathcal{F}_n$ and
so 
\begin{align*}
\left( A \cap \lbrace \tau \leq n
\rbrace \right ) \cup \left (A \cap \lbrace \sigma \leq n \rbrace
\right ) \cup \left ( \lbrace \tau \leq n
\rbrace \setminus \lbrace \sigma \leq n\rbrace\right)^c &= A \cap \lbrace \sigma \leq n
\rbrace \in \mathcal{F}_n
\end{align*}
which shows $A \in \mathcal{F}_\sigma$.
\end{proof}

\begin{xca}Suppose $\tau$ is a discrete optional
  time with respect the filtration $\mathcal{F}_0 \subset
  \mathcal{F}_1 \subset \cdots$, then $\tau$ is $\mathcal{F}_\tau$-measurable.
\end{xca}
\begin{proof}
For any $n, m$, we have 
\begin{align*}
\lbrace \tau = m \rbrace  \cap \lbrace \tau \leq n \rbrace &=
\begin{cases}
\emptyset & \text{if $m > n$} \\
\lbrace \tau = m \rbrace & \text{if $m\leq n$}
\end{cases}
\end{align*}
hence in all cases is in $\mathcal{F}_n$.
\end{proof}

\begin{xca}Suppose $\tau$ and $\sigma$ are discrete optional
  times with respect the filtration $\mathcal{F}_0 \subset
  \mathcal{F}_1 \subset \cdots$. Then each of $\lbrace \sigma < \tau
  \rbrace$, $\lbrace \sigma \leq \tau
  \rbrace$ and $\lbrace \sigma = \tau
  \rbrace$ is in $\mathcal{F}_{\sigma} \cap \mathcal{F}_{\tau}$.
\end{xca}
\begin{proof}
It suffice to prove two of the three since the third set can be
constructed using finite unions or intersections of the other two.
First we show that $\lbrace \sigma < \tau \rbrace \in
\mathcal{F}_\tau$.
Pick an $n$ and we calculate
\begin{align*}
\lbrace \sigma < \tau \rbrace \cap \lbrace \tau \leq n \rbrace &=
\cup_{m\leq n}\lbrace \sigma < \tau \rbrace \cap \lbrace \tau = m
\rbrace \\
&= \cup_{m\leq n}\lbrace \sigma < m \rbrace \cap \lbrace \tau = m
\rbrace \\
\end{align*}
Now each $\lbrace \sigma < m \rbrace \in \mathcal{F}_m \subset
\mathcal{F}_n$ and each $\lbrace \tau = m
\rbrace \in \mathcal{F}_m \subset
\mathcal{F}_n$ by definition of optional time so the union is and we
have shown $\lbrace \sigma < \tau \rbrace \in \mathcal{F}_\tau$.  The
same argument clearly shows that the other sets are in
$\mathcal{F}_\tau$ as well.  To see that all sets are in
$\mathcal{F}_\sigma$, it suffices to note for example that 
\begin{align*}
\lbrace \sigma < \tau \rbrace^c &= \lbrace \tau \leq \sigma \rbrace
\in \mathcal{F}_\sigma
\end{align*}
by what we have already proven. Apply the closure of $\sigma$-algebras
under complement to get the result.
\end{proof}

\begin{xca}Let $\sigma$ and $\tau$ be optional times with respect to
  the filtration $\mathcal{F}_0 \subset \mathcal{F}_1 \subset
  \cdots$.  Show that 
\begin{align*}
\cexpectationlong
{\mathcal{F}_\tau}{\cexpectationlong{\mathcal{F}_\sigma}{\xi}} &=
\cexpectationlong
{\mathcal{F}_\sigma}{\cexpectationlong{\mathcal{F}_\tau}{\xi}} = \cexpectationlong
{\mathcal{F}_{\sigma \wedge \tau}}{\xi}
\end{align*}
\end{xca}
\begin{proof}
The first thing to do is show how to calculate conditional
expectations with respect to $\sigma$-algebras of the form
$\mathcal{F}_\sigma$ for an arbitrary optional time $\sigma$.  Given
an integrable random variable $\xi$ we let $M^\xi_n =
\cexpectationlong{\mathcal{F}_n}{\xi}$ be the martingale generated by
$\xi$.  We claim
\begin{align*}
\cexpectationlong{\mathcal{F}_\sigma}{\xi} &= M^\xi_\sigma
\end{align*}
TODO:  Dude, this is just optional stopping (at least for the
uniformly integrable case); is that supposed to be available?
To see this, pick an $A \in \mathcal{F}_\sigma$ and then note that for
every $n$, use the fact that $A \cap \lbrace \sigma = n \rbrace \in
\mathcal{F}_n$ and the telescoping rule for conditional expectation to see
\begin{align*}
\expectation{\characteristic{A} \characteristic{\lbrace \sigma = n
    \rbrace } \xi} &= 
\expectation{\characteristic{A} \characteristic{\lbrace \sigma = n
    \rbrace } \cexpectationlong{\mathcal{F}_n}{\xi} } = 
\expectation{\characteristic{A} \cexpectationlong{\mathcal{F}_n}{\characteristic{\lbrace \sigma = n
    \rbrace } \xi} } 
\end{align*}
which is easy to extend by linearity 
\begin{align*}
\expectation{\characteristic{A} \xi} &= \sum_{n=0}^\infty \expectation{\characteristic{A} \characteristic{\lbrace \sigma = n
    \rbrace } \xi} = \sum_{n=0}^\infty \expectation{\characteristic{A} \cexpectationlong{\mathcal{F}_n}{\characteristic{\lbrace \sigma = n
    \rbrace } \xi}} = \expectation{\characteristic{A}
  \sum_{n=0}^\infty \cexpectationlong{\mathcal{F}_n}{\characteristic{\lbrace \sigma = n
    \rbrace } \xi}} \\
&= \expectation{\characteristic{A} M^\xi_\sigma}
\end{align*}
Using this formula twice we have
\begin{align*}
\cexpectationlong{\mathcal{F}_\sigma}{\cexpectationlong{\mathcal{F}_\tau}{\xi}}
&=
\cexpectationlong{\mathcal{F}_\sigma}{M^\xi_\tau}
\\
&= \sum_{n=0}^\infty \characteristic{\lbrace \sigma = n \rbrace}
\cexpectationlong{\mathcal{F}_n}{M^\xi_\tau} \\
&= \sum_{n=0}^\infty \sum_{m=0}^\infty \characteristic{\lbrace \sigma = n \rbrace}
\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\mathcal{F}_m}{\characteristic{
    \lbrace \tau = m \rbrace } \xi}} \\
\end{align*}
Now consider each term $\characteristic{\lbrace \sigma = n \rbrace}
\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\mathcal{F}_m}{\characteristic{
    \lbrace \tau = m \rbrace } \xi}}$; there are two cases. If $m \leq n$
then since $\mathcal{F}_m \subset \mathcal{F}_n$ we can write
\begin{align*}
\characteristic{\lbrace \sigma = n \rbrace}
\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\mathcal{F}_m}{\characteristic{
    \lbrace \tau = m \rbrace } \xi}} &= \characteristic{\lbrace \sigma = n \rbrace}\cexpectationlong{\mathcal{F}_m}{\characteristic{
    \lbrace \tau = m \rbrace } \xi} = \characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau = m \rbrace }\cexpectationlong{\mathcal{F}_m}{ \xi} 
\end{align*}
If $n \leq m$ then because $\mathcal{F}_n \subset \mathcal{F}_m$ and
the telescoping rule,
\begin{align*}
\characteristic{\lbrace \sigma = n \rbrace}
\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\mathcal{F}_m}{\characteristic{
    \lbrace \tau = m \rbrace } \xi}} &= 
\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\mathcal{F}_m}{\characteristic{\lbrace
      \sigma = n \rbrace} \characteristic{
    \lbrace \tau = m \rbrace } \xi}}  = \cexpectationlong{\mathcal{F}_n}{\characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau = m \rbrace } \xi} 
\end{align*}
These two forms are a bit different and are not equivalent because we
cannot ascertain the $\mathcal{F}_{m \wedge n}$-measurability of $\characteristic{\lbrace \sigma = m \rbrace}\characteristic{
    \lbrace \tau = m \rbrace }$.  However, we do know that $\lbrace
  \sigma > m \rbrace = \lbrace \sigma \leq m \rbrace^c$ is
  $\mathcal{F}_m$-measurable and $\lbrace
  \tau > n \rbrace = \lbrace \tau \leq n \rbrace^c$ is
  $\mathcal{F}_n$-measurable.  So if we sum using the case $n \leq m$,
  we get,
\begin{align*}
\sum_{m>n} \characteristic{\lbrace \sigma = n \rbrace}
\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\mathcal{F}_m}{\characteristic{
    \lbrace \tau = m \rbrace } \xi}} 
&= \sum_{m>n} \cexpectationlong{\mathcal{F}_n}{\characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau = m \rbrace } \xi} \\
&= \cexpectationlong{\mathcal{F}_n}{\characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau > n \rbrace } \xi} \\
&=\characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau > n \rbrace }  \cexpectationlong{\mathcal{F}_n}{\xi} \\
&= \sum_{m>n} \characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau=m \rbrace }  \cexpectationlong{\mathcal{F}_n}{\xi} 
\end{align*}
So this shows us how to get everything into a common form if we break
up the sum properly, 
\begin{align*}
\cexpectationlong{\mathcal{F}_\sigma}{\cexpectationlong{\mathcal{F}_\tau}{\xi}} 
&= \sum_{n=0}^\infty \sum_{m=n+1}^\infty \characteristic{\lbrace \sigma = n \rbrace}
\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\mathcal{F}_m}{\characteristic{
    \lbrace \tau = m \rbrace } \xi}} + \\
&\sum_{m=0}^\infty \sum_{n=m}^\infty \characteristic{\lbrace \sigma = n \rbrace}
\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\mathcal{F}_m}{\characteristic{
    \lbrace \tau = m \rbrace } \xi}}  \\
&=\sum_{n=0}^\infty \sum_{m=n+1}^\infty \characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau=m \rbrace }  \cexpectationlong{\mathcal{F}_n}{\xi} + \\
&\sum_{m=0}^\infty \sum_{n=m}^\infty \characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau=m \rbrace }  \cexpectationlong{\mathcal{F}_m}{\xi}
  \\
&= \sum_{m=0}^\infty \sum_{n=0}^\infty \characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau=m \rbrace }  \cexpectationlong{\mathcal{F}_{m\wedge
      n}}{\xi} \\
&= M^\xi_{\sigma \wedge \tau} = \cexpectationlong{\mathcal{F}_{\sigma
    \wedge \tau}}{\xi}
\end{align*}
\end{proof}

\begin{xca}\label{SumOfOptionalTimes}Let $\sigma$ and $\tau$ be $\mathcal{F}$-optional times on
  either $\integers_+$ or $\reals_+$.
  Show that $\sigma + \tau$ is $\mathcal{F}$-optional.
\end{xca}
\begin{proof}
For the case of $\integers_+$ valued optional times we pick $n \geq 0$
and note that
\begin{align*}
\lbrace \sigma + \tau = n \rbrace = \cup_{m=0}^n \lbrace \sigma = m
\rbrace \cap \lbrace \tau = n-m \rbrace
\end{align*}
which is in $\mathcal{F}_n$ since for $0 \leq m \leq n$ we have
$\lbrace \sigma = m \rbrace \in \mathcal{F}_m \subset \mathcal{F}_n$
and $\lbrace \tau = n - m \rbrace \in \mathcal{F}_{n-m} \subset
\mathcal{F}_n$.

Pick $t \in $ and note that it suffices to show
\begin{align*}
\lbrace \sigma + \tau > t \rbrace &= \lbrace \sigma > t \rbrace \cup \cup_{\substack{q < t \\ q \in
    \rationals}} \lbrace \sigma > q \rbrace \cap \lbrace \tau > t - q \rbrace
\end{align*}
by reasoning similar to the discrete case.  To see this equality for
one inclusion note
that for all $q \in \rationals$ we have $\lbrace \sigma > q \rbrace
\cap \lbrace \tau > t - q \rbrace \subset \lbrace \sigma + \tau > t
\rbrace$.  By positivity of $\tau$ we know that $ \lbrace \sigma > t
\rbrace \subset \lbrace \sigma + \tau > t \rbrace $.

For the other inclusion suppose $\sigma(\omega) + \tau(\omega) > t$.
If $\sigma(\omega) \leq t$ then by density of rationals we can pick $q \in
\rationals$ such that $t - \tau(\omega) < q < \sigma(\omega) \leq t$ and
we have $\omega \in \lbrace \sigma > q \rbrace \cap \lbrace \tau > t -
q\rbrace$.  If $\sigma(\omega) > t$ then it follows that $\omega \in
\lbrace \sigma > t \rbrace$ so we are done.
\end{proof}

\begin{xca}Show that a random variable $\xi$ has subexponential tails
  if and only if there exists $C > 0$ such that
  $\expectation{\abs{\xi}^k} \leq C k^C$ for all integers $k > 0$.
\end{xca}
\begin{proof}
TODO: Mimic the proof of Lemma \ref{SubgaussianEquivalence}.
\end{proof}

\begin{xca}Let $X$ be a right continuous
  submartingale then almost surely $X$ is cadlag.
\end{xca}
\begin{proof}
By Theorem \ref{CadlagModificationContinuousMartingale} we know that
there is a null set $A$ such that
the process $Z_t = \characteristic{A^c} \lim_{q \to t^+}X_q$ is a
cadlag process (in fact a cadlag $\overline{\mathcal{F}}^+$-submartingale).
As $X$ is almost surely right continuous, it follows that almost surely $Z = X$ and we
conclude that almost surely $X$ has cadlag sample paths.
\end{proof}

\begin{xca}Suppose we are given $\sigma$-algebras $\mathcal{G},
  \mathcal{H}, \mathcal{F}_1, \mathcal{F}_2, \dotsc$ and define
  $\mathcal{F}_\infty = \bigvee_n \mathcal{F}_n$.  If
  $\cindependent{\mathcal{G}}{\mathcal{H}}{\mathcal{F}_n}$ for all $n
  \in \naturals$ then $\cindependent{\mathcal{G}}{\mathcal{H}}{\mathcal{F}_\infty}$.
\end{xca}
\begin{proof}
By definition of conditional independence we see that for every $G \in
\mathcal{G}$ and $H \in \mathcal{H}$ we have
$\cprobability{\mathcal{F}_n}{G \cap H} =
\cprobability{\mathcal{F}_n}{G} \cprobability{\mathcal{F}_n}{H}$.  By
Levy-Jessen Theorem \ref{JessenConditioningLimits} we conclude
\begin{align*}
\cprobability{\mathcal{F}_\infty}{G \cap H} &=  \lim_{n \to \infty} \cprobability{\mathcal{F}_n}{G \cap H} =
 \lim_{n \to \infty}\cprobability{\mathcal{F}_n}{G}
 \cprobability{\mathcal{F}_n}{H} = \cprobability{\mathcal{F}_\infty}{G}
 \cprobability{\mathcal{F}_\infty}{H}
\end{align*}
which shows the result.
\end{proof}

\begin{xca}Let $B_t$ be a standard Brownian motion and define $\tau =
  \inf \lbrace t > 0 \mid B_t = 1 \rbrace$.  Show that
  $B_\tau = 1$ almost surely and $\expectation{\tau^c} < \infty$ for
  all $0 \leq c < 1/2$.
\end{xca}
\begin{proof}
Note that $\tau < \infty$ almost surely since $\limsup_{t \to \infty}
B_t = \infty$ almost surely and $B_t$ is continuous.  For any $\lambda
\geq 0$ note that $ \lbrace \tau \geq t \rbrace = \lbrace
\sup_{0 \leq s \leq t} B_s \leq 1$.  Since the law of $\sup_{0 \leq s
  \leq t} B_s$ is the same as the law of $\abs{B_t}$ by Lemma
\ref{TailsAndExpectations} we get
\begin{align*}
\expectation{\tau^c} &= c^{-1} \int_0^\infty t^{c-1} \probability{\tau
  \geq t} \, dt = \frac{2}{c \sqrt{2\pi}} \int_0^\infty
\int_0^{1/\sqrt{t}}  t^{c-1} e^{-x^2/2} \, dx \, dt \\
&= \frac{2}{c \sqrt{2\pi}} \int_0^\infty
\int_0^{1/x^2}  t^{c-1} e^{-x^2/2} \, dt \, dx = \frac{2}{\sqrt{2\pi}}
\int_0^\infty x^{-2c} e^{-x^2/2} \, dx 
\end{align*}
For $0 \leq c < 1/2$ an integration by parts shows that this integral
is finite.

Note also that integration by parts also shows that
$\expectation{\tau^c} = \infty$ for $c \geq 1/2$ (as we know must be
true because of the BDG/Optional Stopping argument above).
\end{proof}

\begin{xca}Let $B_t$ be a standard Brownian motion show that for every
  $c \in \reals$ the process $M_t = e^{c B_t - \frac{c^2t}{2}}$ is a martingale.
\end{xca}
\begin{proof}
Adaptedness follows from the fact that $B_t$ is
$\mathcal{F}_t$-measurable and $e^x$ is continuous hence Borel
measurable.  First to see that $M_t$ is integrable we compute by Lemma
\ref{ExpectationRule} and completing the
square
\begin{align*}
\expectation{e^{cB_t}} &= \frac{1}{\sqrt{2\pi t}}
\int_{-\infty}^\infty e^{c x} e^{-x^2/2t} \, dx = \frac{ e^{c^2t/2}}{\sqrt{2\pi t}}
\int_{-\infty}^\infty e^{-(x-ct)^2/2t} \, dx = e^{c^2t/2} < \infty
\end{align*}
If we take $0 \leq s < t < \infty$ then using the pullout rule of
conditional expectation, the fact that $B_t -
B_s$ is independent of $\mathcal{F}_s$ and the above computation of
the expectation to see that
\begin{align*}
\cexpectationlong{\mathcal{F}_s}{e^{c B_t - \frac{c^2t}{2}}}
&=e^{- \frac{c^2t}{2}}\cexpectationlong{\mathcal{F}_s}{e^{c (B_t -B_s)} e^{cB_s} } 
= e^{- \frac{c^2t}{2}}\expectation{e^{c (B_t -B_s)}} e^{cB_s} \\
&= e^{- \frac{c^2t}{2} }e^{\frac{c^2(t-s)}{2}}  e^{cB_s} 
= e^{c B_s - \frac{c^2s}{2}}
\end{align*}
\end{proof}

\begin{xca}Let $B_t$ be a standard Brownian motion, show that $\inf
  \lbrace t > 0 \mid B_t > 0 \rbrace = 0$ a.s.  (Hint: Use
  Blumenthal's 0-1 Law).
\end{xca}
\begin{proof}
Let $\tau = \inf \lbrace t > 0 \mid B_t > 0 \rbrace$.  Clearly the
event 
\begin{align*}
\lbrace \tau = 0 \rbrace &= \cap_{n=1}^\infty \cup_{\substack{0 < q <
    1/n\\ q \in \rationals}} \lbrace B_q > 0 \rbrace
\end{align*}
is  $\mathcal{F}^+_0$-measurable so by Lemma
  \ref{Blumenthal01LawBrownianMotion} we know that it has probability
  0 or 1.  It therefore suffices to show that $\probability { \tau =
    0} \neq 0$.  To see this note that for each $s > 0$ we have
$\lbrace \tau \leq s \rbrace \supset \lbrace B_s > 0 \rbrace$ hence
$\probability{\tau \leq s} \geq 1/2$ and by continuity of measure we
know that $\probability {\tau = 0} = \lim_{s \downarrow 0} \probability{\tau \leq s} \geq 1/2$.
\end{proof}

\begin{xca}[Law of Large Numbers for Brownian Motion]Let $B_t$ be a standard Brownian motion show that $M_t =
  t^{-1}B_t$ is a backward martingale.  From this conclude that
  $t^{-1}B_t \toas 0$ and $t^{-1}B_t \tolp{p} 0$ for all $p > 0$.
\end{xca}
\begin{proof}
Adaptedness and integrability are immediate.  For the backward
martingale property, let $s < t$ and we first find the density function for the
conditional distribution $\cprobability{B_t}{B_s \in \cdot}$.  To find
the joint density $(B_s, B_t)$ we note that $(B_s,B_t) = (x,y)$ if and
only if $(B_s,B_t - B_s) = (x, y - x)$ so by the independence of $B_s$
and $B_t - B_s$ and completing the square we get
\begin{align*}
\probability{B_s = x ; B_t = y} &= \frac{1}{\sqrt{2\pi s} } e^{-x^2/2s}
  \frac{1}{\sqrt{2\pi(t- s)}} e^{-(y-x)^2/2(t-s)} \\
&= \frac{1}{\sqrt{2\pi s}} \frac{1}{\sqrt{2\pi(t- s)}}
    e^{-\frac{t}{2s(t-s)}(x - \frac{s}{t}y)^2} e^{-y^2/2t}
\end{align*}
So we see that the conditional density $B_s$ given $B_t$ is Gaussian
with mean $\frac{s}{t} B_t$.  Thus $\cexpectationlong{B_t}{s^{-1} B_s}
= t^{-1} B_t$.  By the extended Markov property (Lemma
\ref{ExtendedMarkovProperty}) we know that $\cindependent{B_s}{\bigvee_{u \geq t}\sigma(
    B_u)}{B_t}$ and therefore $\cexpectationlong{B_t}{s^{-1} B_s} = \cexpectationlong{\bigvee_{u \geq t}\sigma(
    B_u)}{s^{-1} B_s}$ (Lemma \ref{ConditionalIndependenceDoob}) which
  shows the backward martingale property.

Now we need to show that $\cap_{t > 0} \bigvee_{u \geq t} \sigma(B_u)$
is a trivial $\sigma$-field (Lemma \ref{Blumenthal01LawBrownianMotion});
from that it follows that for all $t > 1$, 
\begin{align*}
t^{-1} B_t &= \cexpectationlong{\bigvee_{u \geq t} \sigma(B_u)}{B_1} 
\end{align*}
and by Jessen-Levy and triviality we have 
\begin{align*}
\cexpectationlong{\bigvee_{u \geq t}
  \sigma(B_u)}{B_1} \toas \cexpectationlong{\cap_{t>0} \bigvee_{u \geq t}
  \sigma(B_u)}{B_1} = \expectation{B_1} = 0
\end{align*}

TODO: Finish the $L^p$ argument; presumably we need $L^p$ boundedness.
\end{proof}

\begin{xca}[Kallenberg Exercise 13.19]\label{BrownianBridgeMartingale}Let $X$ be a Brownian bridge,
  show that $(1-t)^{-1}X_t$ is a martingale with respect to the
  induced filtration and is not $L^1$ bounded on $[0,1)$.
\end{xca}
\begin{proof}
Since $X_t$ is Gaussian with variance $t -t^2$ we have 
\begin{align*}
\frac{1}{1-t} \expectation{\abs{X_t}} &= \frac{1}{1-t}\frac{2}{\sqrt{2\pi(t - t^2)}}
                          \int_0^\infty x e^{-x^2/2(t - t^2)} \, dx = \sqrt{\frac{2t}{\pi(1-t)}}
\end{align*}
which shows that $(1 -t)^{-1}X_t$ is integrable on $[0,1)$ but not
$L^1$ bounded.  

Let $\mathcal{F}_t = \sigma((1-s)^{-1} X_s; 0 \leq s \leq t)$.  If we let $B$ be a Brownian motion then for $0 \leq s < t < 1$ we have
\begin{align*}
\frac{1}{(1-t)(1-s)} \expectation{X_t X_s} 
&=\frac{s(1-t)}{(1-t)(1-s)} = \frac{s}{1-s}
\end{align*}
which is does not depend on $t$.  Since $(1-t)^{-1}X_t$ is Gaussian this implies that for every $0 \leq r_1 < \dotsb < r_m \leq s < t < 1$ we have
$((1-r_1)^{-1}X_{r_1}, \dotsc, (1-r_m)^{-1}X_{r_m}, (1-s)^{-1}X_{s}, (1-t)^{-1}X_{t})$ is a Gaussian random vector and the same is true of
$((1-r_1)^{-1}X_{r_1}, \dotsc, (1-r_m)^{-1}X_{r_m}, (1-t)^{-1}X_{t} - (1-s)^{-1}X_{s})$ (Example \ref{LinearTransformationGaussian}).
By Proposition \ref{GaussianIndependence} we know that $(1-t)^{-1}X_{t} - (1-s)^{-1}X_{s} \Independent ((1-r_1)^{-1}X_{r_1}, \dotsc, (1-r_m)^{-1}X_{r_m})$ and
thus for all $0 \leq s < t < 1$ by Lemma \ref{IndependencePiSystem} we have $(1-t)^{-1}X_{t} - (1-s)^{-1}X_{s} \Independent \mathcal{F}_s$. 
The martingale property follows in the standard way,
\begin{align*}
\cexpectationlong{\mathcal{F}_s}{(1-t)^{-1}X_t}
&=\cexpectationlong{\mathcal{F}_s}{(1-t)^{-1}X_t - (1-s)^{-1}X_s} + 
\cexpectationlong{\mathcal{F}_s}{(1-s)^{-1}X_s} \\
&=\expectation{(1-t)^{-1}X_t - (1-s)^{-1}X_s} + (1-s)^{-1}X_s =   (1-s)^{-1}X_s\\
\end{align*}
\end{proof}

\begin{xca}\label{ProductOfIndependentMarkov}Let $X$ be a Markov process in $(S, \mathcal{S})$  on time scale $T$ with
  transition kernel $\mu_{s,t}$ and
  let $Y$ be a Markov process in $(U, \mathcal{U})$ on time scale $T$ with 
  transition kernel $\nu_{s,t}$.  Show that if $X$ and $Y$ are
  independent
  that $(X,Y)$ is a Markov process in $(S \times U, \mathcal{S}
  \otimes \mathcal{U})$ on time scale $T$ with transition kernel $\mu_{s,t}
  \otimes \nu_{s,t}$ (note that kernel $\mu_{s,t}
  \otimes \nu_{s,t}$ is just the pointwise product measure).
\end{xca}
\begin{proof}
TODO: Finish

Pick $t < u \in T$.  Let $C \in \mathcal{S}$ and $D \in \mathcal{U}$ and compute using the
independence of $X$ and $Y$, let
\begin{align*}
&\probability{(X_u,Y_u) \in A \times B ; (X_t, Y_t) \in C
  \times D} \\
&=\probability{X_u \in A ; X_t \in C} \probability{Y_u \in B ; Y_t \in  D} \\
&=\expectation{\cprobability{X_u \in A}{X_t} ; X_t \in C}
\expectation{\cprobability{Y_u \in B}{Y_t} ;  Y_t \in  D} \\
&=\expectation{\cprobability{X_u \in A}{X_t} ; X_t \in C;
\cprobability{Y_u \in B}{Y_t} ;  Y_t \in  D} \\
\end{align*}
which since sets of the form $ (X_t, Y_t) \in C
  \times D$ are a generating
$\pi$-system of $\sigma(X_t,Y_t)$ we the claim is shown by Lemma
\ref{ConditionalExpectationExtension}.

Finally we conclude that $\cprobability{(X_u, Y_u) \in
  \cdot}{(X_t,Y_t)} = \mu_{t,u} \otimes \nu_{t,u}$ 
by the uniqueness of product measure.
\end{proof}

\begin{xca}[Kallenberg Exercise 17.1]Show that if $M$ is a local
  martingale and $\xi$ is a $\mathcal{F}_0$-measurable random variable 
then $N_t = \xi M_t$ is also a local martingale.
\end{xca}
\begin{proof}
Let $\tau_n$ be a localizing sequence for $(M-M_0)$ so that
$(M-M_0)^{\tau_n}$ is a martingale for all $n \in \naturals$ and
$\tau_n \uparrow \infty$ almost surely.  Let $\sigma_n = \tau_n
\characteristic{\abs{\xi} \leq n}$.  Since $\xi$ is almost surely
finite then it follows that $\sigma_n \uparrow \infty$ almost surely
(specifically on the intersection of the event that $\tau_n \uparrow
\infty$ and $\abs{\xi} < \infty$).  Moreover since $\lbrace \sigma_n
\leq t \rbrace = \lbrace\tau_n \leq t \rbrace \cup \lbrace \abs{\xi}
\leq n \rbrace$ and $\xi$ is $\mathcal{F}_0$-measurable it follows
that $\sigma_n$ are $\mathcal{F}$-optional times.  Now note that
$\xi M$ is $\mathcal{F}$-adapted since $\xi$ is
$\mathcal{F}_0$-measurable
and for each $n \in \naturals$  and $0 \leq t < \infty$ we have
\begin{align*}
\expectation{\abs{\xi M_{t \wedge \sigma_n} - \xi M_0}} 
&= \expectation{\abs{\xi} \abs{M_{t \wedge \tau_n} - M_0} ; \abs{\xi}
  \leq n} 
\leq  n \expectation{( M -
  M_0)^{\tau_n}} < \infty
\end{align*} 
and moreover by the pullout rule of
conditional expectation and the fact that $\tau_n$ localizes $M$,
\begin{align*}
\cexpectationlong{\mathcal{F}_s}{(\xi M_{\sigma_n \wedge t} - \xi M_0)}
&=\cexpectationlong{\mathcal{F}_s}{\xi  (M_{\sigma_n \wedge t} - M_0)
  ; \abs{\xi} \leq n} \\
&= \xi \characteristic{\abs{\xi} \leq n}  \cexpectationlong{\mathcal{F}_s}{M_{\tau_n \wedge t} - M_0} \\
&= \xi \characteristic{\abs{\xi} \leq n} (M_{\tau_n \wedge s} - M_0) \\
&= \xi M_{\sigma_n \wedge s} - \xi M_0
\end{align*}
showing that $(M-M_0)^{\sigma_n}$ is a martingale.
\end{proof}

\begin{xca}[Kallenberg Exercise 17.2]Show that a
  local martingale $M$ with $M \geq 0$ for all $0 \leq t < \infty$
  almost surely and $\expectation{M_0} < \infty$ is a
  supermartingale.  Give an example to show that $M$ is not necessarily a martingale.
\end{xca}
\begin{proof}
Let $\tau_n$ be a localizing sequence for $M$.  We know that $(M -
M_0)^{\tau_n}$ is a martingale but by the integrability and 
$\mathcal{F}_0$-measurability of $M_0$ we
see that in fact $M^{\tau_n}$ is a martingale.  Now 
since $M \geq 0$, $M_{\tau_n \wedge t} \toas M_t$ for all $0 \leq t < \infty$ and 
Fatou's Lemma for conditional expectations we have for all $0 \leq s < t < \infty$
\begin{align*}
\cexpectationlong{\mathcal{F}_s}{M_t} &\leq 
\liminf_{n \to \infty}\cexpectationlong{\mathcal{F}_s}{M_{\tau_n \wedge t}}
= \liminf_{n \to \infty} M_{\tau_n \wedge t} = M_{s}
\end{align*}
almost surely.  Choosing $s=0$ and taking expectations shows that $\expectation{M_t} \leq \expectation{M_0} < \infty$ 
and therefore $M$ is a supermartingale.

TODO: Do the last part.
\end{proof}
\begin{xca}[Kallenberg Exercise 17.4]Let $M_n$ be a sequence of continuous local
  martingales starting at zero and let $\tau_n$ be a sequence of
  optional times, then $(M^*_n)_{\tau_n} \toprob 0$ if and only if
  $[M_n]_{\tau_n} \toprob 0$.  State and prove a corresponding result
  for stochastic integrals.
\end{xca}
\begin{proof}
Define $\tilde{M}_n = M_n^{\tau_n}$ and apply Lemma
\ref{QuadraticCovariationAndContinuity} to conclude that
$\tilde{M}_n^* \toprob 0$ if and only if $[\tilde{M}_n]_\infty \toprob
0$.  Now note that 
\begin{align*}
\tilde{M}^*_n 
&= \sup_{0 \leq t < \infty} \abs{(\tilde{M}_n)_t} 
= \sup_{0 \leq t < \infty} \abs{(M_n)_{\tau_n \wedge t}} 
= (M_n)^*_{\tau_n}
\end{align*}
and by Theorem \ref{OptionalQuadraticCovariation} we get
\begin{align*}
[\tilde{M}_n]_\infty
&= [M^{\tau_n}_n]_\infty
=[M_n]^{\tau_n}_\infty
=[M_n]_{\tau_n}
\end{align*}

The corresponding result for stochastic integrals says that
given continuous local martingales $M_n$, processes $V_n \in
L(M_n)$ and optional times $\tau_n$ we have $\left(\int V_n \,
dM_n\right)^*_{\tau_n} \toprob 0$ if and only if $\int_0^{\tau_n} V_n^2(s) \, d[M_n](s)
\toprob 0$.  This follows from what we have just proven and 
Lemma \ref{BasicPropertiesStochasticIntegralContinuousMartingale} to see
\begin{align*}
[\int V_n \, dM_n]_{\tau_n} &= \int_0^{\tau_n} V_n(s) \, d[M_n](s)
\end{align*}
\end{proof}

\begin{xca}[Kallenberg Exercise 17.6]Let $B_t$ be a Brownian motion starting at zero and $\tau$ be an optional
  time.  Show that $\expectation{\tau^{1/2}} < \infty$ implies
  $\expectation{B_\tau} = 0$ and $\expectation{\tau} < \infty$ implies
  $\expectation{B^2_\tau} = \expectation{\tau}$.
\end{xca}
\begin{proof}For $\tau$ bounded by a constant $T$ these are both consequences of
  Optional Stopping.  For then since $B_t$ is a martingale we have
\begin{align*}
\expectation{B_\tau} &=
\expectation{\cexpectationlong{\mathcal{F}_\tau}{B_T}} =
\expectation{B_T} = 0
\end{align*}
and since $B^2_t - t$ is a martingale we have
\begin{align*}
\expectation{B^2_\tau} - \expectation{\tau}&=
\expectation{\cexpectationlong{\mathcal{F}_\tau}{B^2_T - T}} =
\expectation{B^2_T - T} = 0
\end{align*}
Now consider the sequence of bounded optional times $\tau \wedge n$.
If we have $\expectation{\tau^{1/2}} < \infty$ the we can apply the
BDG inequality (Lemma \ref{BDGInequalities}) to the stopped process
$B^\tau$ (which is a priori only a continuous local martingale) to see that there is a
constant $c_1 > 0$ such that $\expectation{\sup_{0 \leq t \leq \tau}
  \abs{B_t}} \leq c \expectation{\tau^{1/2}} < \infty$ and therefore
since $\abs{B_{\tau \wedge n}} \leq \sup_{0 \leq t \leq \tau}
\abs{B_t}$ we can use Dominated Convergence to see that
$\expectation{B_\tau} = \lim_{n\to \infty} \expectation{B_{\tau \wedge
    n}} = 0$.  Similarly when $\expectation{\tau} < \infty$, we get a constant $c_2 > 0$ such that $\expectation{\sup_{0 \leq t \leq \tau}
  \abs{B^2_t}} \leq c_2 \expectation{\tau} < \infty$ and therefore
Dominated Convergence gives us 
\begin{align*}
\expectation{B^2_\tau} &= \lim_{n \to \infty} \expectation{B^2_{\tau
    \wedge n}} = \lim_{n \to \infty} \expectation{\tau \wedge n} = \expectation{\tau}
\end{align*}

While we're at it, we can provide a different proof that
$\expectation{B_\tau} = 0$ under the weaker assumption
$\expectation{\tau} < \infty$ that doesn't rely on the BDG
inequalities.  As above, it suffices to show that $\abs{B_{\tau \wedge
    t}}$ is dominated by an integrable random variable.  The proof here is taken from Peres and Morters.  For
each integer $k \geq 0$ consider 
\begin{align*}
M_k = \sup_{0 \leq t \leq 1} \abs{B_{t+k} - B_k}
\end{align*}
TODO: Finish
\end{proof}

\begin{xca}[Kallenberg Exercise 17.8] Let $X$ and $Y$ be continuous semimartingales show that
  $[X+Y]_t^{1/2} \leq [X]_t^{1/2} + [Y]_t^{1/2}$ for all $0 \leq t <
  \infty$ almost surely.
\end{xca}
\begin{proof}
From bilinearity and the Cauchy Schwartz inequality Lemma
\ref{CourregeCauchySchwartz} we have
\begin{align*}
[X+Y]_t &= [X]_t + 2[X,Y]_t + [Y]_t \leq  [X]_t + 2\abs{[X,Y]_t} +
          [Y]_t \leq 
[X]_t + 2[X]_t^{1/2}[Y]_t^{1/2} + [Y]_t = ([X]_t^{1/2} + [Y]_t^{1/2})^2
\end{align*}
for all $0 \leq t < t$ almost surely.
The result follows by noting the non-negativity of quadratic
variation and taking square rooots.
\end{proof}

\begin{xca}[Kallenberg Exercise 17.11]Let $X$ be a continuous
  semimartingale and let $U,V \in L(X)$ be such that $U = V$ a.s. on a
  set $A \in \mathcal{F}_0$.  Use Lemma
  \ref{StoppingIntegralsContinuousSemimartingale} to show that $\int U \, dX = \int V \, dX$
  a.s on $A$.
\end{xca}
\begin{proof}
Define 
\begin{align*}
\tau(\omega) &= \begin{cases}
\infty & \text{where $\omega \in A$} \\
0 & \text{when $\omega \notin A$}
\end{cases}
\end{align*}
and note that $\tau$ is an optional time since $A \in
\mathcal{F}_0$. Also note that 
\begin{align*}
\characteristic{[0,\tau(\omega)]}( t)
\cdot U_t(\omega)  &= \begin{cases}
U_t(\omega) & \text{when $\omega \in A$ or $\omega \notin A$ and
  $t=0$} \\
0 & \text{when $\omega \notin A$ and $t > 0$}
\end{cases}
\end{align*}
and similarly with $V$ and therefore we conclude
$\characteristic{[0,\tau]} \cdot U = \characteristic{[0,\tau]} \cdot
V$ almost surely and therefore $\int \characteristic{[0,\tau]}  U \,
dM = \int \characteristic{[0,\tau]}  V \, dM$ almost surely by Lemma
\ref{BasicPropertiesStochasticIntegralContinuousMartingale}.   From Lemma
\ref{StoppingIntegralsContinuousSemimartingale} and the fact that
$\int U \, dX$ starts at zero we get almost surely
\begin{align*}
\characteristic{A} \int_0^t U \, dM &= \characteristic{A} \int_0^t U
\, dM + \characteristic{A^c} \int_0^0 U \, dM 
=\int_0^{t \wedge \tau} U \, dM 
= \int_0^{t} \characteristic{[0,\tau]} U \, dM \\
\end{align*}
and similarly with $V$ and the result follows.
\end{proof}

\begin{xca}[Kallenberg Exercise 17.13]Let $X$ be Brownian bridge then
  $X_{t \wedge 1}$ is a semimartingale.
\end{xca}
\begin{proof}
We know from Exercise \ref{BrownianBridgeMartingale} that
$Y_t = (1-t)^{-1}X_t$ is a martingale on $[0,1)$.  Now apply the local Ito
Lemma to the semimartingale $(1 \wedge t ,(1-t)^{-1}X_t)$ using the
function $f(t,x) = (1-t) x$ on the domain $(-\infty,1) \times \reals$ to see that
\begin{align*}
X_{t \wedge 1} &= f(t,Y_t) - f(0,Y_0) = 
\int_0^t (1-s) \, dY_s -\int_0^t Y_s \, ds
\end{align*}
where the first stochastic integral is continuous local martingale and the second random Stieltjes integral is of finite variation.

TODO: Get all the details around the local Ito stuff spelled out.
\end{proof}

\begin{xca}\label{QuadraticCovariationMatrixVectorStochasticIntegral}Let
  $M$ be a continuous local martingale in $\reals^r$ and let $V \in L(X)$ be an $d
  \times r$ matrix valued process in $L(M)$, then 
\begin{align*}
\left[ \left(\int V_s \, dM_s \right)^{(i)},  \left(\int V_s \, dM_s
  \right)^{(i)} \right]_t &= \sum_{k=1}^r \sum_{l=1}^r \int_0^t
                            V^{ik}_s V^{jl}_s \, d[M^k,M^l]_s \text{ for all
                            $1 \leq i,j \leq d$}
\end{align*}
which we write stylistically as $[\int V \, dM] = \int V d[M] V^T$.
\end{xca}
\begin{proof}
This just follows from bilinearity of quadratic covariation and Lemma \ref{BasicPropertiesStochasticIntegralContinuousMartingale}
\begin{align*}
\left[ \left(\int V_s \, dM_s \right)^{(i)},  \left(\int V_s \, dM_s
  \right)^{(i)} \right]_t 
&= \left[ 
\sum_{k=1}^r \int V^{ik}_s \,dM^{k}_s, 
\sum_{l=1}^r \int V^{jl}_s \,dM^{l}_s 
\right]_t \\
&=\sum_{k=1}^r \sum_{l=1}^r \left[ \int V^{ij}_s \,dM^{k}_s, \int V^{jl}_s \,dM^l_s \right]_t \\
&=\sum_{k=1}^r \sum_{l=1}^r \int V^{ik}_s V^{jl}_s \, d[ M^{k}, M^l ]_s \\
\end{align*}
\end{proof}

\begin{xca}Let $(S,d)$ be a totally bounded metric space.  Prove that $S$ is separable and that every uniformly continuous function $f : S \to \reals$ is bounded.
\end{xca}
\begin{proof}For each $n \in \integers$ we may find a finite set $x^n_1, \dots, x^n_{m_n}$ such that $B(x^n_j; 1/n)$ covers $S$.  The union of $x^n_{j}$ for $n \in \integers$ and $j=1, \dotsc, m_n$ is a countable dense subset of $S$.  Assume that $f$ is uniformly continuous, pick an $\epsilon > 0$ such that $d(x,y) < \epsilon$ implies $\abs{f(x) - f(y)} < 1$.  Now pick $x_1, \dotsc, x_n$ such that $B(x_1; \epsilon), \dotsc, B(x_n; \epsilon)$ covers $S$.  Let $K = \abs{f(x_1)} \vee \dotsb \vee \abs{f(x_n)}$ and note that for every $x$ we may pick $x_j$ such that $x \in B(x_j; \epsilon)$ and therefore $\abs{f(x)} \leq K + 1$.
\end{proof}

\begin{xca}\label{SkorohodInfiniteJ1PushForwardContinuous}Let $(S,r)$ and $(T, r^\prime)$ be metric spaces and $g : S  \to T$ be a continuous.  Define $g_* : D([0,\infty); S) \to D([0,\infty); T)$ by $g_*(f)(t) = g(f(t))$ then $g_*$ is
continuous in the $J_1$ topology.
\end{xca}
\begin{proof}
Since the $J_1$ topology is metrizable it suffices to show that $g_*$ takes convergence sequences convergent sequences.  Let $f, f_1, f_2, \dotsc \in  D([0,\infty); S)$ be such that
$f_n \to f$.  Then by Proposition \ref{SkorohodInfiniteJ1EquivalenceC} we know that for every $T >0$ and every $t, t_1, t_2, \dotsc \in [0,T]$ with $\lim_{n \to \infty} t_n = t$ we have 
\begin{itemize}
\item[(i)] $\lim_{n \to \infty} r(f_n(t_n), f(t)) \wedge r(f_n(t_n), f(t-)) = 0$. 
\item[(ii)] If $\lim_{n \to \infty} r(f_n(t_n), f(t)) = 0$ then for every sequence
$s_n$ such that $t_n \leq s_n \leq T$ and $\lim_{n \to \infty} s_n = t$ we have $\lim_{n \to \infty} r(f_n(s_n), f(t)) = 0$
\item[(iii)] If $\lim_{n \to \infty} r(f_n(t_n), f(t-)) = 0$ then for every sequence
$s_n$ such that $0 \leq s_n \leq t_n$ and $\lim_{n \to \infty} s_n = t$ we have $\lim_{n \to \infty} r(f_n(s_n), f(t-)) = 0$
\end{itemize}
Using continuity of $g$ at $f(t)$ and $f(t-)$ for every $\epsilon>0$ there exists $\delta > 0$ such that $r(x, f(t)) < \delta$ implies $r^\prime(g(x), g(f(t))) < \epsilon$ and 
$r(x, f(t-)) < \delta$ implies $r^\prime(g(x), g(f(t-))) < \epsilon$.  By (i) we may find $N$ such that for all $n \geq N$ we have $r(f_n(t_n), f(t)) \wedge r(f_n(t_n), f(t-)) < \delta$; thus
$r^\prime(g(f_n(t_n)), g(f(t))) \wedge r^\prime(g(f_n(t_n)), g(f(t-))) < \epsilon$ which shows $\lim_{n \to \infty} r^\prime(g(f_n(t_n)), g(f(t))) \wedge r^\prime(g(f_n(t_n)), g(f(t-))) = 0$.

Also that $\lim_{s \to t^-} g(f(s)) = g(f(t-))$ since  $g$ is continuous at $f(t-)$: given $\epsilon > 0$ we take $\delta > 0$ such that $r(x, f(t-)) < \delta$ implies $r^\prime(g(x), g(f(t-))) < \epsilon$; now take $\rho > 0$ such that $t - \rho < s < t$ implies $r(f(s), f(t-)) < \delta$.  For the moment we fix $T>0$ and $0 \leq t \leq T$.  We consider two cases separately.

Case 1: $g(f(t)) = g(f(t-))$.  In this conditions (i), (ii) and (iii) reduce to the assertion that for all $t_n \to t$ we have $g(f_n(t_n)) \to g(f(t))$. From the continuity of $g$ given $\epsilon > 0$ then $g^{-1}(B(g(f(t)), \epsilon))$ is open in $S$ and contains both $f(t)$ and $f(t-)$ (it doesn't matter whether $f(t) = f(t-)$ or not).  We may find a $\delta >0$ such that $B(f(t), \delta) \subset g^{-1}(B(g(f(t)), \epsilon))$ and $B(f(t-), \delta) \subset g^{-1}(B(g(f(t)), \epsilon))$.  By the property (i) of $f$ and the $f_n$ we may find $N>0$ such that $f_n(t_n) \in B(f(t), \delta) \cup B(f(t-), \delta)$ for all $n \geq N$ and we are done.  I think this even easier because for this case (i) is equivalent to (i), (ii) and (iii) for $g_*(f)$ and $g_*(f_n)$ and we have already shown that (i) implies (i).
 
Case 2:  $g(f(t)) = g(f(t-))$ and $f(t) \neq f(t-)$.   We already know that (i) holds for $g_*(f)$ and $g_*(f_n)$.  Suppose $t_n \to t$ and $g(f_n(t_n)) \to g(f(t))$.  Then because $f_n \to f$ we know that $r(f_n(t_n), f(t)) \wedge r(f_n(t_n), f(t-)) \to 0$. If it is not true that $r(f_n(t_n), f(t)) \to 0$ then we can find a subsequence $n_k$ such that $f_{n_k}(t_{n_k}) \to f(t-)$ but by continuity of $g$ we conclude $g(f_{n_k}(t_{n_k})) \to g(f(t-))$ which is a contradiction (recall that $g(f(t-)) = \lim_{s \to t^-} g \circ f(s)$ by continuity of $g$).  Therefore we get $f_n(t_n) \to f(t)$ and for any $s_n \geq t_n$ with $s_n \to t$ we conclude $f_n(s_n) \to f(t)$ hence $g(f_n(t_n)) \to g(f(t))$ by continuity of $g$.  By a similar argument, if we assume that $g(f_n(t_n)) \to g(f(t-))$ then we conclude that $f_n(t_n) \to f(t-)$ and therefore for all $s_n \leq t_n$ with $s_n \to t$ we have $g(f_n(s_n)) \to g(f(t-))$ and thus (iii) holds.

Now we apply Proposition \ref{SkorohodInfiniteJ1EquivalenceC} in the opposite direction to conclude that $g \circ f_n \to g \circ f$ in the $J_1$ topology.
\end{proof}

\begin{xca}Define $\psi : D([0,\infty); \reals) \to D([0,\infty); \reals)$ by $\psi (f) (t) = \sup_{0 \leq s \leq t} f(s)$.  Show that $\psi$ is continuous in the $J_1$ topology.
\end{xca}
\begin{proof}
$\psi(f)$ is non-decreasing and finite therefore it cadlag.  To see continuity suppose that $f_n \to f$ in the $J_1$ topology.  Pick $\lambda_n$ such that $\gamma(\lambda_n) \to 0$
and $\lim_{n \to \infty} \sup_{0 \leq t \leq T} \abs{f_n(t) - f(\lambda_n(t))} =0$ for all $T > 0$.  Fix $T >0$ and note that
for any $\delta > 0$, $n \in \naturals$ and $0 \leq t \leq T$ we may pick $0 \leq w_n, u_n \leq t \leq T$ such that $\sup_{0 \leq s \leq t} f_n(s) \leq f_n(u_n) + \delta$ and 
$\sup_{0 \leq s \leq t} f(\lambda_n(s)) \leq f(\lambda_n(w_n)) + \delta$.  From these two inequalities and the fact that $\psi(f)(\lambda_n(t)) = \sup_{0 \leq s \leq \lambda_n(t)} f(s) = \sup_{0 \leq s \leq t} f(\lambda_n(s))$, we get
\begin{align*}
\psi(f_n)(t) &\leq f_n(u_n) + \delta \leq f(\lambda_n(u_n)) + \delta + \sup_{0 \leq t \leq T} \abs{f_n(t) - f(\lambda_n(t))}  \\
&\leq \sup_{0 \leq s \leq t} f(\lambda_n(s)) + \delta + \sup_{0 \leq t \leq T} \abs{f_n(t) - f(\lambda_n(t))}  \\
&= \psi(f)(\lambda_n(t)) + \delta + \sup_{0 \leq t \leq T} \abs{f_n(t) - f(\lambda_n(t))}  \\
\intertext{and}
\psi(f)(\lambda_n(t)) &\leq f(u) + \delta \leq f_n(w_n) + \delta + \sup_{0 \leq t \leq T} \abs{f_n(t) - f(\lambda_n(t))}  \\
&\leq \sup_{0 \leq s \leq t} f_n(s) + \delta + \sup_{0 \leq t \leq T} \abs{f_n(t) - f(\lambda_n(t))}  \\
&=\psi(f_n)(t) + \delta + \sup_{0 \leq t \leq T} \abs{f_n(t) - f(\lambda_n(t))} 
\end{align*}
thus $\sup_{0 \leq t \leq T} \abs{\psi(f_n)(t) - \psi(f)(\lambda_n(t))} \leq \delta + \sup_{0 \leq t \leq T} \abs{f_n(t) - f(\lambda_n(t))}$ and the result follows by taking the limit as $n \to \infty$ and then letting $\delta \to 0$.
\end{proof}


\appendix
\chapter{Techniques}

This section is a place to collect some of the recurring proof
techniques that one should be familiar with.

\section{Standard Machinery}
The standard measure theory arguments that proceed by showing a result
for indicator functions, simple random variables and the positive
random variables.  TODO:  There are a ton of examples of this such as
Lemma \ref{ChangeOfVariables} and Lemma \ref{ChainRuleDensity}.

\subsection{Monotone Class Arguments}
Part of the standard machinery that has independent utility is the
monotone class argument.  This allows one to demonstrate that a
property holds for an entire $\sigma$-algebra of sets by showing that
property holds for a simpler subclass of sets.  Good examples are
Lemma \ref{UniquenessOfMeasure} and Lemma \ref{IndependencePiSystem}.

\section{Almost Sure Convergence}
When one needs to show almost sure convergence of a sequence of random
variables the Borel Cantelli Theorem is a workhorse.  Good examples of
this are Lemma \ref{SLLNL4} and Lemma
\ref{ConvergenceInProbabilityAlmostSureSubsequence}.

Another technique to use that is related is to show that the sum of
the random variables is integrable.  Then you can conclude that the
sum of random variables is almost surely finite and therefore the
terms of the sequence converge to zero a.s.
Good examples of
this are Lemma \ref{SLLNL2} and Lemma
\ref{ConvergenceInProbabilityAlmostSureSubsequence}.

\section{Bounding Expectations}

A common task that one faces is to provide bounds for an expected
value (or more generally a moment).  For example, one may need to know
that a random variable has a finite expectation for use with the
Dominated Convergence Theorem.

\subsection{Using Tail Bound}
A problem I have encountered is trying to use a tail bound to prove
that an expectation is finite.  The problem that I sometime have is
that I write:
\begin{align*}
\expectation{f(\xi)} = \expectation{ \characteristic{\xi \leq \lambda}
  \cdot f(\xi)} + \expectation{\characteristic{\xi > \lambda} \cdot f(\xi)}
\end{align*}
Often knowing $\xi \leq \lambda$ we can show that the first
expectation is bounded (this is often easy).  The problem is usually
that one might be given a tail bound that controls $\probability{\xi >
  \lambda}$ but there is no control over the behavior of $f(\xi)$ that
allows one to provide a bound for the second expectation.  Are there
general approaches for dealing with this?  Possible answer here is
that one might need to take a different approach and use Lemma
\ref{TailsAndExpectations}.  A good example of how to do this is with 
Lemma \ref{SubgaussianEquivalence}.

TODO: Passing from $L^p$ convergence to almost sure convergence.  Note
that we easily get almost sure convergence along a subsequence.

\section{Proving Inequalities}
\subsection{Using Calculus}
If one wants to show that $f(x) \geq 0$ on an interval $[a,b]$ one of
the easiest ways to show the inequality is to find the minimum of
$f(x)$ on $[a,b]$ and to show this value is bigger than zero.  Finding
the minimum is a lot easier if $f(x)$ is differentiable.  A common
special case one can easily show $f(a) \geq 0$ and $f(b) \geq
0$ and show that $f(x)$ is increasing or decreasing on $[a,b]$ by
showing $f^{\prime}(x)$ is positive or negative.  The problem with
this technique is that it is really a proof technique and requires
that one knows the answer beforehand (e.g. one usually wants to show
$g(x) \leq h(x)$ and the bound $h(x)$ is what you are trying to figure
out).  Sometimes the technique can be
used to guess the answer by taking a simpler known inequality and
antidifferentiating (see Lemma \ref{GaussianTailsElementary} for a
non-trivial example).

\subsection{Using Taylor's Theorem}
Taylor's Theorem is also a good way of both guessing and proving
inequalities; if one can show that the remainder term (in either
integral or Lagrange form usually) is of a particular sign over an
interval an inequality follows.  

\chapter{Integrals}
\begin{align*}
\int_0^\infty e^{-x^2} dx &= \frac{\sqrt{\pi}}{2} \\
\int_0^\infty x^{2n} e^{-x^2} dx &= \frac{\sqrt{\pi} \left(2n-1\right)!!}{2^{n+1}} \\
\int_0^\infty x^{2n+1} e^{-x^2} dx &= \frac{n!}{2}\\
\end{align*}
\begin{align*}
\Gamma(z) &= \int_0^\infty x^{z-1} e^{-x} dx
\end{align*}
\chapter{Inequalities}
From time to time in these notes we'll have a need for some simple
inequalities for elementary functions.  The following Lemma collects
them in one place since they are all proven by use of basic calculus.
\begin{lem}\label{BasicExponentialInequalities}The following
  inequalities hold:
\begin{itemize}
\item[(i)] $1+x \leq e^x$ for all $x \in \reals$.
\item[(ii)] $e^x \leq 1 + 2x$ for all $x \in [0,1]$.
\item[(iii)] $e^x \leq 1 + x + x^2$ for all $x \leq 1$.
\item[(iv)] $\frac{1}{2}(e^x + e^{-x}) \leq e^{x^2/2}$ for all $x \in \reals$.
\item[(v)] $\abs{\sin(x)} < \abs{x}$ for all $x  \neq 0$.
\item[(vi)] $1 - \frac{x^2}{2} \leq \cos(x)$ for all $x \in \reals$.
\item[(vii)] $x + \log(1-x) \leq 0$ for all $x \in [0,1)$.
\item[(viii)] $e^{-x} \leq 1 - (1 - e^{-1}) x$ for all $x \in [0,1]$.
\end{itemize}
\end{lem}
\begin{proof}
Note that for ${x\geq0}$ we can consider $f(x) = e^x - x -1$ and note
that $f(0)=0$ and moreover we can see that $f(x)$ has a global minimum
at $x=0$ since $f'(x) = e^x - 1$ vanishes precisely at $x=0$ and
$f^{\prime \prime}(x)=e^x$ is strictly positive.  Alternative this can
be seen by Taylor's Theorem.  One writes using the Lagrange form of
the remainder $e^x = 1 + x + \frac{x^2}{2} e^c$ for some $c$.
Since the remainder is positive the result follows.

In a similar vein to show (ii), define $f(x) = 1+2x-e^x$ and notice that $f(x)$ has
a global maximum at $x=\ln(2)$ and no other local maximum.  Thus, it
suffices to validate the inequality at the endpoints $x=0$ and $x=1$
which is obvious.

To show (iv) we just manipulate series expansions.
\begin{align*}
\frac{1}{2}\left(e^x + e^{-x}\right) & = \frac{1}{2}\left(\sum_{n=0}^\infty
\frac{x^n}{n!} + \sum_{n=0}^\infty \frac{(-x)^n}{n!}\right) \\
& = \sum_{n=0}^\infty \frac{x^{2n}}{(2n)!} \\
& \leq \sum_{n=0}^\infty \frac{x^{2n}}{2^n n!} = e^{\frac{x^2}{2}}\\
\end{align*}

To see (v),  because the function $x - \sin(x)$ is odd, it suffices to
show that it is strictly positive for $x > 0$.
Clearly $x - \sin(x) > 0$ for $x > 1$.   For $0 < x < 1$ we just use
Taylor's Theorem with Lagrange remainder to see that $\sin(x) = x -
\frac{x^2}{2} \cos(c)$ for some $0 < c < x < 1$.  The remainder is
negative so the result follows.

To show (vi), define $f(x) = \frac{x^2}{2} -1 +  \cos(x)$.  Calculate
the first derivative $f^\prime(x) = x - \sin(x)$.  The function
$f^\prime(x) = 0$ if and only if $x=0$ by (v) and moreover $f^\prime(x)$ changes sign at $x=0$
which shows that $f(0) = 0$ is a strict global minimum.

To show (vii), define $f(x) = x + \log(1 -x)$ and differentiate to see
that $f^\prime(x) = 1 - \frac{1}{1-x} = \frac{-x}{1-x} < 0$ for $x \in
(0,1)$.  Therefore $f(x) \leq f(0)=0$ for $x \in [0,1)$.

To show (viii), let $a = 1 - e^{-1}$ and $f(x) = 1 - ax - e^{-x}$.
Take first derivative $f^\prime(x) = -a + e^{-x}$ which has a zero at
$x = -\ln a \approx 0.5$.  Furthermore $f^{\prime \prime}(x) = -e^{-x}
< 0$ so we have a global maximum at $x = -\ln a$, therefore to show
$f(x) \geq 0$  for $x \in [0,1]$ it suffices to show it at the
endpoints: $f(0) = f(1) = 0$.
\end{proof}

When dealing with characteristic functions, it is useful to have
estimates for the function $e^{ix}$.  We collect a few useful ones
here.
\begin{thm}\label{BasicComplexExponentialInequalities}The following
  inequalities hold:
\begin{itemize}
\item[(i)] $\abs{e^{ix} -1 - ix} \leq \frac{x^2}{2}$ for all $x \in \reals$.
\item[(ii)] $\abs{e^{ix} -1 - ix + \frac{x^2}{2}} \leq x^2 R(x)$ for
  all $x \in \reals$ where $\abs{R(x)} \leq 1$ and $\lim_{x \to 0}
  R(x) = 0$.
\end{itemize}
\end{thm}
\begin{proof}
To see (i) we use Taylor's Theorem with the Lagrange form of the
remainder to write $e^{ix} - 1 - ix = -\frac{x^2}{2} e^{ic}$ for some
$c \in \reals$.  Now take absolute values and use the fact that
$\abs{e^{ic}}=1$.

To see (i) we use Taylor's Theorem with the integral form of the
remainder to write $e^{ix} - 1 - ix = - \int_0^x (x - t) e^{it} \,
dt$.  Now we write 
\begin{align*}
 \int_0^x (x - t) e^{it} \, dt &=  \int_0^x (x - t) (e^{it} - 1) \, dt
+ \int_0^x (x-t) \, dt 
= \int_0^x (x - t) (e^{it} - 1) \, dt + \frac{x^2}{2}
\end{align*}
so that $x^2 R(x) = -\int_0^x (x - t) (e^{it} - 1) \, dt$.  Observe that on the one hand
\begin{align*}
\abs{R(x)}
&=\frac{1}{x^2} \abs{\int_0^x (x - t) (e^{it} - 1) \, dt} 
\leq \frac{\sup_{0 \leq t \leq x} \abs{e^{it} - 1}}{x^2} \int_0^x (x - t) \, dt \\
&=\sup_{0 \leq t \leq x} \abs{e^{it} - 1}
\end{align*}
and thus continuity of $e^{ix}$ implies $\lim_{x \to 0} R(x) = 0$.
On the other hand
\begin{align*}
\abs{\int_0^x (x - t) (e^{it} - 1) \, dt} 
&\leq \int_0^x (x - t) \abs{e^{it} - 1} \, dt
&\leq 2 \int_0^x (x - t) \, dt = x^2
\end{align*}
which shows $\abs{R(x)} \leq 1$.  
\end{proof}

\begin{thm}[Arithmetic Mean Geometric Mean Inequality]\label{AMGM}Let $x_1, \dotsc, x_n$ be non-negative real
  numbers and let $p_1, \dotsc, p_n$ be non-negative real numbers such
  that $\sum_{j=1}^n p_j = 1$ then
\begin{align*}
x_1^{p_1} \dotsm x_n^{p_n} &\leq p_1 x_1 + \dotsm + p_n x_n
\end{align*}
\end{thm}
\begin{proof}
TODO:
\end{proof}

\begin{prop}\label{SimplePowerMean}Let $x_1, \dotsc, x_n$ be non-negative real numbers and $m
  \in \naturals$ then 
\begin{align*}
\left( \frac{x_1 + \dotsm + x_n}{n} \right)^m &\leq \frac{x^m_1 + \dotsm + x^m_n}{n}
\end{align*}
\end{prop}
\begin{proof}
We first validate the result for $m=2$.  To see this case observe that
by Theorem \ref{AMGM} we have for arbitrary non-negative integers
$\alpha$ and $\beta$ and non-negative reals $x,y$
\begin{align*}
x^\alpha y^\beta &= 
(x^{\alpha+\beta})^{\frac{\alpha}{\alpha + \beta}}
+ (y^{\alpha+\beta})^{\frac{\beta}{\alpha + \beta}}
\leq \frac{\alpha}{\alpha + \beta} x^{\alpha+\beta} + 
\frac{\beta}{\alpha + \beta} y^{\alpha+\beta}
\end{align*}
Therefore by the Binomial Theorem and the fact that $\binom{m}{k} = \binom{m}{m-k}$
\begin{align*}
(x_1 + x_2)^m &= \sum_{k=0}^m \binom{m}{k} x_1^k x_2^{m-k}
\leq \sum_{k=0}^m \binom{m}{k} \left(\frac{k}{m} x_1^m + \frac{m-k}{m} x_2^m \right) \\
&= \begin{cases}
\left(x_1^m + x_2^m \right) \sum_{k=0}^{\floor{\frac{m}{2}}} \binom{m}{k} \text{ if $m$ is odd}\\
\left(x_1^m + x_2^m \right) \lbrace\sum_{k=0}^{\frac{m}{2} -1} \binom{m}{k} + \frac{1}{2}\binom{m}{m/2} \rbrace \text{ if $m$ is even}\\
\end{cases} \\
&= 2^{m-1} \left(x_1^m + x_2^m \right)
\end{align*}

Now an easy induction shows that the result holds for any $n=2^k$:
\begin{align*}
\left( \frac{x_1 + \dotsm + x_{2^k}}{2^k} \right)
&\leq
\frac{1}{2} \left \lbrace \left(\frac{x_1 + \dotsm + x_{2^{k-1}}}{2^{k-1}} \right) +
\left(\frac{x_{2^{k-1} + 1} + \dotsm + x_{2^{k}}}{2^{k-1}} \right) \right \rbrace \\
&\leq \frac{x_1^m + \dotsm + x_{2^k}^m}{2^k}
\end{align*}

It remains to extend the result to arbitrary $n$.  Suppose that $2^{k-1} \leq n < 2^k$ and 
define 
\begin{align*}
A &= \left ( \frac{x_1^m + \dotsm + x_n^m}{n} \right)^{1/m}
\end{align*}
they by the result for $2^k$ we get
\begin{align*}
\left( \frac{x_1^m + \dotsm + x_n^m + (2^k - n)A}{2^k} \right)^m 
&\leq
\frac{x_1^m + \dotsm + x_n^m +  (2^k - n)A^m}{2^k} = A^m
\end{align*}
If we take $m^{th}$ roots and collect terms involving $A$ we get $\frac{x_1 + \dotsm + x_k}{k} \leq A$.
Now take $m^{th}$ power and use the definition of $A$ to get the
result.

TODO: I actually think we need this result for non-integral $m$.  Get
the full blown power mean inequality from Steele and then fix up the
BDG vector inequality.
\end{proof}

\begin{thm}\label{StirlingsFormula}
\end{thm}
\end{document}