\documentclass{amsart}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\card}{card}
\DeclareMathOperator*{\RePart}{Re}
\DeclareMathOperator*{\ImPart}{Im}
\DeclareMathOperator*{\median}{Med}
\DeclareMathOperator*{\diag}{Diag}
\DeclareMathOperator*{\interior}{int}
\DeclareMathOperator*{\diam}{diam}

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}

\newtheorem{ex}[thm]{Exercise}
\newtheorem{examp}[thm]{Example}

\begin{document}


\section{\LaTeX Examples}
\newcommand{\Independent}{\perp \! \! \! \perp}
\newcommand{\cindependent}[3]{#1 \Independent_{#3} #2}
\newcommand{\mymacro}{This text was produced by a macro}
\newcommand{\withparams}[2]{This text was produced by a macro with two
  parameters #1, #2}
\newcommand{\expectation}[1]{\textbf{E}\left[#1\right]} 
\newcommand{\sexpectation}[2]{\textbf{E}_{#2}[#1]} 
\newcommand{\cexpectationop}[1]{\textbf{E}^#1} 
\newcommand{\cexpectation}[2]{\textbf{E}^{#1} #2} 
\newcommand{\cexpectationlong}[2]{\textbf{E}\left[ #2 \mid #1 \right]} 
\newcommand{\variance}[1]{\textbf{Var} \left (#1 \right )} 
\newcommand{\covariance}[1]{\textbf{Cov} \left (#1 \right )} 
\newcommand{\scovariance}[2]{\textbf{Cov} \left (#1, #2 \right )} 
\newcommand{\probabilityop}{\textbf{P}} 
\newcommand{\probability}[1]{\textbf{P}\{#1\}} 
\newcommand{\sprobability}[2]{\textbf{P}_{#2}\{#1\}} 
\newcommand{\sprobabilityop}[1]{\textbf{P}_{#1}} 
\newcommand{\cprobability}[2]{\textbf{P}\{#2 \mid #1\}} 
\newcommand{\characteristic}[1]{\textbf{1}_{#1}} 
\newcommand{\pushforward}[2]{#2 \circ #1^{-1}} 
\newcommand{\complexes}{\mathbb{C}} 
\newcommand{\reals}{\mathbb{R}} 
\newcommand{\rationals}{\mathbb{Q}} 
\newcommand{\naturals}{\mathbb{N}} 
\newcommand{\integers}{\mathbb{Z}} 
\newcommand{\distribution}[1]{\textbf{P}^{#1}}
\newcommand{\borel}[1]{\mathcal{B}(#1)} 
\newcommand{\abs}[1]{\left \vert #1 \right \vert}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\toprob}{\overset{P}\to}
\newcommand{\toas}{\overset{a.s.}\to}
\newcommand{\todist}{\overset{d}\to}
\newcommand{\tolp}[1]{\overset{L^#1}\to}
\newcommand{\eqdist}{\overset{d}=}
\newcommand{\kldiv}[2]{D\left( #1 \mid\mid #2 \right)}
\newcommand{\sample}[1]{\boldsymbol{#1}}

\newcommand{\andop}{\wedge}
\newcommand{\orop}{\vee}

\section{Real Analysis}
For purposes of our discussion of measure theory, we often make little
use of the structure of the reals.  In many cases it is with little
effort that we can state results much more generally.  Sometimes the
results will be true of arbitrary sets but in other cases we need the
most basic notions of metric spaces.
\begin{defn}A metric space is a set $S$ together with a function
  $d:SxS \to \reals$ satisfying
\begin{itemize}
\item[(i)]$d(x,y) = 0$ if and only if $x=y$.
\item[(ii)]For all $x,y \in S$, $d(x,y) = d(y,x)$.
\item[(iii)]For all $x,y,z \in S$, $d(x,z) \leq d(x,y) + d(y,z)$.
\end{itemize}
\end{defn}
\begin{lem} Given a metric space $(S,d)$, we have $d(x,y) \geq 0$ for all
  $x,y \in S$.
\end{lem}
\begin{proof}
Let $x,y \in S$ and observe 
\begin{align*}
d(x,y) &= \frac{1}{2} (d(x,y) + d(y,x)) \textrm { by symmetry} \\
&\geq \frac{1}{2} d(x,x) \textrm{ by triangle inequality} \\
&= 0
\end{align*}
\end{proof}
 It's pretty easy to see that standard notions of limits and continuity
extend to the case of metric spaces.
\begin{defn}A sequence of elements $x_n \in S$ converges to $x \in S$
  if for every $\epsilon > 0$, there exists $N > 0$ such that
  $d(x_n,x) < \epsilon$ for all $n > N$.
\end{defn}
\begin{defn}A function between metric spaces $f : (S,d) \to (S', d')$
  is continuous at $x \in S$ if for every $\epsilon>0$, there exists
  $\delta > 0$ such that for $y\in S$ such that $d(x,y)<\delta$ we
  have $d'(f(x),f(y)) < \epsilon$.  A function $f$ that is continuous
  at all points $x \in S$ is said to be continuous.
\end{defn}
\begin{lem}$f : (S,d) \to (S', d')$ is continuous at $x \in S$ if and only if for
  every $x_n \to x$ we have $f(x_n) \to f(x)$.
\end{lem}
\begin{proof}
Suppose $f$ is continuous and let $\epsilon > 0$ be given.  By
continuity, we can pick $\delta > 0$ such that for all $y \in S$
with $d(x,y) < \delta$ we have $d'(f(x), f(y)) < \epsilon$.  Now by
convergence of the sequence $x_n$, we can find $N$ such that for all
$n>N$, we have $d(x_n,x) < \delta$.  Hence for all $n > N$, we have
$d'(f(x), f(x_n)) < \epsilon$.

Now suppose that for every $x_n \to x$ we have $f(x_n) \to f(x)$.  We
argue by contradiction.  Suppose $f$ is not continuous at $x$.  There
exists $\epsilon > 0$ such that we can find $x_n \in S$ such that
$d(x,x_n) < 2 ^ {-n}$ and $d'(f(x_n), f(x)) \geq \epsilon$.  Note that
the sequence $x_n \to x$ but $f(x_n)$ doesn't converge to $f(x)$.
\end{proof}
\begin{defn}For $x\in S$ and $r \geq 0$, the open ball at $x$ or
  radius $r$ is the set
\begin{align*}
B(x;r) = \{y\in S | d(x,y) < r \}
\end{align*}
\end{defn}
\begin{defn}A set $U \subset S$ is open if for every $x \in U$ there
  exists $r>0$ such that $B(x;r) \subset U$.  The complement of an
  open set is called a closed set.
\end{defn}
\begin{lem}A set $A \subset S$ is closed if and only if for every $x_n
  \to x$ with $x_n \in A$, we have $x \in A$.
\end{lem}
\begin{proof}
Suppose $A$ is closed.  Then $A^c$ is open.  Let $x_n \in A$ converge
to $x$.  If $x \notin A$, then $x \in A^c$ and we can find an open
ball $B(x;\epsilon) \subset A^c$.  Pick $N>0$ such that $d(x_n, x) <
\epsilon$ for all $n > N$.  Then $x_n \notin A$ for all $n>N$ which is
a contradiction.

Now suppose $A$ contains all of its limit points.  We show that $A^c$
is open.  Let $x\in A^c$ and suppose the balls $B(x;2^{-n}) \bigcap A
\ne \emptyset$.  Then we can construct a sequence $x_n \in A$ such
that $x_n ->x$.  This is a contradiction, hence for some $n$, we have 
$B(x;2^{-n}) \bigcap A = \emptyset$ and therefore $A^c$ is open.
\end{proof}

As it turns out continuity of a function can be expressed entirely in
terms of open sets.
\begin{lem}A function between metric spaces $f : (S,d) -> (T,d^\prime)$ is continuous
  if and only if for every open subset $U \subset T$, we have $f^{-1}(U)$ is an
  open subset of $S$.
\end{lem}
\begin{proof}For the only if direction, let $U \subset T$ be an open set and pick $x \in
  f^{-1}(U)$.  Now, $f(x) \in U$ and by openness of $U$ we can find $\epsilon
  > 0$ such that $B(f(x); \epsilon) \subset U$.  By continutity of $f$
  we can find a $\delta > 0$ such that for all $y \in S$ with $d(x,y) <
  \delta$ we  have $d^\prime(f(x),f(y)) < \epsilon$.  This is just
  another way of saying $B(x; \delta) \subset f^{-1}(U)$ which shows
  that $f^{-1}(U)$ is open.

For the if direction, pick $x \in S$ and suppose we are given
$\epsilon > 0$.  The ball $B(f(x); \epsilon)$ is an open set in $T$.  
By assumption we know that $f^{-1}(B(f(x); \epsilon))$ is
an open set in $S$ containing $x$.   By definition of openness, we can pick a $\delta > 0$, such that 
$B(x;\delta) \subset f^{-1}(B(f(x); \epsilon))$.  Unwinding this
statement shows that for all $y \in S$ with $d(x,y) < \delta$, we have
$d^\prime(f(x),f(y)) < \epsilon$ and we have show that $f$ is
continuous at $x$.  Since $x \in S$ was arbitrary we have shown $f$ is
continuous on all of $S$.
\end{proof}

\begin{defn}A sequence of elements $x_n \in S$ is said to be a
  \emph{Cauchy sequence}
  if for every $\epsilon > 0$, there exists $N > 0$ such that
  $d(x_n,x_m) < \epsilon$ for all $n,m > N$.
\end{defn}
Note that any convergent sequence is Cauchy.
\begin{lem}If a sequence of elements $x_n \in S$ converges to $x \in
  S$ then it is a Cauchy sequence.
\end{lem}
\begin{proof}Pick $\epsilon > 0$ and then pick $N>0$ so that $d(x_n,x)
  < \frac{\epsilon}{2}$ for all $n > N$.  Then by the triangle
  inequality, $d(x_n, x_m) \leq d(x_n, x) + d(x, x_m) < \epsilon$ for
  $n,m > N$.
\end{proof}
It is also easy to construct examples of Cauchy sequences that do not
converge by looking at spaces with \emph{holes}.
\begin{examp}Consider the sequence $\frac{1}{n}$ on $\reals \setminus
  \lbrace 0 \rbrace$.  It is Cauchy but does not converge.
\end{examp}
The existence of non-convergent Cauchy sequences is in some sense the
definition of what it means for a general metric space to have holes.
This motivates the following definition.
\begin{defn}A metric space $(S,d)$ is said to be \emph{complete} if
  every Cauchy sequence is convergent.
\end{defn}
\begin{defn}The real line $\reals$ is complete.
\end{defn}
\begin{proof}Suppose we are given a Cauchy sequence $x_n$.  Let $a =
  \liminf_{n \to \infty} x_n$ and $b = \limsup_{n \to \infty} x_n $.
  We proceed by contradiction and suppose
  that $a < b$ (note
  that the \emph{completeness axiom} of the reals is used in the
  definition of $\liminf$ and $\limsup$).  Let $M =b-a$ then for any
  $0 < \epsilon < M$, $N>0$
  we can find $k,m > N$ such that $\abs{a - x_k} < \frac{M -
    \epsilon}{2}$ and $\abs{b - x_m} < \frac{M -
    \epsilon}{2}$ thus showing $\abs{x_k - x_m} \geq \epsilon$ and
  contradicting the assumption that $x_n$ was a Cauchy sequence.
\end{proof}

The following is a simple fact about $\reals$.
\begin{lem}\label{IncreasingSequenceWithConvergentSubsequence}Let $x_n$ be a nondecreasing sequence in $\reals$.  Suppose
  there is an infinite subsequence $x_{n_k}$ such that $\lim_{k \to
    \infty} x_{n_k} = x$ , then $\lim_{n \to \infty} x_n = x$.
\end{lem}
\begin{proof}
TODO:  This is actually pretty much obvious.
\end{proof}
In our treatment of measure theory we'll want to have a detailed
understanding of the structure of the topology of the real line.  It
can be described quite simply.
\begin{lem}\label{OpenSetsOfReals}The open sets in $\reals$ are precisely the countable
  unions of disjoint open intervals.
\end{lem}
\begin{proof}Pick an open set $U \subset \reals$.  Define an
  equivalence relation on $U$ such that $a \equiv b$ if and only if
  $[a,b] \subset U$ or $[b,a] \subset U$.  It is easy to see this is an equivalence
  relation.  Reflexivity and symmetry are entirely obvious.
  Transitivity follows from taking a union of intervals (carefully
  taking order into consideration).  

Now, consider the equivalence classes of the relation. As equivalence
classes these sets are disjoint and their union is $U$.  Call the
family of equivalence classes $U_\alpha$.

We have to show that
the equivalence classes are open intervals.  Consider $x \in U_\alpha
\subset U$.  Openness of $U_\alpha$ follows from using openness of $U$ to find a small ball (open interval) around $x \in U$ and noting that every
point of the ball is $\equiv$-related to $x$.  Therefore the same open
ball demonstrates the openness of $U_\alpha$.

To see that equivalence classes are intervals, pick an equivalence
class $U_\alpha$ and consider the open interval $(\inf U_\alpha, \sup
U_\alpha)$.  Since $U_\alpha$ is nonempty and open, $\inf U_\alpha \neq \sup
U_\alpha $ and this interval is non-empty.  By definition of $\inf$ and $\sup$ and the openness of
$U_\alpha$ we can see that $U_\alpha \subset (\inf U_\alpha, \sup
U_\alpha)$ (otherwise we could find an element of $U_\alpha$ bigger
than $\sup$ or less than $\inf$).  On the other hand, suppose we are
given $x \in (\inf U_\alpha, \sup U_\alpha)$.  We can find elements
$y,z \in U_\alpha$ such that $\inf U_\alpha  < y < x < z <\sup  U_\alpha$.
By definition of the equivalence relation, this shows $[y,z] \subset
U_\alpha$ and therefore $x \in U_\alpha$.  Therefore we have shown
that $U_\alpha =  (\inf U_\alpha, \sup U_\alpha)$ is an open interval.

The fact that there are
at most countably many equivalence classes follows from the density
and countability of $\rationals$.
\end{proof} 

\begin{lem}\label{ComplementOfCountableSetDense}Let $A \subset \reals$ be a countable set.  Then $A^c$ is
  dense in $\reals$.
\end{lem}
\begin{proof}
Pick an $x \in \reals$ and consider an interval $I_n = (x - \frac{1}{n}, x +
\frac{1}{n})$ for $n > 0$.  Then if $A^c \cap I_n = \emptyset$ we have
$I_n \subset A$ which implies that $I_n$ is countable.  This is
clearly false (since otherwise we could write the reals as a countable
union of countable sets which would imply the reals themselves are countable).
\end{proof}

Just as an aside at this point, we note that notions of open and
closed set are really all that is needed to make sense of the notions
of convergence and continuity.
\begin{defn}A topological space is a set $S$ together with a
  collection of subsets $\tau$ satisfying
\begin{itemize}
\item[(i)]$\tau$ contains $\emptyset$ and $S$.
\item[(ii)]$\tau$ is closed under arbitrary union.
\item[(iii)]$\tau$ is closed under finite intersection.
\end{itemize}
The collection $\tau$ is called a topology on $S$.  The elements of
$\tau$ are called the open sets of $S$ and the complement of the open
sets are called closed sets.  As we have shown above, if one defines
continuity of a function between topological spaces as inverse images
of open sets being open we have a definition that is a compatible
generalization of the $\epsilon/\delta$ definition of calculus.
\end{defn}

\begin{thm}[Taylor's Formula]\label{TaylorsTheorem}Let $f: \reals\to
  \reals$ be a function which is $m$-times continuously differentiable.  Then for all
  $0 \leq n < m$,
\begin{align*}
f(b) &= \sum_{k=0}^n \frac{(b-a)^k}{k!} f^{(k)}(a) + R_n(b)
\end{align*}
where the \emph{remainder term} is of the form
\begin{align*}
R_n(b) &= \int_a^b
\frac{(b-x)^n}{n!} f^{(n+1)}(x) \, dx
\end{align*}
\end{thm}
\begin{proof}
We proceed by induction.  Note that for $n=1$, then Taylor's Formula simply says $f(b) = f(a) +
\int_a^b f^\prime(x) \, dx$ which is just the Fundamental Theorem of
Calculus.  For the induction step, we integrate the remainder term by parts.  Consider
the integral $\int_a^b \frac{(b-x)^{(n-1)}}{(n-1)!} f^{(n)}(x) \, dx$ and let
$u = f^{(n)}(x)$ and $dv = \frac{(b-x)^{(n-1)}}{(n-1)!} dx$.  Then $du =
f^{(n+1)}(x) dx$ and $v = -\frac{(b-x)^n}{n!}$, so 
\begin{align*}
\int_a^b \frac{(b-x)^{(n-1)}}{(n-1)!} f^{(n)}(x) \, dx &=
-\frac{(b-x)^n}{n!} f^{(n)}(x) \mid_a^b + \int_a^b \frac{(b-x)^n}{n!}
f^{(n+1)}(x) \, dx \\
&= \frac{(b-a)^n}{n!} f^{(n)}(a) + \int_a^b \frac{(b-x)^n}{n!}
f^{(n+1)}(x) \, dx
\end{align*}
which proves the result.
\end{proof}
The version of Taylor's Formula above expresses the ``integral form''
of the remainder term.  It is often useful to transform the remainder term in Taylor's Formula
into the \emph{Lagrange form}.
\begin{lem}\label{LagrangeFormRemainder}There is a number $c \in
  [a,b]$ such that $\int_a^b
R_n(b)= f^{(n+1)}(c) \frac{(b-a)^{n+1}}{(n+1)!}$
\end{lem}
\begin{proof}
By continuity of $f^{(n+1)}(x)$ and compactness of $[a,b]$ we know
that there exist $m, M \in \reals$ such that $m = \min_{x \in [a,b]}
f^{(n+1)}(x)$ and $M = \max_{x \in [a,b]}
f^{(n+1)}(x)$.
Therefore we have the bounds
\begin{align*}
m\frac{(b-a)^{(n+1)}}{(n+1)!} &= m \int_a^b
\frac{(b-x)^n}{n!} \, dx \\
&\leq \int_a^b
\frac{(b-x)^n}{n!} f^{(n+1)}(x) \, dx \\
&\leq M \int_a^b
\frac{(b-x)^n}{n!} \, dx = M\frac{(b-a)^{(n+1)}}{(n+1)!} 
\end{align*}
hence 
\begin{align*}
m \leq \frac {(n+1)!} {(b-a)^{(n+1)}}\int_a^b
\frac{(b-x)^n}{n!} f^{(n+1)}(x) \, dx \leq M
\end{align*}
By continuity of $f^{(n+1)}(x)$ and the Intermediate Value Theorem, we know that $f^{(n+1)}(x)$
takes every value in $[m,M]$ and therefore there exists $c \in [a,b]$
such that $f^{(n+1)}(c) = \frac {(n+1)!} {(b-a)^{(n+1)}}\int_a^b
\frac{(b-x)^n}{n!} f^{(n+1)}(x) \, dx$.
\end{proof}
\begin{lem} Let $X$ be a real normed vector space with a subspace $Y$
  of codimension 1.  Then any bounded linear functional $\lambda$ on
  $Y$ extends to a bounded linear functional on $X$ with the same
  operator norm.
\end{lem}
\begin {proof}
We first assume that $\lambda$ has operator norm $1$.
Let $v$ be any vector that is not in $Y$.  Then every element of $X$
is of the form $y + tv$, hence by linearity all we really have to
choose is the value of $\lambda(v)$ so that the operator norm doesn't
increase.  
First, note that it suffices to show $|\lambda(y+v)| \leq
\|y+v\|$ for all $y$.  For it that if that is true then
\begin{align*}
|\lambda(y + tv)| &= |t\lambda(y/t + v)| \\
&\leq |t| \|y/t + v\| \\
&= \|y + tv\| 
\end{align*}
We rewrite the constraint $|\lambda(y+v)| \leq
\|y+v\|$ for all $y$ as
\begin{align*}
-\lambda(y) - \|y+v\| \leq \lambda(v) \leq  \|y+v\| - \lambda(y)
\end{align*}
To see that it is possible to satisfy the constraint derived above, we
use the triangle inequality (subadditivity) of the operator norm.  For
all $y_1,y_2 \in Y$,
\begin{align*}
\lambda(y_1) - \lambda(y_2) &\leq|\lambda(y_1 - y_2)| \\
&\leq \|y_1 - y_2\| \\
&= \|y_1 + v - v - y_2 \| \\
&\leq \|y_1 + v\| + \|y_2 + v\| 
\end{align*}
From which we conclude by rearranging terms 
\begin{align*}
\sup_{y_2 \in Y} -\lambda(y_2) - \|y_2 + v\| \leq \inf_{y_1 \in Y} \|y_1 + v\| - \lambda(y_1)
\end{align*} 
Picking any value between the two terms of the above inequality
results in a valid extension.
To handle the case of operator norm not equal to $1$, notice that the
extension is trivial if the operator norm is $0$ (i.e. $\lambda=0$), otherwise define the
extension by $\|\lambda\|$ times the extension of $\lambda/\|\lambda\|$.
\end {proof}
\begin{thm}[Hahn-Banach Theorem (Real case)]
Let $X$ be a real normed vector space with a subspace $Y$.  Then any bounded linear functional $\lambda$ on  $Y$ extends to a bounded linear functional on $X$ with the same operator norm.
\end{thm}
\begin{proof}We proceed by using the codimension 1 case proved above
  and then applying Zorn's Lemma.  We define a partial extension of
  $\lambda$ to be a pair $(Y^\prime, \lambda^\prime)$ such that $Y
  \subset Y^\prime \subset X$ and $\lambda^\prime$ is an extension of
  $\lambda$ with the same operator norm.  Put a partial order on the
  set of extensions by declaring $(Y^\prime, \lambda^\prime) \leq
  (Y^{\prime\prime}, \lambda^{\prime\prime})$ if and only if $Y^\prime
  \subset Y^{\prime\prime}$ and $\lambda^{\prime\prime}\mid_{Y^\prime}
  = \lambda^\prime$.

To apply Zorn's Lemma, we need to show that every chain has an upper
bound.  If we are given a chain $(Y_\alpha, \lambda_\alpha)$ then we
define $Z = \cup_\alpha Y_\alpha$ and for any $z \in Z$ we define
$\tilde{\lambda}(z) = \lambda_\alpha(z)$ for any $\alpha$ such that $z \in
Y_\alpha$.  It is immediate that this well defined.  It is easy to
show linearity and to show that $\norm{\tilde{\lambda}} =
\norm{\lambda}$ (TODO: do this).

Now we can apply Zorn's Lemma to conclude that there is a maximal
element $(Y^\prime, \lambda^\prime)$.  The codimension one case show
us that $Y^\prime = X$ for otherwise we can construct an extension
that shows $(Y^\prime, \lambda^\prime)$ is not maximal.
\end{proof}
Note that the use of Zorn's Lemma here is not accidental; the Hahn
Banach Theorem cannot be proven in set theory without the Axiom of
Choice (though according to Tao it can be proven without the full
power of the Axiom of Choice using what is know as the Ultrafilter Lemma).
\subsection{Compactness}

\begin{defn}Let $(S,d)$ be a metric space, then we say $K \subset S$
  is \emph{sequentially compact} if and only if for every sequence $x_1, x_2,
  \dots \in K$ there exists a convergent subsequence $x_{n_j}$ such
  that $\lim_{j \to \infty} x_{n_j} \in K$.
\end{defn}

\begin{defn}Let $(S,d)$ be a metric space, then we say $S$
  is \emph{compact} if and only if for every collection $U_\alpha$ of
  open sets such that $\bigcup_\alpha U_\alpha \supset S$ there
  exists a finite subcollection $U_1, \dots, U_n$ such that
  $\bigcup_{j=1}^n U_j \supset S$.
\end{defn}

\begin{defn}Let $(S,d)$ be a metric space, then we say $S$
  is \emph{totally bounded} if and only if for every $\epsilon >0$
  there exists a finite set of points $F \subset S$ such that for
  every $x\in S$ there is a $y \in F$ such that $d(x,y) < \epsilon$.
\end{defn}

\begin{defn}Let $(S,d)$ be a metric space, then we say $x \in S$
  is \emph{limit point} of a set $A \subset S$ if and only if for
  every open set $U$ containing $x$, $A \cap (U \setminus \lbrace x
  \rbrace) \neq \emptyset$.
\end{defn}

\begin{thm}\label{CompactnessInMetricSpaces}In a metric space $(S,d)$ the following are equivalent
\begin{itemize}
\item[(i)]$S$ is compact
\item[(ii)]$S$ is complete and totally bounded
\item[(iii)]Every infinite subset of $S$ has a limit point
\item[(iv)]$S$ is sequentially compact
\end{itemize}
\end{thm}
\begin{proof}
First we show that (i) implies (ii).  Given $\epsilon > 0$ note that
we have a covering by open balls $\cup_{x \in S} B(x, \epsilon)$.  By
compactness we have a finite set $x_1, \dots, x_m$ such that
$\cup_{i=1}^m B(x_i, \epsilon) = S$.  Thus given $y \in S$, we know
there is an $x_j$ such that $y \in B(x_j, \epsilon)$ and we have shown
total boundedness.  To show completeness, let $x_1, x_2, \dots$ be a
Cauchy sequence in $S$.  For every $m > 0$ we know there exists $N_m$
such that $d(x_{N_m}, x_n) < \frac{1}{m}$ for every $n > N_m$.  Now
define $U_m = \lbrace x \in S \mid d(x_{N_m}, x) > \frac{1}{m}\rbrace$
and note that $U_m$ is open. Furthermore we know that $x_n \notin U_m$
for all $n > N_m$.  By virtue of this latter fact we can see that
there is no finite subset of $U_m$ that covers $S$; for given $U_1,
\dots, U_m$ then $x_n \notin \cup_{k=1}^m U_k$ for any $n > \max(N_1,
\dots, N_m)$.  By compactness of $S$ we know that the $U_m$ do not
cover $S$ and therefore there is an $x \in S \setminus
\cup_{m=1}^\infty U_m$.  For such an $x$, by definition of $U_m$ we
know that $d(x_{N_m}, x) \leq \frac{1}{m}$ for all $m > 0$.  By the
triangle inequality we then get that $d(x_n, x) \leq \frac{2}{m}$ for
all $n > N_m$ and $m > 0$ which shows that $x_n$ converges to $x$.
Thus $S$ is complete.

Next we show that (ii) implies (iii).  Suppose $A \subset S$ is an
infinite set.  By the assumption of total boundedness, for each $n >
0$, we can find a finite set $F_n$ such that for every $y \in S$ there
exists $x \in F_n$ such that $d(x,y) < \frac{1}{n}$.  Since the finite
sets $B(y, 1)$ for $y \in F_1$ cover $S$ there is an $y_1 \in F_1$
such that $A \cap B(y_1, 1)$ is infinite.  Then arguing inductively we
construct for every $n>0$ a $y_n \in F_n$ such that $A \cap B(y_1,1)
\cap \cdots \cap B(y_n, \frac{1}{n})$ is infinite.  Note that for $n > m
>0$, by the triangle inequality using any of the infinite number of
elements in $B(y_n, \frac{1}{n}) \cap B(y_m, \frac{1}{m})$, we have $d(y_n, y_m) < \frac{1}{m} +
\frac{1}{n} < \frac{2}{m}$.  This shows that $y_n$ is a Cauchy
sequence and by assumption we know that this converges to some $y \in
S$ and by the above estimate on $d(y_n, y_m)$, we know that for every
$m > 0$, $d(y, y_m) < \frac{2}{m}$.  Therefore we have the inclusion
$B(y_m, \frac{1}{m}) \subset B(y, \frac{3}{m})$ and therefore $A \cap
B(y, \frac{3}{m})$ is also infinite which shows $y$ is a limit point
of $A$.

Next we show that (iii) implies (iv).  Let $x_1, x_2, \dots$ be an
infinite sequence with an infinite range and by (iii) we can get a limit point $x \in S$.
Thus we can find a subsequence $x_{n_1}, x_{n_2}, \dots$ such that
$x_{n_k} \in B(x, \frac{1}{k})$ which shows that the subsequence
converges.  If the sequence has a finite range then it is eventually
constant and converges.

Lastly let's show that (iv) implies (i).  Pick an open cover
$\mathcal{U}_\alpha$ of $S$.  Our first subtask is to show that there exists a
radius $r > 0$ such that for every $x \in S$, the ball $B(x,r)$ is contained in some
element of $\mathcal{U}_\alpha$.   To that end, for every $x \in S$ let 
\begin{align*}
f(x) &= \sup \lbrace r \mid B(x,r) \subset U_\alpha \text{ for some }
\alpha \rbrace
\end{align*}
We claim that $\inf \lbrace f(x) \mid x \in S \rbrace > 0$.  To verify
the claim, we argue by contradiction and assume we can find a sequence
$x_n$ with $f(x_n) < \frac{1}{n}$ (i.e. the ball $B(x_n, \frac{1}{n})$
is not contained in any $U_\alpha$). 
By sequential compactness we have a convergent subsequence $x_{n_k}$
that converges to $x \in S$.  Because $\mathcal{U}_\alpha$ is a open cover there we
can find an $r > 0$ and $U_\alpha$ such that $B(x, r) \subset
U_\alpha$.  Pick $N_1 > \frac{2}{r}$.  By convergence of $x_{n_k}$ we can find $N_2 > 0$ such that
for $n_k > N_2$ we have $d(x, x_{n_k}) < \frac{r}{2}$.  For $n_k >
\max(N_1, N_2)$, by the triangle inequality we have $B(x_{n_k},
\frac{1}{n_k}) \subset B(x,r) \subset U_\alpha$, so we have
a contradiction.

With the claim verified we return to the problem of proving
compactness.  Pick an arbitrary $x_1 \in S$ and let $c = 2 \wedge
\inf_{x \in S} f(x)$.  We define $x_n$ inductively by the following
algorithm: if there is exists $x_n$ such that $d(x_n, x_j) >
\frac{c}{2}$ for all $j=1, \dots, n-1$ then pick it otherwise stop.
We claim that the algorithm terminates after a finite number of
steps.  If it didn't then we'd have constructed an infinite sequence
$x_n$ such that for all $m,n > 0$ we have $d(x_n,x_m) > \frac{c}{2}$
which implies there is no Cauchy subsequence hence has no convergent
subsequence contradicting sequential compactness.  Therefore there is an $n>0$ such that $S = \cup_{k=1}^n
B(x_k, \frac{c}{2})$; however by construction we know that for every
$x_k$ there is a $U_k$ such that $B(x_k, \frac{c}{2}) \subset U_k$.
Then $U_1, \dots, U_n$ is a finite subcover of $S$ and we are done.
\end{proof}
It is worth noting that the equivalence of the finite subcover
property and sequential compactness does not hold in general
topological spaces.  In general sequential compactness is equivalent
to the weaker property that \emph{countable} open covers have finite
subcovers (sometime this property is refered to as countable
compactness).  It turns out that in these circumstances that the full
power of the finite subcover property is generally needed.

\begin{cor}\label{ClosedSubsetsCompact}Every closed subset of a compact set is compact.
\end{cor} 
\begin{proof}Let $B$ be a compact set and let $A \subset B$ be
  closed.  Then by the previous result and the compactness of $B$, any
  infinite sequence in $A$ has a subsequence that converges to a point
  in $B$.
  Because $A$ is closed the limit of the subsequence is in fact in $A$.
\end{proof}

\begin{thm}\label{ContinuousImageOfCompact}Let $f : (S, d) \to
  (S^\prime, d^\prime)$ be continuous.  If $S$ is compact then $f(S)$
  is compact.
\end{thm}
\begin{proof}Let $U_\alpha$ be an open cover of $f(S)$.  By continuity
  of $f$, $f^{-1}(U_\alpha)$ is an open cover of $S$ and therefore has
  a finite subcover $f^{-1}(U_1), \dots, f^{-1}(U_n)$.  It is easy to
  see that $U_1, \dots, U_n$ is a finite subcover of $f(S)$:  if $y \in
  f(S)$, we can write $y = f(x)$ for $x \in S$; picking $i$ so that $x \in
  f^{-1}(U_i)$, we see that $y \in U_i$.
\end{proof}

The following is a characterization of compact sets in $\reals^n$.
\begin{thm}\label{HeineBorel}[Heine-Borel Theorem]A subset $A \subset
  \reals^n$ is closed and
  bounded if and only if it is compact.
\end{thm}
TODO:  I don't think it is worth doing the proof from scratch; this is
a simple corollary of the result.
\begin{proof}By Lemma \ref{CompactnessInMetricSpaces} it suffices to
  show that a closed and bounded set in $\reals^n$ is complete and
  totally bounded.  Completeness is simple as any Cauchy sequence in
  $A$ converges in $\reals^n$ by completeness of $\reals^n$ but then
  the limit is in $A$ because $A$ is closed.  To see total
  boundedness, pick an $\epsilon > 0$ and then pick $N >
  \frac{\sqrt{n}}{\epsilon}$.  Since $A$ is bounded, there exists $M >
  0$ such that $A \subset [-M, M] \times \cdots \times [-M,M]$.  It
  suffices to show that the latter set is totally bounded.  Pick
  the finite set of points $\lbrace (x_1/N, \dots , x_n/N) \mid -MN \leq x_j
    \leq MN \rbrace$ and note that 
\begin{align*} [-M, M] \times \cdots \times
  [-M,M] \subset \bigcup B((x_1/N, \dots , x_n/N), \epsilon)
\end{align*}
\end{proof}

Before we begin the proof we need a Lemma.
\begin{lem}Suppose $C_0 \supset C_1 \supset \cdots$ is a nested
  sequence of closed and bounded sets $C_k \subset \reals^n$.  Then
  $\cap_k C_k$ is non empty.
\end{lem}
\begin{proof}Here is the proof for $n=1$.  TODO: Generalize.

Let $a_k = \inf C_k$; because $C_k$ is closed we know that $a_k \in
C_k$.  By the nestedness and boundedness of $C_k$, we know that $a_k$
is a non-decreasing bounded sequence and therefore has a limit $a$.
For any fixed $k$, the sequence $a_n \in C_k$ for all $n \geq k$ and
thus $a=\lim_{n \to \infty} a_n \in C_k$.  Since $k$ was arbitrary we
have $a \in \cap_k C_k$ and we're done.
\end{proof}
With the Lemma in hand we can proceed to the proof of Heine-Borel.
\begin{proof}
Suppose $A$ is closed and bounded.  By boundedness there exists $N>0$
such that $A \subset [-N,N] \times \cdots \times [-N,N]$ and by
Corollary \ref{ClosedSubsetsCompact} it suffices to show that $ [-N,N]
\times \cdots \times [-N,N]$ is compact.

Now suppose that we are given an infinite open covering of $ [-N,N]
\times \cdots \times [-N,N]$ by sets $A_\alpha$ such that there is no
finite subcover.  Now bisect each side of the cube so that we can
write it as a union of $2^n$ cubes each of side $N$.  $A_\alpha$
covers each of the subcubes; if all of the subcubes had a finite
subcover of $A_\alpha$ then by taking the union we'd have constructed
a finite subcover of $ [-N,N]
\times \cdots \times [-N,N]$.  Since we've assumed that this isn't
true at least one of the subcubes has no finite subcover.   Pick that
cube, call it $C_1$ and now iterate the construction to create a
nested sequence of cubes $C_k$ where $C_k$ has side of length
$N/2^k$.  Since the $C_k$ are closed and bounded by the previous Lemma
we know that the intersection $\cap_k C_k \neq \emptyset$ and
therefore we can pick $x \in \cap_k C_k$.  Since $A_\alpha$ is a
cover, there exists an $A$ such that $x \in A$.  Because $A$ is open
we can in fact find a ball $B(x,r) \subset A$ for some $r > 0$.  Then
for sufficiently large $k$, $C_k \subset B(x,r) \subset A$ which means
that we have constructed a finite subcover for $C_k$ which is a contradiction.
\end{proof}

\begin{defn}Let $(S,d)$ and $(T, d^\prime)$ be metric spaces, a
  function $f : S \to T$ is said to be \emph{uniformly continuous} if
  for every $\epsilon > 0$ there exists a $\delta > 0$ such that
  $d(x,y) < \delta$ implies $d^\prime(f(x), f(y)) < \epsilon$.
\end{defn}

\begin{thm}Let $f : (S,d) \to (T, d^\prime)$ be a continuous function, if $S$ is
  compact then $f$ is uniformly continuous.
\end{thm}
\begin{proof}
The proof is by contradiction.  Suppose that $f$ is not uniformly
continuous.  Fix an $\epsilon > 0$, for every $n > 0$ we can find
$x_n$ and $y_n$ such that $d(x_n, y_n) < \frac{1}{n}$ but
$d^\prime(f(x_n), f(y_n)) \geq \epsilon$.  Now by compactness and Theorem
\ref{CompactnessInMetricSpaces} we can find a
common convergence subsequence of both $x_n$ and $y_n$.  Let's say
$\lim_{j \to \infty} x_{n_j} = x$ and $\lim_{j \to \infty} y_{n_j} =
y$.  Note that for every $j>0$, 
\begin{align*}
d(x,y) = \lim_{j \to \infty } d(x,y)\leq \lim_{j \to \infty } d(x,
x_{n_j}) + d(x_{n_j}, y_{n_j}) + d(y_{n_j}, y) = 0
\end{align*}
therefore $x=y$ and $f(x)=f(y)$.  

Again using the triangle inequality we see
\begin{align*}
\lim_{j \to \infty} d^\prime(f(x_{n_j}), f(y_{n_j})) \leq \lim_{j \to \infty}
d^\prime(f(x_{n_j}), f(x)) + d^\prime(f(x), f(y)) + d^\prime(f(y), f(y_{n_j})) = 0
\end{align*}
which is the desired contradiction.
\end{proof}

\begin{thm}Let $f : S \to \reals^n$ be a continuous function, if $S$ is
  compact then $f$ is bounded.
\end{thm}
\begin{proof}
By the Heine-Borel Theorem and Theorem \ref{ContinuousImageOfCompact}, we know that $f(S)$ is a closed bounded set.
\end{proof}

A related notion is that of uniform convergence of functions.
\begin{defn}Let $f, f_n : S \to (S, d^\prime)$ be a sequence
  of functions.  The we way that $f_n$ converges to $f$
  \emph{uniformly} if and only if for every $\epsilon > 0$ there
  exists a $N > 0$ such that for all $x \in S$, and $n > N$, $d^\prime(f_n(x), f(x)) < \epsilon$.
\end{defn}

One of the most important points about uniform convergence is that a
uniform limit of continuous functions is continuous.
\begin{defn}\label{UniformLimitContinuousFunctionsIsContinuous}Let $f, f_n : (S,d) \to (S^\prime, d^\prime)$ be a sequence
  of functions where $f_n$ are continuous.  If the $f_n$ converge
  to $f$ uniformly then $f$ is continuous.
\end{defn}
\begin{proof}
Suppose we are given an $\epsilon > 0$ and let $x \in S$.  By uniform
convergence of $f_n$ we may find an $N > 0$ such that
$d^\prime(f_n(y), f(y)) < \frac{\epsilon}{3}$ for all $n \geq N$ and
$y \in S$.  In particular, consider $f_N$.  Since this function is
continuous we may find $\delta > 0$ so that $d(x,y) < \delta$ implies
$d^\prime(f_N(x), f_N(y)) < \frac{\epsilon}{3}$.  So by the triangle
inequality, we have 
\begin{align*}
d^\prime(f(x), f(y)) < d^\prime(f(x), f_N(x)) + d^\prime(f_N(x),
f_N(y)) + d^\prime(f_N(y), f(y)) < \epsilon
\end{align*}
\end{proof}

\subsection{Stone Weierstrass Theorem}

\begin{thm}Let $X$ be a compact Hausdorff space and let $A \subset
  C(X;\reals)$ be a subalgebra which contains a non-zero constant
  function.  The $A$ is dense in $C(X;\reals)$ if and only if $A$
  separates points.
\end{thm}
\begin{proof}TODO:
\end{proof}

\begin{cor}[Fourier Series Approximation]\label{FourierSeries}For
  every continuous $f : \reals^n \to \reals$ such that $f(x + v) =
  f(x)$ for all $x \in \reals$, and $v \in \integers^n$, for every
$\epsilon > 0$  there exists constants $c_{j,k}$ and
$d_{j,k}$ such that 
\begin{align*}
\sup_x \abs{\sum_{j=0}^n \sum_{k=0}^N (c_{j,k} \sin(2k\pi x_j) + d_{j,k}
\cos(2k\pi x_j)) - f(x)} < \epsilon
\end{align*}
\end{cor}
\begin{proof}First we observe that there is a bijection between
  periodic function as in they hypothesis and functions on the
  topological space $T^n = S^1 \times \cdots \times S^1$ (the $n$-torus).
Observe that if one has a uniform approximation to a function
viewed as having a domain $T^n$ then the uniform approximation applies
equally well when considered as a periodic function on $\reals^n$.

It remains to observe that $T^n$ is compact Hausdorff, the functions $\sin(2k\pi x_j)$ an $\cos(2k
\pi x_j)$ separate points and contain the constants so the Stone Weierstrass
Theorem applies.

An alternative approach is a more constructive one using the Fejer kernel.
\end{proof}
\section{Measure Theory}
Measure theory is concerned with the theory of integration.  Thinking
intuitively for a moment, we know that we want to compute
expressions of the form $\int_A f$ in which $A$ is a set and $f$ is a
real valued function on the set $A$.  If we take functions $f$ that
are equal to $1$ on the set $A$, then it is clear from our intuition
from elementary calculus that $\int_A 1$ should correspond the the
size of $A$ in some appropriate sense.  Therefore, even if we set out
to create a theory of integration we will get as a by product a theory
of set measure.  In fact, the development of the theory starts from
the notion of set measures and develops the theory of integration
using that.

Before setting out the definitions, it is worth mentioning that set
theory is a weird and wild territory.  Over the years, mathematicians
have come up with some truly astounding constructs with sets that defy
intuition. The first trivial example is to note the cardinality of $Z$
and $Z^2$ is the same.  A second much deeper example is the
Banach-Tarski Paradox which says in effect that there is a
decomposition of the unit ball in $\reals^3$ into a finite number of
pieces such that the pieces can be rearranged by only translations and
rotations into two copies of the unit ball.  We won't prove the
Banach-Tarski paradox here, but it suffices to say that it shows you
can't have all of the following in a definition of volume;
\begin{itemize}
\item[(i)]Translations are volume preserving.
\item[(ii)]Rotations are volume preserving.
\item[(iii)]All sets are measurable.
\end{itemize}

By now, the time honored approach to these matters is to give up on
the naive idea that all sets can be measured.  Thus the definition of a measure theory comprises a definition of
which sets are measureable, a means of measuring those sets and a
theory of integrating suitable functions using that measure.



\subsection{Measurable Spaces}
\begin{defn}A non-empty collection $\mathcal{A}$ of subsets of a set
  $\Omega$ is called a $\sigma$-algebra if given $A, A_1, A_2, \dots
  \in  \mathcal{A}$ we have
\begin{itemize}
\item[(i)]$A^c \in \mathcal{A}$
\item[(ii)]$\bigcup_n A_n \in \mathcal{A}$
\item[(iii)]$\bigcap_n A_n \in \mathcal{A}$
\end{itemize}
\end{defn}
Note that this definition makes a lot of sense.  Whatever our
definition of the class of measurable sets is, we want to be able to
perform meaningful constructions with those sets.  Thus we want the
set of allowable operations to be as large as possible.  On the other
hand, we know that we can't go beyond countable unions. For the
reals once one allows points to be measurable, allowing uncountable unions would
mean that every set is measurable and we already know we can't have that.
\begin{lem}Let $\sigma$-algebra $\mathcal{A}$ in $\Omega$, and $A_1,
  A_2, \dots \in \mathcal{A}$,
\begin{itemize}
\item[(i)] $\Omega \in \mathcal{A}$
\item[(ii)] $\emptyset \in \mathcal{A}$
\end{itemize}
\end{lem}
\begin{proof}
Since $\mathcal{A}$ is non empty, we can find $A \in \mathcal{A}$.
Thus $\Omega = A \bigcup A^c \in \mathcal{A}$.  Then taking
complements shows $\emptyset \in \mathcal{A}$.
\end{proof}
Note that in many accounts of measure theory, the result of the above lemma is assumed as part of
the definition of a $\sigma$-algebra.

\begin{lem}Given a class $\mathcal{C}$ of $\sigma$-algebras on
  $\Omega$, the intersection is also a $\sigma$-algebra.
\end{lem}
\begin{proof}
Because we have shown that every $\sigma$-algebra contains $\Omega$,
we know that the intersection is non-empty.  Now let $A, A_1, A_2, \dots$
be in every $\sigma$-algebra.  Clearly every $\sigma$-algebra in the
class contains $\bigcap_n A_n$, hence so does the intersection.
Similarly with $\bigcup_n A_n$ and $A^c$.
\end{proof}
Note that a union of $\sigma$-algebras is not necessarily a
$\sigma$-algebra.  However, a union of $\sigma$-algebras generates a
$\sigma$-algebra in an appropriate sense.
\begin{defn}Given a collection $\mathcal{C}$ of subsets of $\Omega$,
  we let $\sigma(\mathcal{C})$ be the smallest $\sigma$-algebra
  containing $\mathcal{C}$.
\end{defn}
Note that the definition makes sense since the set of all subsets of
$\Omega$ is a $\sigma$-algebra.  Therefore, the class of
$\sigma$-algbras containing $\mathcal{C}$ is non-empty and
$\sigma(\mathcal{C})$ is the intersection of of the class by the
previous lemma.

For metric spaces (and general topological spaces) there is an
important $\sigma$-algebra that is associated with the topology.
\begin{defn}Given a metric space $S$, the Borel $\sigma$-algebra
  $\borel{S}$ is the $\sigma$-algebra generated by the open sets on $S$.
\end{defn}

\begin{lem}\label{IntervalsGenerateBorel}The Borel $\sigma$-algebra of $\reals$ is generated by intervals
  of the form $(-\infty, x]$ for $x \in \rationals$.
\end{lem}
\begin{proof}Let $\mathcal{C}$ be the collection of all open
  intervals.
We know that the open sets of $\reals$ are countable
  unions of open intervals.  Therefore, the Borel $\sigma$-algebra is
  generated by the set of open intervals.  Now let $\mathcal{D}$ be
  the set of closed intervals of the form $(-\infty,x]$ for $x \in
  \rationals$.  Pick an open interval
  $(a,b)$ and pick a descreasing sequence or rationals $a_n \downarrow
  a$ and an increasing sequence of rationals $b_n \uparrow b$.  Then
  we have 
\begin{align*}(a,b) &= \bigcup_{n=1}^\infty (a_n,b_n] \\
&= \bigcup_{n=1}^\infty \left ( (-\infty,b_n] \cap (-\infty,a_n] \right )
\end{align*}
which shows that $\mathcal{C} \subset \sigma(\mathcal{D})$ hence 
$\sigma(\mathcal{C}) \subset \sigma(\mathcal{D})$.  However, since the
elements of $\mathcal{D}$ are closed sets and $\sigma$-algebras are
closed under set complement, we have $\mathcal{D} \subset
\sigma{\mathcal{C}}$ and therefore
\begin{align*}
\mathcal{B} = \sigma(\mathcal{C}) \subset \sigma(\mathcal{D}) \subset
\sigma(\mathcal{C}) = \mathcal{B}
\end{align*}
and we have $\sigma(\mathcal{D}) = \mathcal{B}$.
\end{proof}

Next we consider how $\sigma$-algebras behave in the presence of
functions.  Given a function $f:S \to T$ we have the induced map on
sets $f^{-1}: 2^T \to 2^S$ defined by 
\begin{align*}
f^{-1}(B) = \left \{x \in S; f(x) \in B \right \}
\end{align*}
\begin{lem}\label{SetOperationsUnderPullback}For $A, B,B_1,B_2,\dots
  \subset T$, then 
\begin{itemize}
\item[(i)] $f^{-1}(B^c) = \left[
    f^{-1}(B) \right ]^c$
\item[(ii)] $f^{-1} \bigcap_n B_n = \bigcap_n f^{-1}
  B_n$
\item[(iii)] $f^{-1} \bigcup_n B_n = \bigcup_n f^{-1}
  B_n$
\item[(iv)]$f^{-1}(B \setminus A) = f^{-1}(B) \setminus f^{-1}(A)$
\end{itemize}
\end{lem}
\begin{proof}
(i)\begin{align*}
f^{-1}(B^c) &= \left \{x \in S; f(x) \notin B \right \} \\
&= \left \{x \in S; f(x) \in B \right \}^c = \left[ f^{-1}(B) \right ]^c
\end{align*}
(ii)\begin{align*}
f^{-1} \bigcap_n B_n &= f^{-1} \left \{x \in T ; \forall n x \in B_n
\right \} \\
& = \left \{x \in S; \forall n f(x) \in B_n \right \} = \bigcap_n f^{-1}  B_n
\end{align*}
(iii)\begin{align*}
f^{-1} \bigcup_n B_n &= f^{-1} \left \{x \in T ; \exists n x \in B_n
\right \} \\
& = \left \{x \in S; \exists n f(x) \in B_n \right \} = \bigcup_n f^{-1}  B_n
\end{align*}

(iv) follows from (i) and (ii) by writing $B \setminus A = B \cap A^c$.
\end{proof}
\begin{lem}\label{SigmaAlgebraPullback}Given an arbitrary function $f$ between measurable spaces
  $(S,\mathcal{S})$ and $(T,\mathcal{T})$, then
\begin{itemize}
\item[(i)] $\mathcal{S}^\prime = f^{-1} \mathcal{T}$ is a
  $\sigma$-algebra on $S$.
\item[(ii)] $\mathcal{T}^\prime = \left \{A \subset T ; f^{-1}(A) \in
      \mathcal{S} \right \}$ is a $\sigma$-algebra on $T$.
\end{itemize}
The $\sigma$-algebra denoted $\mathcal{T}^\prime$ is often denoted
$f_* \mathcal{S}$.
\end{lem}
\begin{proof}
To show (i), let $A,A_1,A_2,\dots \in \mathcal{S}^\prime$.  Since
$\mathcal{S}^\prime = f^{-1}\mathcal{T}$, there exist $B,B_1,B_2,\dots
\in \mathcal{T}$ such that $A = f^{-1}(B)$ and $A_i = f^{-1}(B_i)$ for
$i=1,2,\dots$.  Now since $\mathcal{T}$ is a $\sigma$-algebra, we know
that $B^c$, $\bigcup_n B_n$ and $\bigcap_n B_n$ are all in
$\mathcal{T}$.  Now using the previous lemma,
\begin{align*}
A^c &= \left[f^{-1}(B)\right]^c &&= f^{-1}(B^c) \in
\mathcal{S}^\prime\\
\bigcap_n A_n &= \bigcap_n f^{-1}B_n &&= f^{-1} \bigcap_n B_n \in
\mathcal{S}^\prime \\
\bigcup_n A_n &= \bigcup_n f^{-1}B_n &&= f^{-1} \bigcup_n B_n \in
\mathcal{S}^\prime \\
\end{align*}
Now to see (ii), first note that $\mathcal{T}^\prime$ is non-empty
since $f^{-1} (\emptyset) = \emptyset \in \mathcal{S}$.  Next, pick $B,B_1,B_2,\dots \in \mathcal{T}^\prime$ so that
$f^{-1}B,f^{-1}B_1,f^{-1}B_2 \in \mathcal{S}$.  Again use the previous
lemma to see
\begin{align*}
f^{-1} B^c &= \left[f^{-1}(B)\right]^c \in
\mathcal{S}\\
f^{-1} \bigcap_n B_n &= \bigcap_n f^{-1}B_n \in
\mathcal{S} \\
f^{-1} \bigcup_n B_n &= \bigcup_n f^{-1}B_n \in
\mathcal{S} \\
\end{align*}
and this shows that $B^c,f^{-1} \bigcap_n B_n, f^{-1} \bigcap_n B_n
\in \mathcal{T}^\prime$.
\end{proof}

\begin{lem}Let $f : S \to T$ be a set function and $f^{-1} : 2^T \to
  2^S$ be the induced function on sets.
\begin{itemize}
\item[(i)]$f^{-1}$ is surjective if and only if $f$ is injective
\item[(ii)]$f^{-1}$ is injective if and only if $f$ is surjective
\item[(iii)]$f^{-1}$ is a bijection if and only if $f$ is a bijection
\end{itemize}
\end{lem}
\begin{proof}
Suppose $f$ is surjective and pick $A, B \subset T$ with $A \neq B$.
Then, possibly switching the names of $A$ and $B$, we have $t \in A
\setminus B$.  By surjectivity we know there exists an $s \in S$ such
that $f(s) = t$ and therefore $s \in f^{-1}(A) \setminus f^{-1}(B)$
showing $ f^{-1}(A) \neq f^{-1}(B)$.  Now if $f$ is not surjective
then there exists $t \in T$ such that there is no $s \in S$ with $f(s)
= t$.  In this case we see that $f^{-1}(T) = S = f^{-1}(T \setminus
\lbrace t \rbrace)$ showing $f^{-1}$ is not injective.

Suppose $f$ is injective and let $B \subset S$ and we claim $B =
f^{-1}(f(B))$.  Clearly $A \subset f^{-1}(f(B))$ and if they are not
equal then there exists $s \in S \setminus B$ such that $f(s) = f(b)$
for some $b \in B$ contradicting injectivity.  If $f$ is not injective
then there exists $s,t \in S$ with $s \neq t$ and $f(s) = f(t)$ and
clearly there can be no $A \subset T$ such that $f^{-1}(A) = \lbrace s \rbrace$.

The statement of (iii) is an immediate consequence of (i) and (ii).
\end{proof}

The definition given for $\sigma(\mathcal{C})$ for a set $\mathcal{C}
\subset 2^\Omega$ as the smallest $\sigma$-algebra containing
$\mathcal{C}$ may lack appeal because of the fact that it is
non-constructive.  It is possible to give a constructive definition of
$\sigma(\mathcal{C})$ by making a transfinite recursive definition.
The following makes use of the theory of ordinal numbers.
\begin{lem}Let $\mathcal{C} \subset 2^\Omega$, and let $\omega_1$ be
  the first uncountable ordinal and define for each countable ordinal
\begin{itemize}
\item[(i)]$\mathcal{C}_{\omega_0} = \mathcal{C}$
\item[(ii)]For a successor ordinal $\alpha$, $\mathcal{C}_\alpha$ is
  the set of countable unions of elements of $\mathcal{C}_{\alpha -
    1}$ and complements of such unions.
\item[(iii)]For a limit ordinal $\alpha$, define $\mathcal{C}_\alpha =
  \bigcup_{\beta < \alpha} \mathcal{C}_\beta$.
\end{itemize}
Then $\bigcup_{\alpha < \omega_1} \mathcal{C}_\alpha = \sigma(\mathcal{C})$.
\end{lem}
\begin{proof}
First we show $\bigcup_{\alpha < \omega_1} \mathcal{C}_\alpha \supset
\sigma(\mathcal{C})$.  Since we know that $\mathcal{C} \subset
\bigcup_{\alpha < \omega_1} \mathcal{C}_\alpha$, it suffices to show
that $\bigcup_{\alpha < \omega_1} \mathcal{C}_\alpha$ is a
$\sigma$-algebra.

It is explicit in the definition for successor
ordinals, that given any $A \in \mathcal{C}_\alpha$, we have $A^c \in  \mathcal{C}_{\alpha+1}$.

To show closure under set union, we suppose that we are
given $A_1, A_2, \dots$ where $A_i \in \mathcal{C}_{\alpha_i}$.  We now
use the fact that given a countable set of countable ordinals, there
is a countable ordinal that bounds them (TODO: Prove this somewhere or
find a good reference).  Thus we may pick a countable ordinal $\hat
\alpha$ such that $\alpha_i < \hat {\alpha}$ for every $i=1,2,\dots$.  Since
$\mathcal{C}_\alpha \subset \mathcal{C}_{\alpha+1}$, we know that $A_i
\in \mathcal{C}_{\hat \alpha}$ for all $i$.  Now simply apply the definition
of $\mathcal{C}_{\hat \alpha + 1}$ to see
$\bigcup_{i=1}^\infty A_i \in \mathcal{C}_{\hat \alpha + 1}$.  Having
proven closure under complement and countable union, use De
Morgan's Law to derive the countable intersection property and we are done.

Now we need to show that $\bigcup_{\alpha < \omega_1} \mathcal{C}_\alpha \subset
\sigma(\mathcal{C})$.  This is an easy transfinite induction on
$\alpha$ using the properties of the $\sigma$-algebra
$\sigma(\mathcal{C})$.
TODO: Write this out.
\end{proof}

\subsection{Measurable Functions}
We've seen that arbitrary set functions can be used to create
$\sigma$-algebras but when we consider functions between
measurable spaces the $\sigma$-algebras are given and it makes sense
to restrict our attention to a class of functions that are compatible
with those $\sigma$-algebras.
\begin{defn}A function $f : (S,\mathcal{S}) \to (T,\mathcal{T})$ is
  called measurable if for every $B \in \mathcal{T}$, we have
  $f^{-1}(B) \in \mathcal{S}$.  When we want to emphasize that the
  measurability is with repsect to particular $\sigma$-algebras we may
  say that $f$ is $\mathcal{S}/\mathcal{T}$-measurable.
\end{defn}
\begin{lem}\label{MeasurableByGeneratingSet}Suppose we are given a function $f : (S,\mathcal{S}) \to
  (T,\mathcal{T})$ and a class of subsets $\mathcal{C} \subset 2^T$
  such that $\sigma(\mathcal{C}) = \mathcal{T}$.  The $f$ is
  measurable if and only if $f^{-1} \mathcal{C} \subset \mathcal{S}$.
\end{lem}
\begin{proof}The only if direction is trivial.  So suppose $f^{-1}  \mathcal{C} \subset \mathcal{S}$.
Now consider $\mathcal{T}^\prime = \left \{ B \subset T; f^{-1} B \in
  S \right \}$.  By our assumption, we have $\mathcal{C} \subset
\mathcal{T}^\prime$.  Furthermore we know from
Lemma \ref{SigmaAlgebraPullback} that $\mathcal{T}^\prime$ is
a $\sigma$-algebra, thus $\sigma(\mathcal{C}) \subset
\mathcal{T}^\prime$ and this shows that $f$ is
$\mathcal{S}/\mathcal{T}$ measurable.
\end{proof}
\begin{lem}\label{CompositionOfMeasurable}Let $f:(S,\mathcal{S}) \to (T,\mathcal{T})$
  and $g:(T,\mathcal{T}) \to (U,\mathcal{U})$ be measurable.  Then $g
  \circ f :(S,\mathcal{S}) \to (U,\mathcal{U})$ is measurable.
\end{lem}
\begin{proof}This follows simply from the fact that $(g \circ
  f)^{-1}(B) = g^{-1}(f^{-1}(B))$ and the measurability of $f$ and $g$.
\end{proof}
Note, from this point forward, when we refer to $\reals$ as a
measurable space, it should be assumed that we are referring to
$\reals$ with the Borel $\sigma$-algebra.  Note that a function
$f:(\Omega, \mathcal{A}) \to \reals$ is measurable if and only if
$\left \{\omega \in \Omega ; f(\omega) \leq x \right \} \in \mathcal{A}$ for all $x \in \reals$ (in fact
it suffices to consider $x \in \rationals$).  It is also very common to consider
extensions of $\reals$ such as $\overline{\reals} = [-\infty,\infty]$
and $\overline{\reals}_+ = [0,\infty]$
obtained by appending points at infinity.  For these spaces we take the $\sigma$-algebra
generated by $\left \{\omega \in \Omega ; f(\omega) \leq x \right \}$
for $x \in \overline{\reals}$ respectively.  It can be shown that
there are natural topologies on each of these compactifications and
the $\sigma$-algebras defined are the Borel $\sigma$-algebras of these topologies.

We will often talk about the
convergence of sequences of measurable functions.  Unless we say
otherwise, it should be understood that this convergence is taken pointwise.
\begin{lem}\label{LimitsOfMeasurable}Let $f_1, f_2, \dots$ be measurable functions from
  $(\Omega, \mathcal{A})$ to $\overline{\reals}$.  Then $\sup_n f_n$,
  $\inf_n f_n$, $\limsup_n f_n$, $\liminf_n f_n$ are all measurable.
\end{lem}
\begin{proof}
To see measurability of $\sup_n f_n$ we suppose that $\omega \in
\Omega$ is such that $\sup_n f_n(\omega) \leq x$, then $x$ is an upper
bound we have $f_n(\omega) \leq x$ for all $n$.  On the other hand, if
we assume that $\omega \in \Omega$ is such that $f_n(\omega) \leq x$
for all $n$ then  $\sup_n f_n(\omega) \leq x$ so we have
\begin{align*}
\left \{\omega ; \sup_n f_n(\omega) \leq x \right \} = \bigcap_n \left
  \{\omega ; f_n(\omega) \leq x \right \} \in \mathcal{A}
\end{align*}
To see that $\inf_n f_n$ is measurable we use the identity $\inf_n f_n
= -\sup_n (-f_n)$.

We also have the definitions
\begin{align*}
\limsup_{n\to \infty} f_n = \inf_n \sup_{k \geq n} f_k \textrm{,} \liminf_{n\to \infty} f_n = \sup_n \inf_{k \geq n} f_k
\end{align*}
and the measurability of $\sup$ and $\inf$ already shown implies the
measurability of $\liminf$ and $\limsup$.
\end{proof}

We now introduce an extremely important class of measurable
functions.  Simple measurable functions will be used to approximate
arbitrary measurable functions and in particular, will serve as the
analogue of Riemann sums when we start to consider integration.

\begin{defn}Given a set $\Omega$ and a set $A
  \subset \Omega$, the \emph{indicator function} $\characteristic{A}$ is
  equal to $1$ on $A$ and $0$ on $A^c$.  A linear combination $c_1
  \characteristic{A_1} + \cdots + c_n \characteristic{A_n}$ is called
  a \emph{simple function}.
\end{defn}

\begin{lem}\label{SimpleFunctions}A function $f : \Omega \to \reals$ is simple if and only it
  takes a finite number of values.  A simple function is measurable if
  an only if $f^{-1}(c_j)$ is measurable for each of its distinct
  values $c_j \in \reals$.
\end{lem}
\begin{proof}
If $f = c_1  \characteristic{A_1} + \cdots + c_n \characteristic{A_n}$
is simple, then since indicator functions take only the value ${0,1}$
it is clear that $f$ can have at most $2^n$ values.

On the other hand, if $f : \Omega \to \reals$ only takes the finite
number of distinct values $c_1, \dots, c_n$ then clearly we may write 
$f = c_1  \characteristic{A_1} + \cdots + c_n \characteristic{A_n}$
where $A_j = f^{-1}(c_j)$.

As regards measurability, first notice that $\characteristic{A}$ is
measurable if and only if $A \in  \mathcal{A}$.  This follows from
that fact that there are only four possible preimages under
$\characteristic{A}$: $A, A^c, \Omega, \emptyset$ and each of these
preimages is the preimage of a measurable subset of $\reals$.

Similarly, if a simple function $f$ has the distinct values $c_1, \dots,
c_n$ (including $0$ if necessary) then clearly for $f$ to be
measurable it is necessary $f^{-1}(c_j)$ is measurable since points
are measurable in $\reals$.   On the hand, there are $2^n$ possible preimages under $f$ and they are all
constructed from unions of the preimages $f^{-1}(c_j)$ so if know that
$f^{-1}(c_j)$ are measurable then so is every $f^{-1}(A)$ for $A
\subset \reals$ (a stronger condition than measurability).
\end{proof}

Note that the representation of a simple function as a linear
combination of indicator functions is not unique.  However, we
have
just shown that a simple function is equally well characterized as a
function that takes a finite number of values.  The canonical
representation of a simple function is a representation such that the
$c_i$ are distinct and non-zero and the $A_i$ are pairwise disjoint; the canonical
representation is unique.
\begin{lem}\label{PointwiseApproximationBySimple}For any positive measurable function $f : (\Omega,
  \mathcal{A}) \to \overline{\reals}_+$ there exist a sequence of simple measurable
  functions $f_1, f_2, \dots$ such that $0 \leq f_n \uparrow f$.
\end{lem}
\begin{proof}
Define
\begin{align*}
f_n(\omega) = 
\begin{cases}k2^{-n} & \text{if $k2^{-n} \leq f(\omega) < (k+1)2^{-n}$
    and $0 \leq k \leq n2^n -1$.} \\
n & \text{if $f(\omega) \geq n$.}
\end{cases}
\end{align*}
Note that $f_n$ is simple since it has at most $2^n + 1$ values $0,
\frac{1}{2^n}, \dots, n$.  $f_n$ is measurable since
$f_n^{-1}(k2^{-n}) = f^{-1}[k2^{-n},(k+1)2^{-n})$ is measurable by measurability of $f$.  Similarly
with $f_n^{-1}(n) = f^{-1} [n,\infty)$ and Lemma \ref{SimpleFunctions}.
\end{proof}

As an application of approximation by simple functions, 
\begin{lem}
Let $f,g : (\Omega, \mathcal{A}) \to \reals$ be measurable functions
and let $a,b \in \reals$.  Then $af + bg$ and $fg$ are measurable and
$f/g$ is measurable when $g \neq 0$ on $\Omega$.
\end{lem}
\begin{proof}As $f$ and $g$ are measurable, we can apply the previous
  lemma to $f_\pm = \pm((\pm f) \wedge 0)$ and $g_{\pm} = \pm((\pm g) \wedge 0)$
  to get measurable simple functions $f_n$ and $g_n$ such that $\lim_{n \to
    \infty} f_n = f$ and $\lim_{n \to
    \infty} g_n = g$.  Basic properties of limits show that $\lim_{n
    \to \infty} (a f_n + b g_n) = a f + b g$, $\lim_{n \to \infty} f_n
  g_n = f g$ and $\lim_{n \to \infty} \frac{f_n}{  g_n} =
  \frac{f}{g}$.  Thus by Lemma \ref{LimitsOfMeasurable} we are done if
  we can show that each of $a f_n + b g_n$, $f_n g_n$ and
  $\frac{f_n}{g_n}$ is measurable.  In fact we will show that each of
  these is simple measurable.

It is easy to see that $a f_n + b g_n$ are also
  measurable simple as are $f_n g_n$.  Let $f_n$ take the values $c_1,
  \dots, c_s$ and let $g_n$ take the values $d_1, \dots, d_t$.  
Clearly the functions  $a f_n + b g_n$, $f_n g_n$ and
  $\frac{f_n}{g_n}$ are simple as each takes at most the values  $a c_i + b d_j$, $c_i d_j$ and
  $\frac{c_i}{d_j}$ for $i=1,\dots,s$ and $j=1,\dots, t$.  
Measurability follows from noting that each
  possible value of the linear combination is created from a finite
  set of combinations of the values of the $f_n$ and $g_n$; hence
  $(af_n + bg_n)^{-1}(c_j)$ is a finite union of intersections of the
  form $f_n^{-1}(x) \cap g_n^{-1}(y)$ where $x,y \in \reals$ are
  values of $f_n$ and $g_n$ respectively.
\end{proof}

\begin{defn}Given two measurable functions $f,g$ on the same measurable space
  $(\Omega, \mathcal{A})$, we say that $f$ is $g$-measurable if
  $\sigma(f) \subset \sigma(g)$.
\end{defn}

The following lemma is extremely useful both conceptually and
practically.  In addition it's proof is a paradigmatic example of a
common measure theoretic argument.
\begin{lem}\label{FunctionalRepresentation}Let $f : (\Omega, \mathcal{A}) \to \reals$ and $g :
  (\Omega, \mathcal{A}) \to (T,\mathcal{T})$ be measurable.  Then $f$
  is $g$-measurable if and only if there exists measurable $h :
  (T,\mathcal{T}) \to \reals$ such that $f = h \circ g$.
\end{lem}
\begin{proof}
For the if direction, assume $f = h \circ g$.  Then for $B \in
\mathcal{B}(\reals)$, we have $f^{-1}(B) = g^{-1}(h^{-1}(B))$.  Now we know
that $h^{-1}(B) \in \mathcal{T}$ and therefore, $f^{-1}(B) \in
\sigma(g)$.

For the only if direction, first assume that $f$ is an indicator
function $\characteristic{A}$.  Our assumption of $g$-measurability
means that there exists $B \in \mathcal{T}$ such that $A =
g^{-1}(B)$.  If we define $h = \characteristic{B}$, then we have $f =
h \circ g$.  Now let us suppose that $f$ is a simple function and take
its canonical representation $f = c_1 \characteristic{A_1} + \cdots + c_n
\characteristic{A_n}$ with $A_i$ disjoint and $c_i$ distinct.  Since $f$ is
$g$-measurable, we know that there exist $B_i \in \mathcal{T}$ such
that $A_i = g^{-1}(B_i)$.  If we define $h=c_1 \characteristic{B_1} + \cdots + c_n
\characteristic{B_n}$, then $f = h \circ g$.

Now if we assume $f \geq 0$, then we know that we can find a sequence
of $g$-measurable simple functions such that $f_n \uparrow f$.  We
have shown that there are $h_n$ such that $f_n = h_n \circ g$.  Define
$h = \limsup_n h_n$ and then note $h$ is $g$-measurable and that 
\begin{align*}
h(g(\omega)) = \limsup_n h_n(g(\omega)) = \limsup_n f_n(\omega) =
\lim_{n \to \infty} f_n(\omega) = f(\omega)
\end{align*}
Lastly, for arbitrary $f$, we write $f = f_+ - f_{-}$ where $f_{\pm}
\geq 0$ and are both $g$-measurable (e.g. $f_{\pm} = (\pm f) \wedge 0$).
We find $h_{\pm}$ such that $f_{\pm} = h_{\pm} \circ g$ and define $h = h_+ - h_{-}$.
\end{proof}

The following definitions and lemma may seem merely technical, but in fact are
an important part of the most common methodology for proving measure
theoretic results.  
\begin{defn}A class $\mathcal{C}$ of subsets of a set $\Omega$ is called a $\lambda$-system if
\begin{itemize}
\item[(i)] $\Omega \in \mathcal{C} $.
\item[(ii)] For all $A,B \in \mathcal{C}$ such that $A \subset B$,
  $B \setminus A \in \mathcal{C}$.
\item[(iii)] $A_n \uparrow A$ for $A_n \in \mathcal{C}$ the $A \in \mathcal{C}$.
\end{itemize}
\end{defn}
\begin{defn}A class $\mathcal{C}$ of subsets of a set $\Omega$ is
  called a $\pi$-system if it is closed under finite intersections.
\end{defn}
The first observation is that the concepts of $\pi$-system and
$\lambda$-system factor the conditions for being a $\sigma$-algebra.
\begin{lem}\label{PiLambdaSigma}If a class $\mathcal{C} \subset 2^\Omega$ is both a
  $\pi$-system and a $\lambda$-system, then it is a $\sigma$-algebra.
\end{lem}
\begin{proof}
First we show closure under set complement.  Let $A \in \mathcal{C}$.
Then since $\Omega \in \mathcal{C}$, we know that $A^c = \Omega
\setminus A \in \mathcal{C}$.  Now note that having closure under set complement
together with closure under finite intersection gives closure under
finite union by De Morgan's law $\left \{ \bigcup_{i=1}^n A_i \right \} ^
c =  \bigcap_{i=1}^n A_i^c $.

Let $A_1, A_2, \dots \in \mathcal{C}$.  Next we show closure under
countable union.  Defining $B_n = \bigcup_{i=1}^n A_i$, we know that
$B_n \in \mathcal{C}$ and clearly $B_n \uparrow \bigcup_{i=1}^\infty
A_i$ and therefore $\bigcup_{i=1}^\infty A_i \in \mathcal{C}$.
Closure under countable intersections follows from closure under
countable unions and the infinite version of De Morgan's Law.
\end{proof}
\begin{thm}[$\pi$-$\lambda$ Theorem]\label{MonotoneClassTheorem}Suppose $\mathcal{C}$ is a $\pi$-system, $\mathcal{D}$ is a
  $\lambda$-system such that $\mathcal{C} \subset \mathcal{D}$.  Then
  $\sigma(\mathcal{C}) \subset \mathcal{D}$.
\end{thm}
\begin{proof}The first thing to note is that the intersection of a
  collection of $\lambda$-systems is also a $\lambda$-system and that
   $2^\Omega$ is a $\lambda$-system.
  Therefore, in a way entirely analogous to $\sigma$-algebras we may
  define the $\lambda$-system generated by a collection of sets as the
  intersection of all $\lambda$-systems containing the collection.

The theorem is proved for general $\mathcal{D}$ if we prove it for the
special case $\mathcal{D} = \lambda(\mathcal{C})$.  To see this
special case, by \ref{PiLambdaSigma} it suffices to show that
$\lambda(\mathcal{C})$ is a $\pi$-system.  A trivial induction
argument shows it suffices to show closure under pairwise
intersection:  for every $A,B \in \lambda(\mathcal{C})$ we have $A
\cap B \in \lambda(\mathcal{C})$.

By definition of $\pi$-algebra, we have closure when $A,B
\in \mathcal{C}$.  Now fix $C \in \mathcal{C}$ and let $\mathcal{A}_C
= \left \{A \subset \Omega ; A \cap C \in \lambda(\mathcal{C}) \right
\}$.  We claim that $\mathcal{A}_C$ is a $\lambda$-system.

To see that $\Omega \in \mathcal{A}_C$ is trivial: $C \cap \Omega
= C \in \mathcal{C} \subset \lambda(\mathcal{C})$.
Suppose $A \supset B$ where $A,B \in \mathcal{A}_C$, then $C
\cap (A \setminus B) = (C \cap A) \setminus (C \cap B) \in
\lambda(\mathcal{C})$.  Suppose $A_1 \subset A_2 \subset \cdots$ with
$A_i \in \mathcal{A}_C$.  $C \cap \bigcup_{i=1}^\infty A_i =
\bigcup_{i=1}^\infty C \cap A_i \in \lambda(\mathcal{C})$ by
distributivity of set intersection over set union and closure of $\lambda$-system under increasing unions.

Now that we know $\mathcal{A}_C$ is a $\lambda$-system containing
$\mathcal{C}$ we know that $\lambda(\mathcal{C}) \subset
\mathcal{A}_C$ and therefore $C \cap A \in \lambda(\mathcal{C})$ for
every $A \in \lambda(\mathcal{C})$ and $C \in \mathcal{C}$.

To finish up the proof, for every $C \in \lambda(\mathcal{C})$, let
$\mathcal{B}_C = \left \{ A \in \Omega ; A \cap C \in
  \lambda(\mathcal{C}) \right \}$.  We have just shown that
$\mathcal{C} \subset \mathcal{B}_C$ and an argument exactly analogous
to the one above shows that $\mathcal{B}_C$ is a $\lambda$-algebra and
therefore $\lambda(\mathcal{C}) \subset \mathcal{B}_C$ proving the result.
\end{proof}

Though we'll see many examples of this along the way, it is worth
making explicit how the Theorem \ref{MonotoneClassTheorem} is applied.
Suppose that one wishes to prove a property holds for a
$\sigma$-algebra $\mathcal{A}$ of sets.  A common sub-case is we'll be trying to show a
property holds for the indicator functions associated with those sets
(those being the most basic building blocks of measurable functions).
The $\pi$-$\lambda$ Theorem allows us to prove the property holds on
$\mathcal{A}$ by showing 
\begin{itemize}
\item[(i)] The collection of all sets satisfying the property is a $\lambda$-system
\item[(ii)] There is a $\pi$-system of sets $\mathcal{P}$ that
  satisfies the property and $\sigma(\mathcal{P}) = \mathcal{A}$.
\end{itemize}
A proof along these lines is referred to as a \emph{monotone class
  argument}.

\subsection{Measures and Integration}
Armed with a way of describing and transforming measurable sets it is
finally time to measure them.
\begin{defn}A \emph{measure} on a measurable space $(\Omega,
  \mathcal{A})$ is a function $\mu : \mathcal{A} \to
  \overline{\reals}_+$ satisfying
\begin{itemize}
\item[(i)] $\mu(\emptyset) = 0$
\item[(ii)] $\mu(\bigcup_{i=1}^\infty A_i) = \sum_{i=1}^\infty \mu(A_i)$
  for $A_1, A_2, \dots \in \mathcal{A}$ disjoint.
\end{itemize}
A triple $(\Omega, \mathcal{A}, \mu)$ is called a \emph{measure space}.
\end{defn}

An important special case of measure theory occurs when the underlying
space has unit measure.  Many of the concepts we have already
discussed have different names when discussing this special case.
\begin{defn}A \emph{probability space} is a measure space $(\Omega,
  \mathcal{A}, P)$ such that $P(\Omega) = 1$.  The measure $P$ is
  called the \emph{probability measure}.  Measurable sets $A \in
  \mathcal{A}$ are referred to as \emph{events}.  Given a measurable
  space $(S, \mathcal{S})$, a measurable function $\xi : \Omega \to S$
  is called a \emph{random element} in $S$.  For the special case in
  which $(S, \mathcal{S}) = (\reals, \mathcal{B}(\reals)$, we call a
  measurable $\xi : \Omega \to \reals$ a \emph{random variable}.
\end{defn}

\begin{lem}\label{ContinuityOfMeasure}Given a measure space $(\Omega, \mathcal{A}, \mu)$, and sets $A_1, A_2, \dots \in \mathcal{A}$.
\begin{itemize}
\item[(i)] If $A_i \uparrow A$ then $\mu A_i \uparrow \mu A$.
\item[(ii)] If $A_i \downarrow A$ and $\mu A_1 < \infty$ then $\mu A_i \downarrow \mu A$.
\end{itemize}
\end{lem}
\begin{proof}To show (i), define $B_1 = A_1$ and $B_i = A_i \setminus
  A_{i-1}$ for $i > 1$.  Clearly, $B_i$ are disjoint and it is equally
  clear that $\bigcup_{i=1}^n B_i = A_n$ and $\bigcup_{n=1} ^\infty
  B_n = A$.
Therefore
\begin{align*}
\mu A_n = \mu \bigcup_{i=1}^n B_i = \sum_{i=1}^n \mu B_i \uparrow
\sum_{i=1}^\infty \mu B_i = \mu \bigcup_{i=1}^\infty  B_i = \mu A
\end{align*}
where we have used finite and countable additivity of $\mu$ over the
$B_i$.

To see (ii), note that $A_1 \setminus A_n \uparrow A_1
\setminus A$ and then under the finiteness assumption $\mu A_1 <
\infty$, we see 
\begin{align*}
\mu (A_1 \setminus A_n) = \mu A_1 - \mu A_n \uparrow
\mu (A_1 \setminus A) = \mu A_1 - \mu A
\end{align*}
Subtract $\mu A_1$ from both sides multiply by $-1$ to get the result.
\end{proof}

\begin{lem}Given a measure space $(\Omega, \mathcal{A}, \mu)$, $\mu
  \left (\bigcup_{i=1}^\infty A_i \right ) \leq \sum_{i=1}^\infty \mu(A_i)$  for $A_1, A_2, \dots \in \mathcal{A}$.
\end{lem}
\begin{proof}First we prove finite subadditivity by an induction argument.  For $n=2$, we note that we may
  write disjoint unions
\begin{align*}
A &= (A \setminus B) \cup (A \cap B) \\
B  &= (B\setminus A) \cup (A \cap B) \\
A \cup B  &= (A\setminus B)  \cup (B\setminus A) \cup (A \cap B) \\
\end{align*}
By finite additivity of measure and positivity of measure, we see $\mu A \cup B = \mu A + \mu B
- \mu A \cap B \leq \mu A + \mu B$. 

For the induction step, assume $\mu \left (\bigcup_{i=1}^{n-1} A_i \right
) \leq \sum_{i=1}^{n-1} \mu(A_i)$, then use the case $n=2$ and
Lemma \ref{ContinuityOfMeasure} to see
\begin{align*}
\mu \left (\bigcup_{i=1}^{n} A_i \right) &= \mu \left
  (\bigcup_{i=1}^{n-1} A_i  \cup A_n\right) \\
&\leq \mu \left (\bigcup_{i=1}^{n-1} A_i \right ) + \mu A_n \\
&\leq \sum_{i=1}^{n-1} \mu(A_i) + \mu A_n = \sum_{i=1}^n \mu(A_i)
\end{align*}

To extend the result to infinite unions, define $B_n = \bigcup_{i=1}^n
A_i$ and note that $B_n \uparrow \bigcup_{i=1}^\infty
A_i$ and that by finite subadditivity, $\mu B_n \leq \sum_{i=1}^n \mu
A_i$.  Taking limits we see
\begin{align*}
\mu \bigcup_{i=1}^\infty A_i = \lim_{n \to \infty} \mu B_n \leq \lim_{n
  \to \infty} \sum_{i=1}^n \mu A_i = \sum_{i=1}^\infty \mu A_i
\end{align*}
\end{proof}

Next up is the definition of integral of a measurable function on a
measure space.  First we proceed by defining the integral for a simple functions.
\begin{defn}
Given a canonical representation of a simple function $f =
c_1 \characteristic{A_1} + \cdots + c_n \characteristic{A_n}$ we
define the integral of $f$ to be
\begin{align*}
\int f d\mu = \mu f = c_1 \mu A_1 + \cdots + c_n \mu A_n
\end{align*}
\end{defn}
Having the definition of the integral of a simple function in terms of
the canonical representation is inconvenient at times when one is
given a simple function that is not known to be in a canonical
representation.  It turns out that the formula above extends to any
representation of the simple function as a linear combination of
indicator functions.  To see that we proceed in steps.
\begin{lem}Given any representation of a simple function $f =
c_1 \characteristic{A_1} + \cdots + c_n \characteristic{A_n}$ with
$A_i$ pairwise disjoint,
\begin{align*}
\int f d\mu = c_1 \mu A_1 + \cdots + c_n \mu A_n
\end{align*}
\end{lem}
\begin{proof}We have to construct the canonical representation of
  $f$.  It is conceptually simple, but there is a bit of notation to
  deal with.
  Let $d_1, d_2, \dots, d_m$ be the distinct values of $c_1, \dots,
  c_n$.  Furthermore, for each $i=1,\dots,m$, let $B_{i,j}$ 
  $j=1,\dots,k_i$ be the set of $A_n$ for which $c_n = d_i$.  Then the
  canonical representation of $f$ is 
\begin{align*}
f = d_1 \characteristic{\bigcup_{j=1}^{k_1} B_{1,j}} + \cdots + d_m \characteristic{\bigcup_{j=1}^{k_m} B_{m,j}}
\end{align*}
and then 
\begin{align*}
\int f d \mu &= d_1 \mu \bigcup_{j=1}^{k_1} B_{1,j} + \cdots + d_m \mu
\bigcup_{j=1}^{k_m} B_{m,j} \\
&= d_1 \sum_{j=1}^{k_1} \mu B_{1,j} + \cdots + d_m 
\sum_{j=1}^{k_m} \mu B_{m,j} \\
&= c_1 \mu A_1 + \cdots + c_n \mu A_n
\end{align*}
\end{proof}
\begin{lem}\label{LinearityIntegralSimpleFunctions}Given two simple functions $f,g$, for all $a,b \in \reals$,
 \begin{align*}
\int \left (af + bg \right ) d \mu = a \int f d \mu + b \int g d \mu
\end{align*}
If $f \geq g$ $a.e.$ then we have
 \begin{align*}
\int f d \mu \geq \int g d \mu
\end{align*}
\end{lem}
\begin{proof}Take the canonical representation of both $f$ and $g$, $f
  = \sum_{i=1}^n c_i \characteristic{A_i}$ and $f
  = \sum_{i=1}^m d_i \characteristic{B_i}$.   Furthermore define $A_0 =
  \Omega \setminus \bigcup_{i=1}^n A_i$ and $B_0 =
  \Omega \setminus \bigcup_{i=1}^m B_i$.  Now consider all of the
  pairs $A_i \cap B_j$ and write 
\begin{align*}
f &= \sum_{i=0}^n \sum_{j=0}^m c_i
  \characteristic{A_i \cap B_j} \\
g &= \sum_{i=0}^n \sum_{j=0}^m d_j
  \characteristic{A_i \cap B_j}
\end{align*}
 where we have defined $c_0=d_0=0$.  Thus, we have the  representation 
\begin{align*}
af + bg = \sum_{i=0}^n \sum_{j=0}^m (ac_i + bd_j)   \characteristic{A_i \cap
  B_j}
\end{align*}
Since the $A_i  \cap B_j$ are pairwise disjoint, we can write
\begin{align*}
\int af+bg &= \int \sum_{i=0}^n \sum_{j=0}^m (ac_i + bd_j)   \characteristic{A_i \cap B_j} \\
&= \sum_{i=0}^n \sum_{j=0}^m (ac_i + bd_j)   \mu A_i \cap B_j \\
&= a \sum_{i=0}^n \sum_{j=0}^m c_i  \mu A_i \cap B_j + b \sum_{i=0}^n
\sum_{j=0}^m d_j  \mu A_i \cap B_j \\
&= a \int f + b \int g
\end{align*}

Using the same representation as above, we see that if $f \geq g$,
then since the $A_i \cap B_j$ are disjoint, we must have $c_i \geq
d_j$ whenever $A_i \cap B_j \neq \emptyset$.  This shows $\int f \geq
\int g$.
\end{proof}
\begin{cor}Given any representation of a simple function $f =
c_1 \characteristic{A_1} + \cdots + c_n \characteristic{A_n}$,
\begin{align*}
\int f  = c_1 \mu A_1 + \cdots + c_n \mu A_n
\end{align*} 
\end{cor}
The corollary above is used so often that we use it without mentioning
it and essentially treat it as the definition of the integral of a
simple function.

Having defined integrals of simple functions, we leverage the fact
that we can approximate positive measurable functions by increasing
sequences of simple functions to define the integral of a positive
measurable function.
\begin{defn}Given a measurable function $f : (\Omega, \mathcal{A},
  \mu) \to \overline{\reals}_+$, we define 
\begin{align*}\int f = \sup_{0 \leq g \leq f} \int g
\end{align*}
where the supremum is taken over positive simple functions $g$.
\end{defn}
Working with the supremum above is a bit inconvenient and it turns out
that it suffices to work with increasing sequences of positive simple
functions.  To see that we first need a lemma.
\begin{lem}Given a measurable function $f : (\Omega,
  \mathcal{A}, \mu) \to \overline{\reals}_+$, a sequence $0 \leq f_1, f_2, \dots$ of simple measurable
  functions such that $f_n \uparrow f$ and a simple measurable
  function $g$ such that $0 \leq g \leq f$, we have $\lim_{n \to \infty} \int f_n
  \, d\mu
  \geq \int g \, d\mu$.
\end{lem}
\begin{proof}Consider the case where $g = \characteristic{A}$ for $A
  \in \mathcal{A}$.  Pick $\epsilon > 0$, and define
\begin{align*}
A_n = \left \{ \omega \in A; f_n(\omega) \geq 1 - \epsilon \right \}
\end{align*}
Since $f_n$ is increasing, so is $A_n$.  Also it is simple to see that
$A_n \subset A$ since $f \geq f_n$ and $A \subset \bigcup_n A_n$ since
for each $\omega \in A$ convergence of $f_n(\omega) \uparrow
f(\omega)$ tells us that there is $N > 0$ such that for $n > N$, we
have $\abs{f_n(\omega) - f(\omega)} < \epsilon$, hence $A_n \uparrow
A$ and 
$\mu A_n \uparrow \mu A = \int g d \mu$.

Now the definition of $A_n$, the positivity of $f_n$ and the
positivity of integration tells us that
$\int f_n d \mu \geq (1 - \epsilon) \mu A_n$, so taking limits we see
\begin{align*}
\lim_{n \to \infty} \int f_n d \mu \geq (1 - \epsilon) \lim_{n \to
  \infty} \mu A_n = (1-\epsilon) \int g d \mu
\end{align*}
Now let $\epsilon \to 0$ to get the result.

To extend the result to arbitrary positive simple functions,  first consider $g
= c \characteristic{A}$ for $c > 0$.  Note that we can apply the
lemma to $\characteristic{A}$ and the functions $\frac{1}{c}f_n
\uparrow \frac{1}{c}f$, to see that $\lim_{n \to \infty}
\frac{1}{c}f_n \geq \mu A$ and multiply both sides by $c$.

Now consider a positive simple function in canonical form $g = c_1
\characteristic{A_1} + \cdots + c_m \characteristic{A_m}$.  Since $g$
is in the canonical form, $c_i > 0$ for $i=1, \dots, m$.  Also, $A_i \cap A_j = \emptyset$ for $i \neq j$
and therefore $g \characteristic{A_i}  = c_i \characteristic{A_i}$.
Now apply the lemma to each $g \characteristic{A_i} $ and the family
$f_n \characteristic{A_i}  \uparrow f \characteristic{A_i} $ and use
linearity of integral and limits.
\end{proof}
\begin{cor}Given a measurable positive function $f : (\Omega, \mathcal{A},
  \mu) \to \overline{\reals}_+$ and any sequence of positive simple functions $0 \leq f_1,
  f_2, \dots$ such that $f_n \uparrow f$, 
\begin{align*}
\int f d \mu = \lim_{n \to \infty} \int f_n d\mu
\end{align*}
\end{cor}
\begin{proof}As $f_n$ are positive simple functions with $f_n \leq f$
  we know each $\int f_n \leq \int f$ and therefore $\lim_{n \to \infty} \int f_n
  d\mu \leq \int f d \mu$.  

To see the other inequality, pick $\epsilon > 0$, and a positive
simple $0 \leq g \leq f$ such that $\int f d \mu - \epsilon \leq \int
g d \mu$.  Apply the above lemma and we see that $\int f d \mu - \epsilon \leq \int
g d \mu \leq \lim_{n \to \infty} \int f_n d \mu$.  Now let $\epsilon \to 0$ to see $\int f d \mu
\leq \int \lim_{n \to \infty} f_n d \mu$.
\end{proof}

\begin{lem}Given $f,g$ positive measurable and $a,b \geq 0$, 
\begin{align*}
\int \left ( a f + b g \right ) d \mu = a \int f d \mu + b \int g d \mu
\end{align*}
and if $f \geq g$,
\begin{align*}
\int f  d \mu \geq \int g d \mu
\end{align*}
\end{lem}
\begin{proof}Linearity follows by taking $0 \leq f_n \uparrow f$ and
  $0 \leq g_n \uparrow g$ and noting that $0 \leq a f_n + b g_n
  \uparrow a f + b g$.  Now apply linearity of integral of simple
  functions Lemma \ref{LinearityIntegralSimpleFunctions}.

Monotonicity follows immediately from noting that any simple $0 \leq h
\leq g$ also satisfies $0 \leq h \leq f$.
\end{proof}
Perhaps the most important basic theorems of measure theory are those
that describe how limits and integrals behave; in particular what
happens we exchange the order of limits and integrals.  There are three
commonly used variants and we are now ready to state and prove the
first.  Before we do that we illustrate three simple examples of the
things that can go wrong when we exchange the order of limits and
integrals.  All of these examples assume the existence of a measure
$\lambda$ on
$(\reals,\mathcal{B}(\reals))$ such that $\lambda([a,b]) = b -a$.  We
will prove later that such a measure exists (it is the \emph{Lebesgue
  measure} on $\reals$).
\begin{examp}[Escape to horizontal infinity]Consider the sequence of functions 
$f_n = \characteristic{[n,n+1]}$.  Note that $\lim_{n \to \infty}
\int f_n \, d \lambda = 1$ but $\int \lim_{n \to \infty}  f_n \, d
\lambda = 0$.
\end{examp}
\begin{examp}[Escape to vertical infinity]Consider the sequence of functions 
$f_n = n \characteristic{[0,\frac{1}{n}]}$.  Note that $\lim_{n \to \infty}
\int f_n \, d \lambda = 1$ but $\int \lim_{n \to \infty}  f_n \, d
\lambda = 0$.
\end{examp}
\begin{examp}[Escape to width infinity]Consider the sequence of functions 
$f_n = \frac{1}{n}\characteristic{[0,n-1]}$.  Note that $\lim_{n \to \infty}
\int f_n \, d \lambda = 1$ but $\int \lim_{n \to \infty}  f_n \, d
\lambda = 0$.
\end{examp}
 In all cases the integral of the limit is strictly less than the limit
of the integrals and in all cases some amount of \emph{mass} has
\emph{escaped to infinty}.  The limit theorems amount to proving the
fact that mass can only be lost when passing to the limit of a
sequence of measurable functions and establishing generally useful
hypotheses that prevent mass from escaping to infinity.

\begin{thm}\label{MCT}[Monotone Convergence Theorem]Given $f, f_1,
  f_2, \dots$ positive measurable functions from
  $(\Omega, \mathcal{A}, \mu)$ to $\overline{\reals}_+$ such that $0 \leq f_n \uparrow f$, we
  have $\int f_n d \mu \uparrow \int f d \mu$.
\end{thm}
\begin{proof}Choose an approximation of each $f_n$ by an increasing
  sequence of positive simple functions $g_{nk} \uparrow f_n$.  For
  each $n,k>0$, define $h_{nk} = g_{1k} \vee \cdots \vee g_{nk}$.
  Note that $h_{nk}$ is increasing in both of its subscripts.
  Furthermore, note that $h_{nk} \leq f_n$ because $g_{ik} \leq f_{i}
  \leq f_n$ for $i \leq n$ by the monotonicity of $f_n$.

We  claim that $h_{kk} \uparrow f$.  To see this, for every $n>0$,
  $h_{kk} \geq g_{nk}$ for $k \geq n$ and therefore
\begin{align*}
\lim_{k \to \infty} h_{kk} \geq \lim_{k \to \infty} g_{nk} = f_n
\end{align*}
By taking limits we get the inequality
\begin{align*}
\lim_{k \to \infty} h_{kk} \geq \lim_{n \to \infty} f_n = f
\end{align*}
We get the opposite inequality because $f_n$ increases to $f$, we know that for
every $k>0$, $h_{kk} \leq f_k \leq f$ and therefore $\lim_{k \to
  \infty} h_{kk} \leq f$.

We have an approximation of $0 \leq h_{kk} \uparrow f$ by simple
functions, now we can calculate the integral of $f$ using $h_{kk}$
\begin{align*}
\int f \, d\mu = \lim_{k \to \infty} \int h_{kk} \, d\mu \leq \lim_{k
  \to \infty} \int f_k \, d\mu \leq \int f \, d\mu
\end{align*}
where we have used the monotonicity of the integral in both
inequalities.  
\end{proof}
\begin{cor}\label{TonelliIntegralSum}[Tonneli's Theorem for Integrals
  and Sums]Given $f_1,
  f_2, \dots$ positive measurable functions from
  $(\Omega, \mathcal{A}, \mu)$ to $\overline{\reals}_+$, we
  have 
\begin{align*}
\int \sum_{n=1}^\infty f_n d \mu = \sum_{n=1}^\infty \int f_n \, d \mu
\end{align*}
\end{cor}
\begin{proof}Note that the sequence partial sums $\sum_{i=1}^n f_i$ is
  increasing in $n>0$.  Now use linearity of integral and apply the Montone Convergence Theorem.
\end{proof}

 In some cases, we may have a sequence of positive functions that are not known
to be increasing.  In those cases, limits may not even exists but we
still have a fundamental inequality
\begin{thm}\label{Fatou}[Fatou's Lemma]Given $f_1, f_2, \dots$
  positive measurable functions from
  $(\Omega, \mathcal{A}, \mu)$ to $\overline{\reals}_+$,
  then $\int \liminf_{n \to \infty} f_n d \mu \leq \liminf_{n \to
    \infty} \int f_n d \mu$.
\end{thm}
\begin{proof}The proof uses the Monotone Convergence Theorem.  To find
  an increasing sequence of positive measurable functions one needn't
  look further than the definition $\liminf_{n \to \infty} f_n =
  \lim_{n \to \infty} \inf_{k \geq n} f_k$.  Since $\inf_{k \geq n}
  f_k \uparrow \liminf_{n \to \infty} f_n$, we know by Monotone
  Convergence that $\lim_{n \to \infty} \int \inf_{k \geq n}
  f_k \, d \mu = \int \liminf_{n \to \infty} f_n \, d \mu$.  

However, we have the following calculation
\begin{align*}
\inf_{k \geq n} f_k &\leq f_k  & &\textrm{for all $k\geq n$ by
  definition of infimum} \\
\int \inf_{k \geq n} f_k \, d\mu &\leq \int f_k \,
d \mu & &\textrm{for all $k\geq n$ by
  monotonicity of integral} \\
\int \inf_{k \geq n} f_k \, d\mu &\leq \inf_{k \geq n} \int f_k \,
d \mu & &\textrm{by
  definition of infimum} \\
\lim_{n \to \infty} \int
\inf_{k \geq n} f_k \, d\mu &\leq \liminf_{n \to \infty} \int f_k \,
d \mu & &\textrm{taking limits and the
  definition of $\liminf$} \\
\int \liminf_{n \to \infty} f_n d \mu &= & & \textrm{by Monotone Convergence}
\end{align*}

In prose, by the  definition of the infimum $\inf_{k \geq n} f_k \leq f_k$ for every
  $k \geq n$, therefore monotonicity of the integral yields $\int
  \inf_{k \geq n} f_k d \mu \leq \int f_k d \mu$ for every
  $k \geq n$ and hence $\int
  \inf_{k \geq n} f_k d \mu \leq \inf_{k\geq n} \int f_k d \mu$.  Now
  take the limit as $n \to \infty$.
\end{proof}

Our last task is to eliminate the assumption of positivity in the
definition of the integral.  
\begin{defn}A measurable function $f$ on the measure space $(\Omega,
  \mathcal{A}, \mu)$ is \emph{integrable} if $\int \abs{f} \, d\mu <
  \infty$.  For any integrable $f$, we define $\int f \, d \mu = \int
  f_+ \, d\mu - \int f_{-} \, d \mu$.
\end{defn}

We've defined the integral of an integrable function in terms of a
canonical decomposition $f = f_+ - f_-$.  It is occasionally useful to
observe that any decomposition of an integrable function as a
difference of positive measurable functions can be used to calculate
the integral.
\begin{lem}Suppose we are given a measure space $(\Omega, \mathcal{A},
  \mu)$ and an integrable function $f : \Omega \to \reals$.  Suppose
  $f = f_1 -f_2$ where $f_i  : \Omega \to \reals$ are positive measurable
  with $\int f_i \, d \mu < \infty$. Then $\int f \, d\mu = \int f_1 \,
  d\mu - \int f_2 \, d\mu$.
\end{lem}
\begin{proof}Write $f = f_+ - f_-$ and note that $f_1 \geq f_+$ and
  $f_2 \geq f_-$.  For example either $f_+(\omega) = 0$ or
  $f_+(\omega) = f(\omega)$ and we know that $f_1(\omega) = f(\omega)
  + f_2(\omega) \geq f(\omega)$.  We also know that $f_1 - f_+ = f_2 -
  f_-$ and we can see that $\int (f_1 - f_+) \, d\mu = \int (f_2 - f_-) \, d\mu 
  < \infty$.  Therefore by
  linearity of integral
\begin{align*}
\int f \, d\mu &= \int f_+ \, d\mu - \int  f_- \, d\mu \\
&= \int f_+ \, d\mu + \int (f_1 - f_+) \, d\mu - \int (f_2 - f_-) \,
d\mu - \int  f_- \, d\mu \\
&= \int f_1 \, d\mu - \int  f_2 \, d\mu
\end{align*}
\end{proof}
Also linearity and monotonicity of integrals extend to the integrable
case.  Linearity of the integral subsumes the previous result.
\begin{lem}Suppose we are given a measure space $(\Omega, \mathcal{A},
  \mu)$ and integrable functions $f,g: \Omega \to \reals$.  The for
  $a,b \in \reals$ we have $\int (af + bg) \, d\mu = a\int f \, d\mu +
  b\int g \, d\mu$ and if $f \geq g$ then $\int f \, d\mu \geq \int g \, d\mu$.
\end{lem}
\begin{proof}
Write $f = f_+ - f_-$ and $g = g_+ - g_-$.  Define 
\begin{align*}
\hat{f}_\pm &= \begin{cases}
a f_\pm & \text{if $a  \geq 0$} \\
-a f_\mp & \text{if $a < 0$}
\end{cases}
\end{align*} 
It is easy to see that $\hat{f}_\pm \geq 0$,
$\int \hat{f}_\pm \, d\mu < \infty$,  $a f = \hat{f}_+ - \hat{f}_-$
and
\begin{align*}
\int af \, d\mu &= \int \hat{f}_+ \, d\mu - \int \hat{f}_- \, d\mu\\
&= \begin{cases}
\int a f_+ \, d\mu - \int af_- \, d\mu & \text{if $a \geq 0$} \\
\int -a f_- \, d\mu - \int -af_+ \, d\mu & \text{if $a < 0$} \\
\end{cases}\\
&= a \int f_+ \, d\mu - a \int f_- \, d\mu = a \int f\, d\mu
\end{align*}  
The same construction and
observations are true with $g$ and $\hat{g}_\pm$.
Then $a f + b g = (\hat{f}_+ + \hat{g}_+) - (\hat{f}_- + \hat{g}_-)$
and we have
\begin{align*}
\int (a f + b g) \, d\mu &= \int (\hat{f}_+ + \hat{g}_+) \, d\mu -
\int (\hat{f}_- + \hat{g}_-) \, d\mu \\
&= \int \hat{f}_+ \, d\mu - \int \hat{f}_- \, d\mu + \int \hat{g}_+ \, d\mu -
\int \hat{g}_- \, d\mu \\
&= a \int f \, d\mu + b \int g \, d\mu
\end{align*}

To see monotonicity, observe that $f \geq g$ if and only if $f_+ \geq
g_+$ and $f_- \leq g_-$.
\end{proof}

Lastly, it is occasionally necessary to deal with integrating
measurable functions that are either infinite on a set of measure zero
or undefined on a set of measure zero.  This is permissible by virtue
of the following Lemma.
\begin{defn}Let $(\Omega,  \mathcal{A}, \mu)$ be a measure space.  We
  say that a property hold \emph{almost everywhere} if the set where
  the property does not hold has measure zero.
\end{defn}
\begin{lem}\label{ZeroIntegralImpliesZeroFunction}Let $f \geq 0$ be a measurable function on $(\Omega,
  \mathcal{A}, \mu)$.  $\int f \, d\mu = 0$ if and only if $f = 0$
  almost everywhere.
\end{lem}
\begin{proof}Clearly this is true by definition for indicator
  functions.  It also is true by positivity and linearity of integral
  for simple functions.  For arbitrary $f\geq 0$, we take and
increasing  approximating sequence of simple functions $f_n \uparrow
f$ and note that $\int f \, d\mu = 0$ and monotonicity of integral
implies $\int f \, d\mu = 0$ for each $n$.  Therefore, $f_n = 0$ almost
everywhere for each $n$ and therefore $f_n = 0$ almost everywhere for
all $n$ by taking a countable union.  This implies $f = 0$ almost
everywhere. 
If on the other hand we assume that $f = 0$ almost
everywhere, then by the increasing nature of $f_n$, we see that $f_n
=0$ for all $n$ almost everywhere and therefore $\int f_n \, d\mu = 0$
for every $n$.  By Monotone Convergence we see that $\int f \, d\mu = 0$.
\end{proof}
Therefore, for the definition of integrability of $f$ can be extended
to allow $f$ to be redefined arbitrarily on a set of measure zero.
 
We have the following limit theorem for limits of integrable
functions.
\begin{thm}\label{DCT}[Dominated Convergence Theorem]Suppose we are given $f, f_1, f_2, \dots$ and $g,g_1,g_2,
  \dots$ measurable functions on $(\Omega,
  \mathcal{A}, \mu)$ such that $\abs{f_n} \leq g_n$, $\lim_{n \to
    \infty} f_n = f$, $\lim_{n \to
    \infty} g_n = g$ and $\lim_{n \to \infty} \int g_n \, d \mu  =
  \int g \, d \mu < \infty$.  Then $\lim_{n \to \infty} \int f_n \,
  d\mu = \int f \, d\mu$.
\end{thm}
\begin{proof}The trick here is to notice that by our assumption, $g_n
  \pm f_n \geq 0$ and we can apply Fatou's Lemma to both sequences.
  Doing so we get
\begin{align*}
\int g \, d\mu \pm \int f \, d\mu &= \int \lim_{n \to \infty} g_n \,
d\mu \pm \int \lim_{n \to \infty}  f_n \, d\mu \\
&=\int \liminf_{n \to \infty} g_n \,
d\mu \pm \int \liminf_{n \to \infty}  f_n \, d\mu \\
&=\int \liminf_{n \to \infty} \left (g_n  \pm  f_n \right ) \, d\mu \\
&\leq \liminf_{n \to \infty} \int \left (g_n  \pm  f_n \right ) \, d\mu \\
&= \liminf_{n \to \infty} \int g_n \, d\mu + \liminf_{n \to \infty} \int \pm  f_n  \, d\mu \\
&= \int g \, d\mu + \liminf_{n \to \infty} \int \pm  f_n  \, d\mu \\
\end{align*}
Now subtract $\int g \, d\mu$ from both sides of the equation and we
get two inequalities $\pm \int f \, d\mu \leq \liminf_{n \to \infty}
\int \pm  f_n  \, d\mu $.  It remains to put these two inequalities
together 
\begin{align*}
\limsup_{n \to \infty} \int f_n \, d\mu &= -\liminf_{n \to \infty}
\int -f_n \, d\mu \\
&\leq \int f \, d\mu \\
&\leq \liminf_{n \to \infty} \int f_n \, d\mu \\
\end{align*}
and the result is proved by the
obvious fact that $\liminf f_n \leq \limsup f_n$.
\end{proof}
Most applications of Dominated Convergence only use the special case in
which the sequence $g_n$ is constant.  We call out this special case
as a corollary of the general theorem.
\begin{cor}Suppose we are given $f, f_1, f_2, \dots$ and $g$ measurable functions on $(\Omega,
  \mathcal{A}, \mu)$ such that $\abs{f_n} \leq g$, $\lim_{n \to
    \infty} f_n = f$ and $\int g \, d \mu < \infty$.  Then $\lim_{n \to \infty} \int f_n \,
  d\mu = \int f \, d\mu$.
\end{cor}
\begin{proof}Let $g_n = g$ for all $n>0$ and use Theorem \ref{DCT}.
\end{proof}

\begin{lem}\label{PushforwardMeasure}Suppose we are given a measure space $(\Omega, \mathcal{A},
  \mu)$, a measurable space $(S, \mathcal{S})$ and measurable
  function $f : \Omega \to S$.  The function $\pushforward{f}{\mu} (A) =
  \mu(f^{-1}(A))$ defines a measure on $(S, \mathcal{S})$.  The
  measure $\pushforward{f}{\mu}$ is called the \emph{push forward} of
  $\mu$ by $f$.
\end{lem}
\begin{proof}Clearly, $\pushforward{f}{\mu} (\emptyset) = \mu(\emptyset) =
  0$.  
If we are given disjoint $A_1, A_2, \dots$ then by and the fact that $\mu$ is a measure,
we know 
\begin{align*}
\pushforward{f}{\mu} \left (\bigcup_{i=1}^\infty A_i \right ) & =
\mu \left (\bigcup_{i=1}^\infty f^{-1}(A_i) \right ) & & \text{by Lemma
\ref{SetOperationsUnderPullback}} \\
& = \sum_{i=1}^\infty \mu (f^{-1}(A_i)) & & \text{by countable
  additivity of measure}\\
&=  \sum_{i=1}^\infty \pushforward{f}{\mu} (A_i) & & \text{by
  definition of push forward}
\end{align*}
\end{proof}

\begin{defn}For a probability space $(\Omega, \mathcal{A}, P)$, a
  measurable space $(S, \mathcal{S})$ and a
  random element $\xi : \Omega \to S$, the measure $\pushforward{\xi}{P}$ is
  called the \emph{distribution} or \emph{law} of $\xi$.  We often
  write $\mathcal{L}(\xi)$ for the law of $\xi$.
\end{defn}
\begin{lem}\label{ChangeOfVariables}[Change of Variables]Suppose we are given a measure space $(\Omega, \mathcal{A},
  \mu)$, a measurable space $(S, \mathcal{S})$, and measurable
  functions $f : \Omega \to S$ and $g : S \to \reals$, then 
\begin{align*}
\int (g \circ f) \, d \mu = \int g \, d (\pushforward{f}{\mu})
\end{align*}
Whenever either side of the equality exists, the other does and they
are equal.
\end{lem}
\begin{proof}
To begin with we assume that $g = \characteristic{A}$ for $A \in
\mathcal{S}$.  The first simple claim is that $\characteristic{A}
\circ f = \characteristic{f^{-1}(A)}$.  This is seen by unfolding
definitions for an $\omega \in \Omega$:
\begin{align*}
(\characteristic{A} \circ f)(\omega) &= \characteristic{A} (f(\omega)) \\
&= \begin{cases}
1 &\text{if $f(\omega) \in A$}\\
0 &\text{if $f(\omega) \notin A$}
\end{cases}\\
&= \begin{cases}
1 &\text{if $\omega \in f^{-1}(A)$}\\
0 &\text{if $\omega \notin f^{-1}(A)$}
\end{cases}\\
&= \characteristic{f^{-1}(A)}(\omega)
\end{align*}
Using this fact the result of the theorem follows for
$\characteristic{A}$ by another simple calculation
\begin{align*}
\int \characteristic{A} \, d (\pushforward{f}{\mu}) &= (\pushforward{f}{\mu})(A)
\\
&= \mu(f^{-1}(A)) \\
&= \int \characteristic{f^{-1}(A)} \, d\mu \\
&= \int (\characteristic{A} \circ f) \, d\mu
\end{align*}

Next we assume that $g = c_1 \characteristic{A_1} + \cdots + c_n
\characteristic{A_n}$ is a simple function. As a general property of
the linearity of composition of functions we can see that 
\begin{align*}
g \circ f
&= c_1 (\characteristic{A_1} \circ f) + \cdots + c_n (\characteristic{A_n} \circ f)
\end{align*}
Coupling this with the result for indicator functions and linearity of
integral we get
\begin{align*}
\int g \, d (\pushforward{f}{\mu}) &= \sum_{i=1}^n c_i \int \characteristic{A_i} \, d (\pushforward{f}{\mu})
\\
&= \sum_{i=1}^n c_i \int (\characteristic{A_i} \circ f) \, d \mu \\
&=  \int \sum_{i=1}^n c_i (\characteristic{A_i} \circ f) \, d \mu \\
&= \int (g \circ f) \, d\mu
\end{align*}

Next we suppose that $g$ is a positive measurable function.  We know
that we can find an increasing sequence of positive simple functions
$g_n \uparrow g$.  Note that $g \circ f$ is positive measurable, $g_n
\circ f$ is positive simple and $g_n \circ f \uparrow g \circ f$.  Now
can use the result proven for simple functions and Monotone
Convergence 
\begin{align*}
\int g \, d(\pushforward{f}{\mu}) &= \lim_{n \to \infty} \int g_n
\, d(\pushforward{f}{\mu}) & &\text{ by Monotone Convergence} \\
&= \lim_{n \to \infty} \int (g_n \circ f)
\, d\mu & &\text{ by result for simple functions} \\
&=\int (g \circ f)
\, d\mu & &\text{ by Monotone Convergence} \\
\end{align*}

The last step is to consider an integrable $g$.  Write it as $g = g_+
- g_-$ for $g_\pm$ positive and use linearity of the integral and the
result just proven for positive functions.
\end{proof}

\begin{defn}Suppose we are given a measure space $(\Omega, \mathcal{A},
  \mu)$ and a positive measurable function $f : \Omega \to \reals_+$.
  We define the measure $f \cdot \mu$ by the formula
\begin{align*}
(f \cdot \mu)(A) = \int \characteristic{A} \cdot f \, d \mu = \int_A f
\, d\mu
\end{align*}
If $\nu$ is a measure of the above form, then we say that $f$ is a
$\mu$\emph{-density} of $\nu$.
\end{defn}
\begin{lem}\label{ChainRuleDensity}Suppose we are given a measure space $(\Omega, \mathcal{A},
  \mu)$, a positive measurable function $f : \Omega \to \reals_+$ and
  and measurable function $g : \Omega \to \reals$, then 
\begin{align*}
\int f g \, d \mu = \int g \, d (f \cdot \mu)
\end{align*}
Whenever either side of the equality exists, the other does and they
are equal.
\end{lem}
\begin{proof}First assume that $g=\characteristic{A}$ is an indicator
  function.  The result is just the definition of the measure $f \cdot
  \mu$:
\begin{align*}
\int \characteristic{A} \, d(f \cdot \mu) = (f \cdot \mu)(A) = \int \characteristic{A}
\cdot f \, d\mu
\end{align*}
Next assume that $g = \sum_{i=1}^n c_i \characteristic{A_i}$ is a
simple function.  Then we can simply apply linearity of the integral
\begin{align*}
\int g \, d(f \cdot \mu) &=\sum_{i=1}^n c_i \int \characteristic{A_i}
\, d(f \cdot \mu)\\
 &=\sum_{i=1}^n c_i \int \characteristic{A_i}\cdot f
\, d\mu  \\
 &=\int g\cdot f
\, d\mu  \\
\end{align*}
For a positive measurable $g$ we pick an increasing approximation by simple
functions $g_n \uparrow g$.  We note that for positive $f$ we have 
 $g_n \cdot f$ positive (not necessarily simple) with $g_n \cdot f \uparrow g \cdot f$.  Thus,
\begin{align*}
\int g \, d(f \cdot \mu) &= \lim_{n \to \infty} \int g_n \, d(f \cdot
\mu) & &\text{definition of integral}\\
&= \lim_{n \to \infty} \int g_n \cdot f \, d \mu & &\text{by result for
  simple functions}\\
&= \int g \cdot f \, d \mu & &\text{by Monotone Convergence}\\
\end{align*}
The last step is to pick an integrable $g = g_+ - g_-$ and use
linearity of integral.  Note also that in this case the two integrals
in question are defined for exactly the same $g$.
\end{proof}

\subsubsection{Standard Machinery}
We've put together a collection of definitions and tools for
talking about integration and proving theorems about integration.
What is probably not clear at this point is that there are some very
useful patterns for how these defintitions, lemmas and theorems are
used.  One such pattern is so commonplace that I have heard it called
the \emph{standard machinery}.  
Suppose one wants to show a result about general measurable
functions.  A proof of the result using the standard machinery proceeds by 
\begin{itemize}
\item[(i)]Demonstrating the result for indicator functions.
\item[(ii)]Arguing by linearity that the result holds for simple functions.
\item[(iii)]Showing the result holds for non-negative measurable
  functions by approximating by an increasing limit of simple
  functions and using the Monotone Convergence Theorem.
\item[(iv)]Showing the result for arbitrary functions by expressing an
  arbitrary measurable function as a difference of non-negative
  measurable functions.
\end{itemize}
The proof of Lemma \ref{ChangeOfVariables} and Lemma
\ref{ChainRuleDensity} are examples of proofs
using the standard machinery.  It is a good idea to get very
comfortable with such arguments as it is quite common in many texts to
leave any such proof as an exercise for the reader.  An important refinement of the standard machinery involves using a
monotone class argument with the $\pi$-$\lambda$ Theorem to demonstrate the result for all indicator
functions.  Recall that to do that, one shows that the collection of
sets whose indicator functions satisfy the theorem is a
$\lambda$-system and to then prove the result a $\pi$-system of sets
such that the $\pi$-system generates the $\sigma$-algebra of the
measurable space.
\subsection{Products of Measurable Spaces}

Given a collection of measurable spaces there is a standard
construction that makes the cartesian product of the spaces into a
measurable space.
\begin{defn}Suppose we are given an index set $T$ and for each $t \in
  T$ we have a measurable space $(\Omega_t, \mathcal{A}_t)$.  The
  \emph{product} $\sigma$\emph{-algebra} $\bigotimes_t \mathcal{A}_t$ on the cartesian product
  $\prod_t \Omega_t$ is the $\sigma$-algebra
  generated by all one dimensional \emph{cylinder sets} $A_t \times
  \prod_{s \neq t} \Omega_s$ for $A_t \in \mathcal{A}_t$.
\end{defn}

TODO: Show that this is the smallest $\sigma$-algebra that make the
projections measurable

TODO: Show that the product of Borel $\sigma$-algebras is the Borel
$\sigma$-algebra with respect to the product topology in the separable
case.  Note that the non-separable case is more subtle and in fact
turns out to be important (especially in statistics)!

The following is an important scenario that we shall often encounter.
Suppose we have a measurable space $(\Omega, \mathcal{A})$ and a
collection of measurable functions $f_t : \Omega \to (S_t,
\mathcal{S}_t)$.  From a purely set-theoretic point of view this
specification of functions is in fact
equivalent to the specification of a single function $f : \Omega \to
\prod_t S_t$ (i.e. if we let $\pi_s : \prod_t S_t \to S_s$ be the
projections then we define $\pi_s(f(\omega)) = f_s(\omega)$).  

\begin{lem}Given a collection of measurable functions $f_t : \Omega
  \to S_t$ and the equivalent function $f : \Omega \to \prod_t S_t$ we
  have $\sigma(\bigwedge_t \sigma(f_t)) = \sigma(f)$.
\end{lem}
\begin{proof}
To see that $\sigma(\bigwedge_t \sigma(f_t)) \subset \sigma(f)$ it
suffices to show that $\sigma(f_t) \subset \sigma(f)$ for all $t \in
T$.  This follows since for any $A_t \in \mathcal{S}_t$, we have
$f_t^{-1}(A) = f^{-1}(A \times  \prod_{s \neq t} \Omega_s)$.  This
fact also shows that $\sigma(f) \subset \sigma(\bigwedge_t
\sigma(f_t))$ since the cylinder sets $A \times  \prod_{s \neq t}
\Omega_s$ generate $\otimes_t \mathcal{S}_t$ by Lemma \ref{MeasurableByGeneratingSet}.
\end{proof}

\subsection{Outer Measures and Lebesgue Measure on the Real Line}
To construct Lebesgue measure on the real line, one proceeds by
demonstrating that one may construct a measure by first constructing a more
primitive object called an outer measure and then proving that outer
measure become measures when restricted to an appropriate collection
of sets.  Having redefined the problem as the construction of outer
measure, one constructs outer measure on real line in a hands on way.

Much of this process that has broader applicability than just the real line,  therefore we state and prove the results
in the more general case.
TODO: Come up with some intuition about outer measure (more
specifically Caratheodory's characterizaiton of sets measurable with
respect to an outer measure; it says in some sense that a measurable
set and its complement have aren't \emph{too} entangled with one another).
\begin{defn}Given a set $\Omega$, an \emph{outer measure} is a
positive  function $\mu : 2^\Omega \to \overline{\reals}_+$ satisfying
\begin{itemize}
\item[(i)] $\mu(\emptyset) = 0$
\item[(ii)] If $A \subset B$, then $\mu(A) \leq \mu(B)$
\item[(iii)] Given $A_1, A_2, \dots \subset \Omega$, then $\mu \left
    (\bigcup_{i=1}^\infty A_i \right ) \leq \sum_{i=1}^\infty \mu(A_i)$.
\end{itemize}
\end{defn}

\begin{defn}Given a set $\Omega$ with outer measure $\mu$, we say a
  set $A \subset \Omega$ is $\mu$\emph{-measurable} if for every $B
  \subset \Omega$,
\begin{align*}
\mu(B) = \mu(A \cap B) + \mu(A^c \cap B)
\end{align*}
\end{defn}

\begin{rem}For every $A,B \subset \Omega$, we have from finite
  subadditivity of outer measure
\begin{align*}
\mu(B) = \mu((A \cap B) \cup (A^c \cap B)) \leq \mu(A \cap B) + \mu(A^c \cap B)
\end{align*}
and therefore to show $\mu$-measurability we only need to show the
reverse inequality.
\end{rem}

\begin{lem}\label{CaratheodoryRestriction}Given a set $\Omega$ with an outer measure $\mu$, let
$\mathcal{A}$ be the collection of $\mu$-measurable sets.  Then
$\mathcal{A}$ is a $\sigma$-algebra and the
  restriction of $\mu$ to $\mathcal{A}$ is a measure.
\end{lem}
\begin{proof}We first note that $A \in \mathcal{A}$ if and only if $A^c
  \in \mathcal{A}$ since the defining condition of $\mathcal{A}$ is
  symmetric in $A$ and $A^c$.

Next we show $\emptyset \in \mathcal{A}$.  To see this,
  take $B \subset \Omega$,
\begin{align*}
\mu(B) &= \mu(\emptyset) + \mu(B) & &\text{since $\mu(\emptyset) = 0$}
\\
& = \mu(\emptyset \cap B) + \mu(B \cap \Omega)
\end{align*}

Next we show that $\mathcal{A}$ is closed under finite intersection.
Pick $A, B \in \mathcal{A}$ and $E \subset \Omega$ and calculate
\begin{align*}
\mu(E) &= \mu(E \cap A) + \mu(E \cap A^c) & &\text{since $A \in
  \mathcal{A}$} \\
&= \mu(E \cap A \cap B) + \mu(E \cap A \cap B^c) +\mu(E \cap A^c) & &\text{since $B \in
  \mathcal{A}$} \\
&\geq \mu(E \cap (A \cap B)) + \mu(E \cap A \cap B^c \cup E \cap A^c)
& & \text{by subadditivity} \\
&\geq \mu(E \cap (A \cap B)) + \mu(E \cap (A \cap B)^c)
& & \text{by monotonicity of $\mu$} \\
\end{align*}
and we have noted that it suffices to show this inequality to show $A
\cap B \in \mathcal{A}$.  Now by De Morgan's Law we conclude that
$\mathcal{A}$ is closed under finite union.

Now we turn to consider the behavior of $\mu$ and show that $\mu$ is
finitely and countably additive over disjoint unions; in fact we show
a bit more.
We let $A,B \in \mathcal{A}$ and let $E \subset \Omega$
be disjoint.
\begin{align*}
\mu(E \cap (A \cup B)) &= \mu(E \cap (A \cup B) \cap A) + \mu(E \cap
(A \cup B) \cap A^c) & & \text{since $A \in \mathcal{A}$} \\
&= \mu(E \cap A) + \mu(E \cap B) & &\text{by set algebra}
\end{align*}
It is easy to see that one can do induction to extend the above result
to all finite disjoint unions.
Now let $A_1, A_2, \dots \in \mathcal{A}$ and $E \subset \Omega$.
Define $U_n = \bigcup_{i=1}^n A_i$ and $U = \bigcup_{i=1}^\infty A_i$.
\begin{align*}
\mu(E \cap U) &\geq \mu(E \cap U_n) & & \text{by monotonicity} \\
&= \sum_{i=1}^n \mu(E \cap A_i) & &\text{by finite additivity and
  disjointness of $A_i$}
\end{align*}
Now take the limit we have $\mu(E \cap U) \geq \sum_{i=1}^\infty \mu(E
\cap A_i)$.  Applying subadditivity of $\mu$ we get the opposite
inequality and we have shown 
\begin{align*}
\mu(E \cap \bigcup_{i=1}^\infty A_i) = \sum_{i=1}^\infty \mu(E
\cap A_i)
\end{align*}
In particular, we can take $E=\Omega$ to show that $\mu$ is countably
additive over disjoint unions.

Having shown how to calculate $\mu$ over countable disjoint unions, we
can show that $U \in \mathcal{A}$. For every $n > 0$,
\begin{align*}
\mu(E) &= \mu(E \cap U_n) + \mu(E \cap U_n^c) \\
&\geq \sum_{i=1}^n \mu(E \cap A_i) + \mu(E \cap U) & & \text{by
  subadditivity and monotonicity} \\
\end{align*}
Take the limit and use the previous claim to see
\begin{align*}
\mu(E) &\geq \sum_{i=1}^\infty \mu(E \cap A_i) + \mu(E \cap U) \\
&= \mu(E \cap U) + \mu(E \cap U^c)
 \end{align*}
thereby showing $U \in \mathcal{A}$.

The last thing to show is that a countable union of elements of
$\mathcal{A}$ are in $\mathcal{A}$.  This follows from what we have
shown about countable
disjoint unions since we have already proven this for complements, finite unions
and intersections and therefore for any $A_1,A_2, \dots$ we can define
$B_n = A_n \setminus \bigcup_{i=1}^{n-1} A_i$ so that
$\bigcup_{i=1}^\infty A_i =\bigcup_{i=1}^\infty B_i$ with the $B_i$ disjoint.
\end{proof}

To define \emph{Lebesgue measure} on $\reals$ we will leverage the
construction above and first define an outer measure by approximating
by intervals.  Given an interval $I \subset \reals$, let $\abs{I}$ be
length of $I$.
\begin{thm}\label{LebesgueMeasure}[Lebesgue Measure]There exists a
  unique measure $\lambda$ on $(\reals, \mathcal{B}(\reals)$ such that
  $\lambda(I) = \abs{I}$ for all intervals $I \subset \reals$.
\end{thm}
Before we begin the proof of the theorem we need first construct an
outer measure.
\begin{lem}\label{LebesgueOuterMeasure}[Lebesgue Outer Measure]Define the function $\lambda : 2^\reals \to \reals$ defined by 
\begin{align*}
\lambda(A) &= \inf_{\{I_k\}} \sum_k \abs{I_k} 
\end{align*}
where the infimum ranges over countable covers of $A$ by intervals.
Then $\lambda$ is
an outer measure.  In addition, $\lambda(I) = \abs{I}$ for every
interval $I \subset \reals$.
\end{lem}
\begin{proof}It is clear that $\lambda$ is positive and $\lambda(\emptyset) = 0$.  It is also
  clear that $\lambda$ is increasing since for any $A \subset B
  \subset \reals$ any cover of $B$ is also a cover of $A$.  

To see subadditivity, take $A_1, A_2, \dots \subset \reals$.  Pick
$\epsilon > 0$ and then for each $A_n$ we take a countable cover by intervals
$I_{n1}, I_{n2}, \dots$ such that $\lambda(A_n) \geq \sum_{k=1}^\infty
\abs{I_{nk}} - \frac{\epsilon}{2^n}$.  Then, the collection of
intervals $I_{nk}$ for $n,k > 0$ is an countable cover of
$\bigcup_{i=1}^\infty A_i$ and therefore
\begin{align*}
\lambda \left (\bigcup_{i=1}^\infty A_i \right ) &\leq
\sum_{n=1}^\infty \sum_{k=1}^\infty \abs{I_{nk}} \\
&\leq \sum_{n=1}^\infty \left ( \lambda(A_n) + \frac{\epsilon}{2^n}
\right )\\
&= \sum_{n=1}^\infty \lambda(A_n) + \epsilon
\end{align*}
Now let $\epsilon \to 0$ and we have proven subadditivity.

To prove that $\lambda(I) = \abs{I}$, we first consider intervals of
the form $I = [a,b]$ with $a < b$.  The family of intervals $(a -
\epsilon, b+\epsilon)$ for $\epsilon > 0$ shows that $\lambda{I} \leq
\abs{I}$ so we only need to show the opposite inequality.
Suppose we are given a countable cover by open intervals $I_1, I_2,
\dots$.  We need to show that $\abs{I} \leq \sum_{k=1}^\infty \abs{I_k}$.
By the Heine-Borel Theorem (Theorem \ref{HeineBorel}), there
is a finite subcover $I_1, \dots, I_n$ and it suffices to show
that $\abs{I} \leq \sum_{k=1}^n \abs{I_k}$ for the finite subcover.

For finite covers we
can proceed by induction.  To begin, consider a cover by a single
interval.  For any $J \supset I$ we know that
$\abs{J} \geq \abs{I}$.

For the induction step, assume that $\inf_{\{I_k\}} \sum_{k=1}^n
\abs{I_k} = \abs{I}$ where the infimum is over covers by $n$
intervals.  Take a cover of $I$ by $n+1$ intervals $I_1, \dots,
I_{n+1}$.  There exists an $I_k$ such that $b \in I_k$.  If we write
$I_k = (a_k,b_k)$, then the rest of the $I_j$ form a cover of
$[a,a_k]$.
\begin{align*}
\abs{I} &= (b-a_k) + (a_k - a) \\
&\leq \abs{I_k} + \sum_{m \neq k} \abs{I_m} & &\text{by induction
  hypothesis applied to $[a,a_k]$} \\
&=\sum_m \abs{I_m}
\end{align*}
It remains to eliminate the restriction to bounded closed intervals.
Clearly every cover of 
$[a,b]$ by open intervals is a cover of $(a,b)$.  On the other hand, every countable cover of $(a,b)$ can be extended to a countable cover of $[a,b]$ by adding
at most two arbitrarily small intervals of the form
$(a-\epsilon,a+\epsilon)$ and $(b-\epsilon,b+\epsilon)$.  An
\emph{epsilon of room} argument shows that $\lambda (a,b) = \lambda
[a,b]$.  Monotonicity of $\lambda$ shows the same is true for half
open intervals.


TODO: Show that outer measure of infinite intervals is infinite.
\end{proof}

\begin{defn}A subset $A \subset \reals$ is \emph{Lebesgue measurable}
  if $A$ is $\lambda$-measurable with respect to the Lebesgue outer measure.
\end{defn}

\begin{lem}\label{BorelsAreLebesgueMeasurable}Every Borel measurable $A \subset \reals$ is also Lebesgue measurable.
\end{lem}
\begin{proof}Since we know that the collection of Lebesgue measurable
  sets is a $\sigma$-algebra, and we know that the Borel algebra on
  $\reals$ is generated by intervals of the form $(-\infty, x]$, it
  suffices to show that each such interval is Lebesgue measurable.

Take an interval $I=(-\infty, x]$, a set $E \subset \reals$ and
$\epsilon > 0$.  Pick a countable covering $I_1, I_2, \dots$ of $E$ by
open intervals so that
$\lambda(E) + \epsilon \geq \sum_{k=1}^\infty \abs{I_k} $.
\begin{align*}
\lambda(E) + \epsilon &\geq \sum_{k=1}^\infty \abs{I_k} \\
&=\sum_{k=1}^\infty \abs{I_k \cap I}  + \sum_{k=1}^\infty \abs{I_k \cap I^c} \\
&=\sum_{k=1}^\infty \lambda(I_k \cap I) + \sum_{k=1}^\infty
\lambda(I_k \cap I^c) \\
&\geq \lambda \left ( \bigcup_{k=1}^\infty I_k \cap I \right )
+\lambda \left ( \bigcup_{k=1}^\infty I_k \cap I^c \right ) &
&\text{by subadditivity} \\
&\geq \lambda(E \cap I) + \lambda(E \cap I^c)
\end{align*}
where the last line holds because $I_k \cap I$ is a countable cover of
$E \cap I$ and similarly for $E \cap I^c$.  Now let $\epsilon \to 0$
to get the result.

TODO: Actually $I_k \cap I$ are half open intervals.  The proof needs
to be extended 
to handle this fact.  Presumably an $\frac{\epsilon}{2^n}$ argument
works here.  Note most definitions of Lebesgue outer measure do not
restrict to open covers (then you have to pay the cost of the
$\frac{\epsilon}{2^n}$ argument to apply Heine Borel).
\end{proof}

\begin{lem}[Uniqueness of measure]\label{UniquenessOfMeasure}Let $(\Omega, \mathcal{A}, \mu)$ be a measure space with
  $\mu$ a finite measure.  Suppose $\nu$ is a finite
  measure on
  $(\Omega, \mathcal{A})$ such that there is a $\pi$-system
  $\mathcal{C}$ such that $\sigma(\mathcal{C})=\mathcal{A}$, $\Omega \in \mathcal{C}$ and for all $A \in
  \mathcal{C}$ we have $\mu(A) = \nu(A)$, then $\mu=\nu$.

If we assume that $\mu$ a $\sigma$-finite measure and $\nu$ is a 
$\sigma$-finite measure such that there exists a partition $\Omega =
\Omega_1 \cup \Omega_2 \cup \cdots$ with $\mu(\Omega_n) =
\nu(\Omega_n) < \infty$, the result holds as well.
\end{lem}
\begin{proof}
First we assume that $\mu$ (and then by hypothesis $\nu$) is finite.
We apply a monotone class
argument.  Consider the collection $\mathcal{D}$ of $A \in
\mathcal{A}$ such that $\mu(A) = \nu(A)$.  We claim that
this collection is a $\lambda$-system.  Since we have assumed
$\mu(\Omega) = \nu(\Omega)$ we have that $\Omega \in \mathcal{D}$.  Now suppose $A
\subset B \in \mathcal{D}$.  By additivity of measure and finiteness
of $\mu$ and $\nu$,
\begin{align*}
\mu(B \setminus A) = \mu(B) - \mu(A) = \nu(B) -\nu(A) =
\nu(B \setminus A)
\end{align*}
Now we assume $A_1 \subset A_2 \subset \cdots \in \mathcal{D}$.  By
continuity of measure (Lemma \ref{ContinuityOfMeasure}) 
\begin{align*}
\mu \left ( \bigcup_i A_i \right ) = \lim_{n \to \infty} \mu(A_i) = \lim_{n \to \infty} \nu(A_i) =
\nu \left ( \bigcup_i A_i \right )
\end{align*}
Application of the $\pi$-$\lambda$ Theorem (Theorem
\ref{MonotoneClassTheorem}) together with the fact that
$\sigma(\mathcal{C}) = \mathcal{A}$ shows that equality holds on all
of $\mathcal{A}$.

Now we handle to the $\sigma$-finite case.  We a partition
$\Omega=\Omega_1 \cup \Omega_2 \cup \cdots$ such that $\mu(\Omega_n) =
\nu(\Omega_n) < \infty$ for all $n$.  Denote $\mu_n$ and
$\nu_n$ the restriction of $\mu$ and $\nu$ to the set $\Omega_n$.  We
note that $\mu_n$ and $\nu_n$ each satisfy the hypothesis of the lemma
for the finite measure case (e.g. $\mu_n(A) = \mu(\Omega_n \cap A)$).  Therefore
we can conclude that $\mu_n = \nu_n$ on all of $\mathcal{A}$ for all
$n$.  For any
$A \in \mathcal{A}$ define $A_n = \cup_{k=1}^n \Omega_k \cap A$ note
that 
\begin{align*}
\mu (A_n) = \sum_{k=1}^n \mu_k(A) = \sum_{k=1}^n \nu_k(A)  = \nu(A_n)
\end{align*}
and $A_1 \subset A_2 \subset \cdots $ with $\cup_{n=1}^\infty A_n =
A$.  Now apply continuity of measure (Lemma \ref{ContinuityOfMeasure}) to see that $\mu(A)=\nu(A)$.
\end{proof}

TODO: Do we need to assume that there is a partition with
$\mu(\Omega_n) = \nu(\Omega_n)$ or can it be derived from the fact
that $\sigma(\mathcal{C}) = \mathcal{A}$.  Is suspect it can be
derived but the applications we have in mind it is trivial to generate
the partition by hand.

 Now we are ready to prove the existence and uniqueness of Lebesgue
measure (Theorem \ref{LebesgueMeasure}).

\begin{proof}The existence of Lebesgue measure clearly follows from
  Lemma \ref{CaratheodoryRestriction} applied to the outer measure
  constructed in Lemma \ref{LebesgueOuterMeasure}.  The fact that the
  $\sigma$-algebra of the restriction contains the Borel sets follows
  from Lemma \ref{BorelsAreLebesgueMeasurable}.

It remains to show uniqueness.  Now clearly the collection of intervals is closed under finite
intersections hence is a $\pi$-system that generates
$\mathcal{B}(\reals)$.  Furthermore, $\reals =
\cup_{n=-\infty}^\infty (n, n+1]$ so we may apply Lemma
\ref{UniquenessOfMeasure} to get uniqueness.
\end{proof}

\begin{defn}A measure space $(\Omega, \mathcal{A}, \mu)$ is
  \emph{$\sigma$-finite} if there exists a countable partition $\Omega
  = \Omega_1 \cup \Omega_2 \cup \cdots$ such that $\mu(\Omega_i) < \infty$.
\end{defn}

\subsubsection{Abstract Version of Caratheodory Extension}
The construction of Lebesgue measure we have given actually has a
broad generalization which we present here.

\begin{defn}A non-empty collection $\mathcal{A}_0$ of subsets of a set
  $\Omega$ is called a \emph{Boolean  algebra} if given any $A, B
  \in \mathcal{A}_0$ we have
\begin{itemize}
\item[(i)]$A^c \in \mathcal{A}_0$
\item[(ii)]$A \cup B \in \mathcal{A}_0$
\item[(iii)]$A \cap B \in \mathcal{A}_0$
\end{itemize}
\end{defn}

Note that it is trivial induction argument to extend the closure
properties to arbitrary finite unions and intersections.

\begin{defn}A \emph{pre-measure} on a Boolean algebra $(\Omega, \mathcal{A}_0)$ is
  a function $\mu_0 : \mathcal{A}_0 \to \overline{\reals}_+$ such that 
\begin{itemize}
\item[(i)]$\mu_0(\emptyset) = 0$
\item[(ii)]For any $A_1, A_2, \dotsc \in \mathcal{A}_0$ such that the
  $A_n$ are disjoint and 
  $\cup_{n=1}^\infty A_n \in \mathcal{A}_0$, we have
  $\mu_0(\cup_{n=1}^\infty A_n) = \sum_{n=1}^\infty \mu_0(A_n)$.
\end{itemize}
\end{defn}

\begin{lem}A pre-measure is finitely additive and montonic.  That is to say given
  any disjoint $A_1, \dotsc, A_n \in \mathcal{A}_0$ we have
  $\mu_0(\cup_{i=1}^n A_i) = \sum_{i=1}^n \mu_0(A_i)$ and given $A
  \subset B$ with $A, B \in \mathcal{A}_0$, we have $\mu_0(A) \leq \mu_0(B)$.
\end{lem}
\begin{proof}
Finite additivity follows by extending the finite sequence to an infinite sequence by appending
copies of the emptyset and using the fact that $\mu_0(\emptyset)=0$.  Monotonicity follows from finite additivity by writing $B = A \cup
B\setminus A$ so that $\mu_0(B) = \mu_0(A) + \mu_0(B\setminus A) \geq \mu_0(A)$.
\end{proof}

Our goal is to show that any pre-measure on a Boolean algebra
$\mathcal{A}_0$ may be
extended to a measure on a $\sigma$-algebra containing
$\mathcal{A}_0$.  We proceed in four steps
\begin{itemize}
\item[1)] Define an outer measure $\mu^*$ from $\mu_0$
\item[2)] Show that all sets in $\mathcal{A}_0$ are $\mu^*$-measurable.
\item[3)] Show that for all sets $A \in \mathcal{A}_0$, $\mu^*(A) = \mu_0(A)$.
\item[4)] Use the Caratheodory restriction to create a
  $\sigma$-algebra and measure.
\end{itemize}

\begin{lem}\label{PremeasureToOuterMeasure}Given a pre-measure $\mu_0$ on a Boolean algebra $(\Omega,
  \mathcal{A}_0)$ then the set function $\mu^* : 2^\Omega \to
  \overline{\reals}_+$ defined by
\begin{align*}
\mu^*(A) &= \inf \lbrace \sum_{n=1}^\infty \mu_0(A_n) \mid A \subset
\cup_{n=1}^\infty A_n \text { and } A_n \in \mathcal{A}_0 \text{ for
  all $n$} \rbrace
\end{align*}
is an outer measure.
\end{lem}
\begin{proof}
Because $\mu_0(\emptyset)$ and $\emptyset \subset \emptyset$ we see
that $\mu^*(\emptyset) = 0$.

Suppose we are given $A \subset B$.  Then if we have a cover $B
\subset \cup_{n=1}^\infty B_n$ where $B_n \in \mathcal{A}_0$, then
this is also a cover of $A$.  Therefore $\mu^*(A)$ is an infimum over
a larger collection of covers than that used in calculating $\mu^*(B)$
hence $\mu^*(A) \leq \mu^*(B)$ (we could actually pick an $\epsilon$
and an approximating cover as below then let $\epsilon \to 0$).

Now to show subadditivity.  Let $A_1, A_2, \dotsc$ be a sequence of
arbitrary subsets of $\Omega$.  If any $\mu^*(A_n) = \infty$ then we
automatically know $\mu^*(\cup_{n=1}^\infty A_n) \leq \sum_{n=1}^\infty \mu^*(A_n)$, so we
may assume that all $\mu^*(A_n) < \infty$.  Let $\epsilon > 0$ be
given and for each $n$ we pick $B_{1n}, B_{2n}, \dotsc$ such that $A_n
\subset \cup_{m=1}^\infty B_{mn}$ and $\sum_{m=1}^\infty \mu_0(B_{mn})
\leq \mu^*(A_n) + \frac{\epsilon}{2^n}$.  Now, we also have that
$\cup_{n=1}^\infty A_n
\subset  \cup_{n=1}^\infty \cup_{m=1}^\infty B_{mn}$ and therefore we
know that $\mu^*(\cup_{n=1}^\infty A_n) \leq \sum_{n=1}^\infty \sum_{m=1}^\infty
\mu_0(B_{mn}) \leq \sum_{n=1}^\infty \mu^*(A_n) + \epsilon$.  Since
$\epsilon$ was arbitrary, we have $\mu^*(\cup_{n=1}^\infty A_n) \leq \sum_{n=1}^\infty
\mu^*(A_n)$ so subadditivity is proven.
\end{proof}

\begin{lem}\label{PremeasureBooleanAlgebraOuterMeasurable}Given a pre-measure $\mu_0$ on a Boolean algebra $(\Omega,
  \mathcal{A}_0)$ and the outer measure $\mu^*$ constructed in Lemma
  \ref{PremeasureToOuterMeasure}, if $A \in \mathcal{A}_0$ then $A$ is
  $\mu^*$-measurable.
\end{lem}
\begin{proof}
Let $A \in \mathcal{A}_0$ and $B \subset \Omega$ and we have to show
$\mu^*(B) \geq \mu^*(A \cap B) + \mu^*(A^c \cap B)$. 
Pick $B_1, B_2, \dotsc$ such that $B_n \in \mathcal{A}_0$ for all $n$
and $\sum_{n=1}^\infty \mu_0(B_n) \leq \mu^*(B) + \epsilon$.  By
finite additivity of $\mu_0$ and the fact that $A, B_n \in \mathcal{A}_0$,
we can write $\mu_0 (B_n) = \mu_0(A \cap B_n) + \mu_0(A^c \cap B_n)$
and therefore $\sum_{n=1}^\infty \mu_0(A \cap B_n) + \sum_{n=1}^\infty
\mu_0(A^c \cap B_n) \leq \mu^*(B) + \epsilon$.  On the other hand, we
know that $A \cap B \subset \cup_{n=1}^\infty A \cap B_n$ so $\mu^*(A
\cap B) \leq \sum_{n=1}^\infty \mu_0( A \cap B_n)$ and similarly with
$A^c$.  Therefore $\mu^*(A \cap B) + \mu^*(A^c \cap B)\leq \mu^*(B) + \epsilon$.
Take the limit as $\epsilon$ goes to zero and we are done.
\end{proof}

\begin{lem}\label{PremeasureOuterMeasureEqual}Given a pre-measure $\mu_0$ on a Boolean algebra $(\Omega,
  \mathcal{A}_0)$ and the outer measure $\mu^*$ constructed in Lemma
  \ref{PremeasureToOuterMeasure}, if $A \in \mathcal{A}_0$ then $\mu^*(A)=\mu_0(A)$.
\end{lem}
\begin{proof}
Suppose we are given $A \in \mathcal{A}_0$.  Since $A$ is a singleton
cover of itself, we know that $\mu^*(A) \leq \mu_0(A)$.  It remains to
show $\mu_0(A) \leq \mu^*(A)$.  If $\mu^*(A) =\infty$ then this is
trivally true so we may assume $\mu^*(A) < \infty$.  Let $\epsilon
>0$ be given and pick $A_1, A_2, \dotsc \in \mathcal{A}_0$ such that
$A \subset \cup_{n=1}^\infty A_n$ and 
$\sum_{n=1}^\infty \mu_0(A_n) \leq \mu^*(A) + \epsilon$.  Our goal now
is to shrink each of the $A_n$ so that we wind up with a partition of
$A$.  Then we will be able to apply the countable additivity of pre-measures.

First, we convert the cover by $A_n$ into a disjoint cover of $A$.  Let
$B_1 = A_1$ and then define $B_n = A_n \setminus (A_1 \cup \cdots \cup
A_{n-1})$ for $n>1$.
By construction, the $B_n$ are disjoint and $\cup_{i=1}^n B_i =
\cup_{i=1}^n A_i$.  Furthermore $B_n \subset A_n$ so by monotonicity
of $\mu_0$ we have $\mu_0(B_n) \leq \mu_0(A_n)$.  Now have $A \subset
\cup_{n=1}^\infty B_n$ with $B_n$ disjoint, $B_n \in \mathcal{A}_0$
for all $n$  and $\sum_{n=1}^\infty
\mu_0(B_n) \leq \mu^*(A) + \epsilon$.  

Lastly we convert the disjoint cover $B_n$ into a partitioning of $A$.  
Consider $C_n = B_n \cap A$.  We still have $C_n \in
\mathcal{A}_0$, $C_n$ disjoint and montonicity implies $\sum_{n=1}^\infty
\mu_0(C_n) \leq \mu^*(A) + \epsilon$.  But now we have
$\cup_{n=1}^\infty C_n = A \in \mathcal{A}_0$ so we may apply
countable additivity of premeasure to conclude $\mu_0(A) = \sum_{n=1}^\infty
\mu_0(C_n) \leq \mu^*(A) + \epsilon$.  Once again, $\epsilon$ was
arbitrary so let it go to zero and we are done.
\end{proof}


TODO: construction that takes us from a semiring to a Boolean algebra.
It is often convenient to start a construction of a measure with a
collection of sets that is so small that it doesn't even form a
Boolean algebra.  For example when constructing Lebesgue measure on
$\reals$ we were really motivated by a desire that the measure of an
interval $(a,b]$ should be $b-a$, yet the set of such intervals on
$\reals$ is not a Boolean algebra.
\begin{defn}A set $\mathcal{D} \subset 2^\Omega$ is called a
  \emph{semiring} if 
\begin{itemize}
\item[(i)]$\emptyset \in \mathcal{D}$
\item[(ii)]if $A, B \in \mathcal{D}$ then $A \cap B \in \mathcal{D}$
\item[(iii)]if $A, B \in \mathcal{D}$ then there exist disjoint $C_1,
  \dotsc, C_n \in \mathcal{D}$ such that $A \setminus B = \cup_{j=1}^n
  C_j$
\end{itemize}
\end{defn}

\begin{examp}The set of intervals $(a,b]$ is a semiring.
\end{examp}

TODO: Other constructions of semirings (e.g. products)

\begin{defn}A set $\mathcal{R} \subset 2^\Omega$ is called a
  \emph{ring} if 
\begin{itemize}
\item[(i)]$\emptyset \in \mathcal{R}$
\item[(ii)]if $A, B \in \mathcal{R}$ then $A \cup B \in \mathcal{R}$
\item[(iii)]if $A, B \in \mathcal{R}$ then $A \setminus B \in \mathcal{R}$
\end{itemize}
\end{defn}

\begin{lem}If $\mathcal{D}$ is a semiring then $\mathcal{R} = \lbrace
  \cup_{j=1}^n C_j \mid C_j \in \mathcal{D} \text{ and the $C_j$ are
    disjoint} \rbrace$ is a ring.  Furthermore it is the smallest ring
  containing $\mathcal{D}$.
\end{lem}
\begin{proof}
The fact that $\emptyset \in \mathcal{R}$ is immediate.  Suppose we
are given $\cup_{i=1}^n A_i$ and $\cup_{j=1}^m B_j$ in
$\mathcal{R}$. Then we have
\begin{align}
\left ( \cup_{i=1}^n A_i \right ) \cap \left ( \cup_{j=1}^m B_j \right
) &= \cup_{i=1}^n \cup_{j=1}^m A_i \cap B_j
\end{align}
which is in $\mathcal{R}$ because each $A_i \cap B_j \in \mathcal{D}$
and they are disjoint by the disjointness since each of $A_i$ and
$B_j$ is a disjoint set of sets.  

We also have 
\begin{align}
\left ( \cup_{i=1}^n A_i \right ) \setminus \left ( \cup_{j=1}^m B_j \right
) &=\left ( \cup_{i=1}^n A_i \right ) \cap \left ( \cup_{j=1}^m B_j \right
)^c \\
&=\cup_{i=1}^n \cap_{j=1}^m A_i \cap B_j^c \\
&=\cup_{i=1}^n \cap_{j=1}^m A_i \setminus B_j
\end{align}
and we know that each $A_i \setminus B_j \in \mathcal{D}$ and we know
that $\mathcal{D}$ is closed under finite intersections thus
$\cap_{j=1}^m A_i \setminus B_j \in \mathcal{D}$.  Furthermore by
disjointness of $A_i$ we have that $\cap_{j=1}^m A_i \setminus B_j$
are disjoint and therefore we have shown that $\left ( \cup_{i=1}^n A_i \right ) \setminus \left ( \cup_{j=1}^m B_j \right
) \in \mathcal{R}$.

To see that $\mathcal{R}$ is the smallest ring containing
$\mathcal{D}$ note simply that it is a ring and any ring containing
$\mathcal{D}$ must contain all of the finite disjoint unions of
elements in $\mathcal{D}$.
\end{proof}

To connect up the concept of rings with that of Boolean algebras we
have the following result.
\begin{lem}Let $\mathcal{R}$ be a ring and define $\mathcal{R}^c =
  \lbrace A^c \mid A \in \mathcal{R}\rbrace$.  Then $\mathcal{A} =
  \mathcal{R} \cup \mathcal{R}^c$ is a Boolean algebra and is the
  Boolean algebra generated by $\mathcal{R}$.  If $\mathcal{R}$ is a
  $\sigma$-ring then $\mathcal{R} \cup \mathcal{R}^c$ is the
  $\sigma$-algebra generated by $\mathcal{R}$.
\end{lem}
\begin{proof}Since Boolean algebras are closed under set complement it
  suffices to show that $\mathcal{A} = \mathcal{R} \cup \mathcal{R}^c$ is a Boolean
  algebra (respectively $\sigma$-algebra).  Closure under set
  complement is immediate from construction.  Closure under set
  intersection follows from handling the three possible cases
\begin{itemize}
\item[(i)]if $A, B \in \mathcal{R}$ then $A\cap B \in \mathcal{R}
  \subset \mathcal{A}$ since $\mathcal{R}$ is a ring.
\item[(ii)]if $A \in \mathcal{R}$ and $B \in \mathcal{R}^c$ then
  $A\cap B = A \cap (B^c)^c = A \setminus B^c \in \mathcal{R}
  \subset \mathcal{A}$ since $B^c \in \mathcal{R}$ and $\mathcal{R}$ is a ring.
\item[(iii)]if $A, B \in \mathcal{R}^c$ then $A \cap B = (A^c \cup
  B^c)^c \in \mathcal{R}^c
  \subset \mathcal{A}$ since $A^c, B^c \in \mathcal{R}$ and $\mathcal{R}$ is a ring.
\end{itemize}
Closure under finite set union follows as usual from De Morgan's Law.

Now if $\mathcal{R}$ is a $\sigma$-ring then 

TODO: Finish
\end{proof}

We have the following result for $\sigma$-rings that is analagous to
Lemma \ref{SigmaAlgebraPullback} proven for $\sigma$-algebras.
\begin{lem}\label{SigmaRingPullback}Given an arbitrary set function $f
  : S \to T$ and $\sigma$-rings $\mathcal{S}$ and $\mathcal{T}$ on
  $S$ and $T$ respectively 
\begin{itemize}
\item[(i)] $\mathcal{S}^\prime = f^{-1} \mathcal{T}$ is a
  $\sigma$-ring on $S$.
\item[(ii)] $\mathcal{T}^\prime = \left \{A \subset T ; f^{-1}(A) \in
      \mathcal{S} \right \}$ is a $\sigma$-ring on $T$.
\end{itemize}
\end{lem}
\begin{proof}
The proof of Lemma \ref{SigmaAlgebraPullback} shows closure under
countable union and intersection.  From these two facts, closure under
set difference follows by writing $B \setminus A = B \cap A^c$.
\end{proof}
 TODO: We have proven abstract Caratheodory construction in the
language of Boolean algebras; fill in a gap that shows that a
countably additive function on a ring actually defines a premeasure as
defined above.

\begin{lem}Let $\mu$ be an additive function on a semiring $\mathcal{D}$.  Let
  $\mu(\cup_{i=1}^n A_i) = \sum_{i=1}^n \mu(A_i)$ for any disjoint
  $A_1, \dotsc, A_n \in \mathcal{D}$.  Then $\mu$ is well defined and
  finitely additive on
  the ring $\mathcal{R}$ generated by $\mathcal{D}$.  If $\mu$ is
  countably additive on $\mathcal{D}$ then $\mu$ is countably additive
  on $\mathcal{R}$ and extends to a measure on $\sigma$-algebra
  generated by $\mathcal{D}$.
\end{lem}
\begin{proof}
\end{proof}

\subsubsection{Product Measures and Fubini's Theorem}

Prior to showing how to construct product measures, we need a
technical lemma.
\begin{lem}[Measurability of Sections]\label{MeasurableSections}Let $(S, \mathcal{S}, \mu)$ be a measure space with $\mu$ a
  $\sigma$-finite measure, let $(T, \mathcal{T})$ be a measurable
  space and $f : S \times T \to \reals_+$ be a positive $\mathcal{S} \otimes \mathcal{T} $-measurable
  function.  Then
\begin{itemize}
\item[(i)]$f(s,t)$ is an $\mathcal{S}$-measurable function of $s \in
  S$ for every fixed $t \in T$.
\item[(ii)] $\int f(s,t) \,  d \mu(s)$ is $\mathcal{T}$-measurable for
  as a function of $t \in T$.
\end{itemize}
\end{lem}
\begin{proof}
To see (i) and (ii),  let us first assume that $\mu$ is a bounded measure.  The proof uses
the standard machinery.  First assume that $f(s,t)
= \characteristic{B \times C}$ for $B \in \mathcal{S}$ and $C \in
\mathcal{T}$. Then note that for fixed $t \in T$, $f(s,t) = \characteristic{B}$ if $t
\in C$ and $f(s,t) = 0$ otherwise; in both cases we see that $f$ is
$\mathcal{S}$-measurable.  Also we calculate, $\int \characteristic{B
  \times C} (s,t) \,  d \mu(s) = \characteristic{C} (t)\int
\characteristic{B} (s) \,  d \mu(s) = \mu(B) \characteristic{C} (t) $
which clearly $\mathcal{T}$-measurable since $\mu(B) < \infty$.

Observe that the set of sets $B \times C$ is a $\pi$-system.  Let
\begin{align*}
\mathcal{H} = \lbrace A \in \mathcal{S} \otimes \mathcal{T} \mid
\characteristic{A}(s,t) \text{ is $\mathcal{S}$-measurable for every
  fixed $t \in T$ and $\int \characteristic{A}(s,t) \, d\mu(s)$ is $\mathcal{T}$-measurable } \rbrace
\end{align*}
and we claim that $\mathcal{H}$ is a $\lambda$-system.  Clearly $S
\times T \in \mathcal{H}$ from what we have already shown.  Suppose
next that $A \subset B$ are both in $\mathcal{H}$.  Note that
$\characteristic{B \setminus A} = \characteristic{B} -
\characteristic{A}$ so each section is a difference of
$\mathcal{S}$-measurable functions hence $\mathcal{S}$-measurable.
Similarly, 
\begin{align*}
\int \characteristic{B \setminus A} (s,t) \, d\mu(s) &= \int
\characteristic{B} (s,t) \, d\mu(s) - \int
\characteristic{A} (s,t) \, d\mu(s) 
\end{align*}
is a difference of $\mathcal{T}$-measurable function hence
$\mathcal{T}$-measurable.

Lastly, suppose that $A_1 \subset A_2 \subset \cdots \in
\mathcal{H}$.  Then $\characteristic{A_i} \uparrow \characteristic{\bigcup A_i}$ and this statement is true when considering each
function as a function on $S \times T$ but also for every
section with fixed $t \in T$.  Hence every section is a increasing limit of $\mathcal{S}$-measurable
functions and therefore $\mathcal{S}$-measurable.  Also we can apply
Montone Convergence Theorem to see that 
\begin{align*}
\int \characteristic{\bigcup A_i}(s,t) \, d\mu(s) = \lim_{n \to
  \infty} \int \characteristic{A_i}(s,t) \, d\mu(s)
\end{align*}
which shows $\mathcal{T}$-measurability.
Now the
$\pi$-$\lambda$ Theorem shows that $\mathcal{H} =  \mathcal{S} \otimes
\mathcal{T}$ and we have the result for all indicators.  

Next, linearity of taking sections and integrals shows that all simple functions
also satisfy the theorem.  Lastly for a general positive $f(s,t)$ we
take an increasing sequence of simple functions $f_n \uparrow f$.
Again, the limit is taken pointwise so every section of $f$ is the
limit of the sections of $f_n$ each of which has been shown
$\mathcal{S}$-measurable.  As the limit of $\mathcal{S}$-measurable
functions, we see that every section $f$ is also
$\mathcal{S}$-measurable.  Since for a fixed $t \in T$, $f_n(s,t)$ is
increasing as a function of $s$ alone we apply the Monotone
Convergence Theorem to see that
\begin{align*}
\int f(s,t) \, d\mu(s) = \lim_{n \to \infty} \int f_n(s,t) \, d\mu(s)
\end{align*}
which shows $\mathcal{T}$-measurability of $\int f(s,t) \, d\mu(s)$
since it is a limit of $\mathcal{T}$-measurable functions.

Now let $\mu$ be a $\sigma$-finite measure on $S$.  Then there is a
disjoint partition $S_1, S_2, \dots$ of $S$ such that $\mu S_n <
\infty$.  Thus, $\mu_n (A) = \mu(A \cap S_n)$ defines a bounded
measure and we know from Lemma \ref{ChainRuleDensity} that for any
measurable $g$, $\int g \, d\mu_n = \int g
\characteristic{S_n} \, d\mu$.
Putting these observations together,
\begin{align*}
\int f(s,t) \,  d \mu(s) &= \int f(s,t) \sum_{n=1}^\infty
\characteristic{S_n}(s) \,  d \mu(s) & & \text{since $S_n$ is a partition
  of $S$} \\
&= \sum_{n=1}^\infty \int f(s,t) 
\characteristic{S_n}(s) \,  d \mu(s) & & \text{by Corollary 
  \ref{TonelliIntegralSum} } \\
&= \sum_{n=1}^\infty \int f(s,t) \,  d \mu_n(s) 
\end{align*}
Since each $\mu_n$ is bounded, we have proven that each $\int f(s,t) \,  d \mu_n(s) $ is
$\mathcal{T}$-measurable hence the same is true for the partial sums
by linearity and then the infinite sum by taking a limit.
\end{proof}

TODO: Come up with an example of a non-measurable function for which all sections are measurable.

\begin{thm}[Fubini-Tonelli Theorem]\label{Fubini}Let $(S, \mathcal{S}, \mu)$
  and $(T, \mathcal{T}, \nu)$ be two $\sigma$-finite measure spaces.
  There exists a unique measure $\mu \otimes \nu$ on $(S \times T,
  \mathcal{S} \otimes \mathcal{T})$ satisfying 
\begin{align*}
(\mu \otimes \nu)(B \times C) &= \mu B \cdot \nu C & &\text{ for all
  $B \in \mathcal{S}$, $C \in \mathcal{T}$.}
\end{align*}
In addition if $f : S \times T \to \reals_+$ is a positive measurable
function then 
\begin{align*}
\int f(s,t) \,  d (\mu \otimes \nu) = \int \left [ \int f(s,t) \, d
\nu(t) \right ] d \mu(s)  = \int \left [ \int f(s,t) \, d \mu(s)
\right ] d\nu(t)
\end{align*}
This last sequence of equalities also holds if $f : S \times T \to \reals$
is measurable and integrable with respect to $\mu \otimes \nu$.
\end{thm}
\begin{proof}Note that the class of sets of the form $A \times B$ for
  $A \in \mathcal{S}$ and $B \in \mathcal{T}$ is clearly a
  $\pi$-system and generates $\mathcal{S} \otimes \mathcal{T}$ by
  definition of the product $\sigma$-algebra.  Furthermore by
  $\sigma$-finiteness of both $\mu$ and $\nu$ we can construct a
  disjoint partition $S \times T = \cup_i \cup_j S_i \times T_j$ with
  $\mu(S_i)\nu(T_j) < \infty$.  Therefore we can apply Lemma
  \ref{UniquenessOfMeasure} to see that the property $(\mu \otimes
  \nu)(A \times B) = \mu(A) \nu(B)$ uniquely determines $\mu \otimes
  \nu$.

To show existence of such a measure, define 
\begin{align*}
(\mu \otimes \nu)(A) = \int \left [\int \characteristic{A}(s,t) \, d
  \nu(t) \right ] d \mu(s)
\end{align*}
The fact that the iterated integrals are well defined follows from
Lemma \ref{MeasurableSections}.  
To see that it is a measure, first
note that it is simple to see $(\mu \otimes \nu)(\emptyset) = 0$.

To prove countable additivity, suppose we are given disjoint $A_1,
A_2, \dots \in \mathcal{S}
\otimes \mathcal{T}$.  By disjointness, we know
$\characteristic{\bigcup_{i=1}^\infty A_i} = \sum_{i=1}^\infty \characteristic{A_i}$.
Now because indicator functions and the inner integrals are positive, we can interchange
integrals and sums twice (Corollary \ref{TonelliIntegralSum}) and get
\begin{align*}
(\mu \otimes \nu)(\bigcup_{i=1}^\infty A_i) &= \int \left [ \int
\characteristic{\bigcup_{i=1}^\infty A_i}(s,t) \, d \nu(t) \right ] d \mu(s)\\
&= \int \left [ \int \sum_{i=1}^\infty \characteristic{A_i}(s,t) \, d
  \nu(t) \right ]  d \mu(s)\\
&= \sum_{i=1}^\infty \int \left [ \int \characteristic{A_i}(s,t) \, d
  \nu(t) \right ] d \mu(s)\\
\end{align*}

It is also clear that for $A = B \times C$ with $B \in \mathcal{S}$
and $C \in \mathcal{T}$, 
\begin{align*}
(\mu \otimes \nu)(B \times C) &= \int \left [ \int \characteristic{B}(s)
\characteristic{C}(t) \, d\nu(t) \right ] d \mu(s)\\
&= \int \characteristic{B}(s) \, d \mu(s) \cdot \int 
\characteristic{C}(t) \, d\nu(t) \\
&= \mu B \cdot \nu C
\end{align*}
Therefore we have proven the existence of the product measure.

The argument proving existence of the product measure applies equally well if we reverse the order
of $\mu$ and $\nu$ and shows that 
\begin{align*}
(\mu \otimes \nu)(B \times C) = \int \left [ \int \characteristic{B
    \times C}(s,t) \, d
\nu(t) \right ] d \mu(s) = \int \left [ \int \characteristic{B \times C}(s,t)
\, d \mu(s) \right ] d \nu(t)
\end{align*}
which proves that the integrals are equal for indicator functions of
sets of the form $B \times C$ and therefore for all indicator
functions by the montone class argument we used at the beginning of
the proof.  At this point, the
standard machinery can be deployed.  Linearity of integrals easily
shows that the equality extends to simple functions.  Lastly suppose
we have a positive measurable function $f(s,t) : S \times T \to
\overline{R}_+$ with a sequence of positive simple functions
$f_n(s,t) \uparrow f(s,t)$.  By the Monotone Convergence Theorem and
monotonicity of integral we know that 
\begin{align*}
0 &\leq \int f_n(s,t) \, d\mu(s) \uparrow \int f(s,t) \, d\mu(s) \\
0 &\leq \int f_n(s,t) \, d\nu(t) \uparrow \int f(s,t) \, d\nu(t) \\
\end{align*}
and therefore we have
\begin{align*}
\int f(s,t) \,  d (\mu \otimes \nu) 
&= \lim_{n \to \infty} \int
 f_n(s,t) \,  d (\mu \otimes \nu)  & & \text{by definition of integral
   of $f$}\\
&= \lim_{n \to \infty} \int \left [
\int f_n(s,t) \,  d \mu(s) \right ] d\nu(t) & &\text{by Tonelli
for simple functions}\\
&= \int \left [
\int f(s,t) \,  d \mu(s) \right ] d\nu(t) & &\text{by Monotone Convergence
   on $\int f_n \, d \mu(s)$}\\
\end{align*}

It is worth pointing out explicitly that even if $f(s,t)$ is never
equal to infinity, the integrals may be equal to infinity on all of
$S$ or $T$ and it is critical that we have phrased the theory of
integration for positive functions in terms of functions with values
in $\overline{\reals}_+$.

TODO: Clean up the following argument; it has all right details but is
more than a bit ragged.  Particularly annoying is that this is the
first time we've talked about defining integrals for signed functions
that take infinite values on a set of measure zero.

Now assume that $f$ is integrable with respect to $\mu \otimes \nu$: 
$\int \abs{f(s,t)} \,  d (\mu \otimes \nu) < \infty$.  We
write $f = f_+ - f_-$ and note that $\int f_\pm(s,t) \,  d (\mu
\otimes \nu) < \infty$ and use Tonelli's Theorem just proven to see
that 
\begin{align*}
\int f_\pm(s,t) \,  d (\mu \otimes \nu) &= \int \left [ \int f_\pm(s,t) \, d
\nu(t) \right ] d \mu(s)  = \int \left [ \int f_\pm(s,t) \, d \mu(s)
\right ] d\nu(t) < \infty
\end{align*}
The finiteness of the iterated integrals implies that the integrands
are almost surely finite and therefore we see that each section 
$\int f_\pm \, d\mu(s)$ and $\int f_\pm \, d\nu(t)$ is almost surely
finite.  The trick is that being almost surely finite isn't good
enough when trying to calculate the iterated integrals of $f$ and we
might run into the awkward situation in which there is a $t \in T$
such that \emph{both} $\int f_+ \, d\mu(s)$ and $\int f_+ \, d\mu(s)$
are infinite.  However define
$N_S = \{ s \in S \mid \int \abs{f} \, d\nu(t) = \infty \}$ and $N_T =
\{ t \in T \mid \int \abs{f} \, d\mu(s) = \infty \}$.  We have noted
that $N_S$ is a $\mu$-null set and that $N_T$ is a $\nu$-null set
hence $N_S \times N_T$ is a $(\mu \otimes \nu)$-null set.  We modify
$f$ so that it is zero on $N_S \times N_T$ by defining 
$\tilde{f}(s,t) = (1 - \characteristic{N_S \times N_T}) f(s,t)$.  Note
the following 
\begin{align*}
\int \tilde{f} d(\mu\otimes \nu) &= \int f d(\mu\otimes \nu) \\
\int \tilde{f} d\mu(s) &= \begin{cases}
\int f d\mu(s) & \text{if $t \notin N_T$} \\ 
0 & \text{if $t \in N_T$} \\ 
\end{cases} \\
\int \tilde{f} d\nu(t) &= \begin{cases}
\int f d\nu(t) & \text{if $s \notin N_S$} \\ 
0 & \text{if $s \in N_S$} \\ 
\end{cases}
\end{align*}
Now we can write $\tilde{f} = \tilde{f}_+ - \tilde{f}_-$ and apply
Tonelli's Theorem to see
\begin{align*}
\int \tilde{f} d(\mu\otimes \nu) &= \int \tilde{f}_+ d(\mu\otimes \nu)
- \int \tilde{f}_- d(\mu\otimes \nu) \\
&= \int \left [ \int \tilde{f}_+ d\mu(s)\right ] d \nu(t) - \int \left
  [ \int \tilde{f}_- d\mu(s)\right ] d \nu(t) \\
&= \int \left [  \int \tilde{f}_+ d\mu(s) - \int \tilde{f}_-
d\mu(s) \right ] d \nu(t) \\
&= \int \left [ \int \tilde{f} d\mu(s)\right ] d \nu(t) \\
\end{align*}

But we know $\int \left [ \int \tilde{f} d\mu(s)\right ] d \nu(t) =
\int \left [ \int f d\mu(s)\right ] d \nu(t)$ so we get the result for
$f$ as well.
\end{proof}

TODO: Royden has some exercises that demonstrate how each of these
hypotheses is necessary (e.g. Counterexample to Fubini for
non-integrable f).  Incorporate them.
\begin{examp}Define the measure space $(\naturals, 2^\naturals,
  \mu)$ where $\mu(A) = \card(A)$.  $\mu$ is called the \emph{counting
    measure}.  
Consider the function 
\begin{align*}
f(s,t) &= \begin{cases}
2 - 2^{-s+1} & \text{if $s=t$}\\
-2 + 2^{-s+1} & \text{if $s = t + 1$} \\
0 & \text{otherwise}
\end{cases}
\end{align*}
on $(\naturals \times \naturals, 2^{\naturals \times \naturals}, \mu
\otimes \mu)$.  Since $\mu \otimes \mu$ is the counting measure on
$\naturals \times \naturals$ it is easy to see that 
\begin{align*}
\int \abs{f(s,t)} \, d ( \mu \otimes \mu) &= \sum_{s=1}^\infty
\sum_{t=1}^\infty \abs{f(s,t)} = \infty
\end{align*}
so $f$ is not integrable.  However in this case both of the iterated
integrals are defined.
For fixed $t$, 
\begin{align*}
\int f(s,t) \, d\mu(s) &=
\sum_{s=1}^\infty f(s,t) = 2^{-t} - 2^{-t+1} = -2^{-t}
\end{align*}
hence 
\begin{align*}
\int \left [ \int f(s,t) \, d\mu(s) \right ] d \mu(t) &=
\sum_{t=1}^\infty -2^{-t} = -1
\end{align*}

For fixed $s$, 
\begin{align*}
\int f(s,t) \, d\mu(s) &=
\sum_{t=1}^\infty f(s,t) = \begin{cases}
1 & \text {if $s=1$}\\
0 & \text {otherwise}
\end{cases}
\end{align*}
and therefore 
\begin{align*}
\int \left [ \int f(s,t) \, d\mu(t) \right ] d \mu(s) &= 1
\end{align*}
This example shows that the positivity of $f$ is a necessary condition
in Tonelli's Theorem and that the assumption of integrability is
necessary in Fubini's Theorem.
\end{examp}


TODO
Outer measures, Caratheodory construction, Lesbegue Measure (existence
and uniqueness), Product Measures and Fubini's Theorem, Radon-Nikodym Theorem and
Fundamental Theorem of Calculus, Differential Change of Variables for
Lebesgue Measure on $\reals^n$ (useful for calculations involving
probability densities).

\begin{lem}[Translation Invariance of Lebesgue Measure]\label{LesbegueTranslationInvariance} Suppose $\mu$ is a measure on $\reals^n$ which is
  translation invariant and for which $\mu([0,1]^n) = 1$, then $\mu =
  \lambda^n$.
\end{lem}
\begin{proof}Suppose we are given a translation invariant measure
  $\mu$ such that $\mu([0,1]^n) = 1$.  By writing boxes as a
  union of cubes and using finite
  and countable additivity together with translation invariance it is
  easy to see that
  for any box $\mathcal{I}_1 \times \cdots \times \mathcal{I}_n$ where
  each $\mathcal{I}_k$ has rational endpoints that we have 
\begin{align*}
\mu\left (\mathcal{I}_1 \times \cdots \times
    \mathcal{I}_n \right ) &= \abs{\mathcal{I}_1} \cdots
  \abs{\mathcal{I}_n} \\
 &= \lambda^n \left (\mathcal{I}_1 \times \cdots \times
    \mathcal{I}_n \right )
\end{align*}
Now fix $\mathcal{I}_2, \dots ,\mathcal{I}_n$ and consider $\nu(A) = \frac{1}{ \abs{\mathcal{I}_2} \cdots  \abs{\mathcal{I}_n}}\mu \left
  (A \times \mathcal{I}_2 \times \cdots \times
    \mathcal{I}_n \right ) $ as a function of $A \in
  \mathcal{B}(\reals)$.  It is easy to see that this is a Borel
  measure and we have already seen that $\nu(\mathcal{I}) =
  \abs{\mathcal{I}}$ for all rational intervals (hence all intervals
    by countable additivity).  Therefore $\nu = \lambda$ is Lebesgue
    measure on $\mathcal{B}(\reals)$ and we have for every $B_1 \in
    \mathcal{B}(\reals)$,
\begin{align*}
\mu\left (B_1 \times \mathcal{I}_2 \times\cdots \times
    \mathcal{I}_n \right ) 
 &= \lambda^n \left (B_1 \times \mathcal{I}_2 \times \cdots \times
    \mathcal{I}_n \right )
\end{align*}
Now iterate the argument $2, \cdots, n$ fixing all but the $i^{th}$
argument to extend to all cylinder sets $B_1 \times \cdots \times B_n$
and we apply the uniqueness of product measures.

Now it remains to show that $\lambda^d$ is indeed translation
invariant.
TODO
\end{proof}
\begin{cor}\label{LesbegueRotationInvariance} Lebesgue measure $\lambda^n$ on $\reals^n$ is invariant
  under orthogonal transformations.
\end{cor}
\begin{proof}Suppose we are given an orthogonal transformation $P$.
  We claim that the measure $\lambda^n_P(A) = \lambda^a(P A)$ is
  translation invariant.   To see this, assume we are given $h \in
  \reals^n$ and note that 
\begin{align*}
\lambda^n_P(A + h) &= \lambda^n(P A + Ph)  & &\text{linearity of $P$} \\
&= \lambda^n(PA) & &\text{translation invariance of $\lambda^n$} \\
&= \lambda^n_P(A) & &\text{definition of $\lambda^n_P$}
\end{align*}
Therefore we know that $\lambda^n_P = c \lambda^n$ for some constant
$c>0$.  Take the unit ball $B^n \subset \reals^n$ and notice that $P
B^n = B^n$ to see that in fact $c = 1$.
\end{proof}
\begin{cor}\label{LesbegueLinearChangeOfVariables}[Linear Change of Variables]For an arbitrary linear transformation $T : \reals^n \to
  \reals^n$, $\lambda^n(T A) = \abs{\det{T}} \lambda^n(A)$ for all
  measurable $A$.
\end{cor}
\begin{proof}Note that by the Singular Value Decompostion, we can
  write $T = U D V$ with $U,V$ orthogonal.  By the rotation invariance
  of $\lambda^n$, we are reduced to the case of a diagonal matrix.  In
  that case, the result is easy.
TODO write down the easy stuff too!
\end{proof}
\subsection{Radon-Nikodym Theorem and Differentiation}
We have seen the construction of measures by integration of a
density.  A productive line of inquiry is to ask if one can
characterize measures that arise through this construction and those
that cannot arise through this construction.  As it
turns out an precise answer may be given for $\sigma$-finite measures;
this is the content of the Radon-Nikodym Theorem.  If one restricts
attention to $\reals$ and considers the Fundamental Theorem of
Calculus for Riemann integrals
\begin{align*}
\frac{d}{dx} \int_0^x f(y) \, dy = f(x)
\end{align*}
one can surmise that there is a connection between the considerations
of the Radon-Nikodym Theorem and the theory of differentiation of
integrals.  This is indeed the case and we will prove the extension of
the Fundamental Theorem of Calculus to Lebesgue integrals using the
Radon-Nikodym Theorem.  Note that it is probably more traditional to
explore the theory of differention of functions of a real variable
without using the more abstract Radon-Nikodym Theorem but if one
intends to cover both one can save some time by proceeding in the way
we have chosen (stolen unabashedly from Kallenberg).

The first step is to develop a couple of tools that may be used to
compare two measures.  The trick is that if one takes the difference
of two measure, one does not get a measure.  However there is a clever
observation that helps to repair the defect.  
\begin{defn}A \emph{ bounded signed measure} on a measurable space $(\Omega,
  \mathcal{A})$ is a bounded function $\nu : \mathcal{A} \to
  \reals$ such that for every disjoint $A_1, A_2, \dots \in
  \mathcal{A}$ such that $\sum_{n=1}^\infty \abs{\nu (A_n)}  < \infty$, we have $\nu(\bigcup_{n=1}^\infty A_n ) =
  \sum_{n=1}^\infty \nu (A_n)$ 
\end{defn}
\begin{defn}Two measures $\mu$ and $\nu$ on a measurable space $(\Omega,
  \mathcal{A})$ are said to be \emph{mutually singular} if there
  exists $A \in \mathcal{A}$ such that $\mu A = 0$ and $\nu A^c = 0$.
  We often write $\mu \perp \nu$.
\end{defn}
\begin{examp}Lebesgue measure and any Dirac measure on $\reals$ are
  mutually singular.
\end{examp}
\begin{examp}Let $f,g$ be positive measurable functions on $\reals$
  such that $\int f \wedge g \, d\lambda= 0$.  Then $f \cdot \lambda$ and $g
  \cdot \lambda$ are mutually singular.
\end{examp}
\begin{defn}Given two measures $\mu$ and $\nu$ on a measurable space $(\Omega,
  \mathcal{A})$ we say that $\nu$ is \emph{absolutely continuous} with
  respect to $\mu$ if for every $A \in \mathcal{A}$ such that $\mu A =
  0 $ we also have $\nu A = 0$.
  We often write $\nu \ll \mu$.
\end{defn}
\begin{examp}Let $f$ be a positive measurable function on the measure
  space $(\Omega,
  \mathcal{A}, \mu)$, then $f \cdot \mu$ is absolutely continuous with
  respect to $\mu$.  We shall soon see that this is the only way to
  construct absolutely continuous measures.
\end{examp}
\begin{thm}\label{HahnDecomposition}[Hahn Decomposition]Given a
  bounded signed measure $\nu$ on a measurable space $(\Omega,
  \mathcal{A})$ there are unique bounded mutually singular positive
  measures $\nu_+$ and $\nu_-$ such that $\nu = \nu_+ - \nu_-$.
\end{thm}
\begin{proof}Let $c=\sup_{A \in \mathcal{A}} \nu(A)$.  The first claim
  is that there is a $A_+ \in \mathcal{A}$ such that $\nu A_+ = c$.
  To see this, first we note the following crude bound.  Suppose we
  are given $A,A^\prime \in \mathcal{A}$ such that $\nu A \geq c -
  \epsilon$ and $\nu A^\prime \geq c - \epsilon^\prime$.  Then 
\begin{align*}
\nu (A \cup A^\prime) &= \nu A + \nu A^\prime - \nu A \cap A^\prime \\
&\geq \nu A + \nu A^\prime - c & &\text{by bound on $\nu$} \\
&\geq c - \epsilon - \epsilon^\prime & &\text{by bounds on $A,A^\prime$}
\end{align*}

Now approximate the supremum by taking $A_1, A_2, \dots \in \mathcal{A}$ such that $\nu
  A_n \geq c - 2^{-n}$ and apply the bound above with countably
  additivity to see 
\begin{align*}
\nu \bigcup_{i=n+1}^\infty A_i \geq c - \sum_{i=n+1}^\infty 2^{-i} = c - 2^{-n}
\end{align*}
There is something a bit confusing about this bound; namely as $n$
is increasing the sets are getting smaller but the bound on the
measure is increasing.  TODO: It is probably worth sorting out exactly
what this is telling us (I think it is just that all of the tails are
equal up to null sets and of measure $c$).
Let $A_+ = \bigcap_{n=1}^\infty \bigcup_{i=n+1}^\infty A_i$ and note by
countable additivity and the boundedness of $\nu$ (see proof of Lemma
\ref{ContinuityOfMeasure}) we have 
\begin{align*} \nu A_+ = \lim_{n \to \infty} \nu
  \bigcup_{i=n+1}^\infty A_i \geq c
\end{align*}
By the definition of $c$ we see that $\nu A_+ = c$.  Now define $A_- =
A_+^c$ and define the restrictions 
\begin{align*}
\nu_+ B &= \nu (A_+ \cap B ) \\
\nu_- B &= \nu ( A_- \cap B )
\end{align*}
TODO: prove decomposition property and uniqueness.
\end{proof}

\begin{thm}[Radon-Nikodym Theorem]\label{RadonNikodym}Let $\mu, \nu$
  be $\sigma$-finite measures on the measurable space $(\Omega,
  \mathcal{A})$.  There exist unique measures $\nu_a \ll \mu$ and
  $\nu_s \perp \mu$ such that $\nu = \nu_a + \nu_s$.  Furthermore,
  there is a unique positive measurable $f : \Omega \to \reals$ such
  that $\nu_a = f \cdot \mu$.
\end{thm}
\begin{proof}TODO
\end{proof}

In addition to the product measure construction we have just seen
there is another important construction for $\reals$.
\begin{defn}A measure $\mu$ on $(\reals, \mathcal{B}(\reals))$ is called
  \emph{locally finite} if $\mu(I) < \infty$ for every finite interval
  $I \subset \reals$.
\end{defn}
\begin{lem}[Lebesgue-Stieltjes
  Measure]\label{LebesgueStieltjesMeasure}There is a 1-1
  correspondence between locally finite measures on $(\reals,\mathcal{B}(\reals))$ and
  nondecreasing right continuous functions $F : \reals \to \reals$ such that $F(0)=0$ given by 
\begin{align*}
\mu((a,b]) = F(b) - F(a)
\end{align*}
\end{lem}
\begin{proof}
Suppose we are given a locally finite measure $\mu$ on
$(\reals,\mathcal{B}(\reals))$.  Define
\begin{align*}
F(x) = \begin{cases}
\mu (0,x] & \text{if $x > 0$}\\
-\mu (x, 0] & \text{if $x < 0$}\\
0 & \text{if $x=0$}
\end{cases}
\end{align*}
Local finiteness of $\mu$ implies that $F$ is well defined.
Monotonicity of $\mu$ implies that $F$ is nondecreasing.  Continuity
of measure implies that $F$ is right continuous.  Clearly, 
\begin{align*}
\mu (a,b] = F(b) - F(a)
\end{align*} and furthermore $F$ is the unique function that satisfies
this property.

On the other hand, given an $F$ that is nondecreasing, right
continuous and satisfies $F(0) = 0$ we define a generalized inverse by 
\begin{align*}
G(y) = \inf \lbrace x \in \reals \mid F(x) \geq y \rbrace
\end{align*}
Note that if $y < w$ then $\lbrace x \in \reals \mid F(x) \geq w
\rbrace \subset \lbrace x \in \reals \mid F(x) \geq y \rbrace$ which
shows that $G$ is a nondecreasing function.  The fact that $G$ is
nondecreasing implies that $G^{-1} (-\infty, y] = (-\infty, x]$ for
some $x \in \reals$ and therefore $G$ is a measurable function.
Furthermore, 
\begin{align*}
G(F(x)) &= \inf\lbrace s \in \reals \mid F(s) \geq F(x)
\rbrace \leq x
\end{align*}
and on the other hand since 
\begin{align*}
G(y) &= \inf\lbrace x \in \reals \mid F(x) \geq y
\rbrace 
\end{align*}
we can find a sequence $x_n \downarrow G(y)$ such that $F(x_n) \geq y$
and therefore by right continuity of $F$ we now that $F(G(y)) =
\lim_{n\to\infty} F(x_n) \geq y$.

Together these two facts show that 
$G(y) \leq c$ if and only if $y \leq F(c)$.  In one direction suppose
$y \leq F(c)$, then applying $G$ to both sides and using the
nondecreasing nature of $G$, we get $G(y) \leq G(F(c)) \leq c$.  In
the other direction, we assume $G(y) \leq c$ and apply $F$ to both
sides and to see
\begin{align*}
F(c) \geq F(G(y)) \geq y
\end{align*}
In a similar way, we see that $c < G(y)$ if and only if $F(c) < y$.

Now we can finish the proof by 
defining $\mu = (\pushforward{G}{\lambda})$ where $\lambda$ is Lebesgue
measure on $\reals$.  We observe that this is an inverse to the
construction of $F$ given above.  
\begin{align*}
\mu (a,b] &= \lambda \left ( \lbrace y \in \reals \mid a < G(y)  \leq b
  \rbrace \right ) \\
&= \lambda (F(a), F(b)] = F(b) - F(a)
\end{align*}

Uniqueness of measure $\mu$ with this property follow by Lemma \ref{UniquenessOfMeasure}.
\end{proof}

Note the choice of the normalizing condition $F(0) = 0$ is somewhat
arbitrary albeit a natural choice when considering arbitrary locally
finite measures on $\reals$.  We will see later that for finite
measures, and probability
measures in particular, it is more useful to pick a different
normalization $\lim_{x \to -\infty} F(x) = 0$.

By the description of all measures on $\reals$ as
Lebesgue-Stieltjes measures, we have set the stage for the
translation of results about measures into results about
nondecreasing, right continuous functions.  In particular, if we apply
the Radon-Nikodym Theorem to we see that any such $F$ may be written
as $F = F_a + F_s$ which represent the absolutely continuous and
singular parts of the decomposition respectively.  If one unwinds the
defining property of $F_a$ from the Lebesgue-Stieltjes integral, one
sees
 that in the absolutely continuous case, $F_a(x) = \int_0^x f \,
 d\lambda$ for an appropriate density $f$.

\begin{thm}[Fundamental Theorem Of Calculus]\label{FundamentalTheoremOfCalculus}Let any nondecreasing, right continuous function $F(x) = \int_0^x
  f \, d\lambda + F_s(x)$ is differentiable a.e. with derivative $F^\prime = f$.
\end{thm}
\begin{proof}
TODO
\end{proof}

\begin{cor}[Integration By Parts]\label{IntegrationByParts}Suppose $f$
  and $g$ are absolutely continuous functions.  Then 
\begin{align*}
\int_a^b f^\prime g d\lambda
  = f(b)g(b) - f(a)g(a) - \int_a^b f g^\prime d \lambda
\end{align*}
\end{cor}
\begin{lem}\label{IntervalSelection}Let $\mathcal{I}$ be an arbitrary
  collection of open intervals of $\reals$.  Let $G = \bigcup_{I \in
    \mathcal{I}} I$ and suppose that $\lambda G < \infty$.  Then there
  exists disjoint $I_1, \dots, I_n$ such that $\sum_{i=1}^n \abs{I_i}
  \geq \frac{\lambda G}{4}$.
\end{lem}
\begin{proof}TODO
\end{proof}
\begin{lem}\label{DifferentiationOnNullSets}Let $\mu$ be a locally finite measure on $(\reals, \mathcal{B}(\reals))$
  and let $F(x) = \mu (0,x]$.  Let $A \in \mathcal{B}$ be a set with
  $\mu A = 0$, then $F^\prime = 0$ almost everywhere $\lambda$ on $A$.
\end{lem}
\begin{proof}The intuition behind the proof is that the derivative
  $F^\prime(x)$ represents the ratio of $\mu$-measure and
  $\lambda$-measure for arbitrarily small intervals around $x \in
  \reals$.  For $x \in A$, we expect the $\mu$-measure and therefore
  the derivative to be $0$.  Since $A$ may not contain any honest
  intervals, there is some finesse required to make the intuition rigorous.

First pick $\delta > 0$ and and open set $G_\delta \supset A$ such that $\mu
G_\delta < \delta$.  

TODO: Prove that such $G_\delta$ exists; this is a fact for arbitrary
Borel $\sigma$-algebras.

For each $\epsilon > 0$, let 
\begin{align*}
A_\epsilon &= \{ x \in A \mid \sup_{h \to 0} \frac{F(x + h) - F(x -
  h)}{h} > \epsilon \} \\
&= \{ x \in A \mid \sup_{h \to 0} \frac{\mu (x - h, x+h]}{2 \epsilon} > \abs{(x-h,x+h]} \} 
\end{align*}
TODO: Prove that $A_\epsilon$ is measurable.

For any $x \in A_\epsilon$ we can pick $h > 0$ small enough so that
$I_x = (x - h, x+h] \subset G_\delta$.  Note that $A_\epsilon \subset
\bigcup_{x \in A_\epsilon} I_x$.  By the previous Lemma
\ref{IntervalSelection} we pick a finite disjoint set $I_1, \dots,
I_n$ and note that
\begin{align*}
\lambda A_\epsilon &\leq \lambda 
\bigcup_{x \in A_\epsilon} I_x \\
&\leq 4 \sum_{k=1}^n \abs{I_k} \\
&\leq 4 \sum_{k=1}^n \frac{\mu I_k}{2 \epsilon} \\
& \leq \frac{\delta}{2\epsilon}
\end{align*}
Now $\delta > 0$ was arbitrary so we see that $\lambda A_\epsilon =
0$.  Since $\epsilon > 0$ was arbitrary so we see that $F^\prime(x) = 0$
almost everywhere on $A$ since the set of points where $F^\prime \neq
0$ is a countable union of $A_\epsilon$ (e.g. take $\bigcup_n A_{\frac{1}{n}}$).
\end{proof}

\subsection{Approximation By Smooth Functions}
In this section we discuss a technique for approximating arbitrary
measurable and integrable functions by smooth functions.  

To start, we establish the existence of an infinitely differentiable
function which is supported on the interval $[-1,1]$.

\begin{lem}The function 
\begin{align*}
f(x) = \begin{cases}
e^\frac{-1}{1-x^2} & \abs{x} < 1\\
0 & \abs{x} \geq 1
\end{cases}
\end{align*} is compactly supported on $[-1,1]$ and has continuous
derivatives of all orders.
\end{lem}
\begin{proof}
It is clear from the definition that $f(x)$ is compactly supported on
$[-1,1]$.  To see that it has continuous derivatives of all orders we
use an induction to prove that for every $n\geq 0$, there exists a
polynomial $P_n(x)$ and a nonnegative integer $N_n$ such that 
\begin{align*}
f^{(n)}(x) = \frac{P_n(x)}{(1 - x^2)^{N_n}} e^\frac{-1}{1-x^2}
\end{align*}
Clearly this is true for $n=0$.  Supposing that it is true for $n >0$,
we calculate using the induction hypothesis, the product rule and
chain rule
\begin{align*}
f^{(n+1)}(x) &= \frac{d}{dx}\frac{P_n(x)}{(1 - x^2)^{N_n}}
e^\frac{-1}{1-x^2} \\
&= \frac{(1 - x^2)^{N_n} P_n^\prime(x) - P_n(x) N_n (1- x^2)^{N_n
    -1}}{(1 - x^2)^{2N_n}}e^\frac{-1}{1-x^2} + \frac{P_n(x)}{(1 - x^2)^{N_n}}\frac{-1}{1-x^2} \frac{-2x}{(1-x^2)^2}
e^\frac{-1}{1-x^2} \\
\end{align*}
which shows the result after creating a common denominator.

It is clear that the derivatives are continuous away from ${-1,1}$ so
it remains to show $\lim_{x \to -1^+} f^{(n)}(x) = 0$ and $\lim_{x \to
  1^-} f^{(n)}(x) = 0$.

Take the former limit.  We write $f^{(n)}(x) = \frac{P_n(x)}{(1 -
  x)^{N_n}(1 + x)^{N_n}} e^\frac{-1}{1-x^2}$ and note that


TODO: Show $\lim_{x \to -1} \frac{1}{(1 + x)^M} e^\frac{-1}{1-x^2} = 0$
for all $M \geq 0$.
\end{proof}

TODO: What is $\int f(x)?$

\begin{lem}\label{ApproximationByMollifiers}Let $\rho(x)$ be a positive
  function in $C^\infty_c(\reals)$ such
  that $\rho(x)$ is supported on $[-1,1]$ and $\int_{-\infty}^\infty
  \rho(x) \, dx = \int_{-1}^1
  \rho(x) \, dx= 1$.  Let $f : \reals \to \reals$ be a continuous function.  Define 
\begin{align*}
f_n(x) = n \int_{-n}^n \rho(n(x - y)) f(y) dy
\end{align*}
Then $f_n \in C_c^\infty(\reals)$, $f_n^{(m)}(x) = n \int_{-n}^n
\rho^{(m)}(n(x -y)) f(y) dy$ and $f_n$ converges to $f$ uniformly on
compact sets.  Furthermore, if $f$ is bounded then $\norm{f_n}_\infty \leq
\norm{f}_\infty$.
\end{lem}
\begin{proof}
First note that because $\rho(x)$ and all of its derivatives are
compactly supported, they are also bounded.  In particular, there is
an $M > 0$ such that $\abs{\rho^\prime(x)} \leq M$.  To clean up the notation
a little bit, define $\rho_n(y) = n\rho(ny)$ so we have
\begin{align*}
f_n(x) &= \int_{-n}^{n} \rho_n(x - y) f(y) dy
\end{align*}
Since the support of $\rho_n(x)$ is contained in
$[-\frac{1}{n},\frac{1}{n}]$, if we fix $x \in \reals$ and
view $\rho_n(x - y)$ as a  function of $y$, its support is
contained in $[x-\frac{1}{n},x+\frac{1}{n}]$.  Thus the support of
$f_n(x)$ is contained in  $[-n-\frac{1}{n},n+\frac{1}{n}]$.

To examine the derivative
of $f_n(x)$, pick $h > 0$ and consider the difference
quotient
\begin{align*}
\frac{f_n(x + h) - f_n(x)}{h} &= \frac{1}{h} \int_{-n}^{n}
(\rho_n(x+h - y) - \rho_n(x - y) ) f(y) dy
\end{align*}
Taylor's Theorem tells us that $\frac{1}{h}(\rho_n(x+h - y) - \rho_n(x - y)) =
\rho_n^\prime(c)$ for some $c \in [x+h - y, x - y]$.  Therefore,
$\abs{\frac{1}{h} (\rho_n(x+h - y) - \rho_n(x - y))f(y)} \leq M\abs{f(y)}$ and by
integrability of $f(y)$ on the interval $[-n,n]$ (i.e. the
integrability of $f(y) \cdot \characteristic{[-n,n]}(y)$ which follows
rom the boundedness of $f(y)$ on the compact set $[-n,n]$) we may use Dominated Convergence to conclude
that 
\begin{align*}
f_n^\prime(x) &=\lim_{h \to 0} \frac{f_n(x + h) - f_n(x)}{h} \\
&= \lim_{h \to 0} \frac{1}{h} \int_{-n}^{n}
(\rho_n(x+h - y) - \rho_n(x - y) ) f(y) dy \\
&= \int_{-n}^{n }
\lim_{h \to 0} \frac{1}{h} (\rho_n(x+h - y) - \rho_n(x - y) ) f(y) dy
\\
&= \int_{-n}^{n} \rho^\prime_n(x - y) f(y) dy 
\end{align*} 
Continuity of $f_n^\prime(x)$ follows from the continuity of $f(y)$
and $\rho^\prime_n(x - y) $ and Dominated Convergence as above.  A simple induction extends the result to derivatives of arbitrary
order.

Next we show the convergence.  Pick a compact set $K \subset \reals$
and  $\epsilon > 0$,
Since $f$ is uniformly continuous on $K$,  there is a $\delta >0$ such
that for any $x \in K$ we have $\abs{x - y} \leq \delta$ implies
$\abs{f(x) - f(y)} \leq \epsilon$.  Pick $N_1 > 0$ such that
$\frac{1}{n} < \delta$ for all $n \geq N_1$.  The hypothesis $\int_{-\infty}^\infty
\rho(y) \, dy = \int_{-1}^1
\rho(y) \, dy = 1$ and simple change of variables shows $\int_{-\infty}^\infty
\rho_n(x - y) \, dy = \int_{x - \frac{1}{n}}^{x + \frac{1}{n}}
\rho_n(x - y) \, dy = 1$ for all $x \in \reals$ and $n > 0$.  Pick $N_2>0$ so that for all $n > N_2$,
we have $K \subset [-n + \frac{1}{n}, n - \frac{1}{n}]$.  Therefore we can write $f(x) = \int_{- n}^{n}
\rho_n(x - y) f(x)  \, dy = 1$ for any $x \in K$ and $n > N_2$.  We
have for any $n \geq \max(N_1, N_2)$
\begin{align*}
\abs{f_n(x) - f(x)} &= \abs{\int_{- n}^{n} (\rho_n(x -
  y)f(y) -
\rho_n(x- y) f(x)) \, dy} \\
&= \abs{\int_{x - \frac{1}{n}}^{x + \frac{1}{n}} (\rho_n(x -
  y)f(y) -
\rho_n(x- y) f(x)) \, dy}  & & \text{since $n>N_2$}\\
&\leq \int_{x - \frac{1}{n}}^{x + \frac{1}{n}} \rho_n(x- y) \abs{f(y)
  - f(x)} \, dy\\
&\leq \epsilon \int_{x -\frac{1}{n}}^{x + \frac{1}{n}} \rho_n(x -  y)
\, dy & &\text{since $\frac{1}{n} < \delta$}\\
&\leq \epsilon & & \text{since $\rho_n$ is positive and
  $\int_{-\infty}^\infty \rho_n(x) \, dx = 1$}
\end{align*}

The last thing to prove is the norm inequality in case $f$ is
bounded.  
\begin{align*}
\abs{f_n(x)} &\leq n \int_{-n}^n \rho(n(x - y)) \abs{ f(y)} dy & &
\text{because $\rho$ is positive} \\
&\leq n \norm{f}_\infty \int_{-\infty}^\infty \rho(n(x - y)) dy = \norm{f}_\infty 
\end{align*}
\end{proof}

Approximation by convolution with a compactly supported bump function
is usually sufficient for our purposes, however it is also useful to
replace the bump function with Gaussians.  

We will need the following fact that is a standard exercise from multivariate calculus
\begin{lem}\label{IntegralGaussian}$\int_{-\infty}^\infty e^{-x^2/2}\, dx = \sqrt{2\pi}$.
\end{lem}
\begin{proof}
By Tonelli's Theorem,
\begin{align*}
\int_{-\infty}^\infty \int_{-\infty}^\infty e^{-(x^2 + y^2)/2} \, dxdy
&= \int_{-\infty}^\infty e^{-x^2/2} \,
dx \int_{-\infty}^\infty e^{-y^2/2} \,
dy = \left(int_{-\infty}^\infty e^{-x^2/2} \,
dx \right)^2
\end{align*}
However, if we switch to polar coordinates and Tonelli's Theorem,
\begin{align*}
\int_{-\infty}^\infty \int_{-\infty}^\infty e^{-(x^2 + y^2)/2} \, dxdy
&= \int_{0}^{2\pi} \int_{0}^\infty e^{-r^2/2} r \, dr d\theta =
\int_{0}^{2\pi} d\theta = 2\pi
\end{align*}
and we are done.
\end{proof}

Now we can see that we may uniformly approximate compactly supported continuous
functions by convolution with Gaussians.
\begin{lem}\label{UniformApproximationByGaussians}Define $\rho(x) =
  \frac{1}{\sqrt{2 \pi} }e^{-x^2/2}$ and let $\rho_n(x) = n\rho(nx)$.
Let $f \in C_c(\reals)$ then define $f_n(x) = (f *
  \rho_n)(x)$.  Then $f_n(x) \in C_c^\infty(\reals)$ and $f_n$
  converges to $f$ uniformly.
\end{lem}
\begin{proof}
The proof is rather similar to that in the preceeding Lemma
\ref{ApproximationByMollifiers}.  By simple change of variables and
Lemma \ref{IntegralGaussian} we see
that $\int_{-\infty}^{\infty} \rho_n(y) \, dy =
\int_{-\infty}^{\infty} \rho_n(x - y) \, dy = 1$ and therefore we
have the trivial identity $f(x) = \int_{-\infty}^{\infty} f(x)
\rho_n(x - y) \, dy$.  Because $f$ has compact support, we know that
$f$ is uniformly continuous, so given $\epsilon  > 0$ we can find
$\delta > 0$ such that $\abs{x -y} < \delta$ implies $\abs{f(x) -f(y)}
< \epsilon$.  Similarly, by compact support $f$ is bounded and we may
assume $f(x) < M$ for some $M > 0$.  Assume we are given
$\epsilon > 0$ then take $\delta>0$ as above and for any $n>0$ we have
Therefore
\begin{align*}
\abs{f*\rho_n(x) - f(x)} &= \abs{\int_{-\infty}^{\infty} \rho_n(x-y)
  (f(y) - f(x)) \, dy} \\
&\leq \int_{\abs{x-y} < \delta} \rho_n(x-y)
  \abs{  f(y) - f(x) } \, dy + \int_{\abs{x-y} \geq \delta} \rho_n(x-y)
  \abs{ f(y) - f(x) } \, dy \\
&\leq \epsilon + 2M \int_{\abs{x-y} \geq \delta} \rho_n(x-y) \, dy
\end{align*}
Now we consider the last term and change integration variables
\begin{align*}
\int_{\abs{x-y} \geq \delta} \rho_n(x-y) \, dy &= \int_{\abs{y} \geq
  \delta} \rho_n(y) \, dy \\
&= \frac{1}{\sqrt{2 \pi}} \int_{\abs{y} \geq
  n\delta} e^{-y^2/2} \, dy \\
&\leq \frac{2}{\sqrt{2 \pi}} \int_{n\delta}^\infty \frac{y}{n\delta}
e^{-y^2/2} \, dy \\
&= \frac{2}{n \delta \sqrt{2 \pi}} e^{-n^2\delta^2/2}
\end{align*}
One point here is the elementary fact that $\lim_{n \to \infty} \frac{2}{n \delta
  \sqrt{2 \pi}} e^{-n^2\delta^2/2} = 0$ but the second point is that this limit does
not depend on $x$.   Thus we may pick $N > 0$ independent of $x$, such
that $\int_{\abs{x-y} \geq \delta} \rho_n(x-y) \, dy  <
\frac{\epsilon}{2M}$ for $n > N$ and therefore
\begin{align*}
\abs{f*\rho_n(x) - f(x)} < 2 \epsilon
\end{align*}
which proves the uniform convergence of $f*\rho_n$.
\end{proof}

\subsection{Daniell-Stone Integrals}

We record some required facts about $\sigma$-rings that are completely
analogous to corresponding facts about $\sigma$-algebras.

\begin{lem}Let $X$ be a topological space and let $\mathcal{B}(X)$ be
  the Borel $\sigma$-algebra on $X$.  If $A$ is a Borel set then
  $\lbrace B \in \mathcal{B}(X) \mid B \subset A \rbrace$ is a
  $\sigma$-ring of sets in $X$.
\end{lem}
\begin{proof}
Clearly it contains the empty set and is closed under countable
union.  To see that it is closed under set difference simply note $B
\setminus C = B \cap C^c \subset B \subset A$ and is clearly a Borel
set of $X$.  
\end{proof}
Note that in fact the set of sets in the previous Lemma is the Borel
$\sigma$-algebra of $A$ with the induced topology.

\begin{lem}\label{IntervalsGenerateBorelPunctured}The $\sigma$-ring of Borel sets of $\reals$ that do not
  contain $0$ is generated by intervals $(-\infty, -c)$ and $(c,
  \infty)$ with $c > 0$.
\end{lem}
\begin{proof}
As noted above the $\sigma$-ring in the statement of the Lemma is the
$\sigma$-algebra of $\reals \setminus \lbrace 0 \rbrace$ in the
induced topology.  We know that open sets of $\reals$ are precisely
countable disjoint unions of open intervals (Lemma
\ref{OpenSetsOfReals}).  For any open interval $(a,b)$ we either have
$(a,b) \subset \reals \setminus \lbrace 0 \rbrace$ or $a < 0 < b$
hence $(a,b) \cap \reals \setminus \lbrace 0 \rbrace = (a,0) \cup
(0,b)$.  We conclude that the open sets of $\reals \setminus \lbrace 0
\rbrace$ are countable disjoint unions of open intervals none of which
contains $0$.  Now one can adapt the proof of Lemma
\ref{IntervalsGenerateBorel} to get the result.
\end{proof}


TODO:  Introduce notation for the $\sigma$-ring generated by a
set of sets.

\begin{lem}\label{SigmaRingPullbackGenerators}Let $f : S \to T$ be a set mapping and let $\mathcal{C}
  \subset 2^T$, then the $\sigma$-ring generated by
  $f^{-1}(\mathcal{C})$ is the same as the pullback of the
  $\sigma$-ring generated by $\mathcal{C}$.
\end{lem}
\begin{proof}
It is clear that the $\sigma$-ring generated by $f^{-1}(\mathcal{C})$
is contained in the pullback of the $\sigma$-ring generated by
$\mathcal{C}$.  To see the reverse conclusion, pushforward the
$\sigma$-ring generated by $f^{-1}(\mathcal{C})$; this is equal to
$\lbrace A \subset T \mid f^{-1}(A) \text{ is in the $\sigma$-ring
  generated by } f^{-1}(\mathcal{C})\rbrace$ and is itself a $\sigma$-ring
(Lemma \ref{SigmaRingPullback}).  It clearly contains $\mathcal{C}$ and therefore
the $\sigma$-ring generated by $\mathcal{C}$ as well.  Therefore the
pullback of the $\sigma$-ring generated by $\mathcal{C}$ is contained
in $\sigma$-generated by $f^{-1}(\mathcal{C})$ and we are done.
\end{proof}

It turns out that having a countably additive set function on a
$\sigma$-ring is almost the same thing as having a measure on the
generated $\sigma$-algebra.  This fact is made precise by the
following result.
\begin{lem}\label{ExtendingMeasuresFromSigmaRing}Let $\mathcal{R}$ be a $\sigma$-ring on a set $S$ and let
  $\mu : \mathcal{R} \to \overline{\reals}_+$ be a function that is
  countably additive on disjoint sets.  Let $\mu_*(E) = \sup \lbrace
  \mu(A) \mid A
  \subset E \text{ and } A \in \mathcal{R} \rbrace$ be the inner measure
  defined by $\mu$ on all of $2^S$.  Let $\mathcal{A} = \mathcal{R}
  \cup \mathcal{R}^c$ be the $\sigma$-algebra generated by
  $\mathcal{R}$.
\begin{itemize}
\item[(i)]If we define $\tilde{\mu}(A) = \mu(A)$ for $A \in
  \mathcal{R}$ and $\tilde{\mu}(A) = \infty$ for $A \in \mathcal{R}^c$
  then $\tilde{\mu}$ is a measure on $\mathcal{A}$.
\item[(ii)]For any $b \in \overline{\reals}_+$ if we define
  $\tilde{\mu}(A) = \mu(A)$ for $A \in
  \mathcal{R}$ and $\tilde{\mu}(A) = \mu_*(A) + b$ for $A \in \mathcal{R}^c$
  then $\tilde{\mu}$ is a measure on $\mathcal{A}$.
\item[(iii)]Every measure on $\mathcal{A}$ that extends $\mu$ on
  $\mathcal{R}$ is of the above form.
\item[(iv)]$\mu$ has a unique extension to $\mathcal{A}$ if and only
  if $\mathcal{R} = \mathcal{A}$ or $\mu_*(A) = \infty$ for every $A \in \mathcal{R}^c$.
\end{itemize}
\end{lem}
\begin{proof}
There is nothing to prove if $\mathcal{R} =
\mathcal{A}$ so we assume otherwise.  Note that in this case there are no
disjoint sets in $\mathcal{R}^c$ (if $A,B \in \mathcal{R}^c$ satisfy
$A \cap B = \emptyset$ then taking complements $A^c \cup B^c = S$
which shows $S \in \mathcal{R}$ which implies $\mathcal{R} =
\mathcal{A}$).

To prove the that the proposed set functions are measures we only need
to show countable additivity over all of $\mathcal{A}$.  By the above
comment we can assume that we have $A_1, A_2, \dots \in \mathcal{R}$
and $A_0 \in \mathcal{R}^c$ which are all disjoint.  Recall that $\cup_{i=0}^\infty
A_i \in \mathcal{R}^c$.  For (i) we have
\begin{align*}
\infty &= \tilde{\mu}(\cup_{i=0}^\infty A_i ) & &\text{by
  definition of $\tilde{\mu}$ on $\mathcal{R}^c$} \\
&=\sum_{i=0}^\infty \mu(A_i) & & \text{since $\tilde{\mu}(A_0)=\infty$}
\end{align*}

For (ii) things are a little more complicated.  First we handle the
case of $b=0$.  Since for any $A \in \mathcal{R}$ we have $\mu_*(A) =
\mu(A)$ we simplify notation and let the extension be denoted by $\mu_*$.  Note that 
for any $\epsilon > 0$ we can find $B_0 \in \mathcal{R}$ such that
$B_0 \subset A_0$ and $\mu(B_0) \geq \mu_*(A_0) -\epsilon$.  Then if
we define $B_i = A_i$ for $i=1,2,\dotsc$ we have the $B_i$ are all disjoint sets in
$\mathcal{R}$ and $\cup_{i=0}^\infty B_i \subset \cup_{i=0}^\infty A_i $.  Therefore
\begin{align*}
\mu_*(\cup_{i=0}^\infty A_i) &= \sup \lbrace \mu(C) \mid C \subset
\cup_{i=0}^\infty A_i \text{ and } C \in \mathcal{R}\rbrace \\
&\geq \mu(\cup_{i=0}^\infty B_i) \\
&= \sum_{i=0}^\infty \mu(B_i) \\
&\geq \sum_{i=0}^\infty \mu_*(A_i) - \epsilon \\
\end{align*}
Since $\epsilon$ was arbitrary we conclude
$\mu_*(\cup_{i=0}^\infty A_i) \geq \sum_{i=0}^\infty \mu_*(A_i)$.

To see the other inequality, for any $\epsilon >0$ we can pick $C \in
\mathcal{R}$ such that $C \subset \cup_{i=0}^\infty A_i$ and $\mu(C)
\geq \mu_*(\cup_{i=0}^\infty A_i) - \epsilon$.  Since $A_0 \in
\mathcal{R}^c$ there is a $B_0 \in \mathcal{R}$ such that $A_0 =
B_0^c$ and therefore $C \cap A_0 = C \cap B_0^c = C\setminus B_0 \in
\mathcal{R}$.  Because $A_i \in \mathcal{R}$ for $i=1,2,\dotsc$ we
know that $A_i \cap C \in \mathcal{R}$ for $i=1,2, \dotsc$.  Putting
these two observations together we know can write $C =
\cup_{i=0}^\infty C_i$ where each $C_i = C \cap A_i \in \mathcal{R}$,
$C_i \subset A_i$ and $C_i$ are disjoint.  Now applying the definition
of $\mu_*$ and countable additivity and monotonicity of $\mu$ we see
\begin{align*}
\mu_*(\cup_{i=0}^\infty A_i) - \epsilon &\leq \mu(C) = \sum_{i=0}^\infty \mu(C_i) \leq \sum_{i=0}^\infty \mu_*(A_i)
\end{align*}
Since $\epsilon > 0$ was arbitrary we conclude
$\mu_*(\cup_{i=0}^\infty A_i)  \leq \sum_{i=0}^\infty
\mu_*(A_i)$ and therefore we have proven $\mu_*(\cup_{i=0}^\infty A_i)  = \sum_{i=0}^\infty
\mu_*(A_i)$.

Now we extend the argument to see that defining $\tilde{\mu}(A) =
\mu_*(A) + b$ on $\mathcal{R}^c$ also defines a measure.  Once again
only countable additivity needs to be shown.  As noted
$\cup_{i=0}^\infty A_i \in \mathcal{R}^c$ so using what we have just
proven for $\mu_*$,
\begin{align*}
\tilde{\mu}(\cup_{i=0}^\infty A_i) &= \mu_*(\cup_{i=0}^\infty A_i ) +
b = \mu_*(A_0) + \sum_{i=1}^\infty \mu(A_i) + b = \sum_{i=0}^\infty \tilde{\mu}(A_i)
\end{align*}

To see (iii) we must show that every extension of $\mu$ to
$\mathcal{A}$ has the form $\mu_* + b$ on $\mathcal{R}^c$ for a particular $b \in
\overline{\reals}_+$.  Let $\tilde{\mu}$ be an extension of $\mu$ to
$\mathcal{A}$.  Suppose we have $A_1, A_2 \in
\mathcal{R}^c$.  From monotonicity we know that
$\mu_*(A) \leq \tilde{\mu}(A)$ for every $A\in \mathcal{R}^c$.  So
there exists constants $b_1, b_2 \in \overline{\reals}_+$ such that
$\tilde{\mu}(A_i) = \mu_*(A_i) + b_i$ for $i=1,2$ and we need to show
that $b_1 = b_2$.   In addition since $A_1 \cup A_2 \in \mathcal{R}^c$, there is a $b$ such that
$\tilde{\mu}(A_1 \cup A_2) = \mu_*(A_1 \cup A_2) + b$.  Note that $A_2 \setminus A_1 = A_2 \cap A_1^c = A_1^c
\setminus A_2^c \in \mathcal{R}$ therefore
\begin{align*}
\mu_*(A_1 \cup A_2) + b &= \tilde{\mu}(A_1 \cup A_2) =
\tilde{\mu}(A_1) + \tilde{\mu}(A_2 \setminus A_1) = \mu_*(A_1) + b_1 +
\mu_*(A_2 \setminus A_1)
\end{align*}
which implies $b = b_1$ since $\mu_*$ is a measure.  The same argument
shows that $b = b_2$ hence we see that $b_1 = b_2$ and we are done.

The claim in (iv) is direct consequence of what we have shown.  If
$\mu_*(A) \neq \infty$ for some $A \in \mathcal{R}^c$ then we have
constructed a uncountably infinite number of distinct extension of
$\mu$ given by $\mu_* + b$ on $\mathcal{R}^c$.  On the other hand if
$\mu_*(A) = \infty$ for all $A \in \mathcal{R}^c$ then we know any
extension must be of the form $\mu_* + b$ on $\mathcal{R}^c$ but these
are all equal to $\infty$ so the uniqueness of the extension is established.
\end{proof}

\begin{examp}It is instructive to consider the scenario of the
  previous Lemma in the context of the specific example of the
  $\sigma$-ring generated by taking the set of Borel sets on $\reals$
  that do not contain $0$ and Lebesgue measure.  We are clearly in the
  non-unique case with this example and the different extensions
  correspond to putting point masses with different weights at $0$.
\end{examp}
We have developed tools that enable us to define measures based on
more primitive set functions and this has allowed us to create very
important measures such as Lebesgue measure on $\reals$.  There is
another broad class of results that exist that allow one to construct
measures.  The basic observation is that a measure begets an integral
that is a linear function from measurable functions to the extended
reals hence it makes sense to pose the question of when a linear
functional on some set of measurable functions arises from a measure.
Being in possession of such results we are in a position to construct
measures by constructing linear functionals instead.  In all cases the
results in the space make some assumptions about the space of
measurable functions on which the functional is defined.  In this
section we consider the first result in this class; one that is
distinguished by the fact that it works on general spaces that do not
possess any topological structure.

\begin{defn}Let $\mathcal{L}$ be a real vector space of real valued
  functions on a set $\Omega$.  We say $\mathcal{L}$ is a \emph{vector
    lattice} if given any $f,g \in \mathcal{L}$ we have $f \vee g \in
  \mathcal{L}$ and $f \wedge g \in \mathcal{L}$.
\end{defn}

\begin{prop}If $\mathcal{L}$ is a real vector space of real valued
  functions on a set $\Omega$ such that for any $f,g \in \mathcal{L}$
  we have $f \vee g \in \mathcal{L}$ then $\mathcal{L}$ is a vector lattice.
\end{prop}
\begin{proof}
Simply note that $f \wedge g = -(-f \vee -g)$.  
\end{proof}

\begin{defn}Given a set $\Omega$ and a vector lattice $\mathcal{L}$ of
  real functions on $\Omega$ a \emph{pre-integral} is a linear
  function $I : \mathcal{L} \to \reals$ such that 
\begin{itemize}
\item[(i)]if $f \in \mathcal{L}$ and $f \geq 0$ then $I(f) \geq 0$
\item[(ii)]if $f_1, f_2, \dots \in \mathcal{L}$ such that $f_n
  \downarrow 0$ then $I(f_n) \downarrow 0$.
\end{itemize}
\end{defn}

To construct a measure that corresponds to a pre-integral we make an
intermediate step using the interpretation of an integral as the area
under a curve.  This will provide us with a measure on the product
space $\Omega \times \reals$ and then we will show how we restrict
this measure in an appropriate way to construct the measure that
generates an integral equivalent to $I$.

\begin{thm}\label{Zaanen}Let $\mathcal{L}$ be a vector lattice of
  functions on a set $\Omega$ with a pre-integral $I$.  For any $f, g \in \mathcal{L}$ such
  that $f \leq g$ we define
\begin{align*}
[f, g) &= \lbrace (\omega, t) \in \Omega \times \reals \mid f(\omega)
\leq t < g(\omega) \rbrace
\end{align*}
, $\mathcal{D} = \lbrace [f,g) \mid f,g \in \mathcal{L}
\text{ such that } f\leq g\rbrace$ and $\nu([f,g)) = I(g - f)$.  Then
$\nu$ is countably additive and extends to a measure on the
$\sigma$-algebra generated by $\mathcal{D}$.
\end{thm}
\begin{proof}
The proof proceeds by showing that $\mathcal{D}$ is a semiring, that
$\nu$ is countably additive on $\mathcal{D}$ and by applying Lemma (TODO:).
\end{proof}

\begin{thm}\label{DaniellStoneTheorem}Let $I$ be a pre-integral on a Stone vector lattice
  $\mathcal{L}$.  Then on the $\sigma$-algebra generated by the
  lattice $\mathcal{L}$ there is a measure $\mu$ such that $I(f) =
  \int f \, d\mu$ for all $f \in \mathcal{L}$.  Futhermore the measure
  $\mu$ is uniquely determined on the $\sigma$-ring generated by $\mathcal{L}$.
\end{thm}
\begin{proof}
We proceed by first defining our measure on the $\sigma$-ring
$\mathcal{R}$ generated by the functions $\mathcal{L}$.  This can be extended (not
necessarily uniquely) to a measure on the $\sigma$-algebra using Lemma \ref{ExtendingMeasuresFromSigmaRing}.
Because we have arranged for all of the functions in $\mathcal{L}$ to
be $\mathcal{R}$ measurable their integrals will not depend on the
extension of $\mu$ to a full $\sigma$-algebra and their integrals will
be determined by the values of $\mu$ on $\mathcal{R}$ alone.

Claim 1: $\mathcal{R}$ is generated by sets of the form $f^{-1}(1,
\infty)$ for $f \in \mathcal{L}$.

Note that for $c > 0$, 
\begin{align*}
f^{-1}(c, \infty) &= 
\lbrace \omega \in \Omega \mid f(\omega) \geq c \rbrace = 
\lbrace \omega \in \Omega \mid \left(f/c\right)(\omega) \geq 1 \rbrace =
\left(f/c\right)^{-1}(1, \infty)
\end{align*}
and since $\mathcal{L}$ is a Stone lattice (a fortiori a real vector
space) we know that $f/c \in \mathcal{L}$.  A similar argument
shows that for $c > 0$, $f^{-1}(-\infty, -c) = (-f/c)^{-1}(1,
\infty)$.  We know that intervals $(-\infty, -c)$ and $(c, \infty)$
generate the $\sigma$-ring on $\reals \setminus \lbrace 0 \rbrace$,
therefore for any $f \in \mathcal{L}$, we have
$f^{-1}(\mathcal{B}(\reals\setminus \lbrace 0 \rbrace)$ is the
$\sigma$-ring generated by sets $f^{-1}(c, \infty)$ and
$f^{-1}(-\infty, -c)$ for $c > 0$
(Lemma \ref{SigmaRingPullbackGenerators}) which are the same as the sets
$(f/c)^{-1}(1, \infty)$ for $c \neq 0$. Thus the $\sigma$-ring generated by $\cup_{f
  \in \mathcal{L}} f^{-1}(\mathcal{B}(\reals\setminus \lbrace 0
\rbrace)$ is contained in the $\sigma$-ring generated by $\cup_{f \in
  \mathcal{L}} f^{-1}(1, \infty)$.

Claim 2: We can define a measure $\mu$ on the $\sigma$-algebra
generated by $\mathcal{L}$.

If suffices to define a countably additive set function on the $\sigma$-ring $\mathcal{R}$
(Lemma \ref{ExtendingMeasuresFromSigmaRing}).  We define the measure by embedding $\mathcal{R}$ as
sub-$\sigma$-ring in $\sigma$-algbra $\mathcal{A}$ constructed in
Theorem \ref{Zaanen}.  To see this, suppose that we have a set $A =
f^{-1}(1, \infty)$ with $f \in \mathcal{L}$ and $f \geq 0$.  For arbitrary $c > 0$,
we define
\begin{align*}
f_n(\omega) &= n(f(\omega) - f(\omega) \wedge 1) \wedge c
= \begin{cases}
0 &  \text{if $\omega \notin A$} \\
n(f(\omega) - 1) \wedge c & \text{if $\omega \in A$}
\end{cases}
\end{align*}
and observe that $f_n \in \mathcal{L}$ and $f_n \uparrow c\characteristic{A}$.  Applying this
observation to graphs of $f_n$ in $\Omega \times \reals$ we see that
$A \times [0,c) = [0, c\characteristic{A}) = \cup_{n=1}^\infty [0, f_n)$ which shows that $A
\times [0,c) \in \mathcal{A}$ for all $c > 0$.  From this it follows
that $A \times [0,c) \in \mathcal{A}$ for all $A \in \mathcal{R}$.  To
see this note that for a fixed $c >0$, the set $\mathcal{R}_c = \lbrace A \times [0,c)
\mid A \in \mathcal{R} \rbrace$ is a $\sigma$-ring and the set
$\lbrace A \subset \Omega \mid A \times [0,c) \in \mathcal{R}_c
\rbrace$ is a $\sigma$-ring (it can be constructed as a pushforward
under an appropriately constructed map or one can see it directly)
that contains sets of the form $f^{-1}(1, \infty)$.  Thus,
$\mathcal{R} \subset \lbrace A \subset \Omega \mid A \times [0,c) \in \mathcal{R}_c
\rbrace$.

Having shown that $\mathcal{R}_c$ is a $\sigma$-ring in $\mathcal{A}$,
we take $c=1$ and define $\mu(A) = \nu(A \times [0,1)$.  That this
is countably additive follows from the fact that $\nu$ is a measure,
so we can extend $\mu$ to the $\sigma$-algebra $\mathcal{R} \cup
\mathcal{R}^c$ in any way we chose.

Now we show how to compute integrals of functions $f \in \mathcal{L}$
with respect to $\mu$ and show that they agree with the pre-integral
$I$.
Claim 3:
\end{proof}

It should be remarked that one can develop a good deal of measure an
integration theory starting from some of the concepts introduced in
this section; indeed for a short period of time it was fashionable to
do this instead of taking the approach of developing the theory of
$\sigma$-algebras, measure and integral in the way we have done.
Alas, that fashion has passed so we content ourselves with the most
streamlined presentation of these ideas we know that gives us Theorem \ref{DaniellStoneTheorem}.

\section{Integrals}
\begin{align*}
\int_0^\infty e^{-x^2} dx &= \frac{\sqrt{\pi}}{2} \\
\int_0^\infty x^{2n} e^{-x^2} dx &= \frac{\sqrt{\pi} \left(2n-1\right)!!}{2^{n+1}} \\
\int_0^\infty x^{2n+1} e^{-x^2} dx &= \frac{n!}{2}\\
\end{align*}
\begin{align*}
\Gamma(z) &= \int_0^\infty x^{z-1} e^{-x} dx
\end{align*}
\section{Inequalities}
From time to time in these notes we'll have a need for some simple
inequalities for elementary functions.  The following Lemma collects
them in one place since they are all proven by use of basic calculus.
\begin{lem}\label{BasicExponentialInequalities}The following
  inequalities hold:
\begin{itemize}
\item[(i)] $1+x \leq e^x$ for all $x \in \bold{R}$.
\item[(ii)] $e^x \leq 1 + 2x$ for all $x \in [0,1]$.
\item[(iii)] $e^x \leq 1 + x + x^2$ for all $x \leq 1$.
\item[(iv)] $\frac{1}{2}(e^x + e^{-x}) \leq e^{x^2/2}$ for all $x \in \bold{R}$.
\item[(v)] $1 - \frac{x^2}{2} \leq \cos(x)$ for all $x \in \bold{R}$.
\item[(vi)] $x + \log(1-x) \leq 0$ for all $x \in [0,1)$.
\item[(vii)] $e^{-x} \leq 1 - (1 - e^{-1}) x$ for all $x \in [0,1]$.
\end{itemize}
\end{lem}
\begin{proof}
Note that for ${x\geq0}$ we can consider $f(x) = e^x - x -1$ and note
that $f(0)=0$ and moreover we can see that $f(x)$ has a global minimum
at $x=0$ since $f'(x) = e^x - 1$ vanishes precisely at $x=0$ and
$f''(x)=e^x$ is strictly positive.

In a similar vein to show (ii), define $f(x) = 1+2x-e^x$ and notice that $f(x)$ has
a global maximum at $x=\ln(2)$ and no other local maximum.  Thus, it
suffices to validate the inequality at the endpoints $x=0$ and $x=1$
which is obvious.

To show (iv) we just manipulate series expansions.
\begin{align*}
\frac{1}{2}\left(e^x + e^{-x}\right) & = \frac{1}{2}\left(\sum_{n=0}^\infty
\frac{x^n}{n!} + \sum_{n=0}^\infty \frac{(-x)^n}{n!}\right) \\
& = \sum_{n=0}^\infty \frac{x^{2n}}{(2n)!} \\
& \leq \sum_{n=0}^\infty \frac{x^{2n}}{2^n n!} = e^{\frac{x^2}{2}}\\
\end{align*}

To show (v), define $f(x) = \frac{x^2}{2} -1 +  \cos(x)$.  Calculate
the first derivative $f^\prime(x) = x - \sin(x)$.  The function
$f^\prime(x) = 0$ if and only if $x=0$.  Clearly this is true if
$x=0$, and clearly $f^\prime(x) \neq 0$ for $\abs{x} > 1$.  For the
interval, $\abs{x} \leq 1$ calculate the second derivative
$f^{\prime\prime}(x) = 1 - \cos(x)$ and note that it is strictly
positive for $\abs{x} \leq 1$ and $x \neq 0$.  Thus, $f^\prime(x)$ is
strictly increasing on the intervals $[-1,0)$ and $(0,1]$ and
therefore $f^\prime(x) \neq 0$ on these intervals as well.  Note also
that this argument shows that $f^\prime(x)$ changes sign at $x=0$
which shows that $f(0) = 0$ is a global minimum.

To show (vi), define $f(x) = x + \log(1 -x)$ and differentiate to see
that $f^\prime(x) = 1 - \frac{1}{1-x} = \frac{-x}{1-x} < 0$ for $x \in
(0,1)$.  Therefore $f(x) \leq f(0)=0$ for $x \in [0,1)$.

To show (vii), let $a = 1 - e^{-1}$ and $f(x) = 1 - ax - e^{-x}$.
Take first derivative $f^\prime(x) = -a + e^{-x}$ which has a zero at
$x = -\ln a \approx 0.5$.  Furthermore $f^{\prime \prime}(x) = -e^{-x}
< 0$ so we have a global maximum at $x = -\ln a$, therefore to show
$f(x) \geq 0$  for $x \in [0,1]$ it suffices to show it at the
endpoints: $f(0) = f(1) = 0$.
\end{proof}

\section{Probability}
Here we begin to focus on the special case of probability spaces.  The
development of measure theoretic probabilty begins with the
assumptions that we are given a
\begin{defn}A \emph{probability space} is a measure space $(\Omega,
  \mathcal{A}, P)$ such that $\probability{\Omega} = 1$.
\end{defn}
Given a measurable function $\xi : \Omega \to (S,
\mathcal{S})$ we will refer to $\xi$ as a \emph{random
  element} of $S$.  The special case of a measurable function $\xi : \Omega \to (\reals, \mathcal{B}(\reals))$
is called a \emph{random variable}.  For a random element $\xi$, by
Lemma \ref{PushforwardMeasure} we can
push forward the probability measure to get a measure $(\pushforward{\xi}{P})$ called the \emph{distribution} or \emph{law} of $\xi$. One
sometimes writes $\mathcal{L}(\xi)$ to denote the distribution of
$\xi$ and one writes $\xi \eqdist \eta$ to denote that $\xi$ and $\eta$ have
the same distribution. 

In probability theory the existence of a probability space is critical
to the formal development of the theory however it is almost always
the case that one is only concerned with results that don't depend on
the exact choice of probability space.  To make this statement more
precise we introduce 
\begin{defn}A probability space $(\Omega^\prime, \mathcal{A}^\prime,
  P^\prime)$ is an \emph{extension} of $(\Omega, \mathcal{A},
  P)$ if there is a surjective measurable map $\pi : \Omega^\prime \to
  \Omega$ such that $P = \pushforward{\pi}{P^\prime}$.
\end{defn}

A result is considered properly \emph{probabilistic} if it is
preserved under extension of sample space.  Note that this is a
cultural statement and not a mathematical theorem.  As an example of a
probabilistic concept, we have the ability to talk about an
\emph{event} $A$ and its probability $\probability{A}$ since given any
$\pi$  we can unambiguously refer to $\pi^{-1}(A)$ as the same event
in $\Omega^\prime$ and we know that probability is preserved.  As an
example of a non-probabilistic concept we have the cardinality of an
event.

In keeping with the philosophy that probabilistic results are
invariant under extension of the underlying probability space, we will
follow common practice and try to avoid explicit mention of the underlying
probability space in many definitions and results.  

\begin{defn}Given a random vector $\xi = (\xi_1, \dots, \xi_n)$ in
  $\reals^n$ we define the \emph{distribution function} to be 
\begin{align*}
F(x_1, \dots, x_n) = \probability {\cap_{i=1}^n \left ( \xi_i \leq x_i
    \right ) }
\end{align*}
\end{defn}
\begin{lem}Let $\xi$ and $\eta$ be random vectors in $\reals^n$ with
  distribution functions $F$ and $G$, then
  $\xi \eqdist \eta$ if and only if $F = G$.
\end{lem}
\begin{proof}This follows from Lemma \ref{UniquenessOfMeasure} by
  noting
  that sets of the form $(-\infty, x_1] \times \cdots \times (-\infty,
  x_n]$ form a $\pi$-system that contains $\reals^n$.
\end{proof}

The construction of Lebesgue-Stieltjes measure shows that every
Borel measure on $\reals$ is determined uniquely by its distribution function.
\begin{lem}Probability measures of $(\reals, \mathcal{B}(\reals))$ are
  in one to one correspondence with $F : \reals \to \reals$ that are
  right continuous, nondecreasing such that $\lim_{x \to -\infty} F(x)
  = 0$ and $\lim_{x \to \infty} F(x) = 1$ via the mapping $F(x) =
  \probability{(-\infty, x]}$.
\end{lem}
\begin{proof}Clearly any probability measure is locally finite so we
  apply Lemma \ref{LebesgueStieltjesMeasure} to create a 1-1
  correspondence with $\hat{F}$, 
  right continuous and nondecreasing such that $\probability{(a,b]} =
  \hat{F}(b) - \hat{F}(a)$.  Now define $F(x) = \hat{F}(x) +
  \probability{(-\infty, 0]}$.
\end{proof}

\begin{defn}The \emph{expectation} of a random variable $\xi$ on a
  probability space $(\Omega, \mathcal{A}, P)$ is
  defined to be 
\begin{align*}
\expectation{\xi} = \int \xi \, dP
\end{align*}
\end{defn}
A very useful corollary to the abstract change of variables Lemma
\ref{ChangeOfVariables} is the following
\begin{lem}[Expectation Rule]\label{ExpectationRule}Let $\xi$ be a random variable and $f: \reals \to \reals$
  be a Borel measurable function.  Then 
\begin{align*}
\expectation{f(\xi)} = \int f \, d (\pushforward{\xi}{P})
\end{align*}
In particular, 
\begin{align*}
\expectation{\xi} = \int x \, d (\pushforward{\xi}{P})
\end{align*}
\end{lem}
\begin{proof}This is just a restatement of Lemma
  \ref{ChangeOfVariables} for the special case of random variables and
  measurable functions on $\reals$.
\end{proof}

The following lemma is useful for relating tail bounds and expectations.
\begin{lem}\label{TailsAndExpectations}Let $\xi$ be a positive random variable with finite
  expectation.  Then $\expectation{\xi} = \int_0^\infty \probability{\xi \geq
    \lambda} d\lambda$.
\end{lem}
\begin{proof}
This is just an application of Tonelli's Theorem,
\begin{align*}
\int_0^\infty \probability{\xi \geq \lambda}d\lambda &= \int_0^\infty \left[\int
\characteristic{\xi \geq \lambda} dP\right] d\lambda \\
&= \int\left[\int_0^\infty \characteristic{\xi \geq \lambda} d\lambda\right] dP \\
&= \int\left[\int_0^{\xi} d\lambda\right] dP \\
&= \int \xi \, dP \\
&= \expectation{\xi} \\
\end{align*}
\end{proof}

\begin{lem}[Cauchy Schwartz Inequality]\label{CauchySchwartz}Let $\xi$
  and $\eta$ satisfy $\expectation{\xi^2}, \expectation{\eta^2} < \infty$ then $\xi \eta$ is integrable and
  $\expectation{\xi \eta}^2 \leq \expectation{\xi^2}
  \expectation{\eta^2}$.
\end{lem}
\begin{proof}
Since we have both $0 \leq (\xi + \eta)^2$ and $0 \leq (\xi - \eta)^2$
we have $\abs{\xi \eta} \leq \frac{1}{2}(\xi^2 + \eta^2)$ which shows
that $\xi \eta$ is integrable.

There are a host of different proofs of Cauchy Schwartz inequality.  Here is perhaps
the simplest one.  Note that for all $t \in \reals$, $0 \leq \expectation{(t \xi +
\eta)^2} = \expectation{\xi^2} t^2 + 2 \expectation{\xi \eta} t +
\expectation{\eta^2}$.  The quadratic formula implies that
$\sqrt{4 \expectation{\xi^2} \expectation{\eta^2} - (2
  \expectation{\xi \eta})^2 } \geq 0$ which in turn implies the
result.

The proof we just provided is probably the slickest one available but
has the disadvantage of being very specific to the quadratic case.  
There is a different proof of Cauchy Schwartz that we provide that
involves two steps that have a broader application.  The idea is to
derive Cauchy Schwartz from the trival fact that for all real numbers
$x,y$ we have $xy \leq \frac{x^2}{2} + \frac{y^2}{2}$ (which we used when showing
integrability of $\xi\eta$).  Applying this fact to $\xi$ and $\eta$ we see
that
\begin{align*}
\expectation{\xi \eta} &\leq \frac{\expectation{\xi^2}}{2} + \frac{\expectation{\eta^2}}{2}
\end{align*}
To finish the proof, we apply a \emph{normalization trick} by defining
$\hat{\xi} = \frac{\xi}{\sqrt{\expectation{\xi^2}}}$ and $\hat{\eta} =
\frac{\eta}{\sqrt{\expectation{\eta^2}}}$ so that
$\expectation{\hat{\xi^2}} = \expectation{\hat{\eta^2}} =1$.  Now we apply the above bound and linearity of expectation to see that
\begin{align*}
\frac{1}{\sqrt{\expectation{\xi^2}}\sqrt{\expectation{\eta^2}}}
\expectation{\xi\eta} &= \expectation{\hat{\xi}\hat{\eta}} \leq 1
\end{align*}
which yields the result.
\end{proof}

Applications of Cauchy Schwatz are ubiquitous in analysis.  Only
slightly less common are applications of the following
generalization.  First a definition
\begin{defn}Given any $p > 0$ and random variable $\xi$, the \emph{$L^p$
  norm} of $\xi$ is 
\begin{align*}
\norm{\xi}_p = \left (\expectation{\abs{\xi}^p} \right )^ {\frac{1}{p}}
\end{align*}
\end{defn}
\begin{lem}[H\"{o}lder Inequality]\label{Holder}Given $p,q,r > 0$ such
  that $\frac{1}{r} = \frac{1}{p} + \frac{1}{q}$ and random variables
  $\xi$ and $\eta$, we have
\begin{align*}
\norm{\xi \eta}_r \leq \norm{\xi }_p \norm{\eta}_q
\end{align*}
\end{lem}
\begin{proof}
We start by assuming that $r=1$.  The proof here is a direct generalization of the second proof we
provided for Cauchy Schwartz.  To get started we need to find a
generalization of the simple fact that $xy \leq \frac{x^2}{2} +
\frac{y^2}{2}$.  
 
The inequality we need is called Young's Inequality and is derived
from the following fact.  Let $f$ be an
continuous increasing function $f : [0,c] \to \reals$ such that $f(0)
= 0$.  Then the area interpretation of integral tells us that for $0
\leq a \leq c$ and $0 \leq b \leq f(c)$ we have
\begin{align*}
ab \leq \int_0^a f(x) \, dx + \int_0^b f^{-1}(x) \, dx
\end{align*}
with equality if and only if $b=f(a)$.  

For our case, we first assume that $r = 1$.  Define $f(x) =
x^{p-1}$ then observe that $f^{-1}(x) = x^{q-1}$ since
$1 = \frac{1}{p} + \frac{1}{q}$ is equivalent to $(p-1)(q-1) = 1$.
Therefore we have Young's Inequality, $ab \leq \frac{a^p}{p} +
\frac{b^q}{q}$.

Now applying the normalization trick by defining $\hat{\xi} =
\frac{\abs{\xi}}{\norm{\xi}_p}$ and $\hat{\eta} =
\frac{\abs{\eta}}{\norm{\eta}_q}$ so that ${\norm{\hat{\xi}}_p} =
{\norm{\hat{\eta}}_q} = 1$.  We now apply Young's Inequality to
$\hat{\xi}$ and $\hat{\eta}$ to see
\begin{align*}
\frac{1}{\norm{\hat{\xi}}_p \norm{\hat{\eta}}_q} \expectation{\abs{\xi \eta}} &=
\expectation{\hat{\xi}\hat{\eta}} \leq \frac{1}{p} + \frac{1}{q} = 1
\end{align*}

Lastly we generalize to general $r>0$.  Given $\frac{1}{r} =
\frac{1}{p} + \frac{1}{q}$ we define $\hat{p} = \frac{p}{r}$ and 
$\hat{q} = \frac{q}{r}$ so that $1 =
\frac{1}{\hat{p}} + \frac{1}{\hat{q}}$ and 
\begin{align*}
\expectation{\abs{\xi \eta}^r} &\leq \norm{\xi^r}_{\hat{p}}
\norm{\eta^r}_{\hat{q}} = \norm{\xi}_p^r \norm{\eta}_q^r 
\end{align*}
Taking $r^{th}$ roots we are done.
\end{proof}
\begin{cor}\label{IncreasingMoments}For $p > r > 0$ and any random variable $\xi$, we
  have $\norm{\xi}_r \leq \norm{\xi}_p$.
\end{cor}
\begin{proof}Define $q = \frac{p -r}{pr} > 0$ and apply H\"older's
  Inequality to see that $\norm{\xi}_r \leq \norm{\xi}_p \norm{1}_q = \norm{\xi}_p $.
\end{proof}
It worth noting that the corollary above is generally true on finite
measure spaces but fails for non-finite measure spaces (e.g. consider
$f(x) = \frac{1}{x}$ which has finite $L^p$ norm on $[1,\infty)$ for
$p > 1$ but infinite $L^1$ norm on $[1,\infty)$).

\subsection{Convexity and Jensen's Inequality}
\begin{defn}A function $\varphi : \reals^n \to \reals$ is said to be
  \emph{convex} if for all $x, y \in \reals^n$ and $t \in [0,1]$, we
  have
\begin{align*}
\varphi( tx + (1-t)y) \leq t\varphi(x) + (1-t)\varphi(y)
\end{align*}
$\varphi$ is said to be \emph{strictly convex} if it is convex and for
all $t \in (0,1)$, 
\begin{align*}
\varphi( tx + (1-t)y) < t\varphi(x) + (1-t)\varphi(y)
\end{align*}
\end{defn}

TODO: Convex functions are continuous

Convex functions are almost surely differentiable.
\begin{lem}\label{ThreeChordLemma}Let $\varphi : [a,b] \to \reals$ be convex.  Then for every
  $a < x < b$, we have
\begin{align*}
\frac{\varphi(x) - \varphi(a)}{x - a} &\leq \frac{\varphi(b) -
  \varphi(a)}{b - a}  \leq \frac{\varphi(b) - \varphi(x)}{b - x} 
\end{align*}
If $\varphi$ is strictly convex then the inequalities may be replaced
by strict inequalities.
\end{lem}
\begin{proof}
Note that we can write $x = t a + (1-t) b$
with $t = \frac{b-x}{b-a} \in [0,1]$.  So applying the definition of
convexity we know that $\varphi(x) \leq t \varphi(a) +
(1-t)\varphi(b)$ and using the fact that $1-t = \frac{x-a}{b-a}$ we get
\begin{align*}
\frac{\varphi(x) - \varphi(a)}{x - a} &\leq \frac{t \varphi(a) +
(1-t)\varphi(b) - \varphi(a)}{x - a} = \frac{1-t}{x-a} (\varphi(b) -
\varphi(a) ) = \frac{\varphi(b) -
  \varphi(a)}{b - a} 
\end{align*}
and in a similar way,
\begin{align*}
\frac{\varphi(b) - \varphi(x)}{b - x} &\geq \frac{ \varphi(b)  - t \varphi(a) -
(1-t)\varphi(b) }{b -x} = \frac{t}{b-x} (\varphi(b) -
\varphi(a) ) = \frac{\varphi(b) -
  \varphi(a)}{b - a} 
\end{align*}
It is clear from the definition of strict convexity that the
inequalities above may be replaced by strict inequalities if $\varphi$ is strictly convex.
\end{proof}
\begin{lem}\label{ConvexHasDini}Let $\varphi : [a,b] \to \reals$ be a convex function, then
  for every $x \in (a,b)$, $D^-\varphi(x)$ and $D^+\varphi(x)$ exist
  and furthermore for $a < x < y < b$ we have
\begin{align*}
D^-\varphi(x) &\leq D^+\varphi(x) \leq \frac{\varphi(y) -
  \varphi(x)}{y - x} \leq D^-\varphi(y) \leq D^+\varphi(y)
\end{align*}
If $\varphi$ is strictly convex then we have
\begin{align*}
D^+\varphi(x) &< \frac{\varphi(y) -  \varphi(x)}{y - x} < D^-\varphi(y)
\end{align*}
\end{lem}
\begin{proof}
Lemma \ref{ThreeChordLemma} shows that for $a < x < b$ and $h > 0$,
$\frac{\varphi(x + h) - \varphi(x)}{h}$ is an increasing function of
$h$ bounded below by $\frac{\varphi(x) - \varphi(a)}{x - a}$.  Thus $D^+\varphi(x) = \lim_{h \downarrow 0} \frac{\varphi(x + h) -
  \varphi(x)}{h}$ is a decreasing limit hence exists.
Similarly $\frac{\varphi(x - h) -
  \varphi(x)}{-h}=\frac{\varphi(x)-\varphi(x - h)}{h}$ is an decreasing
function of $h$ bounded above by $\frac{\varphi(b) - \varphi(x)}{b - x}$.  Thus $D^-\varphi(x) = \lim_{h \downarrow 0} \frac{\varphi(x - h) -
  \varphi(x)}{-h}$ is a bounded increasing limit hence
exists.  

The inequalities follow directly from Lemma \ref{ThreeChordLemma}.
For example, since $D^+\varphi(x) = \lim_{h \downarrow 0} \frac{\varphi(x + h) -
  \varphi(x)}{h}$ and for all $x < x+h < y$, we have $\frac{\varphi(x + h) -
  \varphi(x)}{h} \leq \frac{\varphi(y) -
  \varphi(x)}{y-x}$ we get $D^+\varphi(x) \leq \frac{\varphi(y) -
  \varphi(x)}{y-x}$.  In the strictly convex case, we know that for
any $w$ with
$x < w < y$ we have  by what we have just shown and
another application of Lemma \ref{ThreeChordLemma}
\begin{align*}
D^+\varphi(x) &\leq \frac{\varphi(w) -
  \varphi(x)}{w-x} < \frac{\varphi(y) -
  \varphi(x)}{y-x}
\end{align*}  The case of $D^-\varphi(y)$ follows analogously.
\end{proof}
\begin{cor}\label{ConvexHasSubderivative}Let $\varphi : [a,b] \to
  \reals$ be convex then for $x \in (a,b)$ there exists constants $A,B
  \in \reals$ such that $Ay + B \leq \varphi(y)$ for all $y \in [a,b]$
  and $Ax + B = \varphi(x)$.  If $\varphi$ is strictly convex then we
  may assume that $Ay + B < \varphi(y)$ for $y \neq x$.
\end{cor}
\begin{proof}
By Lemma \ref{ConvexHasDini} we can pick $D^-(x) \leq A \leq D^+(x)$.
Also by that result we know that for all $h > 0$, in fact we
have
\begin{align*}
\frac{\varphi(x) - \varphi(x-h)}{h} &\leq A \leq \frac{\varphi(x+h) - \varphi(x)}{h}
\end{align*}
which gives the result upon clearing denominators and defining $B =
\varphi(x)$.
Once again, the strictly convex case follows easily.
\end{proof}
TODO: Extend this to $\reals^n$ (presumably this can be done by taking
partial Dini Derivatives.

\begin{thm}[Jensen's Inequality]\label{Jensen}Let $\xi$ be a random vector in $\reals^n$
  and $\varphi : \reals^n \to \reals$ be a convex function such that
  $\xi$ and $\varphi(\xi)$ are integrable.  Then 
\begin{align*}
\varphi(\expectation{\xi}) \leq \expectation{\varphi(\xi)}
\end{align*}
If $\varphi$ is strictly convex then we have $\varphi(\expectation{\xi}) = \expectation{\varphi(\xi)}$ if and only if $\xi =
\expectation{\xi}$ a.s.
\end{thm}
\begin{proof}We use the fact that for every $x \in \reals^n$ we have a
  subdifferential $\langle a, y \rangle + b$ that satisfies
\begin{align*}
\langle a, y \rangle + b &\leq \varphi(y) \\
\langle a,x \rangle + b &= \varphi(x)
\end{align*}
In particular, choose such an $a,b \in \reals^n$ for the choice $x =
\expectation{\xi}$.  Then by monotonicity and linearity of integral
\begin{align*}
\expectation{\varphi(\xi)} &\geq \expectation{\langle a, \xi\rangle +
  b} \\
&= \langle a, \expectation{\xi} \rangle + b = \varphi(\xi)
\end{align*}
which gives the result.

If $\varphi$ is strictly convex then when $\xi \neq \expectation{\xi}$,
we have 
\begin{align*}
0 &< \varphi(\xi) - \varphi(\expectation{\xi}) - \langle a , \xi -
\expectation{\xi} \rangle
\end{align*}  Thus if $\varphi(\expectation{\xi}) =
\expectation{\varphi(\xi)}$ using linearity of expectation
\begin{align*}
\expectation{ (\varphi(\xi) - \varphi(\expectation{\xi}) - \langle a , \xi -
\expectation{\xi}\rangle); \xi \neq \expectation{\xi}} &= \expectation{\varphi(\xi) - \varphi(\expectation{\xi}) - \langle a , \xi -
\expectation{\xi}\rangle} \\
&=0
\end{align*}
from which we conclude $\characteristic{\xi \neq \expectation{\xi}} = 0$ a.s.
\end{proof}

\input Independence

\input ConvergenceOfRandomVariables

\section{Lindeberg's Central Limit Theorem}

The Law of Large Numbers tells us that when we are given i.i.d. random
variables $\xi_i$ with finite expectation, we have almost sure
convergence of $\frac{1}{n} \sum_{k=1}^n \xi_k =
\expectation{\xi_i}$.  Using different notation we can say, 
\begin{align*}
\sum_{k=1}^n \xi_k - n \expectation{\xi_i}  = o(n)
\end{align*}
From one point of view, the Central Limit Theorem arises from asking
the question about whether $o(n)$ can be replaced by $o(n^p)$ or
$\mathcal{O}(n^p)$ for $p < 1$.  In this sense the Central Limit
Theorem gives some information about the rate of convergence of the sums
$\frac{1}{n} \sum_{k=1}^n \xi_k$ to their limit.

First some intuition about the Central Limit Theorem.  Let's assume
that we have a sequence of i.i.d. random variables $\xi_i$ such that
$\xi_i$ has moments of all orders (a much stronger assumption than one
needs for the CLT).  We also assume 
\begin{align*}
\expectation{\xi_i} = 0, \expectation{\xi_i^2} =1
\end{align*}
Consider the following computation of the moments
of the partial sums of $\xi_i$.  Let $S_n = \xi_1 + \cdots + \xi_n$.
\begin{align*}
\expectation{S_n^{m+1}} &= \expectation{(\xi_1
  + \cdots + \xi_n) (\xi_1
  + \cdots + \xi_n)^{m}} \\
&= \sum_{i=1}^n \expectation{\xi_i (\xi_n + S_{n-1})^m }\\
&= n \expectation{\xi_n (\xi_n + S_{n-1})^m} \textrm{ TODO: don't know how
  to prove this step}\\
&= n \sum_{j=0}^m \binom{m}{j} \expectation{\xi_n^{j+1}}
\expectation{S_{n-1}^{m-j}} \\
&= n m \expectation{S_{n-1}^{m-1}} + n \sum_{j=2}^m \binom{m}{j} \expectation{\xi_n^{j+1}}
\expectation{S_{n-1}^{m-j}} \\
\end{align*}
Now define $\hat{S}_n = S_n/\sqrt{n}$, and divide both sides of the
above by $n^{\frac{m+1}{2}}$ and we see
\begin{align*}
\expectation{\hat{S}_n^{m+1}} = m \expectation{\hat{S}_n^{m-1}} + \sum_{j=2}^m \binom{m}{j} \frac{1}{n^\frac{j-1}{2}}\expectation{\xi_n^{j+1}}
\expectation{\hat{S}_{n-1}^{m-j}}
\end{align*}
An induction on $m$ together with the observation that $\expectation{\hat{S}_n^0} =
1$ and $\expectation{\hat{S_n}} = 0$ shows that 
\begin{align*}
\lim_{n \to \infty} \expectation{\hat{S}_n^{2m+1}}&=0 \\
\lim_{n \to \infty} \expectation{\hat{S}_n^{2m}}&=\prod_{j=1}^m (2j-1)
= \frac{(2m)!}{2^m m!}\\
\end{align*}
We can recognize that these are the moments of the standard normal
distribution.

The above argument is one path to use to see how Gaussian
distributions might arise when looking at sums of i.i.d random
variables but relies on an unecessarily strong set of assumptions (not
to mention it ignores the fact that moments alone to not characterize
a distribution).

In fact convergence to normal distributions is more general than
i.i.d. variables and we look for a version that has a rather precise set of
assumptions called the Lindeberg conditions.  The statement of the
result and the corresponding notation is unwieldy but the proof itself
doesn't seem to suffer much from the added complexity.  Furthermore
the added generality provides a useful space to explore when examining the limits of asymptotic normality.

\begin{thm}[Lindeberg]\label{LindebergTheorem}Let $\xi_1, \xi_2, \dots$ be independent square
  integrable random variables $\expectation{\xi_m} = 0$ and
  $\expectation{\xi_m^2} = \sigma_m^2 > 0$.  Define
\begin{align*}
S_n &= \sum_{i=1}^n \xi_i \\
\Sigma_n &= \sqrt{\sum_{i=1}^n \sigma_i^2}\\
\hat{S}_n &= \frac{S_n}{\Sigma_n} \\
r_n &= \max_{1 \leq i \leq n} \frac{\sigma_i}{\Sigma_n} \\
g_n(\epsilon) &= \frac{1}{\Sigma_n^2} \sum_{i=1}^n \expectation{\xi_i^2
  \characteristic{|\xi_i| \geq \epsilon \Sigma_n}}
\end{align*}
and let $d \gamma = \frac{1}{\sqrt{2\pi}} e^{\frac{-x^2}{2}} dx$ be the
  distribution of an $N(0,1)$ random variable.  Now for all $\epsilon > 0$, $\varphi \in C^3(\reals;\reals)$ with
bounded 2nd and 3rd derivative,
\begin{align*}
\left \lvert \expectation{\varphi(\hat{S_n})} - \int_\reals \varphi
  d\gamma \right \rvert \leq
\left ( \frac{\epsilon}{6} + \frac{r_n}{2}\right ) \norm{\varphi^{\prime\prime\prime}}_\infty + g_n(\epsilon) \norm{\varphi^{ \prime
\prime}}_\infty
\end{align*}
and 
\begin{align*}
r_n^2 \leq \epsilon^2 + g_n(\epsilon)
\end{align*}
In particular, if $\lim_{n \to \infty} g_n(\epsilon) = 0$ for every
$\epsilon > 0$, then 
\begin{align*}
\lim_{n \to \infty} \left \lvert \expectation{\varphi(\hat{S_n})} - \int_\reals \varphi
  d\gamma \right \rvert = 0
\end{align*}
\end{thm}
Before attacking the proof we note how everything specializes in the
case of i.i.d. random variables.  In this case $\Sigma_n = \sqrt{n}
\sigma$, $\hat{S}_n = \frac{\sum_{i=1}^n \xi_i}{\sqrt{n} \sigma}$ and
$g_n(\epsilon) = \frac{1}{\sigma^2} \expectation{\xi^2 ; \abs{\xi}
  \geq \epsilon \sqrt{n} \sigma}$.  Because $\expectation{\xi^2}
< \infty$ we know that $\xi^2 < \infty$ a.s.  and we have
$\xi^2 \characteristic{\abs{\xi}  \geq \epsilon \sqrt{n} \sigma}
\toas 0$.  Noting $\xi^2 \characteristic{\abs{\xi}  \geq \epsilon \sqrt{n}
  \sigma} \leq \xi^2$, Dominated Convergence tells us that $\lim_{n
\to \infty} g_n(\epsilon) = 0$.

This special case also sheds some light on aspects of the hypotheses.
For example, the $\sqrt{n}$ in the denominator is the only possible
choice to acheive convergence to a random variable with finite
non-zero variance; it is precisely the term requires to make
$\sigma(\hat{S}_n)$ converge to a finite non-zero number (in fact in
the i.i.d. case it makes the sequence constant).  

It is also worth spending some time understanding the nature of
$g_n(\epsilon)$.  First, it is clear from independence and definitions that 
\begin{align*}
\expectation{\hat{S}_n^2} &= \sum_{i=1}^n \expectation{\left
    (\frac{\xi_i}{\Sigma_n} \right )^2} = \frac{1}{\Sigma_n^2}
\sum_{i=1}^n \sigma_i^2 = 1
\end{align*}
but we can also write 
\begin{align*}
g_n(\epsilon) &= \frac{1}{\Sigma_n^2} \sum_{i=1}^n \expectation{\xi_i^2
  \characteristic{|\xi_i| \geq \epsilon \Sigma_n}} = \sum_{i=1}^n
\expectation{\left (\frac{\xi_i}{\Sigma_n}\right )^2
  ; \abs{\frac{\xi_i}{\Sigma_n}} \geq \epsilon}
\end{align*}
So the $\hat{S}_n$ is the sum of $\xi_i$ normalized to maintain a
constant unit variance.  Our assumption that $\lim_{n \to \infty}
g_n(\epsilon) = 0$ is an assertion that in the limit, all of that unit
variance is contained in a bounded region around $0$.  In the
i.i.d. case that is clearly true because all of the unscaled $\xi_n$
have their ``energy'' in a constant fashion, so rescaling is able to
concentrate that energy arbitrarily close to $0$.  It is permissible
to have the energy of the $\xi_n$ moving off to infinity but only if
it travels at a rate less than $\sqrt{n}$.

TODO: Question is it possible to satisfy the Lindeberg condition when
$\lim_{n \to \infty} \Sigma_n < \infty$?

\begin{proof}
Fix an $n >0$ and define $\hat{\xi}_m = \frac{\xi_m}{\Sigma_n}$ and
$\hat{S}_n = \hat{\xi}_1 + \cdots + \hat{\xi}_n$.  Note that
$\expectation{\hat{S}_n^2} = 1$.  Let $\eta_1, \eta_2, \dots$ be independent $N(0,1)$ random variables
that are also independent of the $\xi_i$.  Note that we may have to
extend $\Omega$ in order to arrange this (e.g. extend by $[0,1]$ and
use Theorem \ref{ExistenceCountableIndependentRandomVariables}).  We
rescale each $\eta_i$ so that it has the same variance as
$\hat{\xi_i}$; define $\hat{\eta}_i =
\frac{\sigma_i \eta_i}{\Sigma_n}$ and $\hat{T}_n = \hat{\eta}_1 +
\cdots + \hat{\eta}_n$.  Notice that
$\expectation{\hat{\eta}_m^2} =
\expectation{\hat{\xi}_m^2} = \frac{\sigma_m^2}{\Sigma_n^2}$ and $\hat{T}_n$ is also a $N(0,1)$
random variable.  Therefore, by the Expectation Rule (Lemma
\ref{ExpectationRule}) $\int \varphi \, d\gamma =
\expectation{\varphi(\hat{T}_n)}$ and we can write 
\begin{align*}
\left \lvert \expectation{\varphi(\hat{S_n})} - \int_\reals \varphi
  d\gamma \right \rvert = \left \lvert \expectation{\varphi(\hat{S_n})} - \expectation{\varphi(\hat{T_n})} \right \rvert
\end{align*}
By having arranged for $\hat{\xi}_i$ and $\hat{\eta}_i$ to have same
first and second moments so one should be thinking that we have
constructed a ``second order approximation''.  TODO: What is critical
is that the approximation of the individual $\hat{\xi}_i$ may not be a
good one, the approximation $\hat{S_n}$ by $\hat{T_n}$ is a good one.
Find the critical point(s) in the proof where this comes to light.

The real trick of the proof is to interpolate between
$\varphi(\hat{S_n})$ and $\varphi(\hat{T_n})$ by exchanging
$\hat{\xi}_i$ and $\hat{\eta}_i$ one summand at a time.  By varying
only one summand we will then be able use Taylor's Theorem to
estimate the differences between the terms.  Concretely we write,
\begin{align*}
\varphi(\hat{S_n}) - \varphi(\hat{T_n}) &= \varphi(\hat{\xi}_1 + \cdots + \hat{\xi}_n) -
\varphi(\hat{\eta}_1 + \cdots + \hat{\eta}_n) \\
&= \varphi(\hat{\xi}_1 + \cdots + \hat{\xi}_n) - \varphi(\hat{\eta}_1 +
\hat{\xi}_2 + 
\cdots + \hat{\xi}_n) \\
&+ \varphi(\hat{\eta}_1 +
\hat{\xi}_2 +
\cdots + \hat{\xi}_n) - 
\varphi(\hat{\eta}_1 + \hat{\eta}_2 + \hat{\xi}_3 + \cdots + \hat{\xi}_n) \\
&+ \cdots\\
&+ \varphi(\hat{\eta}_1 +
\cdots  +
\hat{\eta}_{n-1} + \hat{\xi}_n) - 
\varphi(\hat{\eta}_1 + \cdots + \hat{\eta}_n) \\
\end{align*}
Since we have to manipulate these terms a bit, it helps to clean up
the notation by defining:
\begin{align*}
U_m &= \begin{cases}
\hat{\xi}_2 + \cdots + \hat{\xi}_n & \text{if $m=1$} \\
\hat{\eta}_1 + \cdots + \hat{\eta}_{m-1}+\hat{\xi}_{m+1} + \cdots +
\hat{\xi}_n & \text{if $1 < m<n$} \\
\hat{\eta}_1 + \cdots + \hat{\eta}_{n-1} & \text{if $m=n$} \\
\end{cases}
\end{align*}
and then we can write the above interpolation as 
\begin{align*}
\varphi(\hat{S_n}) - \varphi(\hat{T_n}) &= \sum_{m=1}^n \varphi(U_m + \hat{\xi}_m) -\varphi(U_m + \hat{\eta}_m)
\end{align*}
Now we can take absolute values, use the triangle inequality and use
linearity of expectation to see
\begin{align*}
\abs{\expectation{\varphi(\hat{S_n}) - \varphi(\hat{T_n})}} &\leq
\sum_{m=1}^n \abs{\expectation{\varphi(U_m + \hat{\xi}_m)}
  -\expectation{\varphi(U_m + \hat{\eta}_m)}}\\
&=\sum_{m=1}^n \abs{\expectation{\varphi(U_m + \hat{\xi}_m)
  -\varphi(U_m + \hat{\eta}_m)}}
\end{align*}

Now we focus on each term $\varphi(U_m + \hat{\xi}_m) -\varphi(U_m +
\hat{\eta}_m)$ by applying Taylor's Formula (Theorem
\ref{TaylorsTheorem}) to see 
\begin{align*}
\varphi(U_m + x) = \varphi(U_m) + x \varphi^\prime(U_m) +
\frac{x^2}{2} \varphi^{\prime\prime}(U_m) + R_m(x)
\end{align*}
where
\begin{align*}
R_m(x) &=  \int_{U_m}^{U_m+x}
\frac{(U_m + x - t)^2}{2} \varphi^{\prime \prime \prime}(t) \, dt
\end{align*}

For example,applying this expansion with $x = \hat{\xi}_m$, using
linearity of expectation, independence of
$\hat{\xi}_m$ and $U_m$ and Lemma \ref{IndependenceExpectations} we get
\begin{align*}
\expectation{\varphi(U_m + \hat{\xi}_m) } &= \expectation{
  \varphi(U_m)+ \hat{\xi}_m \varphi^\prime(U_m) +
\frac{\hat{\xi}_m^2}{2} \varphi^{\prime\prime}(U_m) +
R_m(\hat{\xi}_m)} \\
&=\expectation{
  \varphi(U_m)} + \frac{\sigma_m^2}{2\Sigma_n^2}\expectation{\varphi^{\prime\prime}(U_m)} + \expectation{R_m(\hat{\xi}_m)}
\end{align*}
and in exactly the same way because we have arrange for $\hat{\xi}_m$
and $\hat{\eta}_m$ to share the first two moments, we get
\begin{align*}
\expectation{\varphi(U_m + \hat{\eta}_m) } &=\expectation{
  \varphi(U_m)} + \frac{\sigma_m^2}{2\Sigma_n^2}\expectation{\varphi^{\prime\prime}(U_m)} + \expectation{R_m(\hat{\eta}_m)}
\end{align*}
Thus, $\expectation{\varphi(U_m + \hat{\xi}_m)
  -\varphi(U_m + \hat{\eta}_m)} = \expectation{R_m(\hat{\xi}_m)} -
\expectation{R_m(\hat{\eta}_m)}$ and 
\begin{align*}
\abs{\expectation{\varphi(\hat{S_n}) - \varphi(\hat{T_n})}} &\leq 
\sum_{m=1}^n \abs{\expectation{R_m(\hat{\xi}_m) }} +  \sum_{m=1}^n \abs{\expectation{R_m(\hat{\eta}_m) }}
\end{align*}

We complete the proof by bounding each expectation above.  On the one hand, there is the Lagrange Form for the
remainder term (Lemma \ref{LagrangeFormRemainder}) that shows that $R_m(x) =
\varphi^{\prime\prime\prime}(c) \frac{x^3}{6}$ for some $c \in [U_m,
U_m+x]$ hence $\abs{R_m(x)} \leq
\norm{\varphi^{\prime\prime\prime}}_\infty \frac{\abs{x}^3}{6}$. On
the other hand, sticking with the integral form of the remainder term, since $t \in [U_m, U_m + x]$ we can bound the term $(U_m + x - t)^2 \leq \abs{x}^2$ in
the integral and integrate to conclude 
\begin{align*}
\abs{R_m(x)} &=  \int_{U_m}^{U_m+x}
\frac{(U_m + x - t)^2}{2} \varphi^{\prime \prime \prime}(t) \, dt \leq
\frac{\abs{x}^2}{2} \int_{U_m}^{U_m+x}
\varphi^{\prime \prime \prime}(t) \, dt \\
&=\frac{\abs{x}^2}{2} \left(\varphi^{\prime \prime}(U_m+x) - \varphi^{\prime \prime}(x)\right) \leq \norm{\varphi^{\prime\prime}}_\infty \abs{x}^2
\end{align*}

With this setup, pick $\epsilon >0$ and first consider the remainder term $R_m(\hat{\xi}_m) $
and a note that we have to be a little careful.  We would like to use
the stronger $3^{rd}$ moment bound however we have not
assumed that $\hat{\xi}_m$ has a finite $3^{rd}$ moment.  So what we
do is truncate $\hat{\xi}_m$ and take a $2^{nd}$ moment bound over the
tail (valid because of the finite variance assumption) and use a
$3^{rd}$ moment bound on the truncated $\hat{\xi}_m$.  The details follow:
\begin{align*}
\abs{\expectation{R_m(\hat{\xi}_m) }} &\leq
\abs{\expectation{R_m(\hat{\xi}_m) ; \abs{\hat{\xi}_m} \leq \epsilon}}
+ \abs{\expectation{R_m(\hat{\xi}_m) ; \abs{\hat{\xi}_m} > \epsilon}}
\end{align*}
We take the sum of first terms and apply the Taylor's formula bound to see 
\begin{align*}
\sum_{m=1}^n \abs{\expectation{R_m(\hat{\xi}_m) ; \abs{\hat{\xi}_m} \leq
    \epsilon}}
&\leq
\frac{\norm{\varphi^{\prime\prime\prime}}_\infty}{6} \sum_{m=1}^n \abs{\expectation{\abs{\hat{\xi}_m}^3
    ; \abs{\hat{\xi}_m} \leq \epsilon}} \\
&\leq
\epsilon \frac{\norm{\varphi^{\prime\prime\prime}}_\infty}{6} \sum_{m=1}^n
\abs{\expectation{\abs{\hat{\xi}_m}^2}} \\
&=
\epsilon \frac{\norm{\varphi^{\prime\prime\prime}}_\infty}{6} \sum_{m=1}^n
\frac{\sigma_m^2}{\Sigma_n^2} = \epsilon \frac{\norm{\varphi^{\prime\prime\prime}}_\infty}{6} \\
\end{align*}
Next take the sum of the second terms to see
\begin{align*}
\sum_{m=1}^n \abs{\expectation{R_m(\hat{\xi}_m) ; \abs{\hat{\xi}_m} >
    \epsilon}}
&\leq
\norm{\varphi^{\prime\prime}}_\infty\sum_{m=1}^n \abs{\expectation{\abs{\hat{\xi}_m}^2
    ; \abs{\hat{\xi}_m} > \epsilon}} \\
&=
\norm{\varphi^{\prime\prime}}_\infty \frac{1}{\Sigma_n^2}\sum_{m=1}^n \abs{\expectation{\abs{\xi_m}^2
    ; \abs{\xi_m} > \epsilon \Sigma_n}} \\
&= \norm{\varphi^{\prime\prime}}_\infty g_\epsilon(n)
\end{align*}
Lastly, to bound the remainder term on $\hat{\eta}_m$ we can directly
appeal to the $3^{rd}$ moment bound because as a normal random
variable $\hat{\eta}_m$ has finite moments of all orders:
\begin{align*}
\sum_{m=1}^n \abs{\expectation{R_m(\hat{\eta}_m) }}
&\leq
\frac{\norm{\varphi^{\prime\prime\prime}}_\infty}{6} \sum_{m=1}^n
\abs{\expectation{\abs{\hat{\eta}_m}^3}} \\
&= \frac{\norm{\varphi^{\prime\prime\prime}}_\infty}{6} \sum_{m=1}^n \frac{\sigma_m^3}{\Sigma_n^3}
\abs{\expectation{\abs{\eta_m}^3}} \\
&= \frac{r_n \norm{\varphi^{\prime\prime\prime}}_\infty}{6} \sum_{m=1}^n \frac{\sigma_m^2}{\Sigma_n^2}
\abs{\expectation{\abs{\eta_m}^3}} \\
&= \frac{r_n \norm{\varphi^{\prime\prime\prime}}_\infty}{6}
\frac{2\sqrt{2}}{\sqrt{\pi}} < \frac{r_n \norm{\varphi^{\prime\prime\prime}}_\infty}{2}
\end{align*}

TODO: We used a calculation of the $3^{rd}$ absolute moment of the
standard normal distribution ($\frac{2\sqrt{2}}{\sqrt{\pi}}$).  We need to record that calculation somewhere.

The last thing to show is the bound on $r_n^2$.  For each $n >0$ and
$1 \leq m \leq n$,
\begin{align*}
\frac{\sigma_m^2}{\Sigma_n^2} &= \frac{1}{\Sigma_n^2} \left(
  \expectation{\xi_m^2 ; \abs{\xi_m} < \epsilon \Sigma_n} +
  \expectation{\xi_m^2 ; \abs{\xi_m} \geq \epsilon \Sigma_n} \right) \\
&\leq \frac{1}{\Sigma_n^2} \left(\epsilon^2 \Sigma_n^2 + \Sigma_n^2
  g_n(\epsilon) \right) = \epsilon^2 + g_n(\epsilon)
\end{align*}
hence $r_n^2 = \max_{1\leq m \leq n} \frac{\sigma_m^2}{\Sigma_n^2}
\leq \epsilon^2 + g_n(\epsilon)$.
\end{proof}

Note that the Lindeberg condition is a sufficient condition but not a
necessary condition for convergence to a normal distribution; but is
not too far off.  Thus it is useful to examine a case in which we
don't satisfy the condition.
\begin{examp}[Failure of Lindeberg Condition]Let $\xi_n$ be a sequence
  of independent random
  variables such that $\xi_n = n$ with probability $\frac{1}{2n^2}$, $\xi_n =
  -n$ with probability $\frac{1}{2n^2}$ and $\xi_n = 0$ with
  probability $1-\frac{1}{n^2}$.  Note that $\variance{\xi_n} =
  (-n)^2 \cdot \frac{1}{2n^2} + 0 \cdot (1 - \frac{1}{2n^2}) +  n^2 \cdot
  \frac{1}{2n^2} = 1$.  
$\sum_{n=1}^\infty
  \probability{\xi_n \neq 0} = \sum_{n=1}^\infty \frac{1}{n^2} <
  \infty$ so by Borel Cantelli, we have $\xi_n$ are eventually $0$
  a.s.; hence $S_n = \sum_{i=1}^n \xi_i$ is bounded a.s. and $\lim_{n
    \to \infty} \frac{S_n}{\sqrt{n}} = 0$ a.s.  Therefore,
  $\frac{S_n}{\sqrt{n}}$ does not converge to a Gaussian in
  distribution.

We know that $\xi_n$ must not satisfy the Lindeberg condition and it
is instructive to perform that calculation explicitly.  Using the
notation of Theorem \ref{LindebergTheorem}, $\Sigma_n = \sqrt{n}$,
thus for any $\epsilon >
0$, and $n > \epsilon^2$, we have 
\begin{align*}
\xi_n \cdot \characteristic{\abs{\xi_n} >
  \epsilon \Sigma_n} &= \xi_n \cdot \characteristic{\abs{\xi_n} >
  \epsilon \sqrt{n}} = \xi_n
\end{align*}
so only a finite number of summands of $\expectation{ \xi_n^2 ;
  \abs{\xi_n} >  \epsilon \sqrt{n}}$ are different from $1$, hence
\begin{align*}
\lim_{n \to \infty} g_n(\epsilon) &=\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n \expectation{ \xi_n^2 ;
  \abs{\xi_n} >  \epsilon \sqrt{n}} = 1
\end{align*}
\end{examp}

TODO: Mention Feller-Lindeberg Theorem that adds an addition
hypothesis that makes the Lindeberg condition equivalent to
asymptotic normality.

The Lindeberg Theorem above doesn't actually prove weak convergence
because of the differentiability assumption on the function $\varphi$.
Our next step is to use approximation arguments to show that we in
fact get weak convergence.  The argument has broader applicability
than the Central Limit Theorem and is just a validation that proving weak
convergence for random vectors only requires use compactly supported
smooth test functions.
\begin{lem}\label{WeakConvergenceWithSmoothTestFunctions}Let $\xi, \xi_1, \xi_2, \dots$ be random vectors in $\reals^N$,
  then $\xi_n \todist \xi$ if and only if $\lim_{n \to \infty}
  \expectation{f(\xi_n)} = \expectation{f(\xi)}$ for all $f \in
  C^\infty_c(\reals^N; \reals)$.
\end{lem}
\begin{proof}
Since any $f \in  C^\infty_c(\reals^N; \reals)$ is bounded we
certainly see that $\xi_n \todist \xi$ implies $\lim_{n \to \infty}
  \expectation{f(\xi_n)} = \expectation{f(\xi)}$.

In the other direction, take an arbitrary $f \in C_b(\reals^N;
\reals)$ and pick $\epsilon > 0$.  By Lemma \ref{ApproximationByMollifiers}, we can find $f_n
\in C^\infty_c(\reals^N; \reals)$ such that $f_n$ converges uniformly
on compact sets and $\norm{f_n}_\infty \leq \norm{f}_\infty$.  
The idea of the proof is to note that for any
$n, k \geq 0$, we have
\begin{align*}
\abs{\expectation{f(\xi_n) - f(\xi)}} &\leq \abs{\expectation{f(\xi_n)
    - f_k(\xi_n)}} + \abs{\expectation{f_k(\xi_n)    - f_k(\xi)}} + \abs{\expectation{f_k(\xi) - f(\xi)}}
\end{align*}
and then to bound each term on the right hand side.  The second term
will be easy to handle because of our hypothesis and the smoothness of
$f_k$.   The first and
third terms will require that we examine the approximation provided by
the uniform convergence of the $f_k$ on all compact sets.


The first task we have
is to pick that compact set; it turns out that it suffices to consider
closed balls centered at the origin.  For any $R \in \reals$ with $R>0$,
 there exists a $\psi_R \in C^\infty_c(\reals^N;\reals)$
with $\characteristic{\abs{x} \leq \frac{R}{2}} \leq \psi_R(x) \leq
\characteristic{\abs{x} \leq R}$,
therefore 
\begin{align*}
\lim_{n \to \infty} \probability{\abs{\xi_n} > R} &= 1 - \lim_{n \to
  \infty} \expectation{\characteristic{\abs{\xi_n} \leq R}} \\
&\leq 1 - \lim_{n \to
  \infty} \expectation{\psi_R(\xi_n)} \\
&=1 - \expectation{\psi_R(\xi)} \\
&\leq 1 - \expectation{\characteristic{\abs{\xi} \leq \frac{R}{2}}} \\
&=\probability{\abs{\xi} > \frac{R}{2}} 
\end{align*}
On the other hand, we know that $\lim_{R \to \infty}
\characteristic{\abs{\xi} \leq \frac{R}{2}} = 0$ a.s. and therefore by
Monotone Convergence, $\lim_{R \to \infty} \probability{\abs{\xi} >
  \frac{R}{2}} = 0$.  Select $R > 0$ such that  
\begin{align*}
\probability{\abs{\xi} >R} &\leq \probability{\abs{\xi} >\frac{R}{2}} \leq \frac{\epsilon}{4\norm{f}_\infty}
\end{align*}
Then we can
pick $N_1 > 0$ such that $\probability{\abs{\xi_n} >R} \leq \frac{\epsilon}{2\norm{f}_\infty}$ for all $n > N_1$.

Having picked $R>0$, we know that $f_n$ converges uniformly to $f$ on
$\abs{x} \leq R$ and therefore we can find a $K > 0$ such that for $k
> K$ and $\abs{x} \leq R$  we have $\abs{f_k(x) - f(x)} < \epsilon$.
Therefore,
\begin{align*}
\abs{\expectation{f_k(\xi) - f(\xi)}} &\leq \expectation{\abs{f_k(\xi)
    - f(\xi)} ; \abs{\xi} \leq R} + \expectation{\abs{f_k(\xi)
    - f(\xi)} ; \abs{\xi} > R} \\
&\leq \epsilon \probability{ \abs{\xi} \leq R} + 2\norm{\xi}_\infty \probability{ \abs{\xi} > R} \\
&\leq \epsilon + \frac{\epsilon}{2} < 2\epsilon
\end{align*}
and via the same calculation, for $n > N_1$
\begin{align*}
\abs{\expectation{f_k(\xi_n) - f(\xi_n)}} &\leq \epsilon + 2\norm{\xi_n}_\infty \probability{ \abs{\xi_n} > R} \leq 2\epsilon
\end{align*}

To finish the proof, pick a single $k > K$ and then we can find $N_2 >
0$ such that for all $n > N_2$, we have $\abs{\expectation{f_k(\xi_n)
    - f_k(\xi)}} < \epsilon$.  Putting these three estimates together,
we have for $n > \max(N_1, N_2)$, 
\begin{align*}
\abs{\expectation{f(\xi_n) - f(\xi)}} &\leq 5 \epsilon
\end{align*}
\end{proof}

We are not going to prove the following but we should talk about it:
\begin{thm}\label{Berry-Esseen Theorem}Let $\xi, \xi_1, \xi_2, \dots$
  be i.i.d with $\expectation{\abs{\xi}^3} < \infty$.  Let $\Phi(x)$
  be the cdf of standard normal and let $G(x) = \probability{\frac{S_n
      - \mu}{\sigma\sqrt{n}} \leq x}$ be the empirical cdf.  Then
  there exists a constant $C > 0$ such that
\begin{align*}
\sup_x \abs{G(x) - \Phi(x)} \leq \frac{C
  \expectation{\abs{\xi}^3}}{\sigma^3 \sqrt{n}}
\end{align*}
\end{thm}
Note the upper bound of the constant $C$ has been reduced to about
$0.5600$.  

\input CharacteristicFunctions

\input Conditioning

\input Martingales

\section{Notes on Lasso}
TODO

\section{Concentration Inequalities}
\begin{lem}[Markov Inequality]\label{MarkovInequality}Let $\xi$ be a positive integrable random variable.  Then $\probability{\xi>t} \leq \frac{E(\xi)}{t}$\end{lem}
\begin{proof}
$E(\xi) \leq E(\xi\characteristic{\{\xi>t\}}) \leq E(t\characteristic{\{\xi>t\}})=t\probability{\xi>t}$
\qedhere
\end{proof}

\begin{lem}[Chebeshev's Inequality]\label{ChebInequality}Let $\xi$ be a random variable with finite mean $\mu$ and finite variance $\sigma$.  Then $\probability{|\xi-\mu|>t} \leq \frac{\sigma^2}{t^2}$\end{lem}
\begin{proof}
$\probability{|\xi-\mu|>t} = \probability{(\xi-\mu)^2 > t^2} \leq \frac{\expectation{(\xi-\mu)^2}}{t^2}=\frac{\sigma^2}{t^2}$
\qedhere
\end{proof}

\begin{lem}[One Sided Chebeshev's Inequality]\label{OneSidedChebInequality}Let $\xi$ be a random variable with finite mean $\mu$ and
  finite variance $\sigma$.  Then $\probability{\xi-\mu>\lambda} \leq
  \frac{\sigma^2}{\sigma^2 + \lambda^2}$\end{lem}
\begin{proof}
First we assume $\expectation{\xi}=0$.  We prove a family of
inequalities for a real parameter $c > 0$.
\begin{align*}
\probability{\xi>\lambda}  
&= \probability{\xi + c > \lambda + c} \\
& \leq \probability{(\xi + c)^2 > (\lambda + c)^2} & \textrm{ because }
\lambda+c>0 \\
& \leq \frac{\expectation{\xi^2} + c^2}{(\lambda + c)^2}
\end{align*}
Now we extract the best estimate by finding the minimum of the right
hand side with respect to $c$.  Differentiating we get a vanishing
first derivative when
$
(\lambda^2 + c^2) 2c = (\expectation{\xi^2} + c^2) 2(\lambda +
c)$.  Divide by $2(\lambda+ c)$  and subtract $c^2$ to get the
  minimum at $c=\expectation{\xi}/\lambda > 0$.  Plug this value in to
  get the final estimate.
\begin{align*}
\frac{
  \expectation{\xi^2} +
  (\frac{\expectation{\xi^2}}{\lambda})^2
}
{(\lambda +
    \frac{\expectation{\xi^2}}{\lambda})^2
} & =
    \frac{\expectation{\xi^2}(1 +
      \frac{\expectation{\xi^2}}{\lambda^2})}
{
  \lambda^2 (1 + \frac{\expectation{\xi^2}}{\lambda^2}
)^2
} \\
& = \frac{\expectation{\xi^2}}{\lambda^2 + \expectation{\xi^2}}
\end{align*}
Now apply the above inequality to the centered random variable $\xi -
\mu$ to get the general result.
\qedhere
\end{proof}

\begin{defn}
We say that a random variable $\xi$ is \emph{subgaussian} if and only if
there exist constants $c, C > 0$ such that $\probability { \abs{\xi}
  \geq \lambda} \leq C e^{-c \lambda^2}$ for all $\lambda > 0$.
\end{defn}

TODO: Show that any Gaussian is subgaussian (independent of its mean?).

TODO: Show any bounded (or almost surely bounded) random variable is
subgaussian.

\begin{examp}Given the nomenclature it isn't surprising that Gaussian
  random variables are subgaussian.  As it turns out it is useful to
  analyze the case of a $N(0,\sigma^2)$ random variable separately
  since it has slightly different behavior than the general $N(\mu,
  \sigma^2)$ case.  Let us assume that $\xi$ is a normal random
  variable with mean $0$ and variance $\sigma^2$.  We have a standard tail estimate for $\lambda \geq \sigma$
\begin{align*}
\probability{\xi \geq \lambda} &= 
\frac{1}{\sqrt{2\pi}\sigma} \int_\lambda^\infty e^{-x^2/2\sigma^2} \, dx \leq 
\frac{1}{\sqrt{2\pi}\sigma} \int_\lambda^\infty \frac{x}{\sigma} e^{-x^2/2\sigma^2} \, dx =
\frac{1}{\sqrt{2\pi}} e^{-\lambda^2/2\sigma^2}
\end{align*}
The $0 \leq \lambda \leq \sigma$ case can easily be handled with a constant
multiplier but we can actually find the constant that gives a tight
bound.  Note that $\frac{1}{\sqrt{2\pi}\sigma} \int_0^\infty e^{-x^2/2\sigma^2} =
\frac{1}{2}$ so we can't do any better than $\probability{\xi \geq
  \lambda} \leq \frac{1}{2} e^{-\lambda^2/2\sigma^2}$; in fact this bound
works for all $\lambda \geq 0$.  We've already shown this for $\lambda
\geq 1$ and $\lambda=0$.  To show the bound on $[0,1]$ we calculate
the derivative 
\begin{align*}
\frac{d}{d\lambda} \left( \frac{1}{2} e^{-\lambda^2/2\sigma^2} -
  \frac{1}{\sqrt{2\pi}\sigma} \int_\lambda^\infty e^{-x^2/2\sigma^2} \, dx \right)
&=\left( -\frac{\lambda}{2\sigma^2} + \frac{1}{\sqrt{2\pi}\sigma}\right) e^{-\lambda^2/2}
\end{align*}
from which we conclude there is a unique maximum of the function at
$\lambda=\sigma \sqrt{\frac{2}{\pi}} \in (0,\sigma)$.  We have already validated
that the function is nonnegative at the endpoints of $[0,\sigma]$ so it must
be nonnegative on the entire interval.  Now by symmetry of $\xi$, the
calculation also shows that $\probability{\xi \leq -\lambda} \leq
\frac{1}{2}e^{-\lambda^2/2\sigma^2}$ and therefore $\probability{\abs{\xi}
  \geq \lambda} \leq e^{-\lambda^2/2\sigma^2}$.

Now for a general $N(\mu, \sigma)$ normal random variable $\xi$ we
have by change of variables
\begin{align*}
\probability{\xi \geq \lambda} &= 
\frac{1}{\sqrt{2\pi} \sigma} \int_\lambda^\infty e^{-(x-\mu)^2/2\sigma^2} \, dx = 
\frac{1}{\sqrt{2\pi}} \int_{(\lambda-\mu)/\sigma}^\infty e^{-x^2/2} \,
dx \leq \frac{1}{\sqrt{2\pi}} e^{-(\lambda-\mu)^2/2\sigma^2}
\end{align*}

TODO: Finish
\end{examp}

\begin{lem}Let $\{\xi_i\}_{i=1}^m$ be jointly independent subgaussian random variables.  Then $\expectation{e^{\sum_{i=1}^m \xi}} =
  \prod_{i=1}^m \expectation{e^{\xi_i}}$.
\end{lem}
\begin{proof}
First show that for a subgaussian $\xi$, we have by dominated
convergence the Taylor expansion 
$$\expectation{e^{t\xi}} = 1 + \sum_{k=1}^\infty
\frac{t^k}{k!}\expectation{\xi^k}$$
The proof of this fact is to exhibit an integrable function that
dominates the sequence of partial sums $1+\sum_{k=1}^n
\frac{t^k\xi^k}{k!}$.  This is obvious if $\xi$ is almost surely
bounded but it's not obvious to me that this should be true for a
subgaussian $\xi$.  TODO: Perhaps we need to use uniform integrability or
something like that in the subgaussian/subexponential case.

In any case, assuming the validity of the above identity for each
$\xi$, we turn to the case of the sum.
\end{proof}

\begin{lem}\label{SubgaussianEquivalence}$\xi$ is subgaussian if and only if there exists $C$ such
  that $\expectation{e^{t\xi}} \leq C e^{Ct^2}$ and if and only if
    there exists $C$ such that $\expectation{|\xi|^k} \leq
    \left(Ck\right)^{\frac{k}{2}}$ for all $t \in \reals$.
\end{lem}
\begin{proof}
Suppose $\xi$ is subgaussian and calculate:
\begin{align*}
\expectation{e^{t\xi}} &= \int_0^\infty \probability{e^{t\xi} \geq
    \lambda} d\lambda \\
&= \int_{-\infty}^\infty \probability{e^{t\xi} \geq e^{t\eta}} t
e^{t\eta} d\eta \\
& = \int_{-\infty}^\infty \probability{\xi \geq \eta} t
e^{t\eta} d\eta \\
& \leq \int_{-\infty}^\infty C t
e^{t\eta - c\eta^2} d\eta  \\
& = C t e^{\frac{t^2}{4c}}\int_{-\infty}^\infty 
e^{-\left(\sqrt{c}\eta - \frac{t}{2\sqrt{c}}\right)^2} d\eta \\
&= C^\prime t e^{\frac{t^2}{4c}} \\
&\leq C^\prime e^{\frac{5c t^2}{4c}} 
\end{align*}

Now assume that we have $\expectation{e^{t\xi}} \leq C e^{Ct^2}$ for
all $t$.  Pick an arbitrary $t>0$ to be
chosen later and proceed by using first order
moment method:
\begin{align*}
\probability{\xi \geq \lambda} &= \probability{e^{t\xi} \geq
  e^{t\lambda}} \\
&\leq \frac{\expectation{e^{t\xi}}}{e^{t\lambda}} \\
&\leq C e^{Ct^2 - t\lambda} \\
\end{align*}
Now we pick $t$ to minimize the upper bound derived above; simple
calculus shows this occcurs at $t=\frac{\lambda}{2C}$.  Subtituting
yields the bound 
\begin{align*}
\probability{\xi \geq \lambda} \leq C e ^ {-\frac{\lambda^2}{4C}}
\end{align*}
For the other tail, we note that our assumption holds equally well for
$-\xi$.  Thus we can use the same method to bound 
\begin{align*}
\probability{\xi \leq -\lambda} = \probability{-\xi \geq \lambda} \leq C e ^ {-\frac{\lambda^2}{4C}}
\end{align*}
therefore taking the union bound we get
\begin{align*}
\probability{|\xi| \geq \lambda} \leq 2 C e ^ {-\frac{\lambda^2}{4C}}
\end{align*}

Now consider absolute moments of subgaussian variables.  We can assume
that $\xi \geq 0$ and calculate as before:
\begin{align*}
\expectation{\xi^k} &= \int_0^\infty \probability{\xi^k \geq x} dx \\
&= k\int_0^\infty \probability{\xi^k \geq y^k} y^{k-1} dy \\
&= k C \int_0^\infty y^{k-1} e^{-cy^2} dy \\
&= k C \frac{c^{k-3}}{2} \int_0^\infty x^{\frac{k}{2}-1} e^{-x} dx \\
&= k C \frac{c^{k-3}}{2} \Gamma\left(\frac{k}{2}\right) \\
&\leq k C \frac{c^{k-3}}{2} \left(\frac{k}{2}\right)^\frac{k}{2} \\
\end{align*}
To go the other direction, assume $\expectation{|\xi|^k} \leq
(Ck)^\frac{k}{2}$ and pick a constant $0 < c < \frac{e}{2C}$ 
\begin{align*}
\expectation{e^{K\xi^2}} &= 1 + \sum_{k=1}^\infty
\frac{t^k\expectation{\xi^{2k}}}{k!}\\
&\leq 1 + \sum_{k=1}^\infty
\frac{(2tCk)^k}{k!}\\
&\leq 1 + \sum_{k=1}^\infty
\left(\frac{2tC}{e}\right)^k < \infty \\
\end{align*}
Now use the elementatry bound $ab \leq \frac{(a^2 + b^2)}{2}$ so see
\begin{align*}
\expectation{e^{t\xi}} &\leq 
\end{align*}
\end{proof}

The definition of subgaussian random variables differs in a minor way
from another in common use in the literature.  In particular, in some descriptions a random
variable $\xi$ is called subgaussian if and only if
$\expectation{e^{t\xi}} \leq e^{\frac{c^2 t^2}{2}}$ for all $t \in \reals$.  The important
difference here compared with the characterization in Lemma
\ref{SubgaussianEquivalence} is that the constant on the right hand side is $1$.
With this defintion, we must add the hypothesis $\expectation{\xi}=0$
to get equivalence with the other definition.

\begin{lem}Suppose $\xi$ is a random variable such that there exists
  $c>0$ for which
\begin{align*}
\expectation{e^{t\xi}} &\leq e^{\frac{c^2 t^2}{2}} \text{ for all $t \in \reals$}
\end{align*}
then $\expectation{\xi}=0$ and $\expectation{\xi^2} \leq c^2$.
\end{lem}
\begin{proof}
By Dominated Convergence and the hypothesis we get
\begin{align*}
\sum_{n=0}^\infty \frac{t^n}{n!}\expectation{\xi^n} &=
\expectation{e^{t\xi}} \leq e^{\frac{c^2 t^2}{2}} = \sum_{n=0}^\infty
\frac{c^{2n}}{2^n n!}t^{2n}
\end{align*}
so in particular by taking only terms up to order $t^2$ and using the
fact that the constant term in on both sides is $1$, we have
\begin{align*}
t \expectation{\xi} + \frac{t^2}{2} \expectation{\xi^2} &= \frac{c^2 t^2}{2}
+
o(t^2) \text { as $t \to 0$}
\end{align*}
If we divide both sides by $t > 0$ and take the limit as $t \to 0^+$ then we
get $\expectation{\xi} \leq 0$.  If we divide by $t < 0$ and take the
limit as $t \to 0^-$ then we get $\expectation{\xi} \geq 0$.  Thus we
can conclude $\expectation{\xi} = 0$.  If we plug that in and divide
by $t^2$ and take the limit as $t \to 0$ then see $\expectation{\xi^2}
\leq c^2$.
\end{proof}
Note that the argument in the proof above doesn't even get off the
ground unless the constant of the bounding exponential is assumed to
be $1$.

The following lemma is useful for the second moment method for
deriving tail bounds.
\begin{lem}Let $\{\xi_i\}_{i=1}^m$ be pairwise independent random
  variables and $c_i$ be scalars.  Then $\variance{\sum_{i=1}^m c_i \xi} =
  \sum_{i=1}^m |c_i|^2\variance{\xi_i}$.
\end{lem}
\begin{proof}
TODO
\end{proof}

\begin{lem}[Bennett's Inequality]\label{Bennett} Let $\{\xi_i\}_{i=1}^m$ be independent random variables with means $\mu_i$ and variances $\sigma_i$.  Set $\Sigma^2 = \sum_{i=1}^m \sigma_i^2$.  If for every $i$, $|\xi_i - \mu_i| \leq M$ almost everywhere then for every $\lambda > 0$ we have $$
\probability{\sum_{i=1}^m [\xi_i - \mu_i] > \lambda} \leq 
e^{
	-\frac{\lambda}{M}\{(1 + \frac{\Sigma^2}{M\lambda})\log(1+\frac{M\lambda}{\Sigma^2}) - 1\}
}
$$
\end{lem}
\begin{proof}
First it is easy to see that by subtracting means we may assume that
$\mu_i=0$.  Then we have $\sigma_i = \expectation{\xi_i^2}$.  We use
the exponential moment method.  We show a family of inequalities
depending on a real parameter $c > 0$ which we will pick later.  First we have 
\begin{align*}
\probability{\sum_{i=1}^m \xi_i > \lambda} & =
\probability{c\sum_{i=1}^m \xi_i > c\lambda}  && \textrm{since } c >0.\\
  & =\probability{e^{c\sum_{i=1}^m \xi_i} > e^{c\lambda}} &&
  \textrm{since } e^x \textrm{ is increasing}\\
  & \leq e^{-c\lambda} \expectation{e^{c\sum_{i=1}^m \xi_i}} && \textrm
  {by Markov's Inequality}\eqref{MarkovInequality}\\
  & = e^{-c\lambda} \prod_{i=1}^m \expectation{e^{c\xi_i}} &&
  \textrm{by independence and boundedness.  TODO: do we really need boundedness?}
\end{align*}
Now we consider an individual term $\expectation{e^{c\xi_i}}$ for an
almost surely bounded $\xi_i$ with zero mean.
\begin{align*}
\expectation{e^{c\xi_i}} &= \expectation{\sum_{k=0}^\infty
  \frac{c^k\xi_i^k}{k!}} = \sum_{k=0}^\infty
  \frac{c^k}{k!}\expectation{\xi_i^k} && \textrm{by dominated
    convergence} \\
& = 1  + \sum_{k=2}^\infty
  \frac{c^k}{k!}\expectation{\xi_i^k} && \textrm{by mean zero} \\
& \leq 1  + \sum_{k=2}^\infty
  \frac{c^k M^{k-2} \sigma_i^2}{k!} && \textrm{by boundedness and
    definition of variance} \\
& \leq e^{\sum_{k=2}^\infty  \frac{c^k M^{k-2} \sigma_i^2}{k!}} &&
\textrm{since } 1+x \leq e^x \eqref{BasicExponentialInequalities} \\
& = e^{\frac{(e^{cM} -1 - cM)\sigma_i^2}{M^2}}
\end{align*}
Therefore,
\begin{align*}
\probability{\sum_{i=1}^m \xi_i > \lambda} &\leq e^{-c\lambda} \prod_{i=1}^m
e^{\frac{(e^{cM} -1 - cM)\sigma_i^2}{M^2}} \\
& = e^{\frac{(e^{cM} -1 - cM)\Sigma^2}{M^2}}
\end{align*}
Now we pick $c>0$ to minimize the bound above ($e^{cM} -1 =
\frac{M\lambda}{\Sigma^2}$ or equivalently $c = \frac{1}{M}\ln(1 + \frac{M\lambda}{\Sigma^2})$).
Substituting yields the final bound
\begin{align*}
\probability{\sum_{i=1}^m \xi_i > \lambda} &\leq e^{-(\lambda + \frac{\Sigma^2}{M})
  \frac{1}{M}\ln(1 + \frac{M\lambda}{\Sigma^2}) + \frac{\lambda}{M}}
\\
& = e^{-\frac{\lambda}{M} \{
  (1+\frac{\Sigma^2}{\lambda M}) \ln(1 + \frac{M\lambda}{\Sigma^2}) -1
\}}
\end{align*}
\end{proof}

\begin{lem}[Bernstein's or Chernoff's Inequality]\label{Bernstein} Let
  $\{\xi_i\}_{i=1}^m$ be independent random variables with means
  $\mu_i$ and variances $\sigma_i$.  Set $\Sigma^2 = \sum_{i=1}^m
  \sigma_i^2$.  If for every $i$, $|\xi_i - \mu_i| \leq M$ almost
  everywhere then for every $\lambda > 0$ we have 
\begin{align*}
\probability{\sum_{i=1}^m [\xi_i - \mu_i] > \lambda} &\leq 
e^{
	-\{\frac{\lambda^2}{2(\Sigma^2 + \frac{1}{3}M\lambda)}\}
}
\end{align*}
\end{lem}
\begin{proof}
TODO
\end{proof}

The next inequality has a pleasing form because the resulting bound is
of the form of a Gaussian random variable.  Such bounds are
interesting enough that they warrant the following definition.
\begin{defn} Let $\xi$ be a real valued random variable with mean
  $\mu$.  We say that $\xi$ has a \emph{subgaussian upper tail} if there
  exists a  constants $C > 0$ and $c > 0$ such that for all $\lambda > 0$,
$$
\probability{[\xi-\mu] > \lambda} \leq Ce^{-c\lambda^2}.
$$
We say that $\xi$ has a \emph{subgaussian tail up to} $\lambda_0$ if the
above bound holds for $\lambda < \lambda_0$.  We say that $\xi$ has a
\emph{subgaussian tail} if both $\xi$ and $-\xi$ have subgaussian upper
tails (or equivalently if $|\xi|$ has a subgaussian tail.
\end{defn}

 The boundedness assumption on the individual random variables in the
above sums can be relaxed to an assumption that the individual random
variables has subgaussian tails.  Moreover, one can generalize the sum
of random variables to an arbitrary linear combination of random
variables on the unit sphere.

\begin{lem}\label{Matousek} Let $\{\xi_i\}_{i=1}^m$ be independent
  random variables with $E[\xi_i]=0$ and $E[\xi_i^2]=1$ and uniform
  subgaussian tails.  Let $\{\alpha_i\}_{i=1}^m$ be real coefficients
satisfying $\sum_{i=1}^m \alpha_i^2 = 1$.  The then random variable
$\eta=\sum_{i=1}^m \alpha_i\xi_i$ has $E[\eta]=0$, $E[\eta^2]=1$ and a
subgaussian tail.
\end{lem}
\begin{proof}
TODO
\end{proof}

\begin{lem}[Exercise 7 Lugosi] Let $\{\xi_i\}_{i=1}^n$ be independent
  random variables with values in $[0,1]$.  Let $S_n = \sum_{i=1}^n
  \xi_i$ and let $\mu=\expectation{S_n}$.  Show that for any
  $\lambda\geq \mu$,
\begin{align*}
\probability{S_n \geq \lambda} \leq
\left(\frac{\mu}{\lambda}\right)^\lambda \left(\frac{n-\mu}{n-\lambda}\right)^{n-\lambda}.
\end{align*}
\end{lem}
\begin{proof}
Use Chernoff bounding.  Looking at the solution, we can pattern match
that we may want to use the convexity of $e^x$ since the solution
seems to reference the endpoints of the interval $[0,n]$; indeed that
is the way to proceed.  TODO: convert the argument below for $n=1$ to
cover general $n$.
To estimate $\expectation{e^{s\xi_i}}$ we first use convexity of
$e^sx$ on the interval $x\in[0,1]$,
\begin{align*}
e^{sx} \leq xe^s + (1-x)
\end{align*}
Substituting $\xi_i$ and taking expectations we get
\begin{align*}
\expectation{e^{s\xi_i}} \leq \mu_i e^s + (1-\mu_i).
\end{align*}
So now we minimize the Chernoff bound by using elementary calculus
\begin{align*}
\frac{d}{ds} \mu_i e^{s(1-\lambda)} + (1-\mu_i)e^{-s\lambda} = \mu_i
(1-\lambda) e^{s(1-\lambda)} + \lambda (1-\mu_i)e^{-s\lambda} 
\end{align*}
which equals $0$ when
$s=\ln\left(\frac{\lambda(1-\mu_i)}{\mu_i(1-\lambda)}\right)$.  This
value is positive when $\lambda \geq \mu$.
Backsubstituing this value and doing some algebra shows
\begin{align*}
e^{-s\lambda}\expectation{e^{s\xi_i}} \leq
\left(\frac{\mu_i}{\lambda}\right)^\lambda
\left(\frac{1-\mu_i}{1-\lambda}\right)^{1-\lambda}
\end{align*}
Note also an argument for a related estimate (Exercise 8) that uses bounds similar to those in Bennett can be made
as follows.  Since $\xi_i \in [0,1]$, we have that $\xi_i^k \leq
\xi_i$.  With this observation, 
\begin{align*}
\expectation{e^{s\xi_i}} &= 1 + \sum_{k=1}^\infty \frac{s^k
  \expectation{\xi_i^k}}{k!} \\
& \leq 1 + \sum_{k=1}^\infty \frac{s^k \mu_i}{k!} \\
&= 1 + \mu_i (e^s -1) \\
&\leq e^{\mu_i(e^s - 1)}
\end{align*}
Now we select $s$ to minimize the Chernoff bound $e^{\mu_i(e^s - 1)
  -s\lambda}$ which simple calculus shows happens at
$s=\ln\left(\frac{\lambda}{\mu_i}\right)$; the location of the minimum
being positive precisely when $\lambda \geq \mu_i$.  Backsubstituting
yields a bound $\left(\frac{\mu_i}{\lambda}\right)^\lambda e^{\lambda
  - \mu_i}$.
\end{proof}

\input Likelihood

\input BrownianMotion

\input MarkovProcesses

\input RealAnalysis2

\section{Exercises}
\begin{ex}Let $f(x)$ be a Lebesgue integrable function on $\reals$.
  Show that there exists a measurable $a(x)$ with $\lim_{x \to \infty}
  a(x) = \infty$ such that $a(x)f(x)$ remains integrable.
\end{ex}
\begin{proof}
It suffices to assume that $f(x) \geq 0$ and $\int f(x) \, dx = 1$.
We know from Fundamental Theorem of Calculus that $g(y) = \int_{-\infty}^y
f(x) \, dx$ is almost everywhere differentiable (and montone) and
$g^\prime(y) = f(y)$.   By
definition $\lim_{y \to \infty} g(y) = 1$.  Now define $h(z) = 1 -
\sqrt{1 -z}$ and note that by the Chain Rule (TODO: Show that the
Chain Rule is still valid for functions that are merely absolutely continuous)
\begin{align*}
\frac{d}{dy} h(g(y)) &= \frac{f(y)}{2 \sqrt{1 - g(y)}}
\end{align*}
Now by the Fundamental Theorem of Calculus again, if we define $a(x) =
\frac{1}{2 \sqrt{1 - g(x)}}$ 
then 
\begin{align*}
\int a(x) f(x) \, dx &= \lim_{y \to \infty} h(g(y)) = h(1) = 1
\end{align*}
but 
\begin{align*}
\lim_{x \to \infty} a(x) &= \lim_{x \to \infty} \frac{1}{2 \sqrt{1 -
    g(x)}} = \infty
\end{align*}
\end{proof}
\begin{ex}Let $\xi$ be a random variable, show that for all $\lambda > 0$,
\begin{align*}
\min_k \expectation{\xi^q} \lambda^{-k} \leq \inf_{s>0} \expectation{e^{s(\xi-\lambda)}}
\end{align*}
Note that this shows that the best moment bound for a tail
probability is always better than the best Chernoff bound.
\end{ex}
\begin{proof}
Let $q = \argmin_k \expectation{\xi^k} \lambda^{-k}$.  Now expand as a
series
\begin{align*}
\expectation{e^{s(\xi-\lambda)}} &= e^{-s\lambda} \sum_{k=0}^\infty
\frac{s^k \expectation{\xi^k}}{k!} \\
& \geq e^{-s\lambda} \expectation{\xi^q} \lambda^{-q} \sum_{k=0}^\infty
\frac{s^k \lambda^k}{k!} = \expectation{\xi^q} \lambda^{-q}
\end{align*}
\end{proof}
\begin{ex}Let $\xi$ be a nonnegative integer valued random variable.
  Show $\probability{\xi \neq 0} \leq \expectation{\xi}$ and 
\begin{align*}
\probability{\xi = 0} \leq \frac{\variance{\xi}}{\variance{\xi} + \left(\expectation{\xi}\right)^2}
\end{align*}
\end{ex}
\begin{proof}
For the first inequality,
\begin{align*}
\probability{\xi \neq 0} = \sum_{k=1}^\infty \probability{\xi = k} \leq
\sum_{k=1}^\infty k\probability{\xi = k} = \expectation{\xi}
\end{align*}
For the second inequality, use Cauchy-Schwartz
\begin{align*}
\left(\expectation{\xi}\right)^2 &\leq
\left(\expectation{\characteristic{\xi > 0}\xi}\right)^2 \\
&\leq \expectation{\xi^2} \probability{\xi > 0}
\end{align*}
Now use $\probability{\xi > 0} = 1 - \probability{\xi=0}$ and
$\variance{\xi} = \expectation{\xi^2} -
\left(\expectation{\xi}\right)^2$ and rearrangement of terms to get
the result.
\end{proof}

\begin{ex}Let $f : S \to T$ be function.  If $\mathcal{T}$ is a
  $\sigma$-algebra on $T$ then $\mathcal{T} \subset f_*
  f^{-1}(\mathcal{T})$.  If $\mathcal{S}$ is a $\sigma$-algebra on
  $S$, then $f^{-1}f_*(\mathcal{S}) \subset \mathcal{S}$.  Find examples where the inclusions are strict.
\end{ex}
\begin{proof}
To see the inclusions just unwind the definitions.  For the first inclusion
\begin{align*}
f_* f^{-1}(\mathcal{T}) &= \lbrace A \subset T \mid f^{-1}(A) \in
f^{-1}(\mathcal{T}) \rbrace \\
&= \lbrace A \subset T \mid f^{-1}(A) =
f^{-1}(B) \text { for some } B \in \mathcal{T} \rbrace \\
&\supset \mathcal{T}
\end{align*}
and for the second
\begin{align*}
f^{-1} f_* (\mathcal{S}) &= \lbrace f^{-1}(A) \mid A \in  f_*
(\mathcal{S}) \rbrace \\
&= \lbrace f^{-1}(A) \mid A \subset T \text{ and } f^{-1}(A) \in
\mathcal{S} \rbrace \\
&\subset \mathcal{S}
\end{align*}

TODO: Find the examples of strict inclusion.
\end{proof}
\begin{ex}Let $f : S \to T$ be a set function and let $\mathcal{C}
  \subset 2^T$ then $f^{-1}(\sigma(\mathcal{C})) = \sigma(f^{-1}(\mathcal{C}))$.
\end{ex}
\begin{proof}
We know that $f^{-1}(\sigma(\mathcal{C}))$ is a $\sigma$-algebra and
clearly $f^{-1}(\mathcal{C}) \subset f^{-1}(\sigma(\mathcal{C}))$
therefore showing $\sigma(f^{-1}(\mathcal{C})) \subset
f^{-1}(\sigma(\mathcal{C}))$.  

To see the reverse inclusion we know that 
\begin{align*}
f_* (\sigma(f^{-1}(\mathcal{C}))) &= \lbrace A \subset T \mid
f^{-1}(A) \in \sigma(f^{-1}(\mathcal{C})) \rbrace
\end{align*}
is a $\sigma$-algebra and clearly $\mathcal{C} \subset f_*
(\sigma(f^{-1}(\mathcal{C})))$.  This implies $\sigma(\mathcal{C}) \subset f_*
(\sigma(f^{-1}(\mathcal{C})))$ and thus by the result of the previous
exercise
\begin{align*}
f^{-1}(\sigma(\mathcal{C})) &\subset 
f^{-1} (f_*(\sigma(f^{-1}(\mathcal{C})))) \subset \sigma(f^{-1}(\mathcal{C}))
\end{align*}
\end{proof}
\begin{ex}Let $f(x) = \abs{x}$.  Show that $f_*(\mathcal{B}(\reals))$
  is a strict $\sigma$-subalgebra of $\mathcal{B}(\reals)$.
\end{ex}
\begin{ex}Let $f : S \to T$ be a function, $\mathcal{C} \in
  2^S$ and define $f_*(\mathcal{C}) =  \{A \subset T \mid
    f^{-1}(A) \in \mathcal{C} \}$.  Show by counterexample that
    $\sigma(f_*(\mathcal{C})) \neq f_*(\sigma(\mathcal{C}))$.
\end{ex}

\begin{ex}Let $A_n$ be a sequence of events. Show that 
\begin{align*}
\probability{A_n \text{ i.o.}} \geq \limsup_{n \to \infty} \probability{A_n}
\end{align*}
\end{ex}
\begin{proof}
Note that we know that for every $k\geq n$, $A_k \subset
\cup_{k=n}^\infty A_k$ and therefore monotonicity of measure implies $\probability{A_k} \leq
\probability{\cup_{k=n}^\infty A_k}$ for $k\geq n$.  Therefore we know
$\sup_{k\geq n} \probability{A_k} \leq
\probability{\cup_{k=n}^\infty A_k}$.

By definition and continuity of measure and applying the above,
\begin{align*}
\probability{A_n \text{ i.o.}} &= \probability{\cap_{n=1}^\infty
  \cup_{k=n}^\infty A_k} \\
&= \lim_{n \to \infty} \probability{\cup_{k=n}^\infty A_k} \\
&\geq \lim_{n \to \infty} \sup_{k \geq n} \probability{ A_k} =
\limsup_{n \to \infty} \probability{ A_n} 
\end{align*}
\end{proof}

\begin{ex}Suppose we toss a coin repeatedly and the probability of
  heads is $0 < p < 1$ (i.e. the coin may be unfair but not
  pathological).  Without using the Strong Law of Large Numbers show
  that the probability of flipping only a finite number heads is $0$.
\end{ex}
\begin{proof}
Let $A_n = \lbrace \text{heads is flipped on the }n^{th}\text{
  toss}\rbrace$.  We know that $\probability{A_n} = p >0$, therefore
$\sum_{n=1}^\infty \probability{A_n} = \infty$.  We also know that
$A_n$ are independent events, therefore the converse of the
Borel-Cantelli Theorem (Theorem \ref{BorelCantelli}) tells us that $\probability{ A_n \text{ i.o.}}
= 1$.  The probability of tossing only a finite number of
heads is $1 - \probability{ A_n \text{ i.o.}} = 0$.
\end{proof}

\begin{ex}A sequence of random variables $\xi_1, \xi_2, \dots$ is said
  to be \emph{completely convergent} to $\xi$ if for every $\epsilon > 0$,
\begin{align*}
\sum_{n=1}^\infty \probability{\abs{\xi_n - \xi} > \epsilon} < \infty
\end{align*}
Show that if $\xi_n$ are independent then complete convergence is
equivalent to almost sure convergence.
\end{ex}
\begin{proof}
First assume that $\xi=0$.  

We first assume complete convergence.  If for a given $\epsilon > 0$, we know $\sum_{n=1}^\infty
\probability{\abs{\xi_n} > \epsilon} < \infty$ then we can apply
Borel Cantelli to conclude that $\probability{\xi_n > \epsilon \text{ i.o.}} =
0$.  Thus there exists a set $A_\epsilon$ of measure zero such that
for all $\omega \notin A_\epsilon$, we can find $N>0$ such that
$\xi_n(\omega) \leq \epsilon$.  Define $A = \cup_{m=1}^\infty A_\frac{1}{m}$,
note that $\probability{A} = 0$ and that for every $\omega \notin A$,
and every $\epsilon >0$ we can pick $\frac{1}{m} < \epsilon$  and then
we know  $N>0$ such that $\xi_n(\omega) \leq \frac{1}{m} \leq \epsilon$

Then if $\xi_n \toas 0$, then there
exists an event $A$ with $\probability{A} = 1$ and such that for any
$\omega \in A$, $\epsilon>0$ we can find $N > 0$ such that
$\abs{\xi_n} < \epsilon$, thus $\probability{\abs{\xi_n} > \epsilon
  \text{ i.o.}} \leq 1 - \probability{A} = 0$.  By independence of
$\xi_n$ and Borel Cantelli we conclude that $\sum_{n=1}^\infty
\probability{\abs{\xi_n} > \epsilon} < \infty$.

Now in the case in which $\xi \neq 0$ we can reduce to the case in
which $\xi = 0$.  Note that by Corollary
\ref{ConstantLimitOfIndependent} to the Kolmogorov 0-1 Theorem, we
know that $\xi$ is almost surely a constant $c$.  Then we can define $\xi_n
- c$ and note that $\xi_n - c$ are independent by Lemma \ref{IndependenceComposition}.
\end{proof}

\begin{ex}Suppose $\eta, \xi_1, \xi_2, \dots$ are random variables with
  $\abs{\xi_n} \leq \eta$ a.s. for all $n > 0$.  Show that $\sup_n
  \abs{\xi_n} \leq \eta$ a.s.
\end{ex}
\begin{proof}
Let $A_n = \lbrace \xi_n \leq \eta \rbrace$ and $A = \cup_n A_n$.
By assumption,
$\probability{A_n} =0$ and therefore by countable subadditivity of measure,
$\probability{A} = 0$.  For all $\omega \notin A$, we know for all
$n>0$, $\xi_n(\omega) \leq \eta(\omega)$ and therefore
$\sup_n\xi_n(\omega) \leq \eta(\omega)$.
\end{proof}

\begin{ex}Suppose $\xi, \xi_1, \xi_2, \dots$ are random variables with
  $\xi_n \toas \xi$ and $\xi < \infty$ a.s.  Let $\eta = \sup_n
  \abs{\xi_n}$ and show that $\eta < \infty$ a.s.
\end{ex}
\begin{proof}
TODO
\end{proof}

\begin{ex}[Kallenberg Ex 3.6]Let $\mathcal{F}_{t,n}$ with $t \in T$ and $n \in \naturals$
  be $\sigma$-algebras such that for a fixed $t$ they are
  nondecreasing in $n$ and for a fixed $n$ they are independent in
  $t$.  Show that the $\sigma$-algebras $\bigvee_n \mathcal{F}_{t,n}$
  are independent.
\end{ex}
\begin{proof}
Because for fixed $t \in T$, we have $\mathcal{F}_{t,0} \subset
\mathcal{F}_{t,1} \subset \cdots$ we can see that $\bigcup_n
\mathcal{F}_{t,n}$ is a $\pi$-system.  Since by definition $\bigcup_n
\mathcal{F}_{t,n}$ generates $\bigvee_n
\mathcal{F}_{t,n}$ by Lemma \ref{IndependencePiSystem} it suffices to
show that $\bigcup_n \mathcal{F}_{t,n}$ are independent.

Pick $A_{t_1} \in \mathcal{F}_{t_1,n_1}, \dotsc, A_{t_m} \in
\mathcal{F}_{t_m,n_m}$. Let $n = n_1 \orop \dotsb \orop n_m$ and use
the nondecreasing property of $\mathcal{F}_{t,n}$ to observe that $A_{t_1} \in \mathcal{F}_{t_1,n}, \dotsc, A_{t_m} \in
\mathcal{F}_{t_m,n}$.  By the assumption that each of $\mathcal{F}_{t_j,n}$ is independent
therefore $\probability {A_1 \cup \dotsb \cup A_m} = \probability{A_1}
\dotsm \probability{A_m}$ and we are done.
\end{proof}

\begin{ex}[Kallenberg Ex 3.7]Let $T$ be an arbitrary index set and let
  $(S_t, \mathcal{B}(S_t))$ be metric spaces with Borel
  $\sigma$-algebras.  For each $t\in T$ suppose have random elements
  random elements $\xi^t, \xi^t_n \in S_t$  for $n \in \naturals$ such
  that $\xi^t_n \toas \xi^t$.  If for each fixed $n \in \naturals$ the
  $\xi^t_n$ are independent show that $\xi^t$ are independent.
\end{ex}
\begin{proof}
Pick a finite subset $\lbrace t_1, \dotsc, t_m \rbrace \subset T$ and
assume we are given bounded continuous functions $f_j : S_{t_j} \to
\reals$ for $j=1, \dotsc, m$.
By Lemma \ref{IndependenceExpectations} and the independence of the
$\xi^{t_j}_n$ we have $\expectation{f_1(\xi^{t_1}_n) \dotsm
  f(\xi^{t_m}_n)} = \expectation{f_1(\xi^{t_1}_n)}\dotsm
 \expectation{ f(\xi^{t_m}_n)}$ for each $n \in \naturals$.  But now
 we can use the boundedness and continuity of the $f_j$ 
\begin{align*}
&\expectation{f_1(\xi^{t_1}) \dotsm f_m(\xi^{t_m})} \\
&=
\expectation{\lim_{n \to \infty} f_1(\xi_n^{t_1}) \dotsm
  f_m(\xi_n^{t_m})} & & \text{by continuity} \\
&= \lim_{n \to \infty} \expectation{f_1(\xi_n^{t_1}) \dotsm
  f_m(\xi_n^{t_m})} & & \text{boundedness of $f_j$ and Dominated
  Convergence}  \\
&= \lim_{n \to \infty} \expectation{f_1(\xi_n^{t_1})} \dotsm
 \expectation{ f_m(\xi_n^{t_m})} & & \text{independence}  \\
&= \expectation{f_1(\xi^{t_1})} \dotsm
 \expectation{ f_m(\xi^{t_m})} & &
 \text{continuity and Dominated Convergence} \\
\end{align*}

We now prove a slight extension of Lemma
\ref{IndependenceExpectations} that shows this is sufficient to see
that $\xi^t$ are independent.  Let $(S,d)$ be a metric space and let
$U \subset S$ be open.  We show how to approximate the indicator
function $\characteristic{U}$ be bounded continuous functions.  Let
$d(x, U^c) = \inf \lbrace d(x,y) \mid y \not U \rbrace$.  Note that
$d(x, U^c)$ is continuous (see proof Lemma
\ref{DistanceToSetLipschitz}).  Let $f_n(x) = 1 \wedge n d(x, U^c)$
and observe that $f_n \uparrow \characteristic{U}$.  Now suppose
$U_{j} \subset S_{t_j}$ are open sets for $j=1, \dotsc,m$ and use the
construction just presented to create bounded continuous functions
$f^j_n \uparrow \characteristic{U_j}$.  Then it is also true that
$f^1_n \dotsm f^m_n \uparrow \characteristic{U_1} \dotsm
\characteristic{U_m}$ and so we can apply Montone convergence to see 
\begin{align*}
\probability{\xi^{t_1} \in U_1 \cap \dotsb \cap \xi^{t_m} \in U_m} &=
\lim_{n \to \infty} \expectation{f^1_n(\xi^{t_1}) \dotsm
f^m_n(\xi^{t_m})} \\
&= \lim_{n \to \infty} \expectation{f^1_n(\xi^{t_1}) } \dotsm
\expectation{f^m_n(\xi^{t_m})} \\
&= \probability{\xi^{t_1} \in U_1} \dotsm\probability{\xi^{t_m} \in
  U_m} 
\end{align*}
Now it suffices to note that the open sets  in a metric space are a
$\pi$-system that generates all of the Borel sets so by Lemma
\ref{IndependencePiSystem} it suffices to
show independence on open sets.
\end{proof}

A simpler subcase of the above
\begin{ex}Let $\xi, \xi_n$ be random elements in a metric space $S$
  such that $\xi_n \toprob \xi$ and each $\xi_n$ is
  $\mathcal{F}_n$-measurable.  Furthermore suppose $\mathcal{G}$ is a
  $\sigma$-algebra such that $\mathcal{F}_n \Independent \mathcal{G}$
  for all $n \in \naturals$, then show $\xi$ is independent of
  $\mathcal{G}$.
TODO: In the proof we mention that $\mathcal{F}_1 \subset
\mathcal{F}_2 \subset \cdots$.  Is that really required?  If not
provide a counter example.
\end{ex}
\begin{proof}
Since $\xi_n \toprob \xi$ we know there is a subsequence  that
converges almost surely.  Note that all of the hypotheses restrict
cleanly to the subsequence so we might as well assume that $\xi_n
\toas \xi$.  By the $\mathcal{F}_n$ measurability of $\xi_n$ we see
that each $\xi_n$ is $\bigvee_n \mathcal{F}_n$-measurable and
therefore $\xi$ is almost surely equal to a $\bigvee_n
\mathcal{F}_n$-measurable function.  It therefore suffices to show
that $\bigvee_n
\mathcal{F}_n \Independent \mathcal{G}$ (TODO: show this simple fact; if $\xi
= \eta$ a.s. and $\xi \Independent \mathcal{G}$ then $\eta \Independent
\mathcal{G}$).  This follows from the fact
that the nestedness of the $\mathcal{F}_n$ implies $\bigcup_n
\mathcal{F}_n$ is a $\pi$-system.  Since by definition it generates $\bigvee_n
\mathcal{F}_n$ we get the result from Lemma \ref{IndependencePiSystem}.
\end{proof}

\begin{ex}Let $\xi_1, \xi_2, \dots$ be independent random variables with values in
  $[0,1]$.  Show that $\expectation{\prod_{n =1}^\infty \xi_n} =
  \prod_{n=1}^\infty \expectation{\xi_n}$.  In particular, for
  independent events $A_n$ we have $\probability{\cup_{n=1}^\infty
    A_n} = \prod_{n=1}^\infty \probability{A_n}$.
\end{ex}
\begin{proof}
Note that because $\xi_n$ have values in $[0,1]$, the partial products
$\prod_{k=1}^n \xi_k \leq 1$ and therefore by Dominated Convergence
and Lemma \ref{IndependenceExpectations}, we have
\begin{align*}
\expectation{\prod_{k =1}^\infty \xi_k} &= \lim_{n \to \infty}
\expectation{\prod_{k=1}^n \xi_k} = \lim_{n \to \infty}
\prod_{k=1}^n\expectation{ \xi_k} = \prod_{k=1}^\infty \expectation{ \xi_k} 
\end{align*}
\end{proof}

\begin{ex}Provide an example of uncorrelated but non-independent
  random variables.
\end{ex}
\begin{proof}See Example \ref{UncorrelatedNotIndependent}.
\end{proof}

\begin{ex}Let $\xi_1, \xi_2, \dots$ be random variables.  Show that
  there exist constants $c_1 > 0, c_2 >0, \cdots$ such that
  $\sum_{n=1}^\infty c_n \xi_n$ converges almost surely.
\end{ex}
\begin{proof}First note that we can make a few assumptions about $\xi_n$ without
loss of generality.  First, we can assume that $\xi_n \geq 0$ for all
$n$; knowing that that will show absolute convergence for all series.  Next,
note that by a comparison test argument, we may further assume that $\xi_n >
0$ for all $n$ (e.g. for a random variable $\xi$ that takes $0$ as a value we can
always create the modification $\xi + \characteristic{\xi^{-1}(0)}$
which is nonzero and dominates $\xi$).  

The idea here is to leverage freshman calculus and use the ratio
test.  We first verify the following almost sure version of the ratio
test: Let $\xi_n$ be positive random variables such that there exists
a $0 < C < 1$ such that
$\sum_{n=1}^\infty \probability{\frac{\abs{\xi_{n+1}}}{\abs{\xi_n}} >
  C} < \infty$, then $\sum_{n=1}^\infty \xi_n$ converges almost
surely.

To verify the claim, we apply Borel Cantelli to conclude that $\probability{\frac{\abs{\xi_{n+1}}}{\abs{\xi_n}} >
  C \text{ i.o.}} = 0$.  Unwinding the definitions in this statement, we see that for
almost every $\omega \in \Omega$, there exists an $N>0$ such that
$\frac{\abs{\xi_{n+1}(\omega)}}{\abs{\xi_n(\omega)}} \leq C$
for all $n > N$.  The ratio test tells us $\sum_{n=1}^\infty
\xi_n(\omega)$  converges and the almost sure convergence is verified.

Now we apply the claim in our case by choosing $C=\frac{1}{2}$ and inductively defining $c_n$ so
that we guarantee $\probability{\frac{c_{n+1} \xi_{n+1}}{c_n \xi_n} >
  \frac{1}{2}} < \frac{1}{n^2}$.  To see that this is possible,
suppose we've defined $c_n$ and note that because $\xi_n > 0$, we know
that $0 < \frac{\xi_{n+1}}{c_n \xi_n} < \infty$.  This tells us that
$\lim_{N \to \infty} \probability{\frac{\xi_{n+1}}{c_n \xi_n} > N} =0$
and therefore we can find $M>0$ such that
$\probability{\frac{\xi_{n+1}}{c_n \xi_n} > N} < \frac{1}{n^2}$ for
all $N\geq M$.  Pick $c_{n+1} = \frac{1}{2M}$ and we are done.

Here is some things that I tried that proved to be a dead end.  Is there
a learning opportunity in looking at this?  Note that almost sure
convergence of $\sum_{n=1}^\infty c_n \xi_n$ is equivalent to
$\probability{\abs{\sum_{n=1}^\infty c_n \xi_n} \geq N \text{
    i.o.}}$.  The idea was to try to find $c_n$ so that we could
provide bounds on $\probability{c_n\abs{\xi_n} \geq N}$ and leverage
those to show bounds on the series.  The problem I had with this
approach is that to go from a bound on $c_n\abs{\xi_n}$ to convergence
of the series meant that $c_n\abs{\xi_n}$ had to decay fast enough to
get convergence.  If we assume a finite moment then Markov could
provide a rate of decay but in the absence of that one has to deal
with the fact that tails of $\xi_n$ can decay increasingly slowly.
I tried a truncation argument but fact that $\xi_n$ are not related
meant that I couldn't figure out how to control the residuals of the
truncations.  Maybe this line of reasoning could be made to work but I
got stuck.

Guolong asks a good follow on question: either prove this or (more
likely) provide a
counterexample on general (non-finite) measure spaces (e.g. Lebesgue measure on $\reals$).
\end{proof}

\begin{ex}Let $\xi_1, \xi_2, \dots$ be positive independent random
  variables, then $\sum_{n=1}^\infty \xi_n$ converges almost surely if
  and only if $\sum_{n=1}^\infty \expectation{\xi_n \wedge 1} <
  \infty$.
TODO: Provide hints
\end{ex}
\begin{proof}One direction is easy and doesn't require the assumption
  of independence; namely assume that $\sum_{n=1}^\infty
  \expectation{\xi_n \wedge 1} < \infty$.
Apply Tonelli's Theorem (Corollary \ref{TonelliIntegralSum}) to
conclude $\expectation{\sum_{n=1}^\infty \xi_n \wedge 1} < \infty $
which implies that $\sum_{n=1}^\infty \xi_n \wedge 1 < \infty$ almost
surely.  For any $\omega \in \Omega$ such that $\sum_{n=1}^\infty
\xi_n(\omega) \wedge 1 < \infty$ this implies $lim_{n \to \infty}
\xi_n(\omega) \wedge 1 = 0$ so there exists an $N_\omega > 0$ such
that $\xi_n(\omega) \wedge 1 = \xi_n(\omega)$  for all $n>N_\omega$ 
and therefore $\sum_{n=1}^\infty\xi_n(\omega) < \infty$ as
well.

Now lets assume $\sum_{n=1}^\infty \xi_n < \infty$.  Since $\xi_n
\wedge 1 \leq \xi_n$ we know that $\sum_{n=1}^\infty \xi_n < \infty$, 
so without loss of generality we
can assume $0 \leq \xi_n \leq 1$.

\begin{align*}
0 &< \expectation{e^{-\sum_{n=1}^\infty \xi_n}} 
&=\expectation{\prod_{n=1}^\infty e^{-\xi_n}} 
&= \prod_{n=1}^\infty \expectation{ e^{-\xi_n}} \\
&\leq \prod_{n=1}^\infty \left ( 1 - a\expectation{\xi_n} \right ) &
&\text{ where $a=1-e^{-1}$ by Lemma
  \ref{BasicExponentialInequalities}} \\
&\leq \prod_{n=1}^\infty e^{- a\expectation{\xi_n} } & & \text{since
  $1+x\leq e^x$ by Lemma \ref{BasicExponentialInequalities}} \\
&= e^{-a \sum_{n=1}^\infty \expectation{\xi_n} }
\end{align*}
which shows that $\sum_{n=1}^\infty \expectation{\xi_n} < \infty$.
\end{proof}

\begin{ex}Suppose $\xi$ is a random variable, let $\mathcal{F}$ be
  a $\sigma$-algebra and let $A$ be a measurable set.  Show that
  $\cexpectationlong{\mathcal{F},A}{\xi} =
  \frac{\cexpectationlong{\mathcal{F}}{\xi ;
      A}}{\cprobability{\mathcal{F}}{A}}$ on $A$.
\end{ex}
\begin{proof}
Note by Localization we know that $\characteristic{A}
\cexpectationlong{\mathcal{F},A}{\xi}  =
\cexpectationlong{\mathcal{F},A}{\xi;A}$, therefore we may assume that
$\xi = \characteristic{A} \xi$ and show
$\cexpectationlong{\mathcal{F},A}{\xi} = \characteristic{A}\frac{\cexpectationlong{\mathcal{F}}{\xi}}{\cprobability{\mathcal{F}}{A}}$ almost surely.

Pick $F \in \mathcal{F}$ and calculate
\begin{align*}
\expectation{\characteristic{A} \frac{\cexpectationlong{\mathcal{F}}
{\xi}}
{\cprobability{\mathcal{F}}{A}}
; A \cap F} 
&= \expectation{\cexpectationlong{\mathcal{F}}{\frac{\xi ; F}
{\cprobability{\mathcal{F}}{A}}} 
; A } & & \text{by pushout}\\
&= \expectation{\cexpectationlong{\mathcal{F}}{\frac{\xi ; F}
{\cprobability{\mathcal{F}}{A}}} 
\cprobability{\mathcal{F}}{A} } \\
&= \expectation{\cexpectationlong{\mathcal{F}}{\xi ; F}} & & \text{by
  pushout} \\
&= \expectation{\xi ; F} = 
\expectation{\xi ; A \cap F} & & \text{by tower property}
\end{align*}
and trivially
\begin{align*}
\expectation{\characteristic{A} \frac{\cexpectationlong{\mathcal{F}}
{\xi}}
{\cprobability{\mathcal{F}}{A}}
; A^c \cap F}  &= 0 = \expectation{\xi ; A^c \cap F}
\end{align*}
Since sets of the form $A \cap F$, $A^c \cap F$ and $F$ for $F \in
\mathcal{F}$ form a $\pi$-system that generate $\sigma(A,
\mathcal{F})$ we have shown the result.
\end{proof}

\begin{ex}Let $A_1, A_2, \cdots$ be a disjoint partition of $\Omega$
  and let $\mathcal{F} = \sigma(A_1, A_2, \dots)$.  Show that for
  every integrable random variable $\xi$ we have
$\cexpectationlong{\mathcal{F}}{\xi} = \sum_{\probability{A_n} \neq 0}
\frac{\expectation{\xi ; A_n}}{\probability{A_n}} \characteristic{A_n}$ almost surely.
\end{ex}
\begin{proof}First note that it is trivial that $\sum_{\probability{A_n} \neq 0}
\frac{\expectation{\xi ; A_n}}{\probability{A_n}}\characteristic{A_n}$ is
$\mathcal{F}$-measurable.  Because the $A_n$ are a disjoint partition,
  they are a $\pi$-system and the it will suffice to show the
  averaging property for the sets $A_n$.
Pick an $A_m$ such that $\probability{A_m} \neq 0$, they by
disjointness of the $A_n$ we get
\begin{align*}
\expectation{\sum_{\probability{A_n} \neq 0}
\frac{\expectation{\xi ; A_n}}{\probability{A_n}}
\characteristic{A_n}; A_m} &= \expectation{\frac{\expectation{\xi ;
    A_m}}{\probability{A_m}} \characteristic{A_m}} = \expectation{\xi ; A_m}
\end{align*}
For any $A_m$ with $\probability{A_m} = 0$ and again applying the
disjointness of the $A_n$ we get
disjointness of the $A_n$ that 
\begin{align*}
0 &= \expectation{\sum_{\probability{A_n} \neq 0}
\frac{\expectation{\xi ; A_n}}{\probability{A_n}}
\characteristic{A_n}; A_m}  = \expectation{\xi ; A_m}
\end{align*}
\end{proof}

\begin{ex}Suppose $\xi$ is a random element in $S$ such that
  $\cprobability{\mathcal{F}}{\xi \in \cdot}$ has a regular version
  $\nu$.  Let $f : S \to T$ be measurable.  Show that
  $\cprobability{\mathcal{F}}{f(\xi) \in \cdot}$ has a regular version
  given by $\pushforward{f} {\nu} (\omega, A) = \nu(\omega, f^{-1}(A))$.
\end{ex}
\begin{proof}
Our hypothesis is that for every $A$, $\cprobability{\mathcal{F}}{\xi
  \in A}(\omega) = \mu(\omega, A)$.  We calculate 
\begin{align*}
\cprobability{\mathcal{F}}{f(\xi) \in A}(\omega) &=
\cexpectationlong{\mathcal{F}}{\characteristic{f^{-1}(A)}(\xi)} \\
&=\int \characteristic{f^{-1}(A)}(s) \, d\mu(\omega, s) & & \text{by
  Theorem \ref{Disintegration}} \\
&=\mu(\omega, f^{-1}(A))
\end{align*}
and we are done.
\end{proof}

\begin{ex}Let $\xi$ be a random element in $S$.  Show that $\xi$ is
  $\mathcal{F}$-measurable if and only if $\delta_\xi$ is a regular
  version of 
  $\cprobability{\mathcal{F}}{\xi \in \cdot}$.

TODO: Refine this statement to include almost sureness...
\end{ex}
\begin{proof}
$\mathcal{F}$-measurability of $\xi$ is equivalent to
$\mathcal{F}$-measurability of $\characteristic{A}(\xi)$ for all $A$
which is equivalent to $\cprobability{\mathcal{F}}{\xi \in A} =
\characteristic{A}(\xi)$ almost surely for all $A$.  Evaluating the
last equality at $\omega$ we see that 
\begin{align*}
\cprobability{\mathcal{F}}{\xi \in A}(\omega) &= \begin{cases}
1 & \text{if $\xi(\omega) \in A$} \\
0 & \text{if $\xi(\omega) \notin A$} 
\end{cases}\\
&= \delta_{\xi(\omega)}(A)
\end{align*}

The fact that $\delta_\xi$ is a probability kernel is simple.  It is
trivial that for fixed $\omega$, $\delta_\xi(\omega)$ is a probability
measure.  If we fix $A$ then $\delta_\xi(\omega)(A)$ is clearly seen
to be measurable since it is just the characteristic function of the
measurable set $A$.
\end{proof}

\begin{ex}Let $\xi$ be an integrable random variable for which
  $\cexpectationlong{\mathcal{F}}{\xi} \eqdist \xi$.  Show that in
  fact $\cexpectationlong{\mathcal{F}}{\xi} = \xi$ a.s.
\end{ex}
\begin{proof}
Here is a simple and conceptual proof in the case that
$\cexpectationlong{\mathcal{F}}{\xi}$ (and therefore $\xi$) take
finitely many values/are simple functions.  Let $y_1 < \cdots < y_n$ be the values of
$\xi$ such that $\probability{\xi = y_i} \neq 0$.   Consider $A_1 = \lbrace \cexpectationlong{\mathcal{F}}{\xi} =
y_1 \rbrace$.  By definition of conditional expectation
$\expectation{\xi ; A_1} =
\expectation{\cexpectationlong{\mathcal{F}}{\xi} ; A_1} = y_1
\probability{A_1}$.  Because $y_1$ is the minimum value of $\xi$ it
follows that we must have $\xi = y_1$ identically on $A_1$.  Since $\xi \eqdist
\cexpectationlong{\mathcal{F}}{\xi} $, we know that $\probability{\xi
  = y_1} = \probability{A_1}$ and therefore $\xi \geq y_2$ almost
surely off of $A_1$.  Now induct.

If we want to apply standard machinery to go from the
simple function case.  Then we could approximate $\xi$ by an
increasing family of simple functions of the form $f_n(\xi)$ but then
we know that $f_n(\xi) \eqdist
f_n(\cexpectationlong{\mathcal{F}}{\xi})$ but not necessarily that $f_n(\xi) \eqdist
\cexpectationlong{\mathcal{F}}{f_n(\xi)}$ which is what we would need
in order to use the simple function case.  All roads seem to lead to a
need to show that $\cexpectationlong{\mathcal{F}}{f(\xi)}$ and
$f(\cexpectationlong{\mathcal{F}}{\xi})$ are equal in some sense
(either a.s. or in distribution).

The idea is to use Jensen's inequality.  First note that
  we can find a strictly convex function $f$ such that $0 \leq f(x)
  \leq \abs{x}$.  Therefore we know that $\expectation{f(\xi)} <
  \infty$.  

Moreover, by Theorem \ref{ExistenceConditionalDistribution} be have a
regular version $\nu$ for $\cprobability{\mathcal{F}}{\xi \in A}$.  By
Theorem \ref{Disintegration} we know that
$\cexpectationlong{\mathcal{F}}{f(\xi)} = \int s \, d\mu(s)$.

Because $\xi \eqdist \cexpectationlong{\mathcal{F}}{\xi}$ we also know
that $f(\xi) \eqdist f(\cexpectationlong{\mathcal{F}}{\xi})$ which
shows us that ... 

TODO: I am aiming to show that $\pushforward{f}{\mu}$ is a regular version
for
$\cprobability{\mathcal{F}}{f(\cexpectationlong{\mathcal{F}}{\xi}) \in
\cdot}$.
If we could get that then we could calculate
\begin{align*}
f(\cexpectationlong{\mathcal{F}}{\xi}) &=
\cexpectationlong{\mathcal{F}}
{f(\cexpectationlong{\mathcal{F}}{\xi})} \\
&= \int s d \pushforward{f}{\mu} (s) & & \text{by Theorem \ref{Disintegration}}\\
&= \int f(s) \, d\mu(s)  & & \text{by Expectation Rule} \\
&= \cexpectationlong{\mathcal{F}}{f(\xi)} & & \text{by Theorem \ref{Disintegration}}
\end{align*}
Now apply the strictly convex case of Jensen's Inequality to conclude
the result.

If we assume finite second moments then there should be a proof of
this by showing that the conditional variance is $0$.  TODO: Define
conditional variance and show the result.
\end{proof}

\begin{ex}Prove or disprove the following statement.  Suppose $\xi \eqdist \eta$, show that for every $A$, 
  $\cprobability{\mathcal{F}}{\xi \in A} =
  \cprobability{\mathcal{F}}{\eta \in A}$ a.s.
\end{ex}
\begin{proof}
This is false.  Let $\Omega = \lbrace 0,1 \rbrace$ with uniform distribution and
power set $\sigma$-algebra.  Let
$\xi(x) = x$ and let $\eta(x) = 1 - x$.  Note that $\xi \eqdist
\eta$ (both have a uniform distribution on $\lbrace 0,1 \rbrace$). Now take $\mathcal{F} = \mathcal{A}$ so that
$\cprobability{\mathcal{F}}{\xi \in A} = \characteristic{\xi \in A}$
and $\cprobability{\mathcal{F}}{\eta \in A} = \characteristic{\eta \in
  A}$ and take $A = \lbrace 0 \rbrace$ or $A = \lbrace 1 \rbrace$.
\end{proof}

\begin{ex}Find $\xi, \eta, \mathcal{F}$ such that $\xi \eqdist \eta$
  but $\cexpectationlong{\mathcal{F}}{\xi} \neq
  \cexpectationlong{\mathcal{F}}{\eta}$ a.s.
\end{ex}
\begin{proof}
Pick sets $A,B,C$ such that $\probability{A} = \probability{B}$ but
$\probability{A \cap C} \neq \probability{B \cap C}$.  Even more
trivially, take $\mathcal{F} = \mathcal{A}$ so that
$\cexpectationlong{\mathcal{F}}{\xi} = \xi$ and similarly with
$\eta$.  Now the statement is equivalent to show two random elements
that not almost surely equal but have the same distribution.
\end{proof}

\begin{ex}Suppose $\xi, \tilde{\xi}$ are integrable random variables
  and $\eta, \tilde{\eta}$ are random elements in $(T, \mathcal{T})$
  such that $(\xi,\eta) \eqdist (\tilde{\xi}, \tilde{\eta})$.  Show
  that $\cexpectationlong{\eta}{\xi} \eqdist \cexpectationlong{\tilde{\eta}}{\tilde{\xi}}$.
\end{ex}
\begin{proof}
First, note the intuition behind the statement.  As a result of
$(\xi,\eta) \eqdist (\tilde{\xi}, \tilde{\eta})$ we can also conclude
that $\xi \eqdist \tilde{\xi}$ and $\eta \eqdist \tilde{\eta}$.
However, we also expect that the conditional distributions on $T$ are
equal (thinking heuristically of a formula like $\cprobability{B}{A} =
\probability{A \cap B} / \probability{B}$).  The first order of
business is to formulate this intuition precisely and prove it.

By Theorem \ref{ExistenceConditionalDistribution} there are
probability kernels $\mu$ and $\tilde{\mu}$ such that
$\cprobability{\eta}{\xi \in A} = \mu(\eta, A)$ and
$\cprobability{\tilde{\eta}}{\tilde{\xi} \in A} =
\tilde{\mu}(\tilde{\eta}, A)$ for all Borel sets $A$.  Our first claim
is that $\mu = \tilde{\mu}$ almost surely with respect to
$\mathcal{L}{\eta}$.

Pick a Borel set $A$ and let $B = \lbrace t \in T \mid \mu(t, A) >
\tilde{\mu}(t, A) \rbrace$. 
\begin{align*}
0 &= \probability{\xi \in A; \eta \in B} - \probability{\tilde{\xi}
  \in A; \tilde{\eta} \in B} & \text{by hypothesis}\\
&=\expectation{\int \characteristic{A \times B}(s, \eta) \,
  d\mu(\eta, s) - \int \characteristic{A \times B}(s, \tilde{\eta}) \,
  d\tilde{\mu}(\eta, s)} & \text{by Theorem \ref{Disintegration}}\\
&=\expectation{\characteristic{B}(\eta) \mu(\eta, A) -
  \characteristic{B}(\tilde{\eta}) \tilde{\mu}(\tilde{\eta}, A)} \\
&=\int \characteristic{B}(t) \mu(t, A) -
  \characteristic{B}(t) \tilde{\mu}(t, A) \, d\mathcal{L}(\eta)(t) &
  \text{by Lemma \ref{ChangeOfVariables} and $\mathcal{L}(\eta) = \mathcal{L}(\tilde{\eta})$.}
\end{align*}
which by choice of $B$ shows that $\mu(t, A) =\tilde{\mu}(t, A)$
almost surely $\mathcal{L}(\eta)$.  We can show this almost surely for all $A =
(-\infty, r]$ with $r \in \rationals$ by taking the union of a
countable number of null sets.  This shows that $\mu = \tilde{\mu}$
a.s.

Having shown equality of the conditional distributions it follows from
Theorem \ref{Disintegration} that if we define $f(t) = \int s \, d\mu(t,
s)$ then we have $\cexpectationlong{\eta}{\xi} = f(\eta)$ and
$\cexpectationlong{\tilde{\eta}}{\tilde{\xi}} = f(\tilde{\eta})$.
Since $\eta \eqdist \tilde{\eta}$ it follows that $f(\eta) \eqdist
f(\tilde{\eta})$ and the result is proven.
\end{proof}

\begin{ex}Suppose $\xi$ is a random element in a Borel space $(S, \mathcal{S})$, let
  $\mathcal{F}$ be a $\sigma$-algebra and let $\eta =
  \cprobability{\mathcal{F}}{\xi \in \cdot}$, show $\cindependent{\xi}{\mathcal{F}}{\eta}$.
\end{ex}
\begin{proof}
First it is worth clarifying the question.  Since we have assume $S$
is Borel then by Theorem \ref{ExistenceConditionalDistribution} be may
assume that $\eta$ is an $\mathcal{F}$-measurable random measure on
$S$.  We are asked to show conditional independence of $\xi$ and
$\mathcal{F}$ relative to this random measure.

By Lemma \ref{ConditionalIndependenceDoob} it will suffice to show for
every $A \in \mathcal{S}$, 
\begin{align*}
\cexpectationlong{\eta}{\xi \in  A} 
&= \cexpectationlong{\eta,\mathcal{F}}{\xi \in  A} 
= \cexpectationlong{\mathcal{F}}{\xi \in  A} 
\end{align*}
where the last equality follows from the $\mathcal{F}$-measurability
of $\eta$.  However this is easily verified since the $\sigma$-algebra
on the space of probability measures $\mathcal{P}(S)$ is the smallest $\sigma$-algebra that makes
evaluation maps $ev_B(\mu) = \mu(B)$ measurable (here $B \in \mathcal{S}$).  Thus we have by
definition of $\eta$, $\cexpectationlong{\mathcal{F}}{\xi \in  A} =
ev_A(\eta)$ which shows that $\cexpectationlong{\mathcal{F}}{\xi \in
  A}$ is in fact $\eta$-measurable.
\end{proof}

\begin{ex}Suppose $\cindependent{\xi}{\zeta}{\eta}$ and
  $\cindependent{\gamma}{(\xi,\eta, \zeta)}{}$, show that
  $\cindependent{\xi}{\zeta}{\eta,\gamma}$ and $\cindependent{\xi}{(\zeta,\gamma)}{\eta}$.
\end{ex}
\begin{proof}
By Lemma \ref{ConditionalIndependenceChainRule},
$\cindependent{\xi}{(\zeta,\gamma)}{\eta}$ is equivalent to
$\cindependent{\xi}{\zeta}{\eta}$ and
$\cindependent{\xi}{\gamma}{\eta, \zeta}$.  The fact that
$\cindependent{\xi}{\zeta}{\eta}$ is a hypothesis whereas
$\cindependent{\xi}{\gamma}{\eta, \zeta}$ follows from another
application of Lemma \ref{ConditionalIndependenceChainRule} to show
that $\cindependent{\gamma}{(\xi,\eta, \zeta)}{}$ is equivalent to
$\cindependent{\gamma}{\zeta}{}$ and
$\cindependent{\gamma}{\eta}{\zeta}$ and
$\cindependent{\gamma}{\xi}{\zeta, \eta}$

Now by Lemma \ref{ConditionalIndependenceChainRule} 
we know $\cindependent{\xi}{(\gamma, \zeta)}{\eta}$ is equivalent to
$\cindependent{\xi}{\gamma}{\eta}$ and
$\cindependent{\xi}{\zeta}{\eta, \gamma}$
hence implies $\cindependent{\xi}{\zeta}{\eta, \gamma}$.
\end{proof}

\begin{ex}Suppose we are given random elements such that $(\xi, \eta,
  \zeta) \eqdist (\tilde{\xi}, \tilde{\eta}, \tilde{\zeta})$, then
  $\cindependent{\xi}{\zeta}{\eta}$ if and only if $\cindependent{\tilde{\xi}}{\tilde{\zeta}}{\tilde{\eta}}$.
\end{ex}
\begin{proof}
First we 
\begin{align*}
\
\end{align*}
\end{proof}


\begin{ex}Suppose $\tau$ and $\sigma$ are discrete optional
  times with respect the filtration $\mathcal{F}_0 \subset
  \mathcal{F}_1 \subset \cdots$. Then $\sigma \wedge \tau$ and
  $\sigma$ and $\sigma \vee \tau$ are optional times.  In addition, 
\begin{align*}
\mathcal{F}_{\tau \wedge
    \sigma} &\subset \mathcal{F}_\sigma \subset \mathcal{F}_{\tau \vee
    \sigma}
\end{align*}
\end{ex}
\begin{proof}
First we show that $\tau \wedge \sigma$ and $\tau \vee \sigma$ are
actually optional times.  This is simple by noting
\begin{align*}
\lbrace \tau \wedge \sigma \leq n \rbrace = \lbrace \tau \leq n
\rbrace \cup \lbrace \sigma \leq n \rbrace \in \mathcal{F}_n
\end{align*}
and
\begin{align*}
\lbrace \tau \vee \sigma \leq n \rbrace = \lbrace \tau \leq n
\rbrace \cap \lbrace \sigma \leq n \rbrace \in \mathcal{F}_n
\end{align*}
If we are given $A \in \mathcal{F}_\sigma$ the by definition for all
$n$, $A \cap \lbrace \sigma \leq n \rbrace \in \mathcal{F}_n$.
Therefore since by definition of optional time we also have $\lbrace
\tau \leq n \rbrace \in \mathcal{F}_n$ we have
\begin{align*}
A \cap \lbrace \tau \vee \sigma \leq n \rbrace &= (A \cap \lbrace \sigma \leq n
\rbrace) \cap \lbrace \tau \leq n \rbrace \in \mathcal{F}_n
\end{align*}
which shows $A \in \mathcal{F}_{\sigma \vee \tau}$.

Now if we assume that $A \in \mathcal{F}_{\sigma \wedge \tau}$, then
for all $n$ we have
\begin{align*}
A \cap \lbrace \tau \wedge \sigma \leq n \rbrace &= A \cap \lbrace \tau \leq n
\rbrace \cup A \cap \lbrace \sigma \leq n \rbrace \in \mathcal{F}_n
\end{align*}
Since we have $\lbrace \sigma \leq n \rbrace, \lbrace \tau \leq n
\rbrace \in \mathcal{F}_n$, then we know that $\lbrace \tau \leq n
\rbrace \setminus \lbrace \sigma \leq n\rbrace \in \mathcal{F}_n$ and
so 
\begin{align*}
\left( A \cap \lbrace \tau \leq n
\rbrace \right ) \cup \left (A \cap \lbrace \sigma \leq n \rbrace
\right ) \cup \left ( \lbrace \tau \leq n
\rbrace \setminus \lbrace \sigma \leq n\rbrace\right)^c &= A \cap \lbrace \sigma \leq n
\rbrace \in \mathcal{F}_n
\end{align*}
which shows $A \in \mathcal{F}_\sigma$.
\end{proof}

\begin{ex}Suppose $\tau$ is a discrete optional
  time with respect the filtration $\mathcal{F}_0 \subset
  \mathcal{F}_1 \subset \cdots$, then $\tau$ is $\mathcal{F}_\tau$-measurable.
\end{ex}
\begin{proof}
For any $n, m$, we have 
\begin{align*}
\lbrace \tau = m \rbrace  \cap \lbrace \tau \leq n \rbrace &=
\begin{cases}
\emptyset & \text{if $m > n$} \\
\lbrace \tau = m \rbrace & \text{if $m\leq n$}
\end{cases}
\end{align*}
hence in all cases is in $\mathcal{F}_n$.
\end{proof}

\begin{ex}Suppose $\tau$ and $\sigma$ are discrete optional
  times with respect the filtration $\mathcal{F}_0 \subset
  \mathcal{F}_1 \subset \cdots$. Then each of $\lbrace \sigma < \tau
  \rbrace$, $\lbrace \sigma \leq \tau
  \rbrace$ and $\lbrace \sigma = \tau
  \rbrace$ is in $\mathcal{F}_{\sigma} \cap \mathcal{F}_{\tau}$.
\end{ex}
\begin{proof}
It suffice to prove two of the three since the third set can be
constructed using finite unions or intersections of the other two.
First we show that $\lbrace \sigma < \tau \rbrace \in
\mathcal{F}_\tau$.
Pick an $n$ and we calculate
\begin{align*}
\lbrace \sigma < \tau \rbrace \cap \lbrace \tau \leq n \rbrace &=
\cup_{m\leq n}\lbrace \sigma < \tau \rbrace \cap \lbrace \tau = m
\rbrace \\
&= \cup_{m\leq n}\lbrace \sigma < m \rbrace \cap \lbrace \tau = m
\rbrace \\
\end{align*}
Now each $\lbrace \sigma < m \rbrace \in \mathcal{F}_m \subset
\mathcal{F}_n$ and each $\lbrace \tau = m
\rbrace \in \mathcal{F}_m \subset
\mathcal{F}_n$ by definition of optional time so the union is and we
have shown $\lbrace \sigma < \tau \rbrace \in \mathcal{F}_\tau$.  The
same argument clearly shows that the other sets are in
$\mathcal{F}_\tau$ as well.  To see that all sets are in
$\mathcal{F}_\sigma$, it suffices to note for example that 
\begin{align*}
\lbrace \sigma < \tau \rbrace^c &= \lbrace \tau \leq \sigma \rbrace
\in \mathcal{F}_\sigma
\end{align*}
by what we have already proven. Apply the closure of $\sigma$-algebras
under complement to get the result.
\end{proof}

\begin{ex}Let $\sigma$ and $\tau$ be optional times with respect to
  the filtration $\mathcal{F}_0 \subset \mathcal{F}_1 \subset
  \cdots$.  Show that 
\begin{align*}
\cexpectationlong
{\mathcal{F}_\tau}{\cexpectationlong{\mathcal{F}_\sigma}{\xi}} &=
\cexpectationlong
{\mathcal{F}_\sigma}{\cexpectationlong{\mathcal{F}_\tau}{\xi}} = \cexpectationlong
{\mathcal{F}_{\sigma \wedge \tau}}{\xi}
\end{align*}
\end{ex}
\begin{proof}
The first thing to do is show how to calculate conditional
expectations with respect to $\sigma$-algebras of the form
$\mathcal{F}_\sigma$ for an arbitrary optional time $\sigma$.  Given
an integrable random variable $\xi$ we let $M^\xi_n =
\cexpectationlong{\mathcal{F}_n}{\xi}$ be the martingale generated by
$\xi$.  We claim
\begin{align*}
\cexpectationlong{\mathcal{F}_\sigma}{\xi} &= M^\xi_\sigma
\end{align*}
To see this, pick an $A \in \mathcal{F}_\sigma$ and then note that for
every $n$, use the fact that $A \cap \lbrace \sigma = n \rbrace \in
\mathcal{F}_n$ and the telescoping rule for conditional expectation to see
\begin{align*}
\expectation{\characteristic{A} \characteristic{\lbrace \sigma = n
    \rbrace } \xi} &= 
\expectation{\characteristic{A} \characteristic{\lbrace \sigma = n
    \rbrace } \cexpectationlong{\mathcal{F}_n}{\xi} } = 
\expectation{\characteristic{A} \cexpectationlong{\mathcal{F}_n}{\characteristic{\lbrace \sigma = n
    \rbrace } \xi} } 
\end{align*}
which is easy to extend by linearity 
\begin{align*}
\expectation{\characteristic{A} \xi} &= \sum_{n=0}^\infty \expectation{\characteristic{A} \characteristic{\lbrace \sigma = n
    \rbrace } \xi} = \sum_{n=0}^\infty \expectation{\characteristic{A} \cexpectationlong{\mathcal{F}_n}{\characteristic{\lbrace \sigma = n
    \rbrace } \xi}} = \expectation{\characteristic{A}
  \sum_{n=0}^\infty \cexpectationlong{\mathcal{F}_n}{\characteristic{\lbrace \sigma = n
    \rbrace } \xi}} \\
&= \expectation{\characteristic{A} M^\xi_\sigma}
\end{align*}
Using this formula twice we have
\begin{align*}
\cexpectationlong{\mathcal{F}_\sigma}{\cexpectationlong{\mathcal{F}_\tau}{\xi}}
&=
\cexpectationlong{\mathcal{F}_\sigma}{M^\xi_\tau}
\\
&= \sum_{n=0}^\infty \characteristic{\lbrace \sigma = n \rbrace}
\cexpectationlong{\mathcal{F}_n}{M^\xi_\tau} \\
&= \sum_{n=0}^\infty \sum_{m=0}^\infty \characteristic{\lbrace \sigma = n \rbrace}
\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\mathcal{F}_m}{\characteristic{
    \lbrace \tau = m \rbrace } \xi}} \\
\end{align*}
Now consider each term $\characteristic{\lbrace \sigma = n \rbrace}
\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\mathcal{F}_m}{\characteristic{
    \lbrace \tau = m \rbrace } \xi}}$; there are two cases. If $m \leq n$
then since $\mathcal{F}_m \subset \mathcal{F}_n$ we can write
\begin{align*}
\characteristic{\lbrace \sigma = n \rbrace}
\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\mathcal{F}_m}{\characteristic{
    \lbrace \tau = m \rbrace } \xi}} &= \characteristic{\lbrace \sigma = n \rbrace}\cexpectationlong{\mathcal{F}_m}{\characteristic{
    \lbrace \tau = m \rbrace } \xi} = \characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau = m \rbrace }\cexpectationlong{\mathcal{F}_m}{ \xi} 
\end{align*}
If $n \leq m$ then because $\mathcal{F}_n \subset \mathcal{F}_m$ and
the telescoping rule,
\begin{align*}
\characteristic{\lbrace \sigma = n \rbrace}
\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\mathcal{F}_m}{\characteristic{
    \lbrace \tau = m \rbrace } \xi}} &= 
\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\mathcal{F}_m}{\characteristic{\lbrace
      \sigma = n \rbrace} \characteristic{
    \lbrace \tau = m \rbrace } \xi}}  = \cexpectationlong{\mathcal{F}_n}{\characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau = m \rbrace } \xi} 
\end{align*}
These two forms are a bit different and are not equivalent because we
cannot ascertain the $\mathcal{F}_{m \wedge n}$-measurability of $\characteristic{\lbrace \sigma = m \rbrace}\characteristic{
    \lbrace \tau = m \rbrace }$.  However, we do know that $\lbrace
  \sigma > m \rbrace = \lbrace \sigma \leq m \rbrace^c$ is
  $\mathcal{F}_m$-measurable and $\lbrace
  \tau > n \rbrace = \lbrace \tau \leq n \rbrace^c$ is
  $\mathcal{F}_n$-measurable.  So if we sum using the case $n \leq m$,
  we get,
\begin{align*}
\sum_{m>n} \characteristic{\lbrace \sigma = n \rbrace}
\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\mathcal{F}_m}{\characteristic{
    \lbrace \tau = m \rbrace } \xi}} 
&= \sum_{m>n} \cexpectationlong{\mathcal{F}_n}{\characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau = m \rbrace } \xi} \\
&= \cexpectationlong{\mathcal{F}_n}{\characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau > n \rbrace } \xi} \\
&=\characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau > n \rbrace }  \cexpectationlong{\mathcal{F}_n}{\xi} \\
&= \sum_{m>n} \characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau=m \rbrace }  \cexpectationlong{\mathcal{F}_n}{\xi} 
\end{align*}
So this shows us how to get everything into a common form if we break
up the sum properly, 
\begin{align*}
\cexpectationlong{\mathcal{F}_\sigma}{\cexpectationlong{\mathcal{F}_\tau}{\xi}} 
&= \sum_{n=0}^\infty \sum_{m=n+1}^\infty \characteristic{\lbrace \sigma = n \rbrace}
\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\mathcal{F}_m}{\characteristic{
    \lbrace \tau = m \rbrace } \xi}} + \\
&\sum_{m=0}^\infty \sum_{n=m}^\infty \characteristic{\lbrace \sigma = n \rbrace}
\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\mathcal{F}_m}{\characteristic{
    \lbrace \tau = m \rbrace } \xi}}  \\
&=\sum_{n=0}^\infty \sum_{m=n+1}^\infty \characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau=m \rbrace }  \cexpectationlong{\mathcal{F}_n}{\xi} + \\
&\sum_{m=0}^\infty \sum_{n=m}^\infty \characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau=m \rbrace }  \cexpectationlong{\mathcal{F}_m}{\xi}
  \\
&= \sum_{m=0}^\infty \sum_{n=0}^\infty \characteristic{\lbrace \sigma = n \rbrace}\characteristic{
    \lbrace \tau=m \rbrace }  \cexpectationlong{\mathcal{F}_{m\wedge
      n}}{\xi} \\
&= M^\xi_{\sigma \wedge \tau} = \cexpectationlong{\mathcal{F}_{\sigma
    \wedge \tau}}{\xi}
\end{align*}
\end{proof}

\begin{ex}Show that a random variable $\xi$ has subexponential tails
  if and only if there exists $C > 0$ such that
  $\expectation{\abs{\xi}^k} \leq C k^C$ for all integers $k > 0$.
\end{ex}
\begin{proof}
TODO: Mimic the proof of Lemma \ref{SubgaussianEquivalence}.
\end{proof}

\begin{ex}Suppose we have $\sigma$-algebras $\mathcal{F}$,
  $\mathcal{G}_1$, $\mathcal{G}_2$, $\mathcal{H}$ with $\mathcal{G}_1
  \subset \mathcal{G}_2$.  If
  $\cindependent{\mathcal{F}}{\mathcal{G}_1}{\mathcal{H}}$
is it true that $\cindependent{\mathcal{F}}{\mathcal{G}_2}{\mathcal{H}}$?
Prove or give a counterexample.
\end{ex}
\begin{proof}
Here is a counterexample in which $\mathcal{G}_1$ is the trivial
$\sigma$-algebra.  Perform two independent Bernoulli trials with rate
$1/2$.  Thus we have sample space $\Omega = \lbrace HH, HT, TT, TH
\rbrace$ with the uniform distribution.  Let $A = \lbrace HH, HT
\rbrace$ (and let $\mathcal{F} = \lbrace \emptyset, \Omega, A, A^c
\rbrace$) and let $B = \lbrace HT, TT \rbrace$ (and let $\mathcal{H} =
\lbrace \emptyset , \Omega, B, B^c \rbrace$).  Note that $A$ and $B$
are independent.  Now let $C = \lbrace HH, TT \rbrace$ (and let
$\mathcal{G}_2 = \lbrace \emptyset, \Omega, C, C^c \rbrace$ and note
that $A$ and $B$ are not conditionally independent given $C$ because
$\cprobability{C}{A \cap B} = 0$ whereas $\cprobability{C}{A} = 1/2$
and $\cprobability{C}{B} = 1/2$
\end{proof}

\begin{ex}Suppose $\mathcal{F}$ is independent of $\mathcal{G}$ and
  $\mathcal{H}$, is it true that $\mathcal{F}$ is independent of
  $\sigma(\mathcal{G}, \mathcal{H})$?  Prove or give a counterexample.
\end{ex}
\begin{proof}
Note that $\mathcal{F}$ is independent of
  $\sigma(\mathcal{G}, \mathcal{H})$ if and only if
  $\cindependent{\mathcal{F}}{\mathcal{G}}{}$
and $\cindependent{\mathcal{F}}{\mathcal{H}}{\mathcal{G}}$.  Because
of this equivalence the previous exercise is a counterexample here as
well.  Using the
notation of the previous exercise, let $\mathcal{F} = \sigma(A)$ and
let $\mathcal{G} = \sigma(C)$ and note that $A$ and $C$ are
independent
by direct calculation (this is also intuitively clear).  We also saw
in the previous exercise that $A$ and $B$ are independent and that $A$
is not conditionally independent of $B$ given $C$; hence $A$ is not
indepndent of $\sigma(B,C)$.

Note that we can also show this directly without using the Lemma. A
little work shows that $\sigma(B,C) = 2^\Omega$; it suffices to
note that $B \cap C = \lbrace TT \rbrace$,  $B^c \cap C^c = \lbrace TH
\rbrace$, $B \cap C^c = \lbrace HT \rbrace$ and $B^c \cap C = \lbrace HH
\rbrace$.  Given this fact
it is easy to see that $A$ is not independent of $\sigma(B,C)$ by noting that, because
$P(A) = 1/2$, it is not independent of itself.

Note also that the the key to the failure here is the fact that $A$, $B$
and $C$ are not jointly independent (they are pairwise independent),
otherwise we could appeal to Lemma \ref{IndependenceGrouping}.
To see the lack of joint independence consider $\probability {A \cap B
\cap C} = 0$.
\end{proof}

\section{Techniques}

This section is a place to collect some of the recurring proof
techniques that one should be familiar with.

\subsection{Standard Machinery}
The standard measure theory arguments that proceed by showing a result
for indicator functions, simple random variables and the positive
random variables.  TODO:  There are a ton of examples of this such as
Lemma \ref{ChangeOfVariables} and Lemma \ref{ChainRuleDensity}.

\subsubsection{Monotone Class Arguments}
Part of the standard machinery that has independent utility is the
monotone class argument.  This allows one to demonstrate that a
property holds for an entire $\sigma$-algebra of sets by showing that
property holds for a simpler subclass of sets.  Good examples are
Lemma \ref{UniquenessOfMeasure} and Lemma \ref{IndependencePiSystem}.

\subsection{Almost Sure Convergence}
When one needs to show almost sure convergence of a sequence of random
variables the Borel Cantelli Theorem is a workhorse.  Good examples of
this are Lemma \ref{SLLNL4} and Lemma
\ref{ConvergenceInProbabilityAlmostSureSubsequence}.

Another technique to use that is related is to show that the sum of
the random variables is integrable.  Then you can conclude that the
sum of random variables is almost surely finite and therefore the
terms of the sequence converge to zero a.s.
Good examples of
this are Lemma \ref{SLLNL2} and Lemma
\ref{ConvergenceInProbabilityAlmostSureSubsequence}.

\subsection{Bounding Expectations}

A common task that one faces is to provide bounds for an expected
value (or more generally a moment).  For example, one may need to know
that a random variable has a finite expectation for use with the
Dominated Convergence Theorem.

\subsubsection{Using Tail Bound}
A problem I have encountered is trying to use a tail bound to prove
that an expectation is finite.  The problem that I sometime have is
that I write:
\begin{align*}
\expectation{f(\xi)} = \expectation{ \characteristic{\xi \leq \lambda}
  \cdot f(\xi)} + \expectation{\characteristic{\xi > \lambda} \cdot f(\xi)}
\end{align*}
Often knowing $\xi \leq \lambda$ we can show that the first
expectation is bounded (this is often easy).  The problem is usually
that one might be given a tail bound that controls $\probability{\xi >
  \lambda}$ but there is no control over the behavior of $f(\xi)$ that
allows one to provide a bound for the second expectation.  Are there
general approaches for dealing with this?  Possible answer here is
that one might need to take a different approach and use Lemma
\ref{TailsAndExpectations}.  A good example of how to do this is with 
Lemma \ref{SubgaussianEquivalence}.

TODO: Passing from $L^p$ convergence to almost sure convergence.
\end{document}