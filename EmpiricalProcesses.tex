\chapter{Empirical Processes}

\section{Definitions}

Let $(S, \mathcal{S}, \mu)$ be a probability space.  Suppose that $\xi, \xi_1, \xi_2, \dotsc$ are i.i.d. random elements in $S$ with $\mathcal{L}(\xi_i) = \mu$.  Let $f \in \mathcal{L}(S, \mathcal{S}, \mu)$ be a square integrable real valued function on $S$ and consider the random variables $f(\xi), f(\xi_1), f(\xi_2), \dotsc$.  By the Expectation Rule (Lemma \ref{ExpectationRule}) we know that 
\begin{align*}
\probability{f(\xi_n) \in A} &= \int \characteristic{f \in A} \, d\mathcal{L}(\xi_n) = \mu \lbrace f \in A \rbrace = \pushforward{f}{\mu}(A)
\end{align*}
so the $f(\xi_n)$ are identically distributed with law $\pushforward{f}{\mu}$.  By Lemma \ref{IndependenceComposition} it follows that the $f(\xi), f(\xi_1), f(\xi_2), \dotsc$ are i.i.d.  Observe that 
\begin{align*}
\expectation{f(\xi)} = \int f \, d\mu 
\end{align*}
and 
\begin{align*}
\expectation{f^2(\xi)} = \int f^2 \, \mu < \infty
\end{align*}
(so $\variance{f(\xi)} = \int f^2 \, d\mu - \left(\int f \, d\mu\right)^2$).  Hence we may apply the Central Limit Theorem \ref{CentralLimitTheorem} to conclude 
\begin{align*}
\sqrt{n} \left( \frac{1}{n} \sum_{i=1}^n f(\xi_i) - \int f \, d\mu \right) \todist N(0, \variance{f(\xi)} )
\end{align*}

TODO: The more general finite dimensional case with $f_1, \dotsc, f_m$.

Empirical process theory explores the conditions under which the convergence of $ \frac{1}{n} \sum_{i=1}^n f(\xi_i)$ to $\int f \, d\mu$ and $\sqrt{n} \left( \frac{1}{n} \sum_{i=1}^n f(\xi_i) - \int f , d\mu \right)$ to $N(0, \variance{f(\xi)})$ can be made ``uniform'' of a family of square integrable functions $\mathcal{F}$.  For the moment the meaning of the word uniform is being left vague.  Several details need to be addressed before we can make a precise statement about the convergence we have in mind.  

\begin{defn}
Let $(S, \mathcal{S}, \mu)$ be a probability space  and suppose we are given a set $\mathcal{F} \subset \mathcal{L}^2(S, \mathcal{S}, \mu)$.  Given $\xi, \xi_1, \xi_2, \dotsc$ are i.i.d. random elements in $S$ with $\mathcal{L}(\xi_i) = \mu$ we now view 
\begin{align*}
\mathds{G}^n_f &= \sqrt{n} \left [ \frac{1}{n} \sum_{i=1}^n \left( f(\xi_i) - \int f \, d\mu \right) \right ]
\end{align*}
as a stochastic process indexed by $\mathcal{F}$ called the \emph{empirical process}.
\end{defn} 
Our observations about the central limit theorem above can be rephrased in the language of stochastic process as say that the finite dimensional distributions converge to multidimensional Gaussians.  It follows that if we are able to make sense of the convergence of the empirical processes to a limiting stochastic process, we know that the limiting stochastic process will be a Gaussian process; in particular one indexed by $\mathcal{F}$ and with finite dimensional distributions $N(0, \covariance{(f_1, \dotsc, f_n)})$.  

The discussion above tells us what the limit of an empirical process will be (should it exist).  Thus we have following definition.
\begin{defn}Let $(S, \mathcal{S}, \mu)$ and $\mathcal{F} \subset \mathcal{L}^2(S, \mathcal{S}, \mu)$ then a centered Gaussian process $\mathds{G}^\mu$ indexed by $\mathcal{F}$ with covariance 
\begin{align*}
\scovariance{\mathds{G}^\mu_f}{\mathds{G}^\mu_g} &= \expectation{\mathds{G}^\mu_f \mathds{G}^\mu_g} = \int (f - \int f \, d\mu)(g - \int g \, d\mu) \, d\mu
\end{align*}
is called a \emph{$\mu$-Brownian bridge indexed by $\mathcal{F}$}.
\end{defn}

By virtue of the next result, we'll be able to learn many things about $\mu$-Brownian bridges from results about isonormal processes.
\begin{prop}\label{prop:GeneralizedBrownianBridgeAndIsonormal}Let $(S, \mathcal{S}, \mu)$, $\mathcal{F} \subset \mathcal{L}^2(S, \mathcal{S}, \mu)$ and $\mathds{G}^\mu$ an $\mu$-Brownian bridge.
\begin{itemize}
\item[(i)] If $f=g$ a.s. then $\mathds{G}^\mu_f = \mathds{G}^\mu_g$ a.s.
\item[(ii)] If $a,b \in \reals$ and $f,g, af+bg \in \mathcal{F}$ then $\mathds{G}^\mu_{af+bg} = a\mathds{G}^\mu_f + b \mathds{G}^\mu_g$ a.s.
\item[(iii)] $\mathds{G}^\mu$ defines an isonormal process on $L^2_0(S, \mathcal{S}, \mu)$
\end{itemize}
\end{prop}
\begin{proof}
Suppose $f,g \in \mathcal{L}^2(S, \mathcal{S}, \mu)$ and $f = g$ $\mu$-a.s. It follows that $\int f , d\mu = \int g \, d\mu$ and moreover 
\begin{align*}
&\expectation{(\mathds{G}^\mu_f - \mathds{G}^\mu_g)^2} = \expectation{(\mathds{G}^\mu_f)^2} - 2\expectation{\mathds{G}^\mu_f \mathds{G}^\mu_g} + \expectation{(\mathds{G}^\mu_g)^2} \\
&=\int (f - \int f \, d\mu)^2 \, d\mu - 2 \int (f - \int f \, d\mu)(g - \int g \, d\mu) \, d\mu + \int (g - \int g \, d\mu)^2\, d\mu \\
&=\int (f - \int f \, d\mu)^2 \, d\mu - 2 \int (f - \int f \, d\mu)^2 \, d\mu + \int (f - \int f\, d\mu)^2\, d\mu = 0
\end{align*}
It follows that $\mathds{G}^\mu_f = \mathds{G}^\mu_g$ a.s.

To see linearity calculate
\begin{align*}
&\expectation{(\mathds{G}^\mu_{a f + b g} - a \mathds{G}^\mu_f - b \mathds{G}^\mu_g)^2} \\
&=\expectation{(\mathds{G}^\mu_{a f + b g})^2} -2 a\expectation{\mathds{G}^\mu_{a f + b g}\mathds{G}^\mu_{f}} -2 b\expectation{\mathds{G}^\mu_{a f + b g}\mathds{G}^\mu_g} - 2 ab\expectation{\mathds{G}^\mu_{f}\mathds{G}^\mu_g} \\
&+a^2\expectation{(\mathds{G}^\mu_{f})^2}  + b^2\expectation{(\mathds{G}^\mu_{g})^2}  \\
&=\int (a f + b g - \sexpectation{a f + b g}{\mu})^2 \, d\mu -2a \int (a f + b g - \sexpectation{a f + b g}{\mu}) (f - \sexpectation{f}{\mu}) \, d\mu \\
&-2 b\int (a f + b g - \sexpectation{a f + b g}{\mu}) (g - \sexpectation{g}{\mu}) \, d\mu -2 ab\int (f - \sexpectation{ f}{\mu}) (g - \sexpectation{g}{\mu}) \, d\mu \\
&+a^2 \int (f  - \sexpectation{f}{\mu})^2 \, d\mu + b^2 \int (g - \sexpectation{g}{\mu})^2 \, d\mu \\
&=\int (a f + b g - \sexpectation{a f + b g}{\mu})^2 \, d\mu -2 \int (a f + b g - \sexpectation{a f + b g}{\mu})^2 \, d\mu \\
&+\int (a f  - \sexpectation{a f}{\mu} - (b g - \sexpectation{b g}{\mu}))^2 \, d\mu = 0
\end{align*}
from which $\mathds{G}^\mu_{af+bg} = a\mathds{G}^\mu_f + b \mathds{G}^\mu_g$ a.s. follows by Lemma \ref{ZeroIntegralImpliesZeroFunction}.
 
Given a $\mu$-Brownian bridge we can also consider the family of centered Gaussian processes on $L^2(S, \mathcal{S}, \mu)$ obtained by defining $\mathds{G}^\mu_{[f]} = \mathds{G}^\mu_{f}$ obtained by picking a representative of each equivalence class $[f] \in L^2(S, \mathcal{S}, \mu)$.  From the above argument we see that every such process is a modification of every other.  In particular the covariances of the representatives.  Suppose that $f,g \in \mathcal{L}_0^2(S, \mathcal{S}, \mu)$ we have
\begin{align*}
\scovariance{\mathds{G}^\mu_f}{\mathds{G}^\mu_g} &= \int (f - \int f \, d\mu)(g - \int g \, d\mu) \, d\mu = \int f g \, d\mu = \langle f,g \rangle
\end{align*}
and therefore a $\mu$-Brownian bridge restricted to $L_0(S, \mathcal{S}, \mu)$ is isonormal.
\end{proof}

The previous proposition also shows that $\mu$-Brownian bridges always exist.  
\begin{prop}\label{ExistenceGeneralizedBrownianBridge}Let $(S, \mathcal{S}, \mu)$ and $\mathcal{F} \subset \mathcal{L}^2(S, \mathcal{S}, \mu)$ then $\mu$-Brownian bridge indexed by $\mathcal{F}$ exists.
\end{prop}
\begin{proof}
Let $L$ be an isonormal process on $L^2(S, \mathcal{S}, \mu)$ (which exists by Proposition \ref{ExistenceIsonormalProcess}).  For any $f \in  \mathcal{L}^2(S, \mathcal{S}, \mu)$ we let $[f]$ be the almost sure equivalence class in $L^2(S, \mathcal{S}, \mu)$.  Clearly $[a f + b g] = a [f] + b[g]$ for all $a,b \in \reals$ and $f, g \in \mathcal{L}^2(S, \mathcal{S}, \mu)$.  Define 
\begin{align*}
\mathds{G}^\mu_f &= L_{[f - \int f \, d\mu]} = L_{[f]} - \int f \, d\mu L_{[1]}
\end{align*}
Since $L$ is a centered Gaussian process it follows that $\mathds{G}^\mu$ is as well.  We can compute the covariance from the definition of an isonormal process
\begin{align*}
\expectation{\mathds{G}^\mu_f \mathds{G}^\mu_g} &= \expectation{L_{[f-\int f \, d\mu]}  L_{[g - \int g\, d\mu]}} = \int (f-\int f \, d\mu) (g - \int g\, d\mu) \, d\mu
\end{align*}

From the linearity of $L$ (Proposition \ref{LinearityIsonormalProcess}) and linearity of the projection $[\cdot] : \mathcal{L}^2 \to L^2$ we know that
\begin{align*}
\mathds{G}^\mu_{af + bg} &= L_{[af + bg]} = L_{a[f] + b[g]} = a L_{[f]} + b L_{[g]} = a \mathds{G}^\mu_f + b \mathds{G}^\mu_g \text{ a.s.}
\end{align*}
\end{proof}

\begin{prop}\label{prop:GeneralizedBrownianBridgePseudometric}Let $(S, \mathcal{S}, \mu)$, $\mathcal{F} \subset \mathcal{L}^2(S, \mathcal{S}, \mu)$ and $\mathds{G}^\mu$ a 
$\mu$-Brownian bridge then 
\begin{align*}
\rho_\mu(f,g) &= \expectation{(\mathds{G}^\mu_f - \mathds{G}^\mu_g)^2}^{1/2}
\end{align*}
defines a pseudometric on $\mathcal{F}$.  $\rho_\mu$ induces a pseudometric on $L^2(S, \mathcal{S}, \mu)$ which is the regular Hilbert metric on $L_0^2(S, \mathcal{S}, \mu)$.
\end{prop}
\begin{proof}
Clearly $\rho_\mu(f,g) \geq 0$,  $\rho_\mu(f,f) \geq 0$ and $\rho_\mu(f,g) = \rho_\mu(g,f)$.  The triangle inequality follows from the Minkowski inequality Lemma \ref{MinkowskiInequality}.

If $f=g$ a.s. then by Proposition \ref{prop:GeneralizedBrownianBridgeAndIsonormal} we have $\mathds{G}^\mu_f = \mathds{G}^\mu_g$ a.s. and therefore $\rho_\mu$ is well defined on $L^2$.  Suppose $f,g \in L^2_0(S,\mathcal{S}, \mu)$ then we have since $\mathds{G}^\mu$ is isonormal on $L^2_0$,
\begin{align*}
\rho_\mu(f,g) &= \expectation{(\mathds{G}^\mu_f - \mathds{G}^\mu_g)^2}^{1/2} = \norm{(f - g)^2}^{1/2}
\end{align*}
\end{proof}

\begin{defn}Let $(S, \mathcal{S}, \mu)$ and $\mathcal{F} \subset \mathcal{L}^2(S, \mathcal{S}, \mu)$ then we say that $\mathcal{F}$ is \emph{pregaussian} if there
exists a $\mu$-Brownian bridge indexed by $\mathcal{F}$ which are bounded and uniformly continuous in the $\rho_\mu$ pseudometric.  If we want to denote the dependence on $G$ then we will write $\rho^G_\mu$.  
\end{defn}

TODO: Apparently pregaussian-ness is actually equivalent to tightness (this is in Van der Vaart and Wellner).  This leads to a different way of thinking about much of the following I believe.  That way of thinking stresses the analogy of Hoffmann-Jorgensen theory with classical weak convergence theory (tightness, Prohorov and FDDs).  I think the following is true:

In fact the relationship between tightness and sample path continuity is not restricted to sample bounded Gaussian processes, it is a general fact about sample bounded processes.
\begin{thm}Let $X$ be a stochastic process indexed by $T$ with bounded sample paths.  Then the law of $X$ is a tight Borel probability measure on $\ell^\infty(T)$ if and only if there exists a pseudometric $\tau$ on $T$ such that $T$ is totally bounded with respect to $\rho$ and $X$ has a version (TODO: which sense of version?) with $\rho$-uniformly continuous sample paths.
\end{thm}

For Gaussian stochastic processes $X$ it turns out that $X$ is tight if and only if the $\rho_\mu$ pseudometric works.

\begin{prop}\label{prop:PregaussianConditions}Let $(S, \mathcal{S}, \mu)$ and $\mathcal{F} \subset \mathcal{L}^2(S, \mathcal{S}, \mu)$ then $\mathcal{F}$ is pregaussian if and only if $\pi_0(\mathcal{F})$ is pregaussian.
\end{prop}
\begin{proof}
Suppose $\pi_0(\mathcal{F})$ is pregaussian and $\mathds{G}^\mu$ is a $\mu$-Brownian bridge which is $\rho_\mu$ uniformly continuous and bounded.  For $f \in \mathcal{F}$, define $H^\mu_f = \mathds{G}^\mu_{\pi_0(f)}$ which is a centered Gaussian process by virtue of the corresponding properties of $\mathds{G}^\mu$.  As for covariances, since $\int \pi_0(f) \, d\mu = \int \pi_0(g) \, d\mu = 0$,
\begin{align*}
\expectation{H^\mu_f H^\mu_g} &= \expectation{\mathds{G}^\mu_{\pi_0(f)}\mathds{G}^\mu_{\pi_0(g)}} = \int \pi_0(f) \pi_0(g) \, d\mu = \int (f - \int f \, d\mu) (g-\int g \, d\mu) \, d\mu
\end{align*}
so $H^\mu$ is a $\mu$-Brownian bridge.  Sample path boundedness of $H^\mu$ is immediate.  To see uniform continuity of $H^\mu$ let $\epsilon>0$ be given and 
pick $\delta > 0$ such that $\rho^G_\mu(f,g) < \delta$ implies $\abs{\mathds{G}^\mu_f - \mathds{G}^\mu_g} < \epsilon$ for all $f,g \in \pi_0(\mathcal{F})$.  If $f,g \in \mathcal{F}$ then
\begin{align*}
\rho^H_\mu(f,g) &= \expectation{(H^\mu_f - H^\mu_g)^2}^{1/2} = \expectation{(\mathds{G}^\mu_{\pi_0(f)} - \mathds{G}^\mu_{\pi_0(g)})^2}^{1/2} = \rho^G_\mu(\pi_0(f), \pi_0(g))
\end{align*}
Therefore $\rho^H_\mu(f,g) < \delta$ implies $\abs{H^\mu_f - H^\mu_g} = \abs{\mathds{G}^\mu_{\pi_0(f)} - \mathds{G}^\mu_{\pi_0(g)}} < \epsilon$.

On the other hand suppose $\mathcal{F}$ is pregaussian and let $H^\mu$ be a $\mu$-Brownian bridge which is $\rho^H_\mu$ uniformly continuous and bounded.  
For every $f \in \pi_0(\mathcal{F})$ we have $f = g - \int g \, d\mu$ for some $g \in \mathcal{F}$, thus for each $f \in \pi_0(\mathcal{F})$ there exists a constant
$c_f$ such that $f+c_f \in \mathcal{F}$.  Note that if $f \in \pi_0(\mathcal{F})$ and $f+c,f+d \in \mathcal{F}$ for constants $c,d \in \reals$ then
\begin{align*}
&\expectation{(H^\mu_{f+c} - H^\mu_{f+d})^2} \\
&= \int \left(f+c - \int(f+c) \, d\mu\right)^2 \, d\mu  -2 \left(f+c - \int(f+c) \, d\mu\right) \left(f+d - \int(f+d) \, d\mu\right)  \, d\mu \\
&+ \int \left(f+d - \int(f+d) \, d\mu\right)^2 \, d\mu  \\
&=0
\end{align*}
so $H^\mu_{f+c}=H^\mu_{f+d}$ a.s.
Define $\mathds{G}^\mu_f = H^\mu_{f+c_f}$; by the previous computation the choice of $c_f$ doesn't change the almost sure equivalence class of $\mathds{G}^\mu_f$.  It is immediate that $\mathds{G}^\mu$ is centered Gaussian and bounded.  To calculate the covariance,
\begin{align*}
\expectation{\mathds{G}^\mu_f \mathds{G}^\mu_g} = \expectation{H^\mu_{f+c_f}  H^\mu_{g+c_g}}  &= \int (f+c_f - \sexpectation{f+c_f}{\mu}) (g+c_g - \sexpectation{g+c_g}{\mu}) \, d\mu\\
&= \int (f- \sexpectation{f}{\mu}) (g - \sexpectation{g}{\mu}) \, d\mu
\end{align*}

To see uniformly continuity let $\epsilon > 0$ be given and pick $\delta > 0$ such that $\rho^H_\mu(f,g) < \delta$ implies $\abs{H^\mu_f - H^\mu_g}<\epsilon$ for $f,g \in \mathcal{F}$.  Suppose $f,g \in \pi_0(\mathcal{F})$ then 
\begin{align*}
\rho^G_\mu(f,g) &= \expectation{(\mathds{G}^\mu_f - \mathds{G}^\mu_g)^2}^{1/2} = \expectation{(H^\mu_{f+c_f} - H^\mu_{g+c_g})^2}^{1/2} = \rho^H_\mu(f+c_f, g+c_g)\\
 \end{align*}
Thus if $\rho^G_\mu(f,g)<\delta$ then $\rho^H_\mu(f+c_f, g+c_g)<\delta$ and $\abs{H^\mu_{f+c_f} - H^\mu_{g+c_g}}=\abs{\mathds{G}^\mu_f - \mathds{G}^\mu_g}<\epsilon$.
\end{proof}

\section{Weak Convergence in Non Separable Spaces}

\subsection{Outer Expectations and Measurable Cover}

\begin{defn}Let $(\Omega, \mathcal{A}, \mu)$ be a measure space and let $f : \Omega \to [-\infty, \infty]$ (not assumed to be measurable).  Then consider the two conditions
\begin{itemize}
\item [(i)] for every measurable $g : \Omega \to [-\infty, \infty]$ with $g \geq f$ everywhere the integral $\int g \, d\mu$ is defined 
\item [(ii)] or there exists a measurable $g : \Omega \to [-\infty, \infty]$  with $g \geq f$ everywhere such that $\int g \, d\mu = -\infty$ 
\end{itemize}
When (i) holds we define the \emph{outer integral}
\begin{align*}
\int^* {f} \, d\mu &= \inf \left \lbrace \int g  \, d\mu \mid \text{ $g$ measurable, } g \geq f \right \rbrace
\end{align*}
and when (ii) holds we define $\int^* f \, d\mu = -\infty$.
If neither (i) nor (ii) hold then we say that the outer integral $\int^* f \, d\mu$ is not defined.  If $\mu$ is a probability measure then we will call $\int^* f \, d\mu$ and \emph{outer expectation}.  If the probability measure is clear in a context we may write $\oexpectation{f}$ for an outer expectation.  Given a subset $A \subset \Omega$ we define the outer probability to be
\begin{align*}
\oprobability{A} &= \inf \lbrace \probability{B} \mid B \supset A, B \in \mathcal{A} \rbrace
\end{align*}
\end{defn}

Note that there is no ambiguity in the above definition of an outer integral (i.e. suppose both (i) and (ii) hold then the existence of the $g$ in (ii) implies that the infimum in (i) is $-\infty$).

This result is analogous to Proposition \ref{IntegrableAlmostEverywhereEqualIntegralEqual} and shows that an outer integral is insensitive to changes in behavior of the integrand on a null set.
\begin{prop}\label{OuterIntegralAlmostEverywhereEqualIntegralEqual}Given $f,g : \Omega \to [-\infty, \infty]$ be functions such that $f = g$ almost everywhere then $\int^* f \, d\mu$ is defined if and only if $\int^* g \, d\mu$ is defined and in that case
\begin{align*}
\int^* f \, d\mu &= \int^* g \, d\mu
\end{align*}
\end{prop}
\begin{proof}
Let $A \in \mathcal{A}$ be a measurable set with $\mu(A) = 0$ and $\lbrace f \neq g \rbrace \subset A$.  Because all of the assertions are symmetric in $f$ and $g$, it suffices to suppose that $\int^* f \, d\mu$ is defined and to show that $\int^* g \, d\mu$ is defined and $\int^* f \, d\mu=\int^* g \, d\mu$.

For the first case suppose there exists a measurable $h$ such that $h \geq f$ and $\int h \, d\mu=-\infty$ so $\int^* f \, d\mu =-\infty$.  It follows that 
\begin{align*}
\tilde{h}(\omega) &= \begin{cases}
\infty & \text{if $\omega \in A$} \\
h(\omega) & \text{if $\omega \notin A$}
\end{cases}
\end{align*}
then $\tilde{h}$ is measurable, $\tilde{h} \geq g$ and $\int \tilde{h} \, d\mu = \int h \, d\mu=-\infty$.  Thus $\int^* g \, d\mu$ is defined and $\int^*f \, d\mu = \int^* g \, d\mu = -\infty$.  
\begin{align*}
\end{align*}

For the second case suppose that $\int h \, d\mu$ is defined for every measurable $h$ with $h \geq f$.  Then $\int^* f \, d\mu$ is the infimum of such integrals $\int h \, d\mu$.  Suppose $h$ is measurable and $h \geq g$.  Defining $\tilde{h}$ as above we know that $\tilde{h} \geq f$ hence $\int \tilde{h} \, d\mu$ is defined.  By Proposition \ref{IntegrableAlmostEverywhereEqualIntegralEqual} this implies that $\int h \, d\mu$ is defined.  As $h$ was arbitrary, this implies that $\int^* g \, d\mu$ is defined and is equal to the infimum the integrals $\int h \, d\mu$.  Let $h$ be measurable with $h \geq f$ then $\tilde{h} \geq g$ and $\int \tilde{h} \, d\mu = \int h \, d\mu$.  Arguing symmetrically we see that we have the set equality 
\begin{align*}
\left \lbrace \int h  \, d\mu \mid \text{ $h$ measurable, } h \geq f \right \rbrace = \left \lbrace \int h  \, d\mu \mid \text{ $h$ measurable, } h \geq g \right \rbrace
\end{align*}
so it follows that $\int^* f \, d\mu = \int^* g \, d\mu$.
\end{proof}

Note that by arguments similar to those in the preceeding proof it is easy to see that in the definition of the outer integral we can replace the condition $h \geq f$  in (i) and (ii) with $h \geq f$ almost everywhere.

TODO: Here I am exploring the possibility of an analogue of Proposition \ref{SignedIntegralMonotonicity}.  Note that for outer expectations we will prove the existence of measurable covers.  Thus I believe this result will be implied by Lemma \ref{SignedIntegralMonotonicity}.
\begin{prop}\label{OuterIntegralMonotonicity}Let $(\Omega, \mathcal{A}, \mu)$ be a measure space and let $f,g: \Omega \to [-\infty, \infty]$ be functions with
$f \leq g$ then one of the following is true
\begin{itemize}
\item[(i)]Both $\int^* f \, d\mu$ and $\int^* g \, d\mu$ are defined and $\int^* f \, d\mu \leq \int^* g \, d\mu$
\item[(ii)]$\int^* f \, d\mu$ is undefined, $\int^* g \, d\mu$ is defined and $\int^* g \, d\mu = \infty$
\item[(iii)]$\int^* g \, d\mu$ is undefined, $\int^* f \, d\mu$ is defined and $\int^* f \, d\mu = -\infty$
\item[(iv)]Both $\int^* f \, d\mu$ and $\int^* g \, d\mu$ are undefined
\end{itemize}
\end{prop}
\begin{proof}
Suppose that $\int^* g \, d\mu$ is undefined and $\int^* f \, d\mu$ is defined.  Suppose $\int h \, d\mu$ is defined for every measurable $h$ with $h \geq f$. Since $f \leq g$ it would follow that the same is true of $g$ which is a contradicts the assumtion that $\int^* g \, d\mu$ is not defined.  It follows that $\int^* f \, d\mu = -\infty$.  
 
Suppose $\int^* f \, d\mu$ is undefined and $\int^* g \, d\mu$ is defined.  Note that there cannot be a measurable $h$ with $h \geq g$ and $\int h \, d\mu = -\infty$.  That same $h$ would imply that $\int^* f \, d\mu = -\infty$ is defined which is a contradiction.  Thus we know that for every measurable $j \geq g$ the integral $\int j \, d\mu$ is defined.  Since $\int^* f \, d\mu$ is not defined there exists a measurable $h \geq f$ for which $\int h \, d\mu$ is not defined; that is to say $\int h_\pm \, d\mu = \infty$.  Pick an measurable arbitrary measurable $j \geq g \geq f$; as noted $\int j \, d\mu$ exists hence either $\int j_- \, d\mu<\infty$ or $\int j_+ \, d\mu<\infty$.  We claim that $\int j_+ \, d\mu = \infty$.  
To see this note that we have $f_+ \leq h_+$ and $f_+ \leq g_+ \leq j_+$, so $f_+ \leq h_+ \minop j_+$.  If $\int h_+ \minop j_+ \, d\mu < \infty$ it would follow that $f \leq h_+ \minop j_+ - h_-$ and 
\begin{align*}
\int (h_+ \minop j_+ - h_-) \, d\mu &= \int  (h_+ \minop j_+) \, d\mu  - \int  h_- \, d\mu = -\infty
\end{align*} 
which implies $\int f \, d\mu=-\infty$ is defined.  Thus $\int h_+ \minop j_+ \, d\mu = \infty$ hence $\int j_+ = \infty$.  

From the claim and property (i) for $g$ we know that $\int j_- \, d\mu < \infty$ hence $\int j \, d\mu = \infty$.  Since $j \geq g$ was arbitrary it follows that $\int g \, d\mu = \infty$.

Lastly assume that both $\int^* f \, d\mu$ and $\int^* g \, d\mu$ are defined.  If there is a measurable $h$ with $h \geq g$ and $\int h \, d\mu = -\infty$ then $\int^* g \, d\mu = -\infty$.  That $h$ also implies $\int^* f \, d\mu=-\infty$ so the result holds in this case.  Thus we assume that such an $h$ doesn't exist and therefore $\int^* g \, d\mu$ is infimum of all $\int h \, d\mu$ with $h$ measurable and $h \geq g$.  If $\int^* f \, d\mu=-\infty$ then (i) follows trivially so we assume $\int^* f \, d\mu > -\infty$ and 
$\int^* f \, d\mu$ is the infimum of integrals $\int h \, d\mu$ with $h$ measurable and $h \geq f$.  The latter set of integrals is a superset of the set of integrals defining $\int^* g \, d\mu$ hence it follows that $\int^* f \, d\mu \leq \int^* g \, d\mu$.
\end{proof}


We show how to calculate outer expectations in terms of ordinary expectations.
\begin{defn}Let $\mathcal{J}$ be a subset of $\mathcal{L}^0(\Omega, \mathcal{A}, P, [-\infty, \infty])$ the we say that $f \in \mathcal{L}^0$ is an \emph{essential infimum of $\mathcal{J}$} if 
for every $j \in \mathcal{J}$ we have $f \leq j$ a.s. and moreover for any $g \in \mathcal{L}^0$ such that $g \leq j$ a.s. for every $j \in \mathcal{J}$ we also have $g \leq f$ a.s.
\end{defn}

\begin{prop}\label{AlmostSureUniquenessEssentialInfimum}If $\mathcal{I} \subset \mathcal{J}$ the then the essential infimum of $\mathcal{J}$ is less than or equal to the essential infimum of $\mathcal{I}$ almost surely.  If $f$ and $g$ are both essential infima of $\mathcal{J}$ then $f = g$ a.s.
\end{prop}
\begin{proof}
Let $g$ be the essential infimum of $\mathcal{J}$.  Clearly for all $j \in \mathcal{I} \subset \mathcal{J}$ we know $g \leq j$ a.s.  so by definition the essential infimum of $\mathcal{I}$ is greater than or equal to $f$ a.s.  By the first part of the proposition, we have both $f \leq g$ a.s. and $g \leq f$ a.s. hence $f=g$ a.s.
\end{proof}

\begin{thm}\label{ExistenceMeasurableCover}For any $\mathcal{J} \subset \mathcal{L}^0(\Omega, \mathcal{A}, P, [-\infty, \infty])$ and essential infimum of $\mathcal{J}$ exists.  Given $f : \Omega \to (-\infty, \infty)$ let
$\mathcal{J}_f = \lbrace j \in \mathcal{L}^0 \mid j \geq f \text{ everywhere} \rbrace$ then there is an essential infimum $f^*$ of $\mathcal{J}_f$ such that $f^* \geq f$ everywhere.  $\int f^* \, dP$ is 
defined if and only if $\int^* f \, dP$ is defined and when they are defined we have
\begin{align*}
\int f^* \, dP &= \int^* f \, dP
\end{align*}
If $f = \characteristic{A}$ for some $A \subset \Omega$ (not necessarily measurable) then $f^*$ may be chosen to be $\characteristic{A^*}$ for a measurable set $A^* \supset A$.  Moreover \begin{align*}
\oprobability{A} &= \int^* \characteristic{A} dP = \probability{A^*}
\end{align*}
\end{thm}
\begin{proof}
Let 
\begin{align*}
\mathcal{J}_1 &= \lbrace j_1 \minop \dotsb \minop j_n \mid n \in \naturals \text{ and } j_1, \dotsc, j_n \in \mathcal{J} \rbrace
\end{align*}
Clearly if $j,k \in \mathcal{J}_1$ the also $j \minop k \in \mathcal{J}_1$.  
\begin{clm} $g$ is an essential infimum of $\mathcal{J}$ if and only if $g$ is an essential infimum of $\mathcal{J}_1$.
\end{clm}
Assume $g$ is an essential infimum of $\mathcal{J}$.  The for any $j_1 \minop \dotsb \minop j_n  \in \mathcal{J}_1$ we know that $g \leq j_i$ a.s. for $i=1, \dotsc, n$ so by taking the interesection of the events $\lbrace g \leq j_1 \rbrace \cap \dotsb \cap \lbrace g \leq j_n \rbrace$ we know that $g \leq j_1 \minop \dotsb \minop j_n$ a.s.  Given $h$ such that $h \leq j_1 \minop \dotsb \minop j_n$ a.s. for every subset $\lbrace j_1, \dotsc, j_n \rbrace \subset \mathcal{J}$ in particular we see that $h \leq j$ a.s. for every $j \in \mathcal{J}$ and therefore $h \leq g$ a.s.  In the other direction assume that $g$ is an essential infimum of $\mathcal{J}_1$.  As above it follows immediately that $g \leq j$ a.s. for every $j \in \mathcal{J}$.  If $h \leq j$ a.s. for every $j \in \mathcal{J}$ then taking an interesection of almost sure events we see that $h \leq j_1 \minop \dotsb \minop j_n$ a.s. for every $\lbrace j_1, \dotsc, j_n \rbrace \subset \mathcal{J}$ and it follows that $h \leq g$ a.s.

By the previous claim we may replace $\mathcal{J}$ with $\mathcal{J}_1$ and therefore assume from here that $j,k \in \mathcal{J}$ implies $j \minop k \in \mathcal{J}$.

Note that $\tan : (-\pi/2, \pi/2) \to (-\infty, \infty)$ is a homeomorphism and since $\lim_{x \to \pm \pi/2} \tan x = \pm \infty$ we may extend $\tan$ to a homeomorphism between $[-\pi/2, \pi/2]$ and $[-\infty, \infty]$.  Therefore for every $j \in \mathcal{J}$ we can consider $\tan^{-1} j : \Omega \to [-\pi/2, \pi/2]$.  We now pick a sequence $j_1, j_2, \dotsc$ such that
\begin{align*}
\lim_{n \to \infty} \int \tan^{-1} j_n \, dP = \inf_{j \in \mathcal{J}} \int \tan^{-1} j \, dP
\end{align*}
By assumption on $\mathcal{J}$ we know that $j_1 \minop \dotsb \minop j_n \in \mathcal{J}$ and by construction the sequence $j_1 \minop \dotsb \minop j_n$ is decreasing and it follows that 
$\lim_{n \to \infty} j_1 \minop \dotsb \minop j_n = g$ where $g = \inf_n j_1 \minop \dotsb \minop j_n \in \mathcal{L}^0$ by Lemma \ref{LimitsOfMeasurable}.   

\begin{clm} $\int \tan^{-1} g \, dP =\inf_{j \in \mathcal{J}} \int \tan^{-1} j \, dP$
\end{clm}
By the definition of infimum we know that
$\int \tan^{-1}  j_1 \minop \dotsb \minop j_n \, dP \geq \inf_{j \in \mathcal{J}} \int \tan^{-1} j \, dP$ and by monotone convergence we know that 
\begin{align*}
\int \tan^{-1} g \, dP &= \lim_{n \to \infty} \int \tan^{-1}  j_1 \minop \dotsb \minop j_n \, dP \geq \inf_{j \in \mathcal{J}} \int \tan^{-1} j \, dP
\end{align*}
On the other hand 
\begin{align*}
\int \tan^{-1} g \, dP &= \lim_{n \to \infty} \int \tan^{-1}  j_1 \minop \dotsb \minop j_n \, dP \leq \lim_{n \to \infty} \int \tan^{-1}  j_n \, dP 
= \inf_{j \in \mathcal{J}} \int \tan^{-1} j \, dP
\end{align*}
and the claim follows.

\begin{clm} $g$ is an essential infimum for $\mathcal{J}$.
\end{clm}
Given any $k \in \mathcal{J}$ because $k \minop g \leq g$ everywhere we know that $\int \tan^{-1} k \minop g \, dP \leq \int  \tan^{-1} g \, dP = \inf_{j \in \mathcal{J}} \int \tan^{-1} j \, dP$ hence in fact
 $\int (\tan^{-1} g - \tan^{-1} k \minop g) \, dP = 0$ which since $\tan^{-1} g \geq \tan^{-1} k \minop g$ implies $\tan^{-1} g = \tan^{-1} k \minop g$ a.s. (equivalently $g \leq k$ a.s.).  Lastly if $h \leq j$ a.s. for every $j \in \mathcal{J}$ then $h \leq j_1 \minop \dotsb \minop j_n$ a.s. for all $n \in \naturals$ and
\begin{align*}
h &\leq \lim_{n \to \infty} j_1 \minop \dotsb \minop j_n = g \text{ a.s. }
\end{align*}

In the special case of $\mathcal{J}_f$ it is easy to see that $\mathcal{J}_f$ is closed under the minimum and therefore we can construct an essential infimum as $g=\lim_{n \to \infty} j_1 \minop \dotsb \minop j_n$ for a sequence $j_n \geq f$ everywhere.  It follows that $g \geq f$ everywhere.

Now we turn to considering the expectations.  First suppose that $\int f^* \, dP$ is defined (i.e. at most one of $\int f^*_\pm \, dP = \infty$).  Suppose there exists a measurable $g \geq f$ such
that $\int g_\pm \, dP = \infty$.  By definition of the essential infimum we know that $f^* \leq g$ a.s. which implies that $\int f^*_- \, dP \geq \int g_- \, dP = \infty$ so we may conclude $\int f^*_+ \, dP < \infty$ and $\int f^* \, dP = -\infty$.  This shows us that $\int^* f \, dP$ is defined (and equal to $-\infty$).    On the other hand suppose that $\int^* f \, dP$ is defined. Suppose that $\int f_\pm^* \, dP = \infty$ then since $f^*$ is measurable and $f^* \geq f$  everywhere there must exist a measurable $g$ with $g \geq f$, $\int g \, dP = -\infty$.  By definition of essential infimum we see that $f^* \leq g$ a.s. hence $\int f^*_+ \, dP \leq \int g_+ \, dP < \infty$ which is contradiction.  When $\int^* f \, dP$ and $\int f^* \, dP$ exist, on the one hand $f^* \geq f$ implies
\begin{align*}
\int f^* \, dP &\leq \int^* f \, dP
\end{align*}
and on the other hand for all $j \geq f$ everywhere we have $f^* \leq j$ a.s. hence
\begin{align*}
\int f^* \, dP &\leq \inf_{j \geq f} \int j \, dP = \int^* f \, dP
\end{align*}

If we consider $\characteristic{A}$ then it is clear from the definition that $\int^* \characteristic{A} \, dP \leq \oprobability{A}$.  On the other hand, we claim that we can find measurable sets $\dotsb \supset A_n \supset A_{n+1} \supset \dotsb$ such that $\left( \characteristic{A} \right)^* = \lim_{n \to \infty} \characteristic{A_n}$.  First note that if $f^*$ is a measurable cover of $\characteristic{A}$ then $\characteristic{f^* \geq 1}$ is also a measurable cover of $\characteristic{A}$.  This follows since $f^* \geq f \geq 0$ therefore $f^* \geq \characteristic{f^* \geq 1} \geq \characteristic{A}$ everywhere.  By the construction above we know that there are measurable $j_1, j_2, \dotsc$ with $j_n \geq \characteristic{A}$ and $j_1 \minop \dotsb \minop j_n  \downarrow f^*$.  Define $A_n = \lbrace j_1 \minop \dotsb \minop j_n \geq 1 \rbrace$.  Since $f^* =  \characteristic{f^* \geq 1}$ a.s. we know that $\lim_{n\to \infty} j_1 \minop \dotsb \minop j_n \in \lbrace 0, 1 \rbrace$ a.s.  Since
$j_1 \minop \dotsb \minop j_n$ is a decreasing sequence we know that $ j_1 \minop \dotsb \minop j_n \downarrow 0$ if and only if $j_1 \minop \dotsb \minop j_n < 1$ eventually if and only if $\characteristic{A_n} = 0$ eventually if and only if $\lim_{n \to \infty} \characteristic{A_n} = 0$.  Similarly $ j_1 \minop \dotsb \minop j_n \downarrow 1$ if and only if $j_1 \minop \dotsb \minop j_n \geq 1$ for all $n \in \naturals$ if and only if $\characteristic{A_n} = 1$ for all $n \in \naturals$ if and only if $\lim_{n \to \infty} \characteristic{A_n} = 1$.  It follows that $f^* = \lim_{n \to \infty} \characteristic{A_n}$ a.s. hence
\begin{align*}
\oprobability{A} &\leq \lim_{n \to \infty} \probability{A_n} = \int f^* \, dP = \int^* f \, dP
\end{align*}
\end{proof}

\begin{defn}Given a measure space $(\Omega, \mathcal{A}, \mu$) and $f : \Omega \to [-\infty, \infty]$ an essential infimum of $\mathcal{J}_f$ is called the \emph{minimal measurable majorant} or the \emph{measurable cover} of $f$ with respect to $\mu$.
\end{defn}
We have just proven that measurable covers for arbitrary functions always exist on probability spaces.  TODO: Apparently it is not hard to bootstrap to showing that measurable covers exist on all $\sigma$-finite measure spaces.  Put this in the exercises or an appendix and prove it.  It is also know that measurable covers may not exist in the non $\sigma$-finite case.

TODO: We really don't need Proposition \ref{OuterIntegralMonotonicity} do we?
\begin{prop}\label{OuterExpectationMonotonicity}Let $(\Omega, \mathcal{A}, P)$ be a probability space and let $f,g: \Omega \to [-\infty, \infty]$ be functions with
$f \leq g$ then one of the following is true
\begin{itemize}
\item[(i)]Both $\int^* f \, dP$ and $\int^* g \, dP$ are defined and $\int^* f \, dP \leq \int^* g \, dP$
\item[(ii)]$\int^* f \, dP$ is undefined, $\int^* g \, dP$ is defined and $\int^* g \, dP = \infty$
\item[(iii)]$\int^* g \, dP$ is undefined, $\int^* f \, dP$ is defined and $\int^* f \, dP = -\infty$
\item[(iv)]Both $\int^* f \, dP$ and $\int^* g \, dP$ are undefined
\end{itemize}
\end{prop}
\begin{proof}
Let $f^*$ and $g^*$ be the measurable cover of $f$ and $g$ respectively.  Note that $f^* \leq g^*$ a.s. by Proposition \ref{AlmostSureUniquenessEssentialInfimum}.  Now apply   Proposition \ref{SignedIntegralMonotonicity} to $f^*$ and $g^*$ and use Theorem \ref{ExistenceMeasurableCover} to get the result.
\end{proof}

\begin{prop}\label{BasicPropertiesOfMeasurableCover} Let $(\Omega, \mathcal{A}, P)$  be a probability space and let $f,g : \Omega \to (-\infty, \infty]$ then
\begin{itemize}
\item[(i)] $(f+g)^* \leq f^* + g^*$ a.s.
\item[(ii)] $(f - g)^* \geq f^* - g^*$ a.s. whenever both sides are defined a.s. 
\item[(iii)] If $A$ is measurable, $g$ is measurable and $f \leq g$ a.s. on $A$ we have $f^* \leq g$ a.s. on $A$.  
\item[(iv)] If $A$ is measurable, $g$ is measurable and $f = g$ a.s. on $A$ we have $f^* = g$ a.s. on $A$.
\item[(v)] $(f \maxop g)^* = f^* \maxop g^*$ a.s.
\end{itemize}
\end{prop}
\begin{proof}
To see (i), by Theorem \ref{ExistenceMeasurableCover} we know that $f^* \geq f > -\infty$ and $g^* \geq g > -\infty$ everywhere thus $f^* + g^*$ is well defined everywhere (possibly $+\infty$).  The function
$f^* + g^*$ is measurable and $f+g \leq f^* + g^*$ therefore it follows that $(f+g)^* \leq f^* + g^*$.

To see (ii), by assumption $f^* - g^*$ is defined a.s. therefore $f^*$ is finite a.s. on $\lbrace g^* = \infty \rbrace$.  It follows that $f^* - g^* = -\infty$ a.s. on $\lbrace g^* = \infty \rbrace$ and therefore it is trivial that $(f-g)^* \geq f^* - g^*$ a.s. on $\lbrace g^* = \infty \rbrace$.  On $\lbrace -\infty < g^* < \infty \rbrace$ we know that $g \leq g^* < \infty$ hence we may write $f = (f - g) + g$ and by the first part of this result we get $f^* \leq (f -g)^*+ g^*$ a.s. which implies $f^* - g^* \leq (f - g)^*$.

For (iii), assume that $A$ is measurable, $g$ is measurable and $f \leq g$ a.s. on $A$.  Consider $h = g \cdot \characteristic{A} + f^* \cdot \characteristic{A^c}$.  Since $f^* \leq f$ everywhere we have
$\lbrace h \geq f \rbrace \subset \lbrace g  \geq f \rbrace$ hence is a null set.  Therefore $f^* \leq h$ a.s and therefore $f^* \leq g$ a.s. on $A$.

For (iv), if $A$ is measurable, $g$ is measurable and $f = g$ a.s. on $A$ then by the previous fact we have $f^* \leq g$ a.s on $A$.  On the other hand, $g = f \leq f^*$ a.s so it follows that $f^* = g$ a.s on $A$.

For (v), note that $(f \maxop g)^*$ is measurable and $f \leq f \maxop g \leq (f \maxop g)^*$ hence by (iii) we have $f^* \leq (f \maxop g)^*$.  Similarly $g^* \leq (f \maxop g)^*$ a.s. hence $f^* \maxop g^* \leq (f \maxop g)^*$ a.s.  For the opposite inequality because $f \leq f^*$ and $g \leq g^*$ we know that $f \maxop g \leq f^* \maxop g^*$.  Since $f^* \maxop g^*$ is measurable (iii) implies $(f \maxop g)^* \leq f^* \maxop g^*$ a.s.
\end{proof}

\begin{lem}\label{MeasurableCoverTriangleInequality}Let $(\Omega, \mathcal{A}, P)$  be a probability space  and $V$ be a vector space with seminorm $\norm{\cdot}$ then given $f,g : \Omega \to V$ we have
\begin{align*}
\norm{f+g}^* \leq (\norm{f}+\norm{g})^* \leq \norm{f}^* + \norm{g}^* \text{ a.s.}
\end{align*}
and
\begin{align*}
\norm{c f}^* &= \abs{c} \norm{f}^* \text{ a.s. for every $-\infty < c < \infty$}
\end{align*}
\end{lem}
\begin{proof}
By the triangle inequality we know $\norm{f+g} \leq \norm{f}+\norm{g}$ hence $\norm{f+g}^* \leq (\norm{f}+\norm{g})^*$ a.s.  By Proposition \ref{BasicPropertiesOfMeasurableCover} we have $ (\norm{f}+\norm{g})^* \leq \norm{f}^* + \norm{g}^*$ a.s.

By measurability of constant functions it is clear that $\norm{0}^* = 0$ hence we may assume that $c \neq 0$.  Suppose that $g$ is measurable and $\norm{cf} \leq g$.  This equivalent to $\norm{f} \leq g/\abs{c}$ and it follows that $\abs{c} \norm{f}^* \leq g$ a.s.  Suppose $h$ is measurable and for every measurable $g$ with $g \geq \norm{cf}$ everywhere we have $g \geq h$ a.s.   If $g$ is measurable such that $g \geq \norm{f}$ everywhere then $\abs{c} g \geq \norm{c f}$ everywhere hence we conclude $g \geq h/\abs{c}$ a.s. Therefore by the maximality of the measurable cover $f^*$ we have $\norm{f}^* \geq h/\abs{c}$ a.s. equivalently $\abs{c}\norm{f}^* \geq h$ a.s.
\end{proof}

\subsection{Independence, Iterated Outer Integrals and Limit Theorems}

TODO: Make some comments about how to think about independence.  When you don't have measurability you can't say $\probability{\xi \in A; \eta \in B} = \probability{\xi \in A} \probability{\eta \in B}$.  The answer is not to define independence as $\oprobability{\xi \in A; \eta \in B} = \oprobability{\xi \in A} \oprobability{\eta \in B}$ (WHY?  I suspect there are simple answers why this doesn't describe our intuition about independence).  Rather we take a structural approach and say that $\xi$ and $\eta$ are independent when they are defined on a product measurable space $(\Omega \times \Omega^\prime, \mathcal{A} \otimes \mathcal{A}^\prime)$ and $\xi$ depends only on the first coordinate and $\eta$ depends only on the second coordinate.

Under an assumption of independence the measurable cover of a sum is the sum of measurable covers.
\begin{prop}\label{MeasurableCoverSumOfIndependent}For $i=1, \dotsc, n$ let $(\Omega_i, \mathcal{A}_i, P_i)$  be probability spaces and $f_i : \Omega_i \to (-\infty, \infty]$ be functions.  If we define $g : \Omega_1 \times \dotsb \times \Omega_n \to (-\infty, \infty]$ by
\begin{align*}
g(\omega_1, \dotsc, \omega_n) &= f_1(\omega_1) + \dotsb + f_n(\omega_n)
\end{align*}
then we have $g^*(\omega_1, \dotsc, \omega_n) = f^*_1(\omega_1) + \dotsb + f^*_n(\omega_n)$.
\end{prop}
\begin{proof}
By an induction argument it is clear that we may assume $n=2$.  Let $P = P_1 \otimes P_2$.   Since for $i=1,2$ we have $f_i(\omega_i) \leq f_i^*(\omega_i)$ everywhere and $f_i^*$ measurable we have $g(\omega_1, \omega_2) \leq f_1^*(\omega_1) + f_2*(\omega_2)$ everywhere and therefore $g^*(\omega_1, \omega_2) \leq f_1^*(\omega_1) + f_2*(\omega_2)$ $P$-a.s.  If $\probability{g^* < f_1^* + f_2^*}>0$ then since
\begin{align*}
\lbrace g^* < f_1^* + f_2^* \rbrace &= \cup_{\substack{t,q,r \in \rationals \\ q+r > t }} \lbrace g^* < t < f_1^* + f_2^*,  q < f_1^*, r < f_2^* \rbrace
\end{align*}
by subadditivity there exists $t,q,r \in \rationals$ with $q+r > t$ and 
\begin{align*}
\probability{g^* < t < f_1^* + f_2^*,  q < f_1^*, r < f_2^*} > 0
\end{align*}  
Define $A = \lbrace g^* < t < f_1^* + f_2^*,  q < f_1^*, r < f_2^* \rbrace$ and for each $\omega_1 \in \Omega_1$ define the section $A_{\omega_1} = \lbrace \omega_2 \mid g^*(\omega_1, \omega_2) < t < f_1^*(\omega_1) + f_2^*(\omega_2),  q < f_1^*(\omega_1), r < f_2^*(\omega_2) \rbrace$.  By Lemma \ref{MeasurableSections} each $A_{\omega_1}$ is measurable and by Tonelli's Theorem \ref{Fubini} we have
\begin{align*}
0 &< \probability{A} = \iint \sprobability{A_{\omega_1}}{2} \, P_1(d\omega_1)
\end{align*}
hence $\sprobability{\sprobability{A_{\omega_1}}{2} > 0}{1} > 0$.  

\begin{clm}There exists $\omega_1 \in \Omega_1$ such that $\sprobability{A_{\omega_1}}{2} > 0$ and $f_1(\omega_1) > q$.
\end{clm}
We establish the claim by contradiction.  If $f_1 \leq q$ everywhere on $\lbrace \sprobability{A_{\omega_1}}{2} > 0 \rbrace$ then by Proposition \ref{BasicPropertiesOfMeasurableCover} we have $f_1^* \leq q$ $P_1$-a.s. on $\lbrace \sprobability{A_{\omega_1}}{2} > 0 \rbrace$.  In particular because $\sprobability{\sprobability{A_{\omega_1}}{2} > 0}{1} > 0$ it follows that there exists $\omega_1$ with $f^*_1(\omega_1) \leq q$ and $\sprobability{A_{\omega_1}}{2} > 0$.  On the other hand if $\sprobability{A_{\omega_1}}{2} > 0$ then $A_{\omega_1} \neq \emptyset$ and it follows that $f_1^*(\omega_1) > q$ which is the desired contradiction.

By the previous claim pick $\omega_1$ with $\sprobability{A_{\omega_1}}{2} > 0$ and $f_1(\omega_1) > q$.  For any $\omega_2 \in A_{\omega_1}$, by choice of $\omega_1$ and definition of $A$ we have
\begin{align*}
q + f_2(\omega_2) \leq f_1(\omega_1) + f_2(\omega_2) \leq g^*(\omega_1, \omega_2)
\end{align*}
hence $f_2 \leq g^*(\omega_1, \cdot) - q$ on $A_{\omega_1}$.  By measurability of $A_{\omega_1}$, $g^*(\omega_1, \cdot) - q$ we may apply Proposition \ref{BasicPropertiesOfMeasurableCover}  to conclude that $f^*_2 \leq g^*(\omega_1, \cdot) - q$ $P_2$-a.s. on $A_{\omega_1}$.  Thus $A_{\omega_1} \cap \lbrace f^*_2 \leq g^*(\omega_1, \cdot) - q \rbrace \neq \emptyset$ and we may pick $\omega_2$ such that $(\omega_1, \omega_2) \in A$ and 
\begin{align*}
q + f_2^*(\omega_2) &\leq g^*(\omega_1, \omega_2) < t < q+r
\end{align*}
which implies $f_2^*(\omega_2) < r$ yielding a contradiction.
\end{proof}

Under an assumption of independence and nonnegativity the measurable cover of a product is the product of measurable covers.  In addition, if a function on a product does not depend one of the factors of the product, the measurable cover may be chosen so that it also does not depend on that factor of the product.
\begin{prop}\label{MeasurableCoverProductOfIndependent}For $i=1, \dotsc, n$ let $(\Omega_i, \mathcal{A}_i, P_i)$  be probability spaces and $f_i : \Omega_i \to [-\infty, \infty]$ be functions and let $f_i^*$ be the measurable cover of $f_i$ with repsect to $P_i$.  If either $f_i \geq 0$ for $i=1, \dotsc, n$ or $n=2$ and $f_1 \equiv 1$ and we define $g : \Omega_1 \times \dotsb \times \Omega_n \to (-\infty, \infty]$ by
\begin{align*}
g(\omega_1, \dotsc, \omega_n) &= f_1(\omega_1) \dotsb f_n(\omega_n)
\end{align*}
then we have $g^*(\omega_1, \dotsc, \omega_n) = f^*_1(\omega_1) \dotsb f^*_n(\omega_n)$.
\end{prop}
\begin{proof}
In the case that $f_i \geq 0$ for $i=1, \dotsc, n$  an induction argument shows that it suffices to consider the case $n=2$.  Let $P = P_1 \otimes P_2$.  If $0 \leq f_i \leq f_i^*$ for $i=1, 2$ then $g(\omega_1, \omega_2) \leq f_1^*(\omega_1) f_2^*(\omega_2)$.  In the case that $f_1 \equiv 1$ it trivially follows that 
\begin{align*}
g(\omega_1, \omega_2) &= f_2(\omega_2)  \leq  f_2^*(\omega_2) = f_1^*(\omega_1) f_2^*(\omega_2)
\end{align*}  
In both cases we see that $g^*(\omega_1,\omega_2) \leq f_1^*(\omega_1) f_2^*(\omega_2)$ $P$-a.s.  
We argue by contradiction, so assume that $\probability{g^*(\omega_1,\omega_2) < f_1^*(\omega_1) f_2^*(\omega_2)}>0$.

First consider the case in which $f_1 \equiv 1$ so that $f_1^* \equiv 1$ as well.  Writing
\begin{align*}
\lbrace g^*(\omega_1,\omega_2) < f_1^*(\omega_1) f_2^*(\omega_2)\rbrace =\cup_{t \in \rationals} \lbrace g^*(\omega_1,\omega_2) < t < f_1^*(\omega_1) f_2^*(\omega_2)\rbrace
\end{align*}
and using subadditivity there exists $t \in \rationals$ so that if we we have
$\probability{g^*(\omega_1,\omega_2) < t < f_1^*(\omega_1) f_2^*(\omega_2)} > 0$.  
 By Tonelli's Theorem \ref{Fubini} we
have
\begin{align*}
0 < \probability {g^*(\omega_1,\omega_2) < t < f_1^*(\omega_1) f_2^*(\omega_2)} = \int \sprobability{g^*(\omega_1,\omega_2) < t < f_2^*(\omega_2)}{2} \, P_1(d\omega_1)
\end{align*}
so there exists $\omega_1$ such that $\sprobability{g^*(\omega_1,\omega_2) < t < f_2^*(\omega_2)}{2}>0$.  
Since $f_2 = g \leq g^*$ everywhere, by Proposition \ref{BasicPropertiesOfMeasurableCover}  we conclude $f_2^* \leq t$ a.s. on $\lbrace g^*(\omega_1,\omega_2) < t < f_2^*(\omega_2) \rbrace$ which is a contradiction.

Now consider the case in which $f_1,f_2 \geq 0$.  Via a similar argument as in Proposition \ref{MeasurableCoverSumOfIndependent} there exists $t, q, r \in \rationals$ with $t>0$,$q>0$, $r>0$, $qr>t$ so that if we define 
\begin{align*}
A &= \lbrace g^*(\omega_1,\omega_2) < t < f_1^*(\omega_1) f_2^*(\omega_2); f_1^*(\omega_1) > q ; f_2^*(\omega_2) > r \rbrace
\end{align*}
then $\probability{A}>0$.  Also as in that Proposition consider the sections $A_{\omega_1}$. 

\begin{clm} There exists $\omega_1$ such that $\sprobability{A_{\omega_1}}{2}>0$ and $f_1(\omega_1) > q$.
\end{clm}
We argue by contradiction.   By Tonelli's Theorem \ref{Fubini} we have
\begin{align*}
0 < \probability{A} = \int \sprobability{A_{\omega_1}}{2} \, P_1(d\omega_1)
\end{align*}
and conclude $\sprobability{\sprobability{A_{\omega_1}}{2}>0}{1}>0$.
 If $f_1 \leq q$ everywhere on $\lbrace \sprobability{A_{\omega_1}}{2}>0 \rbrace$ then by Proposition \ref{BasicPropertiesOfMeasurableCover} we have $f_1^* \leq q$ $P_1$-a.s. on $\lbrace \sprobability{A_{\omega_1}}{2}>0 \rbrace$, i.e. $\sprobability{\sprobability{A_{\omega_1}}{2}>0; f_1^* \leq q}{1}=\sprobability{\sprobability{A_{\omega_1}}{2}}{1}>0$.
In particular, there exists $\omega_1$ with $f_1^*(\omega_1) \leq q$ and $A_{\omega_1} \neq \emptyset$.  The latter statement implies that $f_1^*(\omega_1) > q$ which is a contradiction.

By the claim we select $\omega_1$ with $\sprobability{A_{\omega_1}}{2}>0$ and $f_1(\omega_1) > q$.  Pick $\omega_2 \in A_{\omega_1}$ and then note
\begin{align*}
f_2(\omega_2) &= \frac{g(\omega_1, \omega_2)}{f_1(\omega_1)} < \frac{g(\omega_1, \omega_2)}{q} \leq \frac{g^*(\omega_1, \omega_2)}{q} 
\end{align*}
hence by Proposition \ref{BasicPropertiesOfMeasurableCover} we have $f_2^* \leq \frac{g^*(\omega_1, \cdot)}{q}$ $P_2$-a.s. on $A_{\omega_1}$.  Thus $\sprobability{f_2^* \leq \frac{g^*(\omega_1, \cdot)}{q}; A_{\omega_1}} {2} = \sprobability{A_{\omega_1}} {2} >0$ and in particular there exists $\omega_2 \in \lbrace f_2^* \leq \frac{g^*(\omega_1, \cdot)}{q} \rbrace \cap A_{\omega_1}$.  For such an
$\omega_2$ we have 
\begin{align*}
q f_2^*(\omega_2) \leq g^*(\omega_1, \omega_2) < t < qr
\end{align*}
hence $f_2^*(\omega_2)<r$ which is a contradicts the fact that $(\omega_1,\omega_2) \in A$ implies $f_2^*(\omega_2)>r$.
\end{proof}

\begin{cor}\label{OuterProbabilityIndependentEvents}For $i=1, \dotsc, n$ let $(\Omega_i, \mathcal{A}_i, P_i)$  be probability spaces and $A_i \subset \Omega_i$ then
\begin{align*}
\left( P_1 \otimes \dotsb \otimes P_n \right)^* (A_1 \times \dotsb \times A_n) &= \left( P_1 \right)^*(A_1)  \dotsb \left( P_n \right)^*(A_n)
\end{align*}
\end{cor}
\begin{proof}
By Theorem \ref{ExistenceMeasurableCover} and Proposition \ref{MeasurableCoverProductOfIndependent}
\begin{align*}
\left( P_1 \otimes \dotsb \otimes P_n \right)^* (A_1 \times \dotsb \times A_n) 
&= \int \left( \characteristic{A_1} \dotsb \characteristic{A_n} \right)^* \, d \left( P_1 \otimes \dotsb \otimes P_n \right)  \\
&= \int \characteristic{A_1}^* \dotsb \characteristic{A_n}^* \, d \left( P_1 \otimes \dotsb \otimes P_n \right)   \\
&= \int \characteristic{A_1}^* \, dP_1 \dotsb \int \characteristic{A_n}^* \, dP_n  \\
&= P_1^*(A_1) \dotsb P_n^*(A_n)\
\end{align*}
\end{proof}

\begin{lem}Let $(\Omega, \mathcal{A}, P) = \prod_{j=1}^3 (\Omega_i, \mathcal{A}_i, P_i)$ and for each $i=1,2,3$ let $\pi_i : \Omega \to \Omega_i$ be the $i^{th}$ coordinate projection.  Suppose that $f : \Omega_1 \times \Omega_3 \to (-\infty, \infty)$ is a bounded function and let $g : \Omega \to (-\infty, \infty)$ be defined by $g(\omega_1,\omega_2,\omega_3) = f(\omega_1, \omega_3)$ then 
\begin{align*}
\cexpectationlong{(\pi_1, \pi_2)^{-1} (\mathcal{A}_1 \otimes \mathcal{A}_2)}{g^*} &= \cexpectationlong{\pi_1^{-1} (\mathcal{A}_1)}{g^*} \text{ $P$-a.s.}
\end{align*}
\end{lem}
\begin{proof}
By Proposition \ref{MeasurableCoverProductOfIndependent} we may select $g^*$ so that it doesn't depend on $\omega_2$; in particular we have $g^* \Independent \pi_2^{-1}(\mathcal{A}_2)$ (see Example \ref{IndependenceOnProductSpaces}).  If $A_1 \in \mathcal{A}_1$ and $A_2 \in \mathcal{A}_2$ the
$(\pi_1, \pi_2)^{-1}(A_1 \times A_2) = \pi_1^{-1}(A_1) \cap \pi_2^{-1}(A_2)$ and 
\begin{align*}
\expectation{ g^* ; \pi_1^{-1}(A_1) \cap \pi_2^{-1}(A_2)} &= \expectation{ g^* ; \pi_1^{-1}(A_1)} \probability{ \pi_2^{-1}(A_2)}  \\
&=\expectation{ \cexpectationlong{\pi_1^{-1}(\mathcal{A}_1)}{g^*} ; \pi_1^{-1}(A_1)} \probability{ \pi_2^{-1}(A_2)} \\
&=\expectation{ \cexpectationlong{\pi_1^{-1}(\mathcal{A}_1)}{g^*} ; \pi_1^{-1}(A_1) \cap \pi_2^{-1}(A_2)} 
\end{align*}
Since $\cexpectationlong{\pi_1^{-1}(\mathcal{A}_1)}{g^*}$ is $\mathcal{A}_1 \otimes \mathcal{A}_2$-measurable and sets of the form $A_1 \times A_2$ are a $\pi$-system generating $\mathcal{A}_1 \otimes \mathcal{A}_2$ the result follows from Lemma \ref{ConditionalExpectationExtension}.
\end{proof}

\begin{prop}\label{OuterTailProbabilities}Let $(\Omega, \mathcal{A}, P)$ be a probability space, let $f : \Omega \to (-\infty, \infty)$ be a function and $f^*$ the measurable cover of $f$. For all $-\infty < t < \infty$, 
\begin{itemize}
\item[(i)] $\oprobability{f > t} = \probability{f^* > t}$
\item[(ii)] for all $\epsilon > 0$, 
\begin{align*}
\oprobability{f \geq t} &\leq \probability{f^* \geq t} \leq \oprobability{f \geq t - \epsilon}
\end{align*}
\end{itemize}
\end{prop}
\begin{proof}
Since $f \leq f^*$ a.s. we have $\lbrace f > t \rbrace \subset \lbrace f^* > t \rbrace$ hence $\oprobability{f > t} < \oprobability{f^* > t}$.  Let $A$ be a measurable cover of $\lbrace f > t \rbrace$.  Note that $f \leq t$ on $A^c$ hence by Proposition \ref{BasicPropertiesOfMeasurableCover}  we have $f^* \leq t$ on $A^c$ showing
\begin{align*}
\probability{f^* > t} &= 1 - \probability{f^* \leq t} \geq 1 - \probability{A^c} = \probability{A} = \oprobability{f > t}
\end{align*}
so (i) is proven.

To see (ii), by subadditivity of outer measure, $\oprobability{f \geq t} \leq \oprobability{f^* \geq t}=\probability{f^* \geq t}$ which shows the first inequality of (ii).   Let $\epsilon > 0$ be given and let $0 < \delta < \epsilon$ be chosen arbitrarily.  By subadditivity and (i) we have
\begin{align*}
\probability{f^* > t - \delta} &\leq \probability{f^* > t - \epsilon} = \oprobability{f > t - \epsilon} \leq \oprobability{f \geq t - \epsilon}
\end{align*}
Now use the fact that $\lbrace f^* \geq t \rbrace = \cap_{0 < \delta <\epsilon} \lbrace f^* > t - \delta \rbrace$ and continuity of measure Lemma \ref{ContinuityOfMeasure} to see that
\begin{align*}
\probability{f^* \geq t} &= \lim_{\delta \to 0^+} \probability{f^* > t - \delta} \leq \oprobability{f \geq t - \epsilon}
\end{align*}
\end{proof}

\begin{thm}\label{FubiniOuterExpectation}Let $(S, \mathcal{S}, P)$ and $(T, \mathcal{T}, Q)$ be probability spaces, $f : S \times T \to [0, +\infty]$ and let $f^*$ be the measurable cover of $f$ with respect to $P \otimes Q$, then 
\begin{align*}
\int^* \left [ \int^* f(s,t) \, Q(dt) \right ] \, P(ds) &\leq \int^* f(s,t) \, (P \otimes Q)(ds,dt)
\end{align*}
If $Q$ is purely atomic (i.e. there exist $t_1, t_2, \dotsc \in T$ such that $Q = \sum_{n=1}^\infty Q \lbrace t_n \rbrace \delta_{t_n}$) then 
\begin{align*}
\int^* \left [ \int f(s,t) \, Q(dt) \right ] \, P(ds) &\leq \int^* f(s,t) \, (P \otimes Q)(ds,dt) = \int \left [ \int^* f(s,t) \, P(ds) \right ] \, Q(dt) 
\end{align*}
\end{thm}
\begin{proof}
For each $s \in S$ let $f_s : T \to [0,+\infty]$ be the section $f_s(t) = f(s,t)$.  For every fixed $s \in S$ we have $f_s(t) = f(s,t) \leq f^*(s,t)$ and sections of $f^*$ are $\mathcal{S}$-measurable (Lemma \ref{MeasurableSections}) thus $f_s^*(t) \leq f^*(s,t)$ $Q$-a.s.   It follows from this inequality and Theorem \ref{ExistenceMeasurableCover} that 
\begin{align*}
\int^* f(s,t) \, Q(dt) &= \int f^*_s(t) \, Q(dt) \leq \int f^*(s,t) \, Q(dt)
\end{align*}
We have observed Tonelli's Theorem \ref{Fubini} implies $\int f^*(s,t) \, Q(dt)$ is $\mathcal{S}$-measurable and therefore $\left [ \int^* f(s,t) \, Q(dt) \right]^* \leq \int f^*(s,t) \, Q(dt)$ $P$-a.s.  and it follows by another application of Tonelli's Theorem \ref{Fubini} that
\begin{align*}
\int^*\left [ \int^* f(s,t) \, Q(dt) \right] \, P(ds) &= \int \left [ \int^* f(s,t) \, Q(dt) \right]^* \, P(ds) \\
&\leq \int \int f^*(s,t) \, Q(dt) P(ds) = \int f(s,t) \, (P \otimes Q)(ds,dt)
\end{align*}

Now we assume that $Q$ is purely atomic and $Q = \sum_{n=1}^\infty Q \lbrace t_n \rbrace \delta_{t_n}$.

\begin{clm}If $Q$ is purely atomic then every $f : T \to [0,\infty]$ is measurable in the $Q$-completion $\mathcal{T}^Q$ and $\int^* f(t) \, Q(dt) = \int f(t) \, \tilde{Q}(dt)$ where $\tilde{Q}$ denotes the unique extension of $Q$ to $\mathcal{T}^Q$ (Lemma \ref{CompletionOfMeasure}).
\end{clm}
The first part of the claim is equivalent to saying that every set is $\mathcal{T}^Q$-measurable.  So let $A \subset T$ and define $B = A \cap \lbrace t_1, t_2, \dotsc \rbrace$.  Clearly $B \subset A$ and $A \setminus B \subset T \setminus \lbrace t_1, t_2, \dotsc \rbrace$ hence it follows that $A \Delta B = A \setminus B$ is a $Q$-null set.  For the second part of the claim,
since $f$ is $\mathcal{T}^Q$-measurable by Lemma \ref{CompletionOfSigmaAlgebra} there exists a $\mathcal{T}$-measurable $g$ such that $f=g$ $Q$-a.s.  If $f^*$ is the $Q$-measurable cover of $f$ then by Proposition \ref{BasicPropertiesOfMeasurableCover} it follows that $f^* = g$ $Q$-a.s.  hence by  Theorem \ref{ExistenceMeasurableCover}, Proposition \ref{AlmostEverywhereEqualIntegralEqual} and Lemma \ref{CompletionOfMeasure}
\begin{align*}
\int^* f \, dQ &= \int f^* \, dQ = \int g \, dQ = \int f \, d \tilde{Q}
\end{align*}

In what follows we abuse notation slightly and use the notation $Q$ to denote the extension of $Q$ to the completion $\mathcal{T}^Q$.  By the claim and the first part of this result
\begin{align*}
\int^* \left [ \int f(s,t) \, Q(dt) \right ] \, P(ds) = \int^* \left [ \int^* f(s,t) \, Q(dt) \right ] \, P(ds) \leq \int^* f(s,t) \, (P \otimes Q)(ds,dt)
\end{align*}

For the equality in the second part of this result we first need the following claim.  For $t \in T$ we now let  $f_t : S \to [0,\infty]$ represent the section $f_t(s) = f(s,t)$.  
\begin{clm} $f^*(s,t_n) = f_{t_n}^*(s)$ $P$-a.s. 
\end{clm}
From the first part of the proof we know $f^*(s,t_n) \geq f_{t_n}^*(s)$ $P$-a.s. so it suffices to show $f^*(s,t_n) \leq f_{t_n}^*(s)$ $P$-a.s.  If $j : S \to [0,\infty]$ is $\mathcal{S}$-measurable and satisfies $j \geq f_{t_n}$ everywhere.  Define $h(s,t) = j(s) \characteristic{t_n}(t) + f^*(s,t) \characteristic{T \setminus \lbrace t_n \rbrace }(t)$.  Then
$h(s,t_n) = j(s) \geq f(s,t_n)$ and for $t \neq t_n$ we have $h(s,t) = f^*(s,t) \geq f(s,t)$.  Since $h$ is $\mathcal{S} \otimes \mathcal{T}$-measurable we conclude that $h \geq f^*$ $(P \otimes Q)$-a.s.  and therefore since $Q\lbrace y_n \rbrace > 0$  we have $f^*(s, y_n) \leq h(s, y_n) = j(s)$ $P$-a.s. From the definition, we conclude that $f^*(\cdot, y_n) \leq f_{t_n}^*$ $P$-a.s.

Now we simply compute with  Theorem \ref{ExistenceMeasurableCover}, Tonelli's Theorem \ref{Fubini} and the explicit formula for the integral with respect to $Q$,
\begin{align*}
\int^* f(s,t) \,  (P \otimes Q)(ds,dt) &= \int f^*(s,t) \, (P \otimes Q)(ds,dt)  \\
&= \int \left [ \int f^*(s,t) \, P(ds) \right ] \,   Q(dt)\\
&= \sum_{n=1}^\infty Q \lbrace t_n \rbrace \int f^*(s,t_n) \, P(ds) \\
&= \sum_{n=1}^\infty Q \lbrace t_n \rbrace \int f_{t_n}^*(s) \, P(ds) \\
&= \sum_{n=1}^\infty Q \lbrace t_n \rbrace \int^* f(s, t_n) \, P(ds) \\
&= \int \left [ \int^* f(s, t) \, P(ds) \right ] \,   Q(dt)\\
\end{align*}

TODO: Dudley says this last part (the equality) is an immediate consequence of Tonelli, my proof is quite a few lines...  Looking at my notes I am pretty confused here and should probably look for a simpler proof (though I don't see it following immediately from Tonelli). 

TODO: Delete this cruft once I'm sure it is irrelevant:

By the claim and taking an intersection of a countable number of $P$-a.s. events we know that for $P$-almost every $s$ we have $f^*(s,t_n) \leq f_{t_n}^*(s)$ for all $n \in \naturals$ and since $Q\lbrace t_1, t_2, \dotsc \rbrace = 1$ it follows that $f^*(s,t) \leq \sum_{n=1}^\infty f_{t_n}^*(s) \characteristic{t_n}(t)$ $(P \otimes Q)$-a.s.   
hence by Theorem \ref{ExistenceMeasurableCover}, Tonelli's Theorems \ref{TonelliIntegralSum}  and \ref{Fubini} 
\begin{align*}
\int^* f(s,t) \,  (P \otimes Q)(ds,dt) &= \int f^*(s,t) \, (P \otimes Q)(ds,dt)  \\
&\leq \int \sum_{n=1}^\infty f_{t_n}^*(s) \characteristic{t_n}(t) \,  (P \otimes Q)(ds,dt) \\
&= \sum_{n=1}^\infty\int \left [ \int f_{t_n}^*(s) \, P(ds) \right ] \characteristic{t_n}(t) \,   Q(dt)\\
&= \sum_{n=1}^\infty \int \left [ \int^* f(s, t_n) \, P(ds) \right ] \characteristic{t_n}(t) \,   Q(dt)\\
&= \int \left [ \int^* f(s, t) \, P(ds) \right ] \,   Q(dt)\\
\end{align*}
where in the last line we have used the fact that an integral over $T$ is determined by the values of the integrand at $t_1, t_2, \dotsc$.  From the first part of the Theorem we know that $\int \left [ \int^* f(s, t) \, P(ds) \right ] \,   Q(dt) \leq \int^* f(s,t) \,  (P \otimes Q)(ds,dt)$ so the equality follows.

\end{proof}


\begin{thm}[Monotone Convergence Theorem for Outer Expectations]\label{MCTOuterExpectation}Let $f_n \uparrow f$ with $\oexpectation{f_1} > - \infty$ then
$\oexpectation{f_n} \uparrow \oexpectation{f}$.
\end{thm}
\begin{proof}
Since $f_1 \leq f_n$ and $f_1 \leq f$ and $\oexpectation{f_1} > - \infty$, Proposition \ref{OuterExpectationMonotonicity} implies that $\oexpectation{f_n}$ and $\oexpectation{f}$ are all defined. 
We know that $\int^* f_n dP = \int f_n^* dP$ and we know that $f_n \leq f_{n+1}$ implies $\mathcal{J}_{f_{n+1}} \subset \mathcal{J}_{f_n}$ hence $f_n^* \leq f_{n+1}^*$ a.s.  (Proposition \ref{AlmostSureUniquenessEssentialInfimum}).  Therefore $g=\lim_{n \to \infty} f^*_n$ exists, is measurable and by Monotone Convergence Theorem for signed functions (Theorem \ref{SignedMCT})
\begin{align*}
\lim_{n \to \infty} \oexpectation{f_n} &= \lim_{n \to \infty} \int f_n^* dP = \int g dP
\end{align*}  
By the previous argument 
we also have $f^*_n \leq f^*$ a.s. and therefore $g \leq f^*$ a.s.  Suppose $\probability{g < f^*} > 0$.  Since $f_n \leq g$ everywhere (in particular on $\lbrace g < f^* \rbrace$) we know that $f \leq g$ on $\lbrace g < f^* \rbrace$ and therefore by Proposition \ref{BasicPropertiesOfMeasurableCover} we have $f^* \leq g$ a.s. on $\lbrace g < f^* \rbrace$ which is a contradiction.  It follows that $g = f^*$ a.s. and therefore $\int g dP = \oexpectation{f}$ and the result follows.
\end{proof}

TODO: Somewhere note explicity that for any $f \geq 0$ we have $\int^* f \, d\mu$ exists (trivial that $\int h \, d\mu$ exists for any measurabe $h$ with $h\geq f \geq 0$)?
\begin{thm}[Fatou's Lemma for Outer Expectations]\label{FatouOuterExpectation}Given $f_1, f_2, \dots$
  positive functions from
  $(\Omega, \mathcal{A}, P)$ to $[0,\infty]$,
  then $\int^* \liminf_{n \to \infty} f_n d P \leq \liminf_{n \to
    \infty} \int^* f_n d P$.
\end{thm}
\begin{proof}
This is essentially the same proof as Theorem \ref{Fatou}.  For each $k \in \naturals$ let $g_k = \inf_{m \geq k} f_m$.  Then $g_k \geq 0$ and $0 \leq g_k \uparrow \liminf_{n \to \infty} f_n$ so by Theorem \ref{MCTOuterExpectation} and 
\begin{align*}
\int^* \liminf_{n \to \infty} f_n \, dP &= \lim_{k \to \infty} \int^* g_k \, dP = \lim_{k \to \infty} \int^* \inf_{m \geq k} f_m \, dP \\
&\leq \lim_{k \to \infty} \inf_{m \geq k}\int^* f_m \, dP  = \liminf_{n \to \infty} \int^* f_n \, dP
\end{align*}
\end{proof}

\subsection{Perfect Functions}

One of the most useful facts of integration theory is the abstract change of variables formula Lemma \ref{ChangeOfVariables} (and the special case for probability theory: the Expectation Rule Lemma \ref{ExpectationRule}): $\int (f \circ g) \, d\mu = \int f \, d\pushforward{g}{\mu}$.  We want an analogous result for outer expectations.  To get such a result we clearly need $g$ to be measurable (otherwise we can't even make sense of $\pushforward{g}{\mu}$).  In fact we need to require a bit more of $g$ which leads to the following definition.

\begin{defn}Let $(\Omega, \mathcal{A}, P)$ be a probability space, $(S, \mathcal{S})$ be a measurable space and $g : \Omega \to S$ be measurable.  For any $f : S \to (-\infty, \infty)$ let $f^*$ be the measurable cover with respect to $\pushforward{g}{P}$.  We say $g$ is \emph{perfect} if for every $f : S \to (-\infty, \infty)$ we have $(f \circ g)^* = f^* \circ g$ $P$-a.s.
\end{defn}

It is useful to have some conditions for $g$ that are equivalent to it being perfect.  TODO: Note that van der Vaart and Wellner call $g$ perfect if $(f \circ g)^* = f^* \circ g)$ for all bounded $f$ but don't mention that this implies $(f \circ g)^* = f^* \circ g$ for all $f$: this seem to be implied by Dudley's theorem below just make sure there isn't a gap in the proof that assumed boundedness of $f$.
\begin{thm}\label{PerfectFunctions}Let $(\Omega, \mathcal{A}, P)$ be a probability space, $(S, \mathcal{S})$ be a measurable space and $g : \Omega \to S$ be measurable.  For any $f : S \to (-\infty, \infty)$ let $f^*$ be the measurable cover with respect to $\pushforward{g}{P}$.  The following are equivalent
\begin{itemize}
\item[(i)] $g$ is perfect
\item[(ii)] For every $C \subset S$ we have $(\characteristic{C} \circ g)^* = \characteristic{C}^* \circ g$ $P$-a.s.
\item[(iii)] For every $A \in \mathcal{A}$ there exists $B \in \mathcal{S}$ such that $B \subset g(A)$ and $P(g^{-1}(B)) \geq P(A)$
\item[(iv)] For every $A \in \mathcal{A}$ with $P(A) > 0$ there exists $B \in \mathcal{S}$ such that $B \subset g(A)$ and $P(g^{-1}(B)) > 0$
\end{itemize}
\end{thm}
\begin{proof}
TODO:
\end{proof}

\begin{cor}\label{cor:OuterExpectationRule}$(\Omega, \mathcal{A}, P)$ be a probability space, $(S, \mathcal{S})$ be a measurable space and $g : \Omega \to S$ be perfect.  For any $f : S \to (-\infty, \infty)$
\begin{align*}
\int^* (f \circ g) \, dP &= \int^* f \, d\pushforward{g}{P}
\end{align*}
\end{cor}
\begin{proof}
We compute using Theorem \ref{ExistenceMeasurableCover}, Theorem \ref{PerfectFunctions}, Proposition \ref{IntegrableAlmostEverywhereEqualIntegralEqual} and Lemma \ref{ExpectationRule}
\begin{align*}
\int^* (f \circ g) \, dP &= \int (f \circ g)^* \, dP = \int (f^* \circ g) \, dP = \int f^* \, d\pushforward{g}{P} = \int^* f  \, d\pushforward{g}{P}
\end{align*}
\end{proof}

We will often require the fact that coordinate projections on product spaces are perfect.
\begin{prop}\label{prop:CoordinateProjectionsArePerfect}Let $(S, \mathcal{S}, \mu)$ and $(T, \mathcal{T}, \nu)$ be probability spaces and let $(\Omega, \mathcal{A}, P) = (S\times T, \mathcal{S}\otimes \mathcal{T}, \mu \otimes \nu)$.  Then the coordinate projection $\pi : \Omega \to T$ is perfect.
\end{prop}
\begin{proof}
TODO : Note that Dudley provides a proof of this fact doesn't rely on the $n=2$ case of Proposition \ref{MeasurableCoverProductOfIndependent}; it seems to me this is a simple corollary of that right?
\end{proof}

\subsection{Weak Convergence}

\begin{defn}Let $(S,d)$ be a metric space then a \emph{random net in $S$} is a $\mathcal{I}$ a directed set and for every $\alpha \in \mathcal{I}$ a probability space $(\Omega_\alpha, \mathcal{A}_\alpha, P_\alpha)$ and a (not necessarily measurable) map $\xi_\alpha : \Omega_\alpha \to S$.  We will often denote the random net simply as $\net{\xi}{\alpha}$ or $\xi_\alpha$.
\end{defn}

\begin{defn}Let $(S,d)$ be a metric space with the Borel $\sigma$-algebra and $\xi_\alpha$ a random net in $S$.  Let $\mu$ be a Borel probability measure on $S$ then we that \emph{$\xi_\alpha$ converges weakly to $\mu$}
if and only if the net $\soexpectation{f(\xi_\alpha)}{\alpha}$ converges to $\int f(\xi) \, d\mu$ for every bounded continuous function $f : S \to \reals$.  We denote convergence in distribution as $\xi_\alpha \toweakhj \mu$.  If $(\Omega, \mathcal{A}, P)$ is a probability space and $\xi : \Omega \to S$ is a measurable random element then we say that \emph{$\xi_\alpha$ converges in distribution to $\xi$} if and only if $\xi_\alpha$ converges weakly to $\law{\xi}$.  We denote convergence in distribution as $\xi_\alpha \todisthj \xi$.
\end{defn}

TODO: Portmanteau and Skorohod-Dudley-Wichura

\begin{thm}[Portmanteau Theorem]\label{PortmanteauTheoremHoffmanJorgensen}Let $(S,d)$ be a metric space, $\xi_\alpha$ a random net in $S$ and $\mu$ be a
 Borel probability measure.  The following are equivalent
\begin{itemize}
\item[(i)] $\xi_\alpha$ converge weakly to $\mu$.
\item[(ii)] $\int f \, d\mu_n \to \int f \, d\mu$
  for all bounded Lipschitz functions $f$.
\item[(iii)] $\limsup_{\alpha} \oprobability{\xi_\alpha \in C} \leq
  \mu(C)$ for all closed sets $C$
\item[(iv)] $\liminf_{\alpha} \iprobability{\xi_\alpha \in U} \geq
  \mu(U)$ for all open sets $U$
\item[(v)] $\lim_{\alpha} \oprobability{\xi_\alpha \in A} = \lim_{\alpha} \iprobability{\xi_\alpha \in A} = \mu(A)$ for all
  $\mu$-continuity sets $A$.
\end{itemize}
\end{thm}
\begin{proof}
TODO:
\end{proof}

\subsection{Tightness and Prohorov's Theorem}

\begin{defn}\label{defn:AsymptoticMeasurability}Let $(S,d)$ be a metric space with the Borel $\sigma$-algebra and $\xi_\alpha$ a random net in $S$.  We say that $\xi_\alpha$ is \emph{asymptotically measurable} if for every bounded continuous function $f : S \to \reals$ be have
\begin{align*}
\int^* f(\xi_\alpha) \, dP_\alpha - \int_* f(\xi_\alpha)  \, dP_\alpha \to 0
\end{align*}
\end{defn}

\begin{defn}\label{defn:AsymptoticTightness}Let $(S,d)$ be a metric space with the Borel $\sigma$-algebra and $\xi_\alpha$ a random net in $S$.  We say that $\xi_\alpha$ is \emph{asymptotically tight} if for every $\epsilon > 0$ there exists a compact set $K$ such that for every $\delta > 0$ 
\begin{align*}
\liminf_{\alpha} \iprobability{\xi_\alpha \in K^\delta} \geq 1 - \epsilon
\end{align*}
\end{defn}

\begin{prop}Let $(S,d)$ be a metric space  with the Borel $\sigma$-algebra, $\xi_\alpha$ a random net in $S$ and $\mu$ Borel probability measure on $S$ with $\xi_\alpha \toweakhj \mu$ then
\begin{itemize}
\item[(i)] $\xi_\alpha$ is asymptotically measurable
\item[(ii)] $\xi_\alpha$ is asymptotically tight if and only if $\xi$ is tight
\end{itemize}
\end{prop}
\begin{proof}
To see (i) let $f : S \to \reals$ be bounded continuous then we know that $\soexpectation{f(\xi_\alpha)}{\alpha} \to \int f(\xi) \, d\mu$.  On the other hand, $-f$ is also bounded and continuous and therefore 
\begin{align*}
\int_* f(\xi_\alpha) \, dP_\alpha &= - \int^* -f(\xi_\alpha) \, dP_\alpha \to -\int -f(\xi) \, d\mu = \int f(\xi) \, d\mu
\end{align*}

To see (ii) let $\epsilon > 0$.  Assume that $\mu$ is tight and pick a compact set $K$ with $\mu(K) > 1 - \epsilon$.  By the Portmanteau Theorem \ref{PortmanteauTheoremHoffmanJorgensen} we know that for every $\delta>0$,
\begin{align*}
\liminf_\alpha \iprobability{\xi_\alpha \in K^\delta} &\geq \mu(K^\delta) \geq \mu(K) \geq 1 -\epsilon
\end{align*}
Conversely assume that $\xi_\alpha$ is asymptotically tight and pick a compact $K$ with $\liminf_{\alpha} \iprobability{\xi_\alpha \in K^\delta} \geq 1 - \epsilon$ for all $\delta>0$.  Again by the Portmanteau Theorem \ref{PortmanteauTheoremHoffmanJorgensen} we get
\begin{align*}
\mu(\overline{K^\delta}) &\geq \limsup_\alpha \oprobability{\xi_\alpha \in \overline{K^\delta}} \geq \liminf_{\alpha} \iprobability{\xi_\alpha \in K^\delta} \geq 1 - \epsilon
\end{align*}
We claim that $\cap_{\delta > 0} \overline{K^\delta} = K$.  Suppose $x \notin K$ then $d(x,K) > 0$ since otherwise we pick $x_n \in K$ such that $d(x,x_n) < 1/n$ and the by compactness of $K$ we get that $x_n \to x$ along a subsequence which contradicts the fact that $K$ is closed (Lemma \ref{CompactnessInMetricSpaces}).  Now observe $x \notin \overline{K^{d(x,K)/2}}$ to see that $\cap_{\delta > 0} \overline{K^\delta} \subset K$ (the opposite inclusion is obvious. The result follows by continuity of measure (Lemma \ref{ContinuityOfMeasure})
\begin{align*}
\mu(K) = \lim_{\delta \to 0} \mu(\overline{K^\delta}) \geq 1-\epsilon
\end{align*}
\end{proof}


In the following theorem that extends Theorem \ref{Prohorov} to the non-separable case, there are separate statements for nets and sequences; recall that there are subnets of a sequence that are not sequences.  More generally, it may be helpful to recall the subtely in the definition of a subnet (Definition \ref{defn:Subnet}).
\begin{thm}[Prohorov's Theorem]\label{ProhorovHoffmanJorgensen}Let $(S,d)$ be a metric space  with the Borel $\sigma$-algebra  
\begin{itemize}
\item[(i)] Let $\xi_\alpha$ a random net in $S$ that is asymptotically measurable and asymptotically tight then there exists a tight Borel measure $\mu$ and a subnet $\beta$ of $\alpha$ such that $\xi_\beta \toweakhj \mu$.
\item[(ii)] Let $\xi_n$ be a random sequence in $S$ that is asymptotically measurable and asymptotically tight then there exists a tight Borel measure $\mu$ and a subsequence $n_j$ such that $\xi_{n_j} \toweakhj \mu$.
\end{itemize}
\end{thm}
\begin{proof}
Let $C_b(S)$ denote the space of bounded continuous $f : S \to \reals$ and let $\norm{f}$ be the supremum of $f$.  Consider the product $Y = \Pi_{f \in C_b(S)} [-\norm{f}, \norm{f}]$ with the product topology and recall that, by Tychonoff's Theorem \ref{Tychonoff}, $Y$ is compact.  The random net $\xi_\alpha$ defines a net $\net{y}{\alpha}$ in $Y$ given by $y_\alpha(f) = \int^* f(\xi_\alpha) \, dP_\alpha$.  By compactness of $Y$ and Theorem \ref{CompactnessAndNets} we know that there is a convergent subnet of $\net{y}{\alpha}$.  

TODO: Finish
\end{proof}

\section{Asymptotic Equicontinuity}

\begin{defn}Let $(S,\mathcal{S}, \mu)$ be a probability space, $\tau$ be a pseudometric on $S$, $\mathcal{F} \subset \mathcal{L}^2(S, \mathcal{S}, \mu)$ and $\mathds{G}^n$ be the empirical process defined by $\mu$.  We say that $\mathcal{F}$ is \emph{asymptotically equicontinuous with respect to $\mu$ and $\tau$} if for every $\epsilon > 0$ there $\delta > 0$ and $N \in \naturals$ such that for all $n \geq N$ 
\begin{align*}
\oprobability{\sup \lbrace \abs{\mathds{G}^n_{f-g}} \mid f,g \in \mathcal{F}, \tau(f,g) < \delta \rbrace > \epsilon} < \epsilon
\end{align*}
\end{defn}

\begin{defn}Let $(S,\mathcal{S}, \mu)$ and $\mathcal{F} \subset \mathcal{L}^2(S, \mathcal{S}, \mu)$ then we say that $\mathcal{F}$ is \emph{Donsker class} if $\mathcal{F}$ is pregaussian and $\mathds{G}^n \todist \mathds{G}^\mu$.
\end{defn}

\begin{thm}\label{DonskerClassesAndAsymptoticEquicontinuity}Let $(S,\mathcal{S}, \mu)$ and $\mathcal{F} \subset \mathcal{L}^2(S, \mathcal{S}, \mu)$ then the following are equivalent
\begin{itemize}
\item[(i)] $\mathcal{F}$ is a Donsker class
\item[(ii)] $\mathcal{F}$ is totally bounded under the pseudometric $\rho_\mu$ and is asymptotically equicontinuous with respect to $\mu$ and $\rho_\mu$.
\item[(iii)] There exists a pseudometric $\tau$ on $S$ such that $\mathcal{F}$ is totally bounded under $\tau$ and is asymptotically equicontinuous with respect to $\mu$ and $\tau$.
\end{itemize}
\end{thm}
\begin{proof}
(i) implies (ii): TODO

(ii) implies (iii) is immediate.

We now move on to the main part of the theorem, (iii) implies (i).  

\begin{clm} For any countable dense $\mathcal{H} \subset \mathcal{G}$, $\mathds{G}^\mu$ has sample functions uniformly continuous on $\mathcal{H}$.
\end{clm}
Let $\epsilon > 0$ be given and pick $\delta > 0$ and $N \in \naturals$ as per the asymptotic equicontinuity condition.  Let $h_1, h_2, \dotsc$ be an enumeration of $\mathcal{H}$ and for each $m \in \naturals$ define $\mathcal{H}_m = \lbrace h_1, \dotsc, h_m \rbrace$.  By the finite dimensional Central Limit Theorem we know that $(\mathds{G}^n_{h_1}, \dotsc, \mathds{G}^n_{h_m}) \todist (\mathds{G}_{h_1}, \dotsc, \mathds{G}_{h_m})$.
\begin{align*}
\probability{\max \lbrace \abs{\mathds{G}^\mu_{f-g}} \mid f,g \in \mathcal{H}_m, \tau(f,g) < \delta \rbrace > \epsilon} 
&=\lim_{n \to \infty} \probability{\max \lbrace \abs{\mathds{G}^n_{f-g}} \mid f,g \in \mathcal{H}_m, \tau(f,g) < \delta \rbrace > \epsilon} \\
&\leq \lim_{n \to \infty} \oprobability{\sup \lbrace \abs{\mathds{G}^n_{f-g}} \mid f,g \in \mathcal{F}, \tau(f,g) < \delta \rbrace > \epsilon} < \epsilon
\end{align*}
Now letting $\mathcal{H}_m \uparrow \mathcal{H}$ and use continuity of measure to get
\begin{align*}
\probability{\sup \lbrace \abs{\mathds{G}^\mu_{f-g}} \mid f,g \in \mathcal{H}, \tau(f,g) < \delta \rbrace > \epsilon}  \leq \epsilon
\end{align*}
from which we get uniform continuity of $\mathcal{G}^\mu$ on $\mathcal{H}$.  TODO: Make sure all the details are correct here.

\begin{clm} $\mathcal{F}$ is pregaussian
\end{clm}
To see that the $\tau$-uniformly continuous functions on $\mathcal{F}$ is separable in the supremum norm, by Proposition \ref{QuotientOfPseudometricSpace} we know that this space is isomorphic to the uniformly continuous functions on the quotient of $\mathcal{F}$ under the equivalence relation $\tau(f,g) = 0$.  Proposition \ref{QuotientOfPseudometricSpace}  also tells us that the latter space is a totally bounded metric space and therefore the uniformly continuous functions is separable in the supremum norm by Proposition \ref{SpaceOfUniformlyContinuousFunctionsOnTotallyBoundedSet}.  So pick a countable dense subset $\mathcal{H}$.  

Let $G^\mu$ be a 

\end{proof}

\section{Bootstrap}

\begin{defn}A random variable $\epsilon$ with values in $\lbrace -1, 1 \rbrace$ such that $\probability{\epsilon = 1} = \probability{\epsilon = -1} = 1/2$ is called a \emph{Rademacher variable}.
\end{defn}

Some basic motivation for what follows (e.g. Koltchinski's St. Flour notes have some useful stuff in its introduction).   Also Pollard tries to motivate symmetrization techniques but I haven't gotten much out of what he has to say.

TODO: Dudley calls this a desymmetrization fact.  What the heck does that mean?  I actually think this is what Van der Vaart and Wellner call Ottaviani's inequality.  
\begin{lem}Let $(\Omega, \mathcal{A}, P)$ and $(\Omega^\prime, \mathcal{A}^\prime, P^\prime)$ be probability spaces form the product $(\Omega \times \Omega^\prime, \mathcal{A} \otimes  \mathcal{A}^\prime, P \otimes P^\prime)$ and let $\pi : \Omega \times \Omega^\prime \to \Omega$ and $\pi^\prime : \Omega \times \Omega^\prime \to \Omega^\prime$ be the coordinate projections.  Suppose $T$ be a set, $X_t : \Omega \to \reals^T$ and $Y_t : \Omega^\prime \to \reals^T$ be stochastic processes (TODO: Do we really need measurability of $X_t$ and $Y_t$?; I suspect not.).   Then
\begin{itemize}
\item[(i)] For any $s>0$ and $u > 0$ such that $\sup_{t \in T} P^\prime( \abs{Y_t} \geq u )< 1$ we have
\begin{align*}
\inf_{t \in T}  P^\prime( \abs{Y_t} < u ) P^*(\sup_{t \in T} \abs{X_t} > s) &\leq (P \otimes P^\prime)^* (\sup_{t \in T} \abs{X_t \circ \pi  - Y_t \circ \pi^\prime} > s - u)
\end{align*}
\item[(ii)] If $\theta > \sup_{t \in T} \int Y_t^2 \, dP^\prime$ and $s>0$ then
\begin{align*}
P^*(\sup_{t \in T} \abs{X_t} > s) &\leq 2 (P \otimes P^\prime) ( \sup_{t \in T} \abs{X_t \circ \pi  Y_t \circ \pi^\prime} > s - (2\theta)^{1/2})
\end{align*}
\end{itemize}
\end{lem}
\begin{proof}
To see (i), by the Tonelli Theorem \ref{FubiniOuterExpectation} we have
\begin{align*}
&(P \otimes P^\prime)^* (\sup_{t \in T} \abs{X_t \circ \pi  - Y_t \circ \pi^\prime} > s - u) \\
&=\int^* \characteristic{\sup_{t \in T} \abs{X_t \circ \pi  - Y_t \circ \pi^\prime} > s - u}(\omega, \omega^\prime) (P \otimes P^\prime)(d\omega, d\omega^\prime) \\
&\geq \int^* \left [ \int^* \characteristic{\sup_{t \in T} \abs{X_t \circ \pi  - Y_t \circ \pi^\prime} > s - u}(\omega, \omega^\prime) P^\prime(d\omega^\prime) \right ] P(d\omega)\\
&\geq \int^* \characteristic{\sup_{t \in T} \abs{X_t} > s}(\omega) \left [ \int^* \characteristic{\sup_{t \in T} \abs{X_t \circ \pi  - Y_t \circ \pi^\prime} > s - u}(\omega, \omega^\prime) P^\prime(d\omega^\prime) \right ] P(d\omega)\\
&\geq P(\sup_{t \in T} \abs{X_t} > s)  \inf_{\lbrace w \in \reals^T \mid \sup_{t \in T} \abs{w_t} > s \rbrace} P^\prime (\sup_{t \in T} \abs{w_t  - Y_t} > s - u)\\
&\geq P(\sup_{t \in T} \abs{X_t} > s) \inf_{t \in T} P^\prime(\abs{Y_t} > u) \\
\end{align*}
where in the last line we observe that for $w \in \reals^T$ such that $\sup_{t \in T} \abs{w_t} > s$ we can pick $t_0$ such that $\abs{w_{t_0}} > s$; for that $t_0$ we see that $\abs{Y_{t_0}} < u$ implies $\abs{w_{t_0} - Y_{t_0}} > s - u$ so by monotonicity 
\begin{align*}
\inf_{t \in T} P^\prime ( \abs{Y_t} < u) &\leq P^\prime ( \abs{Y_{t_0}} < u) \leq P^\prime(\abs{w_{t_0} - Y_{t_0}} > s - u) \leq \sup_{t \in T} P^\prime(\abs{w_t - Y_t} > s - u)
\end{align*}

To see (ii) note that by Markov's Inequality
\begin{align*}
P^\prime(\abs{Y_t} \geq (2\theta)^{1/2}) &\leq \frac{\expectation{Y_t^2}}{2\theta} \leq \frac{\sup_{t \in T} \expectation{Y_t^2}}{2\theta} \leq \frac{1}{2}
\end{align*}
Therefore $\inf_{t \in T} P^\prime(\abs{Y_t} < (2\theta)^{1/2}) \geq \frac{1}{2}$ and (ii) follows from (i).
\end{proof}

TODO: In this result check if it is true for stochastic processes and not just the empirical processes (the second part looks to be specific to the empirical process case in fact requires $\mathcal{F} \subset L^2(S, \mathcal{S}, P)$)
\begin{lem}For any $\lambda > 0$ and $n \in \naturals$,
\begin{itemize}
\item[(i)]
\begin{align*}
\oprobability{\norm{\sum_{i=1}^n \epsilon_i f(\xi_i)} > \lambda} &\leq 2 \max_{1 \leq k \leq n} \oprobability{\norm{\sum_{i=1}^k f(\xi_i)} > \lambda/2}
\end{align*}
\item[(ii)] Let $\alpha^2 = \sup_{f \in \mathcal{F}} \int (f - \expectation{f})^2 \, dP$, then for every $n \in \naturals$ and $\lambda > \sqrt{2n} \alpha$ 
\begin{align*}
\oprobability{\norm{\sum_{i=1}^n (f(\xi_i) - \expectation{f})} > \lambda} &\leq 
4 \oprobability{\norm{\sum_{i=1}^n \epsilon_i f(\xi_i)} > (t - \sqrt{2n} \alpha)/2}
\end{align*}
\end{itemize}
\end{lem}
\begin{proof}
Let $E_n = \lbrace 0, 1 \rbrace^n$ and let $\tilde{\epsilon}_n = (\epsilon_1, \dotsc, \epsilon_n)$ so $\tilde{\epsilon}_n : \Omega^\prime \to E_n$.  
Using subadditivity of outer probability  and Corollary \ref{OuterProbabilityIndependentEvents}
\begin{align*}
\oprobability{\norm{\sum_{i=1}^n \epsilon_i f(\xi_i)} > \lambda}
&= \oprobability{\norm{\sum_{i=1}^n \epsilon_i f(\xi_i)} > \lambda ; \cup_{\tau \in E_n} \lbrace \tilde{\epsilon}_n = \tau \rbrace } \\
&\leq \sum_{\tau \in E_n} \oprobability{\norm{\sum_{i=1}^n \epsilon_i f(\xi_i)} > \lambda ; \tilde{\epsilon}_n = \tau} \\
&= \sum_{\tau \in E_n} \oprobability{\norm{\sum_{\tau_i=1} f(\xi_i) -\sum_{\tau_i=-1} f(\xi_i) } > \lambda ; \tilde{\epsilon}_n = \tau} \\
&= \sum_{\tau \in E_n} (P^n)^*\left (\norm{\sum_{\tau_i=1} f(\xi_i) -\sum_{\tau_i=-1} f(\xi_i) } > \lambda \right )  P^\prime \left(\tilde{\epsilon}_n = \tau\right ) \\
&= \sum_{\tau \in E_n} (P^n)^*\left (\norm{\sum_{\tau_i=1} f(\xi_i) -\sum_{\tau_i=-1} f(\xi_i) } > \lambda \right )  \frac{1}{2^n}\\
&\leq \frac{1}{2^n} \sum_{\tau \in E_n} \left [ (P^n)^*\left (\norm{\sum_{\tau_i=1} f(\xi_i)} > \lambda/2 \right) + (P^n)^*\left (\norm{\sum_{\tau_i=-1} f(\xi_i) } > \lambda/2 \right )  \right ] \\
\end{align*}

TODO: I think the following makes sense...  
\begin{clm} $(P^n)^*\left (\norm{\sum_{\tau_i=1} f(\xi_i)} > \lambda/2 \right)= (P^n)^*\left (\norm{\sum_{i=1}^{\card{\tau_i=1}} f(\xi_i)} > \lambda/2 \right)$ 
\end{clm}
Let $k = \card{\tau_i = 1}$.  (Note: here we are using measurability and the i.i.d. property).  Let $\pi : \Omega \to \prod_{\tau_i =1} S^i$ be the projection map onto the $i$ for which $\tau_i=1$.  By Proposition \ref{prop:CoordinateProjectionsArePerfect}, projection maps are perfect and therefore we can apply the Expectation Rule Corollary \ref{cor:OuterExpectationRule}
\begin{align*}
(P^n)^*\left (\norm{\sum_{\tau_i=1} f(\xi_i)} > \lambda/2 \right) &= (\pushforward{\pi}{P^n})^*\left (\norm{\sum_{\tau_i=1} f(\xi_i)} > \lambda/2 \right) \\
&= (P^k)^*\left (\norm{\sum_{i=1}^k f(\xi_i)} > \lambda/2 \right) 
\end{align*}
noting that the value of the right hand side depends on the set $\tau_i=1$ only through the value of $k$.  In particular the value is equal to $(P^n)^*\left (\norm{\sum_{i=1}^{\card{\tau_i=1}} f(\xi_i)} > \lambda/2 \right)$

Now by the previous claim we get

TODO:
\end{proof}

TODO: Clarify the general stochastic process setting versus the empirical process setting.  

When thinking about stochastic processes $X$, we often think of the index set $T$ as representing time.  In the theory of empirical processes that intuition has to be cast aside and one is usually thinking about the index set as being some set of functions $\mathcal{F}$.  In the standard setup assume that we have two probability spaces $(S, \mathcal{S}, P)$ and $(\Omega^\prime, \mathcal{A}^\prime, P^\prime)$.  For every $n \in \naturals$ we consider the product probability space
$(S^n \times \Omega^\prime, \mathcal{S}^n \otimes \mathcal{A}^\prime, P^n \otimes P^\prime)$ and for each $1 \leq i \leq n$ the coordinate projections $X_i : S^n \times \Omega^\prime \to S$.  We can then form the \emph{empirical measure} $\mathds{P}^n = \frac{1}{n} \sum_{i=1}^n \delta_{X_i}$ and the \emph{empirical process} 
$\mathds{G}^n = \sqrt{n}( \mathds{P}^n - P)$ (TODO: show that $\mathds{P}^n$ is measurable).  An alternative way of thinking about these objects is to use the random measures to integrate a measurable function on $S$, $\mathds{P}^n_f = \frac{1}{n} \sum_{i=1}^n f(X_i)$ and $\mathds{G}^n_f = \sqrt{n}(\mathds{P}^n_f - \expectation{f})$.  Clearly for each such $f$, $\mathds{P}^n_f$ is a sum of a composition of measurable functions hence is measurable; thus each of of $\mathds{P}^n$ and $\mathds{G}^n$ defines a stochastic process indexed by a set of measurable functions.  In some cases it is also useful to consider the \emph{partial sum process} $\sum_{i=1}^n f(X_i)$.   Note that as a special case of indexing by measurable functions is indexing by a set of characteristic functions (or equivalently indexing by a set of measurable subsets of $S$).  In the general theory of empirical process there are many results that aren't specific to processes defined above and in those cases we'll strive to formulate them in the most general way possible.

TODO: Actually some of the inequalities are not supposed to even require meaurability of the $X_t$.  So we aren't really even talking about stochastic processes here: simply families of functions!

\begin{defn}Let $(\Omega \times \Omega^\prime, \mathcal{A} \otimes \mathcal{A}^\prime, P \otimes P^\prime)$ be a product of probability spaces and $V$ be a vector space with seminorm $\norm{\cdot}$.  A stochastic process $X$ on time scale $T$ is said to be \emph{symmetric} if $X$ and $-X$ have the same distribution.
\end{defn}

TODO: Van der Vaart and Wellner have versions of these inequalities for symmetric processes; do we need those for the bootstrap?
\begin{lem}[Hoffman-J\o rgensen Inequalities]\label{HoffmanJorgensenInequalities}Let $(\Omega_i,\mathcal{A}_i, P_i)$ for each $i=1, \dotsc, n$ and $(\Omega^\prime, \mathcal{A}^\prime, P^\prime)$ be probability spaces and let $(\Omega, \mathcal{A}, P)$ be the product $\Omega_1 \times \dotsb \times \Omega_n \times \Omega^\prime$.  Let $T$ be an index set and for every $i=1, \dotsc, n$ suppose $X^i_t : \Omega \to \reals$ be the pullback of a function $\Omega_i \to \reals$ (i.e. $X^i_t$ depends only on the $i^{th}$ coordinate).  For any $t \in T$ and $k=1, \dotsc, n$ let $S^k_t = X^1_t + \dotsb + X^k_t$.  For any $\lambda,\eta>0$ we have
\begin{align*}
\oprobability{\max_{1 \leq k \leq n} \sup_{t \in T} \abs{S^k_t} > 3\lambda + \eta} 
&\leq \oprobability{\max_{1 \leq k \leq n} \sup_{t \in T} \abs{S^k_t} > \lambda}^2 + \oprobability{\max_{1 \leq k \leq n} \sup_{t \in T} \abs{X^k_t} > \eta}
\end{align*}
\end{lem}
\begin{proof}
Let for any $Y : T \to \reals$ let $\norm{Y} = \sup_{t \in T} \abs{Y_t}$.  
By Proposition \ref{OuterTailProbabilities} and Proposition \ref{BasicPropertiesOfMeasurableCover} we have
\begin{align*}
\oprobability{\max_{1 \leq k \leq n} \norm{S^k} > 3\lambda + \eta} 
&= \probability{\max_{1 \leq k \leq n} \norm{S^k}^* > 3\lambda + \eta} 
= \probability{\max_{1 \leq k \leq n} \norm{S^k}^* > 3\lambda + \eta} 
\end{align*}
and similarly with the other outer probabilities.  For each $k=1, \dotsc, n$ let $A_k$ be the event that $k$ is the first index with $\norm{S^{k}}^* > \lambda$:
\begin{align*}
A_k &= \left \lbrace \norm{S^1}^* \leq \lambda \right \rbrace \cap \dotsb \cap  
\left \lbrace \norm{S^{k-1}}^* \leq \lambda \right \rbrace 
cap \left \lbrace \norm{S^{k}}^* > \lambda \right \rbrace 
\end{align*}
Thus $\left \lbrace \max_{1 \leq k \leq n} \norm{S^k}^* > \lambda\right \rbrace$ is the disjoint union of $A_1, \dotsc, A_n$.  For every $k=1, \dotsc, n$, since $\norm{\cdot}$ is a norm we have by Lemma \ref{MeasurableCoverTriangleInequality} for every $j \geq k$,
\begin{align*}
\norm{S^j}^* &= \norm{S^{k-1} + X^k + S^j - S^k}^* 
\leq \norm{S^{k-1}}^* + \norm{X^k}^* + \norm{S^j - S^k}^*
\end{align*}
so (noting that of course $S^j-S^k=0$ when $k=j$),
\begin{align*}
A_k &\subset \left \lbrace \max_{1 \leq j \leq n} \norm{S^j}^* \leq \lambda + \sup_{1 \leq j \leq n} \norm{X^j}^* + \max_{k < j \leq n} \norm{S^j - S^k}^*  \right \rbrace \text{ a.s.}
\end{align*}
Thus on $A_k$, $\max_{1 \leq j \leq n} \norm{S^j}^* > 3\lambda + \eta$ implies either $\sup_{1 \leq j \leq n} \norm{X^j}^* > \eta$ or $\max_{k < j \leq n} \norm{S^j - S^k}^* > 2\lambda$ so by subadditivity and the fact that $A_k$ is independent of $\left \lbrace \max_{k < j \leq n} \norm{S^j - S^k}^* \right \rbrace$
\begin{align*}
&\probability{\max_{1 \leq j \leq n} \norm{S^j}^* > 3\lambda + \eta} 
= \sum_{k=1}^n \probability{\max_{1 \leq j \leq n} \norm{S^j}^* > 3\lambda + \eta ; A_k} \\
&\leq \sum_{k=1}^n \probability{\sup_{1 \leq j \leq n} \norm{X^j}^* > \eta ; A_k} + \sum_{k=1}^n \probability{\max_{k < j \leq n} \norm{S^j - S^k}^* > 2\lambda ; A_k} \\
&\leq \probability{\sup_{1 \leq j \leq n} \norm{X^j}^* > \eta ; \cup_{k=1}^n A_k} + \sum_{k=1}^n \probability{\max_{k < j \leq n} \norm{S^j - S^k}^* > 2\lambda} \probability{A_k} \\
&\leq \probability{\sup_{1 \leq j \leq n} \norm{X^j}^* > \eta} + \sum_{k=1}^n \probability{\max_{k \leq  j \leq n} \norm{S^j}^* > \lambda} \probability{A_k} \\
&\leq \probability{\sup_{1 \leq j \leq n} \norm{X^j}^* > \eta} + \probability{\max_{1 \leq  j \leq n} \norm{S^j}^* > \lambda}^2\\
\end{align*}
\end{proof}

\begin{thm}\label{HoffmanJorgensenMomentInequalities}Let $(\Omega_i,\mathcal{A}_i, P_i)$ for each $i=1, \dotsc, n$ and $(\Omega^\prime, \mathcal{A}^\prime, P^\prime)$ be probability spaces and let $(\Omega, \mathcal{A}, P)$ be the product $\Omega_1 \times \dotsb \times \Omega_n \times \Omega^\prime$.  Let $T$ be an index set and for every $i=1, \dotsc, n$ suppose $X^i_t : \Omega \to \reals$ be the pullback of a function $\Omega_i \to \reals$ (i.e. $X^i_t$ depends only on the $i^{th}$ coordinate).  For any $t \in T$ and $k=1, \dotsc, n$ let $S^k_t = X^1_t + \dotsb + X^k_t$.  Then for any $0 < p < \infty$ there exist a constant $C_p$ such that
\begin{align*}
\oexpectation{\max_{1 \leq k \leq n} \norm{S^k}^p} &\leq C_p \left( \oexpectation{\max_{1 \leq k \leq n} \norm{X^k}^p} + F^{-1}(1/(24^p))^p \right)
\end{align*}
where $F(u) = \oprobability{\max_{1 \leq k \leq n} \norm{S^k}>u}$.
\end{thm}
\begin{proof}
We use Theorem \ref{ExistenceMeasurableCover}, the Lemma \ref{TailsAndExpectations}, a change of variables and Lemma \ref{HoffmanJorgensenInequalities} to see that for all $\lambda > 0$,
\begin{align*}
&\oexpectation{\max_{1 \leq k \leq n} \norm{S^k}^p} 
= \expectation{\max_{1 \leq k \leq n} (\norm{S^k}^*)^p}  
= p \int_0^\infty t^{p-1} \probability{\max_{1 \leq k \leq n} \norm{S^k}^* > t}  dt \\
&= 4^p p \int_0^\infty t^{p-1} \probability{\max_{1 \leq k \leq n} \norm{S^k}^* > 4t}  dt \\
&= 4^p p \int_0^\lambda t^{p-1} \probability{\max_{1 \leq k \leq n} \norm{S^k}^* > 4t}  dt  + 4^p p\int_\lambda^\infty t^{p-1} \probability{\max_{1 \leq k \leq n} \norm{S^k}^* > 4t}  dt \\
&\leq 4^p p \int_0^\lambda  t^{p-1}  dt  + 4^p p \int_\lambda^\infty t^{p-1} \probability{\max_{1 \leq k \leq n} \norm{S^k}^* > t}^2  dt \\
&\qquad + 4^p p \int_\lambda^\infty t^{p-1} \probability{\max_{1 \leq k \leq n} \norm{X^k}^* > t}  dt \\
&\leq (4\lambda)^p  + 4^p p \probability{\max_{1 \leq k \leq n} \norm{S^k}^* > \lambda} \int_0^\infty t^{p-1} \probability{\max_{1 \leq k \leq n} \norm{S^k}^* > t}  dt \\
&\qquad + 4^p p \int_0^\infty t^{p-1} \probability{\max_{1 \leq k \leq n} \norm{X^k}^* > t}  dt \\
&= (4\lambda)^p  + 4^pF(\lambda) \oexpectation{\max_{1 \leq k \leq n} \norm{S^k}^p} + 4^p \oexpectation{\max_{1 \leq k \leq n} \norm{X^k}^p}
\end{align*}
Now pick $\lambda = u_p = F^{-1}(\frac{1}{24^p})$ so that $4^p F(\lambda) \leq 1/2$ and rearrange terms to get
\begin{align*}
\oexpectation{\max_{1 \leq k \leq n} \norm{S^k}^p}  \leq 2 4^p  \oexpectation{\max_{1 \leq k \leq n} \norm{X^k}^p} + 2 (4 F^{-1}(1/(24^p)))^p
\end{align*}
\end{proof}

\begin{lem}\label{JensenConsequencesForOuterExpectations}Let $(\Omega, \mathcal{A})$ be a measurable space with probability measures $P$ and $Q$
\begin{itemize}
\item[(i)]If $\mathcal{F} \subset \mathcal{L}^1(\Omega, \mathcal{A}, P)$ then 
\begin{align*}
\norm{\int f \, dP}_{\mathcal{F}} &\leq \int^* \norm{f}_{\mathcal{F}} \, dP
\end{align*}
\item[(ii)]If $\mathcal{F} \subset \mathcal{L}^0(\Omega, \mathcal{A})$  such that for all $f \in \mathcal{F}$,  $\int f \, dQ=0$ then
\begin{align*}
\int^* \norm{f(x)}_{\mathcal{F}} \, (P \otimes Q)(dx,dy) &\leq \int^* \norm{f(x)+f(y)}_{\mathcal{F}} \, (P \otimes Q)(dx,dy) 
\end{align*}
\end{itemize}
\end{lem}
\begin{proof}
To prove (i), since $\abs{f} \leq \sup_{f \in \mathcal{F}} \abs{f} \leq \left( \sup_{f \in \mathcal{F}} \abs{f} \right)^*$ we have for all $f \in \mathcal{F}$,
\begin{align*}
\abs{\int f \, dP} &\leq \int \abs{f} \, dP \leq \int \left( \sup_{f \in \mathcal{F}} \abs{f} \right)^* \, dP = \int^* \norm{f}_{\mathcal{F}} \, dP
\end{align*}
Now take the supremum over all $f$ to conclude that $\norm{\int f \, dP}_{\mathcal{F}} \leq \int^* \norm{f}_{\mathcal{F}} \, dP$.

To see (ii), let $x \in \Omega$ be fixed for the moment.  Thinking of $y \in \Omega$ as variable,
\begin{align*}
\abs{f(x) + f(y)} &\leq \sup_{f \in \mathcal{F}} \abs{f(x) + f(y)} \leq \left(\sup_{f \in \mathcal{F}} \abs{f(x) + f(y)} \right)^*
\end{align*}
where the measurable cover on the right hand side is taken with respect to $Q$ (and depends on $x \in \Omega$).  Thus $\int \abs{f(x) + f(y)} \, Q(dy) \leq \int^* \sup_{f \in \mathcal{F}} \abs{f(x) + f(y)} \, Q(dy)$ and taking the supremum over $f \in \mathcal{F}$ we get for each $x \in \Omega$,
\begin{align}\label{JensenConsequencesForOuterExpectations:Ineq}
\norm{\int \abs{f(x) + f(y)} \, Q(dy)}_{\mathcal{F}} &\leq \int^* \sup_{f \in \mathcal{F}} \abs{f(x) + f(y)} \, Q(dy) = \int^* \norm{f(x)+f(y)}_{\mathcal{F}} \, Q(dy)
\end{align}
Now for a fixed $x \in \Omega$ and $f \in \mathcal{F}$ we know that the function $z \mapsto \abs{f(x) + z}$ is convex so by the Jensen's Inequality Theorem \ref{Jensen} we get 
\begin{align*}
\abs{f(x)} &= \abs{f(x) + \int f(y) \, Q(dy)} \leq \int \abs{f(x) + f(y)} \, Q(dy) 
\end{align*}
Taking the supremum over $f \in \mathcal{F}$ and the outer integral over $x$, we get using Corollary \ref{cor:OuterExpectationRule}, \eqref{JensenConsequencesForOuterExpectations:Ineq} and the Tonelli Theorem \ref{FubiniOuterExpectation}
\begin{align*}
\int^* \norm{f(x)}_{\mathcal{F}} \, (P\otimes Q)(dx, dy) &= \int^* \norm{f(x)}_{\mathcal{F}} \, P(dx) \leq \int^* \norm {\int \abs{f(x) + f(y)} \, Q(dy)}_{\mathcal{F}}  \, P(dx) \\
&\leq \int^* \left[ \int^* \norm{f(x) + f(y)}_{\mathcal{F}} \, Q(dy) \right ] \, P(dx) \\
&\leq \int^* \norm{f(x) + f(y)}_{\mathcal{F}}  \, (P \otimes Q)(dx,dy)
\end{align*}

TODO: I think we actually need the following generalization of (ii):
To see (ii), let $x_1 \in \Omega$ be fixed for the moment.  Thinking of $x_2, \dotsc, x_n \in \Omega$ as variable,
\begin{align*}
\abs{f(x_1) + \dotsb + f(x_n)} &\leq \sup_{f \in \mathcal{F}} \abs{f(x_1) + \dotsb + f(x_n)} \leq \left(\sup_{f \in \mathcal{F}} \abs{f(x_1) + \dotsb + f(x_n)} \right)^*
\end{align*}
where the measurable cover on the right hand side is taken with respect to $P_2 \otimes \dotsb \otimes P_n$ (and depends on $x_1 \in \Omega$).  Thus 
\begin{align*}
&\int \abs{f(x_1) + \dotsb + f(x_n)} \, (P_2 \otimes \dotsb \otimes P_n)(dx_2, \dotsc, dx_n) \\
&\qquad \leq \int^* \sup_{f \in \mathcal{F}} \abs{f(x_1) +  \dotsb + f(x_n)} \, (P_2 \otimes \dotsb P_n)(dx_2, \dotsc, dx_n)
\end{align*} 
and taking the supremum over $f \in \mathcal{F}$ we get for each $x_1 \in \Omega$,
\begin{align}
&\norm{\int \abs{f(x_1) + \dotsb + f(x_n)} \, (P_2 \otimes \dotsb \otimes P_n)(dx_2, \dotsc, dx_n)}_{\mathcal{F}} \\
&\qquad \leq \int^* \sup_{f \in \mathcal{F}} \abs{f(x_1) + \dotsb + f(x_n)} \, (P_2 \otimes \dotsb \otimes P_n)(dx_2, \dotsc, dx_n) \\
&\qquad = \int^* \norm{f(x_1)+ \dotsb + f(x_n)}_{\mathcal{F}} \, (P_2 \otimes \dotsb \otimes P_n) (dx_2, \dotsc, dx_n)
\end{align}\label{eq:JensenConsequencesForOuterExpectations:Ineq2}
Now for a fixed $x_1 \in \Omega$ and $f \in \mathcal{F}$ we know that the function $z \mapsto \abs{f(x_1) + z}$ is convex so by the Jensen's Inequality Theorem \ref{Jensen} we get 
\begin{align*}
\abs{f(x_1)} &= \abs{f(x_1) + \int ( f(x_2) + \dotsb + f(x_n) )\, (P_2 \otimes \dotsb \otimes P_n)(dx_2, \dotsc, dx_n)} \\
&\leq \int \abs{f(x_1) + \dotsb + f(x_n)} \, (P_2 \otimes \dotsb \otimes P_n)(dx_2, \dotsc, dx_n) 
\end{align*}
Taking the supremum over $f \in \mathcal{F}$ and the outer integral over $x_1$, we get using Corollary \ref{cor:OuterExpectationRule}, \eqref{eq:JensenConsequencesForOuterExpectations:Ineq2}, monotonicity of outer expectation (Proposition \ref{OuterExpectationMonotonicity}) and the Tonelli Theorem \ref{FubiniOuterExpectation}
\begin{align*}
&\int^* \norm{f(x_1)}_{\mathcal{F}} \, (P_1 \otimes \dotsb \otimes P_n)(dx_1, \dotsc, dx_n) 
= \int^* \norm{f(x_1)}_{\mathcal{F}} \, P(dx_1) \\
&\leq \int^* \norm {\int \abs{f(x_1) +  \dotsb + f(x_n)} \, (P_2 \otimes \dotsb \otimes P_n)(dx_2, \dotsc, dx_n)}_{\mathcal{F}}  \, P_1(dx_1) \\
&\leq \int^* \left[ \int^* \norm{f(x_1) + \dotsb + f(x_n)}_{\mathcal{F}} \, (P_2 \otimes \dotsb \otimes P_n)(dx_2, \dotsc, dx_n) \right ] \, P_1(dx_1) \\
&\leq \int^* \norm{f(x_1) + \dotsb + f(x_n)}_{\mathcal{F}}  \, (P_1 \otimes \dotsb \otimes P_n)(dx_1, \dotsc, ,dx_n)
\end{align*}
\end{proof}

\subsection{Poissonization}

\begin{lem}[Le Cam's Poissonization Lemma]\label{LeCamPoissonization}For each $i \in \naturals$ let $X^i$ be a centered stochastic process in $\reals$ indexed by a set $T$ defined on a probability space $(\Omega_i, \mathcal{A}_i, P_i)$.  For each $i \in \naturals$ let $X^{i,1}, X^{i,2}, \dotsc$ be independent copies of $X^i$ so that the $X^{i,j}$ are defined on the product $(\Omega, \mathcal{A}, P) = (\prod_{i=1}^\infty \Omega_i^\infty, \otimes_{i=1}^\infty \mathcal{A}_i^{\infty}, \otimes_{i=1}^\infty P_i^\infty$.  Suppose that $N_1, N_2, \dotsb$ are independent Poisson random variables of rate $1$ defined on $(\Omega^\prime, \mathcal{A}^\prime, Q)$.  We consider the $X^{i,j}$ and $N_i$ to be mutually independent random elements defined on $\Omega \times \Omega^\prime$.  Then
\begin{align*}
\oexpectation{\norm{\sum_{i=1}^n X^i}_T} &\leq \frac{e}{e-1} \oexpectation{\norm{\sum_{i=1}^n \sum_{j=1}^{N_i} X^{i,j}}_T}
\end{align*}
\end{lem}
\begin{proof}
Since each $N_i$ is Poisson with rate $1$ we have 
\begin{align*}
\expectation{N_i  \minop 1} &= \frac{1}{e} \sum_{n=0}^\infty \frac{(n \minop 1)}{n!} = \frac{1}{e} \sum_{n=1}^\infty \frac{1}{n!} = \frac{e-1}{e}
\end{align*}
Therefore by (i) of Lemma \ref{JensenConsequencesForOuterExpectations} and the Tonelli Theorem \ref{FubiniOuterExpectation} we have
\begin{align*}
\frac{e-1}{e} \oexpectation{\norm{\sum_{i=1}^n X^i}_T} 
&= \int^* \norm{ \sum_{i=1}^n \frac{e-1}{e}  X^i}_T \, dP \\
&= \int^* \norm{\int \sum_{i=1}^n (N_i \minop 1)  X^i \, dQ}_T \, dP \\
&\leq \int^* \int \norm{ \sum_{i=1}^n (N_i \minop 1) X^i}_T dQ \, dP \\
&\leq \int^* \norm{ \sum_{i=1}^n (N_i \minop 1) X^i}_T \, d(P \otimes Q) \\
&= \int \left [ \int^* \norm{ \sum_{i=1}^n (N_i \minop 1) X^i}_T \, dP\right ] \, dQ \\
\end{align*}

Fix $N_1, \dotsc, N_n$.  The idea at this point is for $N_i=0$ replace the term $(N_i \minop 1) X^i=0$ by $0$, for $N_i=1$ we replace $(N_i \minop 1) X^i=X^i$ by $X^{i,1}$, for $N_i=2$ we replace $(N_i \minop 1) X^i$ by $X^{i,1} + X^{i,2}$ and so on.  For the first two substitutions we obviously haven't changed anything, however for the remaining substitutions we need to argue as in (ii) of Lemma \ref{JensenConsequencesForOuterExpectations} to get
\begin{align*}
\frac{e-1}{e} \oexpectation{\norm{\sum_{i=1}^n X^i}_T}  &\leq \int \left [ \int^* \norm{\sum_{i=1}^n \sum_{j=1}^{N_i} X^{i,j}}_T \, dP \right ] \, dQ
\end{align*}

Here is a detailed proof of the variant of  Lemma \ref{JensenConsequencesForOuterExpectations}  that is needed.  To simplify notation, let $\mu$ be the product $P_{11} \otimes \dotsb \otimes P_{n1}$
and let $\eta$ denote the product of $P_{ij}$ for $i=1, \dotsc, n$ and $j=2,\dotsc, N_i$.  Let $\omega_{11}, \dotsc, \omega_{n1} \in \Omega$ be fixed for the moment.  Thinking of $\omega_{i2}, \dotsc, \omega_{iN_i} \in \Omega$ as variable,
\begin{align*}
\abs{\sum_{i=1}^n \sum_{j=1}^{N_i} X_t^{i,j}(\omega_{ij})} &\leq \sup_{t \in T} \abs{\sum_{i=1}^n \sum_{j=1}^{N_i} X_t^{i,j}(\omega_{ij})} \leq \left(\sup_{t \in T} \abs{\sum_{i=1}^n \sum_{j=1}^{N_i} X_t^{i,j}(\omega_{ij})} \right)^*
\end{align*}
where the measurable cover on the right hand side is taken with respect to $\eta$ (and depends on $\omega_{11}, \dotsc, \omega_{n1} \in \Omega$).  Thus 
\begin{align*}
\int \abs{\sum_{i=1}^n \sum_{j=1}^{N_i} X_t^{i,j}(\omega_{ij})} \, d\eta 
&\leq \int^* \sup_{t \in T} \abs{\sum_{i=1}^n \sum_{j=1}^{N_i} X_t^{i,j}(\omega_{ij})} \, d\eta
&=\int^* \norm{\sum_{i=1}^n \sum_{j=1}^{N_i} X_t^{i,j}(\omega_{ij})}_T \, d\eta
\end{align*} 
and taking the supremum over $t \in T$ we get for each $\omega_{11}, \dotsc, \omega_{n1} \in \Omega$,
\begin{align}\label{eq:LeCamPoissonizationIneq}
\norm{\int \abs{\sum_{i=1}^n \sum_{j=1}^{N_i} X_t^{i,j}(\omega_{ij})} \, d\eta}_{T} 
&\leq \int^* \norm{\sum_{i=1}^n \sum_{j=1}^{N_i} X_t^{i,j}(\omega_{ij})}_T \, d\eta
\end{align} 
Now for a fixed $\omega_{11}, \dotsc, \omega_{n1}  \in \Omega$ and $t \in T$ we know that the function $z \mapsto \abs{\sum_{i=1}^n X^{i,1}_t(\omega_{i1})+ z}$ is convex so by the Jensen's Inequality Theorem \ref{Jensen} and the fact that $X^i$ is centered we get 
\begin{align*}
\abs{\sum_{i=1}^n X^{i,1}_t(\omega_{i1})} &= \abs{\sum_{i=1}^n X^{i,1}_t(\omega_{i1}) + \int \sum_{i=1}^n \sum_{j=2}^{N_i} X_t^{i,j}(\omega_{ij}) \, d\eta} 
\leq \int \abs{\sum_{i=1}^n \sum_{j=1}^{N_i} X_t^{i,j}(\omega_{ij})} \, d\eta
\end{align*}
Taking the supremum over $t \in T$ yields
\begin{align*}
\norm{\sum_{i=1}^n X^{i,1}_t(\omega_{i1})}_T &\leq \norm{\int \abs{\sum_{i=1}^n \sum_{j=1}^{N_i} X_t^{i,j}(\omega_{ij})} \, d\eta}_T
\end{align*}
Now take the outer integral over $\omega_{11}, \dotsc, \omega_{n1}$ and use Corollary \ref{cor:OuterExpectationRule}, equation \eqref{eq:LeCamPoissonizationIneq}, monotonicity of outer expectation (Proposition \ref{OuterExpectationMonotonicity}) and the Tonelli Theorem \ref{FubiniOuterExpectation} to get
\begin{align*}
&\int^* \norm{\sum_{i=1}^n X^i_t(\omega_{i1})}_{T} \, dP
= \int^* \norm{\sum_{i=1}^n X^i_t(\omega_{i1})}_{T} \, d\mu \\
&\leq \int^* \norm {\int \abs{\sum_{i=1}^n \sum_{j=1}^{N_i} X_t^{i,j}(\omega_{ij})} \, d\eta}_{T}  \, d\mu\\
&\leq \int^* \left[ \int^* \norm{\sum_{i=1}^n \sum_{j=1}^{N_i} X_t^{i,j}(\omega_{ij})}_{T} \, d\eta \right ] \, d\mu \\
&\leq \int^* \norm{\sum_{i=1}^n \sum_{j=1}^{N_i} X_t^{i,j}(\omega_{ij})}_{T}  \, d(\mu \otimes \eta) \\
&\leq \int^* \norm{\sum_{i=1}^n \sum_{j=1}^{N_i} X_t^{i,j}(\omega_{ij})}_{T}  \, dP \\
\end{align*}
\end{proof}

If we let $V$ be separable Banach space then addition makes $V$ into a measurable Abelian group (TODO: Show this somewhere; the key is separability is required so that $\mathcal{B}(V \times V) = \mathcal{B}(V) \otimes \mathcal{B}(V)$ and therefore continuity of addition implies Borel measurability).  By Lemma \ref{Convolution}, given two $\sigma$-finite signed Borel measures $\mu$ and $\nu$ on $V$ we can define the convolution of $\mu$ and $\nu$ as 
\begin{align*}
(\mu * \nu)(A) &= \int \mu(A - v) \, \nu(dv) 
\end{align*}
and the convolution operation is associative and commutative.  
\begin{defn}Given a finite signed measure $\mu$, for any $k \in \naturals$ we can take the $k$-fold convolution $\mu^k = \mu * \dotsb * \mu$.  Define $\mu^0 = \delta_0$ and the exponential $e^{\mu} = \sum_{k=0}^\infty \frac{\mu^k}{k!}$
\end{defn}

Note that it is simple to see that the convolution of finite signed measures is finite and in particular 
\begin{align*}
(\mu * \nu)(V) &= \int \mu(V - v) \, \nu(dv) = \mu(V - v) \int \, \nu(dv) = \mu(V) \nu(V)
\end{align*}
It follows that $e^{\mu}(V) = \sum_{k=0}^\infty \frac{\mu(V)^k}{k!} = e^{\mu(V)}$.  Thus if $\mu \geq 0$ and we define $\poisop{\mu} = e^{-\mu(V)} e^\mu $ then $\pois{\mu}$ is a probability measure.

\begin{prop}\label{PoissonOperator}Let $\mu$ and $\nu$ be finite measures on a separable Banach space $V$ then $\pois{\mu + \nu} = \pois{\mu} * \pois{\nu}$ and for every $v \in V$ and $c > 0$ we have $\pois{c\delta_v} = \law{N_c v}$ where $N_c$ is a Poisson random variable with parameter $c$.
\end{prop}
\begin{proof}
Consider the convolution
\begin{align*}
((\mu + \nu) * (\mu + \nu))(A) &= \int (\mu + \nu)(V - v) \, (\mu + \nu)(dv) \\
&= \int \mu(V - v) \, \mu(dv) + \int \mu(V - v) \, \nu(dv) + \int \nu(V - v) \, \mu(dv) + \int \nu(V - v) \, \nu(dv) \\
&= (\mu * \mu)(A) + 2 (\mu * \nu)(A) + (\nu * \nu)(A)
\end{align*}
From which it follows that $e^{\mu + \nu} = e^{\mu} * e^{\nu}$ and therefore 
\begin{align*}
\pois{\mu + \nu} &= e^{-(\mu + \nu)(V)} e^{\mu + \nu} = e^{-\mu(V)} e^{\mu} * e^{-\nu(V)} e^{\nu} = \pois{\mu} * \pois{\nu}
\end{align*}

Now look at the self convolution of a point measure,
\begin{align*}
(\delta_v * \delta_v)(A) &= \int \delta_v(A - w) \, \delta_v(dw) = \delta_v(A - v) = \delta_{2v}(A)
\end{align*}
so by induction $\delta_v^k = \delta_{kv}$ and thus 
\begin{align*}
e^{c \delta_v} (A) &= \sum_{k=0}^\infty \frac{c^k \delta_{kv}(A)}{k!} = \sum_{k=0}^\infty \delta_{kv}(A) \probability{N_c = k} e^c \\
&= e^c \probability{N_c v \in A} = e^c \law{N_c v}(A) 
\end{align*}
Since $\pois{c \delta_v}$ is proportional to $e^{c \delta_v}$ is a probability measure it follows that $\pois{c \delta_v} = \law{N_c v}$.
\end{proof}


TODO: At this point Dudley is bringing in separability assumption; how will that play out when combined with inequalities defined in terms of general processes $X^i$?  Do we assume that $X^i$ takes values in a separable subset of $\ell^\infty(T)$?  For example Dudley combines convolutions with Le Cam's Lemma (the latter of which was stated for general stochastic processes in $\reals^T$ which isn't even a normed space).

\subsection{Triangular Array}




