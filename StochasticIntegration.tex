\section{Stochastic Integration}

\subsection{Local Martingales}

\begin{defn}Let $M_t$ be an $\mathcal{F}$-adapted process, we say $M$ is a \emph{local martingale} if there exists a sequence of optional times $\tau_n$ such that $\tau_n \uparrow \infty$ a.s. and $M^{\tau_n} - M_0$ is an $\mathcal{F}$-martingale for all $n$.  We say that $\tau_n$ is a \emph{localizing sequence} for $M$.
\end{defn}

It is useful  to note that a local martingale can be localized to martingales with nice properties.  In the general case we can always assume that we localize to a uniformly integrable martingale.
\begin{lem}\label{LocalMartingaleLocalizeToUniformlyIntegrable}Let $M$ be a local martingale with a localizing sequence $\tau_n \uparrow \infty$, then $\tau_n \wedge n$ is a localizing sequence such that $M^{\tau_n \wedge n}$ is a uniformly integrable martingale for each $n \in \naturals$.
\end{lem}
\begin{proof}
It is clear that $\tau_n \wedge n$ is a sequence of optional times such that $\tau_n \wedge n \uparrow \infty$.  Moreover, since $(M-M_0)^{\tau_n}$ is a cadlag martingale, $n$ is a bounded optional time and $(M-M_0)^{\tau_n \wedge n}_t = (M-M_0)_{\tau_n \wedge n \wedge t} = ((M-M_0)^\tau_n)^n_t$ the Optional Sampling Theorem \ref{OptionalSamplingContinuous} tells us that $(M-M_0)^{\tau_n \wedge n}$ is closable hence uniformly integrable.
\end{proof}

Even better is the fact is that continuous local martingales can always be localized to bounded martingales.  This is one of the general facts that makes the theory of continuous local martingales easier than the general case.
\begin{lem}\label{ContinuousLocalMartingaleLocalizeToBounded}Let $M$ be a continuous local martingale and for each $n \in \integers_+$ let $\tau_n = \inf \lbrace t \geq 0 \mid \abs{M_t} \geq n \rbrace$ then $\tau_n$ is a localizing sequence for $M$.
\end{lem}
\begin{proof}
By continuity and $\mathcal{F}$-adaptedness of $M$ and the fact that $[t, \infty)$ is closed we know that $\tau_n$ is an optional time (Lemma \ref{HittingTimesContinuous}). It is clear that $\abs{M_t} \geq n$ implies $\abs{M_t} \geq n-1$ and therefore $\tau_n$ is an increasing sequence.  By continuity of $M$ we know that $M$ is bounded on bounded intervals and therefore $\tau_n \uparrow \infty$ a.s.  

It remains to show that $(M - M_0)^{\tau_n}$ is a martingale for every $n \in \integers_+$.   Let $\sigma_m$ be a localizing sequence for $M$.  From Optional Sampling we know that $(M - M_0)^{\tau_n \wedge \sigma_m} = ((M-M_0)^{\sigma_m})^{\tau_n}$ is a martingale for every $m,n \in \integers_+$.  Furthermore for fixed $n$ and every $m \in \integers_+$ since $\sigma_m \uparrow \infty$ a.s. we know that $(M-M_0)^{\tau_n \wedge \sigma_m}_t \toas (M-M_0)^{\tau_n}_t$.  Morever $\abs{(M-M_0)^{\tau_n \wedge \sigma_m}_t } = \abs{M_{\tau_n \wedge \sigma_m \wedge t} - M_0} \leq \abs{M_0} + n$.  Since $M_0$ is integrable (TODO: Do we really know this with the Kallenberg definition of a local martingale?; if not what replaces it do we define $\tau_n = \inf \lbrace t \geq 0 \mid \abs{M_t - M_0} \geq n \rbrace$?) so that by Dominated Convergence we get $(M-M_0)^{\tau_n \wedge \sigma_m}_t \tolp{1} (M-M_0)^{\tau_n}_t$ as well. Using both forms of convergence and the martingale property of $M^{\tau_n \wedge \sigma_m}$, for each $s < t$ we get the equality
\begin{align*}
\cexpectationlong{\mathcal{F}_s}{(M - M_0)^{\tau_n}_t} &= \lim_{m \to \infty} \cexpectationlong{\mathcal{F}_s}{(M - M_0)^{\tau_n \wedge \sigma_m }_t} \\
&= \lim_{m \to \infty} (M - M_0)^{\tau_n \wedge \sigma_m }_s =  (M - M_0)^{\tau_n }_s \text{ a.s.}
\end{align*}
which shows that $M^{\tau_n}$ is a martingale.
\end{proof}

It will occasionally be important to know when we may conclude a local martingale is actually a martingale.  The simplest case is that of a bounded local martingale (not necessarily continuous).
\begin{lem}\label{BoundedLocalMartingaleIsMartingale}Let $M$ be a bounded local martingale then it follows that $M$ is a uniformly integrable martingale.
\end{lem}
\begin{proof}
Let $\tau_n$ be a localizing sequence for $M$ so that $M_{t \wedge \tau_n} - M_0$ is a martingale so that $\cexpectationlong{\mathcal{F}_s}{M_{t \wedge \tau_n} - M_0} = M_{s \wedge \tau_n} - M_0$ for every $s < t$.  Now boundedness of $M$ implies boundedness of $M_{t \wedge \tau_n} - M_0$ and therefore we may apply Dominated Convergence for conditional expectations (Lemma \ref{DCTConditional}) and the fact that $\tau_n \uparrow \infty$ a.s. to conclude that 
\begin{align*}
\cexpectationlong{\mathcal{F}_s}{M_{t} - M_0} &= \lim_{n \to \infty} \cexpectationlong{\mathcal{F}_s}{M_{t \wedge \tau_n} - M_0} = \lim_{n\to \infty} M_{s \wedge \tau_n} - M_0 =  M_{s} - M_0 
\end{align*}
almost surely.  Thus $M$ is a martingale.  Since $M$ is bounded, in fact it is uniformly integrable by Example \ref{DominatedImpliesUniformlyIntegrable}
\end{proof}

\begin{lem}Let $\mathcal{F}$ be a right continuous filtration, $M$ be a cadlag $\mathcal{F}$-local martingale with localizing sequence $\tau_n$ and let $\sigma_n$ be an arbitrary sequence of bounded optional times such that $\sigma_n \uparrow \infty$, then $\tau_n \wedge \sigma_n$ is a localizing sequence for $M$.  In particular the space of cadlag $\mathcal{F}$-local martingales is a linear space.
\end{lem}
\begin{proof}
First we claim that every local martingale $M$ has localizing sequence of bounded optional times.  This follows from picking an arbitrary localizing sequence $\tau_n$ and then noting that $\tau_n \wedge n$ is also a localizing sequence as $\tau_n \wedge n \uparrow \infty$ a.s. and $M^{\tau_n \wedge n} -M_0 = (M^{\tau_n})^n - M_0$ is a martingale from Optional Sampling (Theorem \ref{OptionalSamplingContinuous}) since $M^{\tau_n}$ is a cadlag martingale, $\mathcal{F}$ is right continuous and $n$ is a bounded optional time.

Given $\tau_n$ and $\sigma_n$ as in the hypothesis and by our first claim we assume that each $\tau_n$ and $\sigma_n$ is bounded.  It is clear that $\tau_n \wedge \sigma_n$ is a sequence of optional times such that $\tau_n \wedge \sigma_n \uparrow \infty$ and again applying Optional Sampling we see that $M^{\tau_n \wedge \sigma_n} -M_0 = (M^{\tau_n})^{\sigma_n} - M_0$ is a martingale.  

Lastly if we are given $M$ and $N$ local martingales, take $\tau_n$ and $\sigma_n$ to be bounded localizing sequences for $M$ and $N$ respectively and by the previous claim, we know that $\tau_n \wedge \sigma_n$ is a joint localizing sequence for $M$ and $N$.   Therefore $(aM + bN)^{\tau_n \wedge \sigma_n} - a M_0 - b N_0$ is a martingale for all $n \geq 0$.
\end{proof}

\begin{lem}\label{LocalMartingaleLocalProperty}Let $\tau_n$ be a sequence of optional times such that $\tau_n \uparrow \infty$ a.s.  and let $M$ be an $\mathcal{F}$-adapted process.  Then $M$ is a local martingale if an only if $M^{\tau_n}$ is for all $n\geq 0$.
\end{lem}
\begin{proof}
TODO:
\end{proof}

\begin{lem}\label{ContinuousLocalMartingaleBoundedVariation}Let $M$ be a continuous local martingale with locally bounded variation then $M = M_0$ a.s.
\end{lem}
\begin{proof}
We first reduce to the case in which $M$ is a martingale with locally bounded variation and $M_0=0$.  Let $\tau_n$ be a localizing sequence for $M$ then if we can show that $M_{\tau_n \wedge t} - M_0=0$ a.s. for all $n \geq 0$ and $t \geq 0$ then as $\tau_n \to \infty$ we can conclude that $M_t = M_0$ a.s. for all $t \geq 0$.

Next note that since $M$ is locally of bounded variation we have optional times $\tau_n$ such that $\tau_n \uparrow \infty$ such that $M^{\tau_n}$ is of bounded variation.  This implies that $M$ is of bounded variation on every interval $[0,t]$.  Therefore we can define the total variation process $V_t = TV_0^t(M)$.  Since $M$ is continuous, $V_t$ is continuous (Lemma \ref{ContinuityOfTotalVariation}) and by defintion of total variation it is clear that $V_t$ is $\mathcal{F}$-adapted.  Now define $\sigma_n = \inf \lbrace t \geq 0 \mid V_t = n \rbrace$; we know by continuity of $V_t$ that $\sigma_n$ is an optional time (Lemma \ref{HittingTimesContinuous}) and that $M_{\sigma_n \wedge t}$ is a continuous martingale.  Since $M$ is of locally finite variation we know that $\sigma_n \uparrow \infty$ and as before if we can show that $M_{\sigma_n \wedge t}=0$ a.s. for all $n \geq 0$ and $t \geq 0$ then it will follow that $M_t = 0$ for all $t \geq 0$.

Now we have reduced to the case in which $M$ is a continuous martingale with $M_0=0$ and bounded variation.  So fix $t > 0$ and define the partition $t_{n,k} = kt/n$ for all $n>0$ and $k=0, 1, \dotsc, n$.  If we define
\begin{align*}
\zeta_n &= \sum_{k=1}^n \left(M_{t_{n,k}} - M_{t_{n,k-1}}\right)^2 \leq V_t \max_{1 \leq k \leq n} \abs{M_{t_{n,k}} - M_{t_{n,k-1}}}
\end{align*}
then using the continuity of $M$ we know that $M$ is uniformly continuous on $[0,t]$ and therefore we have $\lim_{n \to \infty} \zeta_n = 0$ a.s.  Moreover we have
\begin{align*}
\zeta_n &\leq \sum_{k=1}^n \sum_{j=1}^n \abs{M_{t_{n,k}} - M_{t_{n,k-1}}} \abs{M_{t_{n,j}} - M_{t_{n,j-1}}} = V_t^2
\end{align*}
Since $V_t$ is bounded we can apply Dominated Convergence, the martingale property of $M_t$ and the fact that $M_0=0$ to conclude
\begin{align*}
0 &= \lim_{n \to \infty} \expectation{\zeta_n} = \sum_{k=1}^n \expectation{M_{t_{n,k}}^2} - \expectation{M_{t_{n,k-1}}^2} = \expectation{M_t^2}
\end{align*}
and from this we conclude that $M_t = 0$ a.s.  Taking the union of a countable number of sets of probability zero we see that almost surely $M_q = 0$ for all $q \in \rationals_+$.  Since $M_t$ is continuous we conclude that almost surely $M_t = 0$ for all $t \in \reals_+$.
\end{proof}

\subsection{Stieltjes Integrals}

There are a few simple facts about Stieltjes integrals that we want to describe in the stochastic setting as they will play a part in the general theory of stochastic integration.  First we record the formula for the restriction of a Lebesgue-Stieltjes measure to an interval.
\begin{lem}\label{RestrictionOfLebesgueStieltjesMeasure}Let $F$ be a right continuous function of bounded variation on $[a,b]$, let $[c,d] \subset [a,b]$.  If we let $\mu_F$ denote the signed Lebesgue-Stieltjes measure associated with $F$ and we let
\begin{align*}
F^{[c,d]}(s) &=
F((s \wedge d) \vee c) = 
\begin{cases}
F(c) & \text{if $s < c$}\\
F(s) & \text{if $c \leq s \leq d$} \\
F(d) & \text{if $d < s$} \\
\end{cases}
\end{align*}
then $F^{[c,d]}$ is right continuous of bounded variation on $[a,b]$ and $\mu_F \mid_{[c,d]} = \mu_{F^{[c,d]}}$.  
\end{lem}
\begin{proof}
First suppose that $F$ is non-decreasing and right continuous.  It is elementary that $F^{[c,d]}$ is also non-decreasing and right continuous.  For any half open interval $(x,y] \subset [a,b]$ we have 
\begin{align*}
\mu_F\mid_{[c,d]}((x,y]) &= \mu_F ([c,d] \cap (x,y]) = \mu_F ((d \wedge x) \vee c, (d \wedge y) \vee c]) \\
&= F((d \wedge y) \vee c) - F((d \wedge x) \vee c) = F^{[c,d]}(y) - F^{[c,d]}(x) = \mu_{F^{[c,d]}}((x,y])
\end{align*}
and as we know that $\mu_F \mid_{[c,d]}$ is locally finite, by Lemma \ref{LebesgueStieltjesMeasure} we get $\mu_F\mid_{[c,d]} = \mu_{F^{[c,d]}}$.

In the case of $F$ is right continuous  of bounded variation, then if we write $F = F_+ - F_-$ as a difference of right continuous non-decreasing functions then it is also true $F^{[c,d]} = F^{[c,d]}_+ - F^{[c,d]}_-$ and clearly each $F^{[c,d]}_\pm$ is non-descreasing which show us that $F^{[c,d]}$ is of bounded variation.  Moreover, using the result for non-descreasing functions
\begin{align*}
\mu_F\mid_{[c,d]} &= \mu_{F_+}\mid_{[c,d]} - \mu_{F_-} \mid_{[c,d]} = \mu_{F^{[c,d]}_+} - \mu_{F^{[c,d]}_-} = \mu_{F^{[c,d]}}
\end{align*}
and we are done.
\end{proof}

The simplest type of stochastic integral arises for a process that has right continuous paths with locally finite variation.  In this case, we can just apply the ordinary theory of Lebesgue-Stieltjes integrals pointwise to the process.  
\begin{defn}Let $F$ be an cadlag adapted process and locally finite variation and let $V$ be a jointly measurable process then we define a new process $\int V_s \, dF_s$ by
\begin{align*}
\left(\int V_s \, dF_s\right)_t(\omega) &= \int_0^t V_s(\omega) \, dF(\omega)_s && \text{for all $t \geq 0$ and $\omega \in \Omega$}
\end{align*}
We usually write $\left(\int V_s \, dF_s\right)_t = \int_0^t V_s \, dF_s$.
\end{defn}

The fact that the integral defined as above is actually a process requires verification.  In addition we show that when $V$ is progressive then the resulting process is adapted.
\begin{lem}\label{StochasticStieltjesIntegral}If $F$ is a cadlag process of locally finite variation (not necessarily adapted) and $V$ is a jointly measurable process then $\int_0^t V_s \, dF_s$ is a cadlag process of locally finite variation.  If in addition $F$ is $\mathcal{F}$-adapted and $V$ is $\mathcal{F}$ - progressively measurable then $\int_0^t V_s \, dF_s$ is $\mathcal{F}$-adapted.
\end{lem}
\begin{proof}
If we denote by $\mu_F$ the signed Lebesgue-Stieltjes measure constructed from $F$ and let $\cup_{j=1}^n (a_j, b_j]$ be a disjoint union of intervals, then we have by finite additivity $\mu_F(\cup_{j=1}^n (a_j, b_j]) = \sum_{j=1}^n (F(b_j) - F(a_j))$ which measurable by the measurability of $F$.  As the set of disjoint unions of half open intervals is a ring (Example \ref{RingOfDisjointUnionHalfOpenIntervals}) and therefore a $\pi$-system that generates the Borel $\sigma$-algebra we know $\mu_F$ is a kernel by monotone classes (specifically Lemma \ref{KernelMeasurability}).  If $V$ is jointly measurable then the same is true of $\characteristic{[0,t]} V$ for every $t \geq 0$ and therefore $\int_0^t V_s \, dF_s$ is measurable by Lemma \ref{KernelTensorProductMeasurability}.  The fact that $\int_0^t V_s \, dF_s$ is cadlag and has locally finite variation follow pointwise from Corollary \ref{StieltjesIntegralBoundedVariationAndContinuous}.

Note also that for any $t \geq 0$ we have by Lemma \ref{ChainRuleDensity} and Lemma \ref{RestrictionOfLebesgueStieltjesMeasure} 
\begin{align*}
\int_0^t V_s \, dF_s &= \int_0^\infty \characteristic{[0,t]} V_s \, dF_s = \int_0^\infty V^t_s \, dF\mid_{[0,t]}(s) = \int_0^\infty V^t_s \, dF^t_s  
\end{align*} 
where $F^t(s) = F(t \wedge s)$ and $V^t_s = V_{t \wedge s}$. If we assume that $F$ is adapted it follows that $F_s^t$ is $\mathcal{F}_t$ measurable for all $s \geq 0$ and by the argument above we see that $\mu_{F^t}$ is an $\mathcal{F}_t$-measurable kernel.  If $V$ is progressive then by writing $V^t(\omega, s) = V \mid_{\Omega \times [0,t]}(\omega, s \wedge t)$ which shows that $V^t$ is $\mathcal{F}_t \otimes \mathcal{B}([0,\infty))$-measurable.  Now applying Lemma \ref{KernelTensorProductMeasurability} we get $\mathcal{F}_t$-measurability of $\int_0^t V_s \, dF_s$.
\end{proof}

Because of the previous result we make the following definition for the space of integrands that we'll initially concern ourselves with.
\begin{defn}If $F$ is a cadlag process of locally finite variation then let $L(F)$ be the space of progressive processes $V$ that are pointwise integrable with respect to $F$.
\end{defn}

Because we use stochastic Stieltjes integrals in defining general stochastic integrals we record the following simple facts.  Both of these facts have analogues for general stochastic integrals as well.
\begin{lem}\label{ChainRuleStieltjes}Let $F$ be a cadlag process of locally finite variation, let $V \in L(F)$ and let $U$ be a progressive process.  $U \in L(\int V \, dF)$ if and only if $UV \in L(F)$ and moreover
\begin{align*}
\int_0^t U_s V_s \, dF_s &= \int_0^t U_s \, d\int V_s \, dF_s
\end{align*}
\end{lem}
\begin{proof}
Initially assume that $U$ and $V$ are both positive.  Note that by definition of the Lebesgue-Stieltjes measure we have pointwise for any finite interval $(a,b]$,
\begin{align*}
\mu_{\int V_s \, dF_s}((a,b]) &= \int_0^b V_s \, dF_s - \int_0^a V_s \, dF_s = \int_0^\infty \characteristic{(a,b]} V_s \, dF_s
\end{align*}
and therefore we have $\mu_{\int V_s \, dF_s} = V \cdot \mu_F$ (i.e. $V$ is a $\mu_F$-density of $\mu_{\int V_s \, dF_s}$); the result now follows from Lemma \ref{ChainRuleDensity}.  The rest of the result follows from writing $U = U_+ - U_-$ and $V = V_+ - V_-$ and using linearity.
\end{proof}

We also want to record the behavior of a stochastic Stieltjes integral under stopping.
\begin{lem}\label{StoppingStieltjes}Let $F$ be a cadlag process of locally finite variation, let $V \in L(F)$ and let $\tau$ be an optional time then
\begin{align*}
\int_0^{t \wedge \tau} V_s \, dF_s &= \int_0^t \characteristic{[0,\tau]} V_s \, dF_s = \int_0^t V_s \, F^{\tau}_s
\end{align*}
\end{lem}
\begin{proof}
This follows immediately by writing $\int_0^\infty \characteristic{[0,t]} \characteristic{[0,\tau]} V_s \, dF_s$ and pointwise using the fact that $\mu_F \mid_{[0, \tau]} = \mu_{F^\tau}$ (Lemma \ref{RestrictionOfLebesgueStieltjesMeasure}).
\end{proof}

\subsection{Stochastic Integrals}

The process of defining stochastic integrals follows the standard path of defining integrals for a subclass of integrands for which the definition and existence of the associated integral is easy to see.  Then one uses approximations to extend the class of integrands.  We begin by defining that initial subclass of integrands and define integrals of them with respect to an arbitrary martingale.
\begin{defn}Let $\tau_1 \leq \tau_2 \leq \dotsb$ be optional times, let $\xi_1, \xi_2, \dotsc$ be bounded random variables and assume $\xi_k$ is  $\mathcal{F}_{\tau_k}$-measurable.  Then we say that 
\begin{align*}
V_t &= \sum_{k=1}^\infty \xi_k \characteristic{\tau_{k} > t}
\end{align*}
is a \emph{predictable step process}.  Given a predictable step process and a process $M$ we define the \emph{elementary stochastic integral}
\begin{align*}
\int_0^t V \, dM &= \sum_{k=1}^\infty \xi_k \left (M_t - M_{\tau_k \wedge t} \right)
\end{align*}
In case $\tau_n = \tau_{n+1} = \dotsb$ and $\xi_n = \xi_{n+1} = \dotsb$ we say that $V$ is a \emph{finite predictable step process}.
\end{defn}
Note that in the definition of a stochastic integral for a predictable step process there is no need to consider convergence questions since for each $t \geq 0$ the sum that defines the integral has only finitely many non-zero terms.

TODO: The definition of the elementary stochastic integral isn't quite justified as we haven't shown that it only depends on $V$ and not a particular representation of $V_t = \sum_{k=1}^\infty \xi_k \characteristic{\tau_{k} > t}$.  To show this it seems like it would be helpful to have a canonical representation for a predictable step process.  At some point we also may need the fact that the space of such processes (at least the finite linear combinations) is a vector space or algebra (as per Rogers and Williams).

If one defines the vector space spanned by $\xi \characteristic{(\sigma, \tau]}$ then there is a standard (but not unique) form $\sum_{j=1}^n \xi_j \characteristic{(\sigma_j, \tau_j]}$ where $\sigma_j$ and $\tau_j$ are optional times satisfying $\sigma_1 \leq \tau_1 \leq \sigma_2 \leq \tau_2 \leq \dotsb \leq \sigma_n \leq \tau_n$.  To see this we first need a simple preliminary fact.  If $\sigma$ and $\tau$ are optional times and $\xi$ is either $\mathcal{F}_\sigma$-measurable then $\xi \characteristic{\sigma < \tau}$ is $\mathcal{F}_{\sigma \wedge \tau}$-measurable.  This follows from noting that for all $t \in \reals$, 
\begin{align*}
\lbrace \xi \characteristic{\sigma < \tau} \leq t \rbrace = 
\begin{cases}
\lbrace \sigma \geq \tau \rbrace \cup ( \lbrace \xi \leq t \rbrace \cap \lbrace \sigma < \tau \rbrace) & \text{if $t \geq 0$} \\
\lbrace \xi \leq t \rbrace \cap \lbrace \sigma < \tau \rbrace & \text{if $t < 0$} \\
\end{cases}
\end{align*}
and since $\lbrace \sigma \geq \tau \rbrace$ is $\mathcal{F}_{\sigma \wedge \tau}$-measurable it suffices to show that $\lbrace \xi \leq t \rbrace \cap \lbrace \sigma < \tau \rbrace \in \mathcal{F}_{\sigma \wedge \tau}$ for all $t \in \reals$.  Thus pick $s \in \reals$ and using the $\mathcal{F}_\sigma$-measurability of $\xi$ and the $\mathcal{F}_{\sigma \wedge \tau}$-measurability of $\lbrace \sigma < \tau \rbrace$ we get
\begin{align*}
\lbrace \xi \leq t \rbrace \cap \lbrace \sigma < \tau \rbrace \cap \lbrace \sigma \wedge \tau \leq s \rbrace = 
\lbrace \xi \leq t \rbrace \cap \lbrace \sigma \leq s \rbrace \cap \lbrace \sigma < \tau \rbrace \cap \lbrace \sigma \wedge \tau \leq s \rbrace \in \mathcal{F}_s
\end{align*}

Now considering the decomposition of the intersection of two half open intervals in $\reals$ into 3 disjoint parts we see
\begin{align*}
&\xi_1 \characteristic{(\sigma_1, \tau_1]} + \xi_2 \characteristic{(\sigma_2, \tau_2]}  = \\
&(\xi_1 \characteristic{\sigma_1 < \sigma_2} + \xi_2 \characteristic{\sigma_2 < \sigma_1} ) \characteristic{(\sigma_1 \wedge \sigma_2 , (\sigma_1 \vee \sigma_2) \wedge \tau_1 \wedge \tau_2]} + \\
&(\xi_1 + \xi_2) \characteristic{(\sigma_1 \vee \sigma_2, \tau_1 \wedge \tau_2 \vee \sigma_1 \vee \sigma_2]} + \\
&(\xi_1 \characteristic{\tau_1 > \tau_2} + \xi_2 \characteristic{\tau_2 > \tau_1}) \characteristic{(\sigma_1 \vee \sigma_2 \vee (\tau_1 \wedge \tau_2), \tau_1 \vee \tau_2 ]}
\end{align*}
By our claim above get that $\xi_1 \characteristic{\sigma_1 < \sigma_2} + \xi_2 \characteristic{\sigma_2 < \sigma_1}$ is $\mathcal{F}_{\sigma \wedge \tau}$-measurable.  By $\mathcal{F}_{\sigma_1}$-measurability of $\xi_1$ and $\mathcal{F}_{\sigma_2}$-measurability of $\xi_2$  we get $\mathcal{F}_{\sigma_1 \vee \sigma_2}$-measurability of $\xi_1 + \xi_2$.  Lastly we know also that $\lbrace \tau_1 > \tau_2 \rbrace$ and  $\lbrace \tau_2 > \tau_1 \rbrace$ are $\mathcal{F}_{\tau_1 \wedge \tau_2}$-measurable so $\xi_1 \characteristic{\tau_1 > \tau_2} + \xi_2 \characteristic{\tau_2 > \tau_1}$ is $\mathcal{F}_{\sigma_1 \vee \sigma_2 \vee (\tau_1 \wedge \tau_2)}$-measurable.  Moreover it is clear that we have the inequalities
\begin{align*}
\sigma_1 \wedge \sigma_2 &\leq (\sigma_1 \vee \sigma_2) \wedge \tau_1 \wedge \tau_2 \leq 
\sigma_1 \vee \sigma_2 \leq \tau_1 \wedge \tau_2 \vee \sigma_1 \vee \sigma_2 \leq 
\sigma_1 \vee \sigma_2 \vee (\tau_1 \wedge \tau_2) \leq \tau_1 \vee \tau_2
\end{align*}
and therefore the result is shown.

The representation for a predictable step process we have given in the definition is occasionally not the most convenient one.  Given $V_t = \sum_{k=1}^\infty \xi_k \characteristic{\tau_{k} > t}$ if we define $\eta_n = \sum_{k=1}^n \xi_k$ and therefore 
\begin{align*}
V_t &= \sum_{k=1}^\infty \xi_k \characteristic{t > \tau_k} = \sum_{k=1}^\infty \xi_k \sum_{j=k}^\infty \characteristic{(\tau_j, \tau_{j+1}]}(t)  \\
&= \sum_{j=1}^\infty \sum_{k=1}^j \xi_k \characteristic{(\tau_j, \tau_{j+1}]}(t)  = \sum_{j=1}^\infty \eta_j \characteristic{(\tau_j, \tau_{j+1}]}(t)  \\
\end{align*}
and 
\begin{align*}
\int_0^t V \, dM &= \sum_{k=1}^\infty \xi_k \left(M_{t} - M_{\tau_k \wedge t} \right) = \sum_{k=1}^\infty \xi_k \sum_{j=k}^\infty \left(M_{\tau_{j+1} \wedge t} - M_{\tau_j \wedge t} \right)  \\
&= \sum_{j=1}^\infty \sum_{k=1}^j \xi_k \left(M_{\tau_{j+1} \wedge t} - M_{\tau_j \wedge t} \right) = \sum_{j=1}^\infty \eta_j \left(M_{\tau_{j+1} \wedge t} - M_{\tau_j \wedge t} \right)
\end{align*}
In what follows we will feel free to switch between these representations without comment.

The first order of business is to establish conditions under which an elementary stochastic integral is a martingale.  To do this we need the following characterization of the martingale property.  
\begin{lem}\label{MartingaleOptionalTimeCriterion}Let $M_t$ be an integrable adapted process on an index set $T$.  Then $M$ is a martingale if and only if $\expectation{M_\sigma} = \expectation{M_\tau}$ for all $T$-valued optional times $\sigma$ and $\tau$ that take at most two values.
\end{lem}
\begin{proof}
Restricting $M_t$ to the union of the ranges of $\tau$ and $\sigma$ we can apply Lemma \ref{ExpectationStoppedMartingaleDiscrete} to conclude $\expectation{M_\sigma} = M_0 = \expectation{M_\tau}$.  In the other direction, let $s,t \in T$ with $s < t$.  Let $A \in \mathcal{F}_s$ and define $\sigma = s \characteristic{A^c} + t \characteristic{A}$ and note that $\sigma$ is an optional time.  Now, applying our hypothesis to the optional time $\sigma$ and the deterministic optional time $s$,  we get $\expectation{M_t ; A} = \expectation{M_\sigma} - \expectation{M_s; A^c}  = \expectation{M_s} - \expectation{M_s; A^c}  = \expectation{M_s; A}$ which shows $\cexpectationlong{\mathcal{F}_s}{M_t} = M_s$ a.s.
\end{proof}

\begin{lem}\label{StochasticIntegralFinitePredictableStepProcess}Suppose $\mathcal{F}$ is a filtration,  $\tau_1 \leq \tau_2 \leq \dotsb \leq \tau_n$ are bounded $\mathcal{F}$-optional times, $M_t$ is a martingale
and either 
\begin{itemize}
\item[(i)] each $\tau_k$ is countably valued
\item[(ii)]$\mathcal{F}$ and $M$ are right continuous
\end{itemize}
Then if
\begin{align*}
V_t &= \sum_{k=1}^n \xi_k \characteristic{\tau_{k} > t}
\end{align*}
is a finite predictable step process then $\int_0^t V \, dM$ is a martingale.  If we assume that $M$ is a local martingale then $\int_0^t V \, dM$ is a local martingale.
\end{lem}
\begin{proof}
By definition of elementary stochastic integral and linearity, it suffices to show that $N_t = \xi \left (M_t - M_{\tau \wedge t} \right)$ is a martingale whenever either $\tau$ is a countably valued optional time or $\mathcal{F}$ and $M$ are right continuous and $\xi$ is a bounded $\mathcal{F}_\tau$-measurable random variable.  In the first case, by restricting $M_t$ to the range of $\tau$ we can apply the Optional Sampling Theorem \ref{OptionalSamplingDiscrete} to the bounded optional time $\tau \wedge t$ to conclude that $M_{\tau \wedge t}$ is integrable and in the second case we can apply the continuous time Optional Sampling Theorem \ref{OptionalSamplingContinuous} to conclude that $M_{\tau \wedge t}$ is integrable.  This together with the integrability of $M_t$ and boundedness of $\xi$ shows that $N_t$ is integrable.  If we note that $N_t = \xi \characteristic{ \tau \leq t} \left (M_t - M_{\tau \wedge t} \right)$ then because $ \xi \characteristic{ \tau \leq t}$ and $M_t$ are $\mathcal{F}_t$-measurable and $M_{\tau \wedge t}$ is $\mathcal{F}_{\tau \wedge t}$-measurable (hence $\mathcal{F}_t$-measurable) we see that $N_t$ is adapted.  Lastly let $\sigma$ be an countably valued optional time then by the $\mathcal{F}_\tau$-measurability of $\xi$ we have and either the Optional Sampling Theorem \ref{OptionalSamplingDiscrete} or the Optional Sampling Theorem \ref{OptionalSamplingContinuous} we get
\begin{align*}
\cexpectationlong{\mathcal{F}_\tau}{N_\sigma} &= \xi \cexpectationlong{\mathcal{F}_\tau}{M_\sigma - M_{\tau \wedge \sigma}} = \xi \left(M_{\tau \wedge \sigma} - M_{\tau \wedge \sigma} \right) = 0
\end{align*}
and by the tower property of conditional expectations we get $\expectation{N_\sigma} = 0$.  Now by Lemma \ref{MartingaleOptionalTimeCriterion} we see that $N_t$ is a martingale.

Now let us assume that $M$ is a local martingale.  To see that $\int_0^t V \, dM$ is a local martingale let $\sigma_n$ be a localizing sequence and note that
\begin{align*}
\left( \int_0^t V \, dM\right)^{\sigma_n} &= \sum_{k=1}^n \xi_k \left( M_{\sigma_n \wedge t} - M_{\sigma_n \wedge\tau_k \wedge t} \right) = \int_0^t V \, dM^{\sigma_n}
\end{align*}
is a martingale with localizing sequence $\sigma_n$  by the first part of the Lemma.
\end{proof}

\begin{lem}\label{StochasticIntegralPredictableStepProcess}Suppose $\mathcal{F}$ is a filtration,  $\tau_1 \leq \tau_2 \leq \dotsb \leq \dotsb$ are bounded $\mathcal{F}$-optional times with $\tau_n \uparrow \infty$, $M_t$ is an $L^2$ martingale with $M_0$, $V_t$ is a predictable step process with $\abs{V_t} \leq 1$ and either
\begin{itemize}
\item[(i)] each $\tau_k$ is countably valued
\item[(ii)]$\mathcal{F}$ and $M$ are right continuous
\end{itemize}
then $\int_0^t V \, dM$ is an $L^2$-martingale and $\expectation{\left(\int_0^t V \, dM\right)^2} \leq \expectation{M^2_t}$.
\end{lem}
\begin{proof}
We let $V_t = \sum_{k=1}^\infty \eta_k \characteristic{(\tau_k, \tau_{k+1}]}$.  We start by taking an arbitrary $n >0$ and defining $V^n_t =  \sum_{k=1}^n \eta_k \characteristic{(\tau_k, \tau_{k+1}]}$ so that $V^n$ is a finite predictable step process.  By Lemma \ref{StochasticIntegralFinitePredictableStepProcess} shows that $\int_0^t V^n \, dM$ is a martingale.  The $L^2$ bound for $V^n$ follows from Optional Sampling (Theorem \ref{OptionalSamplingDiscrete} or Theorem \ref{OptionalSamplingContinuous} depending on which hypothesis we choose).  The critical point is that for any $1 \leq k < j \leq n$ we have for each cross term term of the stochastic integral
\begin{align*}
&\expectation{\eta_j \eta_k \left (M_{\tau_{j+1} \wedge t} - M_{\tau_j \wedge t} \right)  \left (M_{\tau_{k+1} \wedge t} - M_{\tau_k \wedge t} \right)} \\
&=\expectation{\eta_j \eta_k \left (M_{\tau_{j+1} \wedge t} - M_{\tau_j} \right)  \left (M_{\tau_{k+1} } - M_{\tau_k} \right) ; t > \tau_j} \\
&=\expectation{ \eta_j \eta_k \cexpectationlong{\mathcal{F}_{\tau_j}}{M_{\tau_{j+1} \wedge t} - M_{\tau_j }}  \left(M_{\tau_{k+1} } - M_{\tau_k} \right) ; t > \tau_j} =0
\end{align*}
and 
\begin{align*}
\expectation{ \left (M_{\tau_{k+1} \wedge t} - M_{\tau_k \wedge t} \right)^2} &= \expectation{M^2_{\tau_{k+1} \wedge t}} - 2\expectation{M_{\tau_{k+1} \wedge t}M_{\tau_{k} \wedge t}} + 
\expectation{M^2_{\tau_k \wedge t}} \\
&=\expectation{M^2_{\tau_{k+1} \wedge t}} - 2\expectation{\cexpectationlong{\mathcal{F}_{\tau_k \wedge t}}{M_{\tau_{k+1} \wedge t}}M_{\tau_{k} \wedge t}} + 
\expectation{M^2_{\tau_k \wedge t}} \\
&= \expectation{M^2_{\tau_{k+1} \wedge t}} - \expectation{M^2_{\tau_k \wedge t}}
\end{align*}
Using the above facts, the fact that $M_0 = 0$ and the bound on $V$ 
\begin{align*}
\expectationop \left( \int_0^t V^n \, dM \right)^2 &= \expectationop \sum_{k=1}^n \eta^2_k \left (M_{\tau_{k+1} \wedge t} - M_{\tau_k \wedge t} \right)^2 \\
&\leq \expectationop \sum_{k=1}^n \left (M_{\tau_{k+1} \wedge t} - M_{\tau_k \wedge t} \right)^2 \\
&= \sum_{k=1}^n \expectation{M^2_{\tau_{k+1} \wedge t}} - \expectation{M^2_{\tau_k \wedge t}} \\
&\leq \sum_{k=1}^\infty \expectation{M^2_{\tau_{k+1} \wedge t}} - \expectation{M^2_{\tau_k \wedge t}} \\
&=  \lim_{n \to \infty} \expectation{M^2_{\tau_n \wedge t}} = \expectation{M^2_t}
\end{align*}

Now in the general case, we get the $L^2$ bound by Fatou's Lemma Theorem \ref{Fatou}
\begin{align*}
\expectationop \left( \int_0^t V \, dM \right)^2 &= \liminf_{n \to \infty} \expectationop  \left( \int_0^t V^n \, dM \right)^2 \leq \expectation{M^2_t}
\end{align*}
In addition, the $L^2$ bound shows that the family $\int_0^t V \, dM, \int_0^t V^1 \, dM, \int_0^t V^2 \, dM, \dotsc$ is uniformly integrable (Lemma \ref{BoundedLpImpliesUniformlyIntegrable}) and therefore for every $t \geq 0$ and the martingale property of $\int_0^t V^n \, dM$ we get for $u < t$,
\begin{align*}
\cexpectationlong{\mathcal{F}_u}{\int_0^t V \, dM} &= \cexpectationlong{\mathcal{F}_u}{\lim_{n \to \infty} \int_0^t V^n \, dM} \\
&= \lim_{n \to \infty} \cexpectationlong{\mathcal{F}_u}{ \int_0^t V^n \, dM} \\
&= \lim_{n \to \infty} \int_0^u V^n \, dM = \int_0^u V \, dM 
\end{align*}
(to exchange the limits with the conditional expectation, use the fact that for each $A \in \mathcal{F}_u$ we can see that $\characteristic{A} \int_0^t V^n \, dM$ is uniformly integrable then use Theorem \ref{LpConvergenceUniformIntegrability})
showing $\int_0^t V \, dM$ is an $L^2$-martingale.
\end{proof}

\begin{defn}Two processes $X$ and $Y$ on a time scale $T$ are said to be \emph{versions} of one another if $\probability{X_t = Y_t} = 1$ for every fixed $t \in T$.   One also says that $Y$ is a modification of $X$ (and vice versa).  Two processes $X$ and $Y$ on a time scale $T$ are said to be \emph{indistinguishable} if $\probability{X_t = Y_t \text{ for all } t \in T} = 1$.
\end{defn}

While it is trivial that two indistinguishable processes are versions of one another it is also simple that for continuous processes on time scales that are separable the two notions are equivalent.  The following is the case that is most important for us.
\begin{prop}Let $X$ and $Y$ be cadlag processes on a time scale $\reals_+$.  Then $X$ and $Y$ are versions of one another if and only if they are indistinguishable.
\end{prop}
\begin{proof}
Let $A_X$ be the event that $X$ has cadlag sample paths and similarly with $Y$.  Let $B = \cap_{\substack{ q \geq 0 \\ q \in \rationals}} \lbrace X_q = Y_q\rbrace$.  If $X$ and $Y$ are versions then by taking a countable intersection of almost sure events we have $\probability{A_X \cap A_Y \cap B} = 1$.  Morever on the event $A_X \cap A_Y \cap B$ by right continuity we have for all $t \geq 0$, $X_t = \lim_{q \downarrow t} X_q = \lim_{q \downarrow t} Y_q = Y_t$ and therefore $A_X \cap A_Y \cap B = A_X \cap A_Y \cap \lbrace X_t = Y_t \text { for all } t\geq 0\rbrace$ and it follows that $\probability {X_t = Y_t \text { for all } t\geq 0} \geq \probability{A_X \cap A_Y \cap \lbrace X_t = Y_t \text { for all } t\geq 0\rbrace} = 1$.
\end{proof}

When constructing spaces of processes is often the case that we'll need to identify indistinguishable processes, thus we make explicit note of the following simple fact.
\begin{prop}Indistinguishability of processes is an equivalence relation.
\end{prop}
\begin{proof}
Reflexivity and symmetry are immediate from the defintion, and transitivity follows from the fact that $\lbrace X_t = Z_t \text{ for all } t \in T \rbrace \supset \lbrace X_t = Y_t \text{ for all } t \in T \rbrace \cap \lbrace Y_t = Z_t \text{ for all } t \in T\rbrace$ and $\probability{\lbrace X_t = Y_t \text{ for all } t \in T \rbrace \cap \lbrace Y_t = Z_t \text{ for all } t \in T\rbrace} = 1$.
\end{proof}

\begin{defn}Fix a probability space $(\Omega, \mathcal{A}, P)$ and suppose $\mathcal{F}$ is a right continuous and complete filtration.  Let $\mathcal{M}^2$ be the space of $L^2$ bounded continuous $\mathcal{F}$-martingales such that $M_0 = 0$ a.s.  up to indistinguishability.  That is to say that for all $M \in \mathcal{M}^2$ there exists $C \geq 0$ such that for all $0 \leq t < \infty$ we have $\norm{M_t}_2 \leq C$.  For $M, N \in \mathcal{M}^2$, define $\langle M, N \rangle = \langle M_\infty, N_\infty \rangle = \expectation{M_\infty N_\infty }$.
\end{defn}
\begin{lem}\label{ContinuousL2MartingalesHilbert}The space $\mathcal{M}^2$ is a Hilbert space.
\end{lem}
\begin{proof}
The fact that $\mathcal{M}^2$ is a vector space follows immediately from linearity of conditional expectation, the linearity of the space $C([0,\infty); \reals)$ and the triangle inequality of the $L^2$ norm on $C([0,\infty); \reals)$. 
 
To see that we have an inner product on $\mathcal{M}^2$, first observe that if $\langle M, M \rangle = \norm{M_\infty}^2_2 = 0$ then $M_\infty = 0$ a.s. hence since $M$ is closable it follows that $M_t = \cexpectationlong{\mathcal{F}_t}{M_\infty} = 0$ a.s.  for all $0 \leq t < \infty$.  Since $M$ is continuous it follows that $M=0$ a.s.  Symmetry of $\langle \cdot,\cdot \rangle$ follows immediately from symmetry of the $L^2$ inner product on $C([0,\infty); \reals)$.  Supposing $M$ and $N$ are both $L^2$ bounded continuous martingales we know they are closable hence $M_t = \cexpectationlong{\mathcal{F}_t}{M_\infty}$ and similarly for $N$.  It then follows from linearity of conditional expectation that $(aM + bN)_\infty = aM_\infty + bN_\infty$ for any $a,b \in \reals$.  From this fact we see that  for all $M,N,R \in \mathcal{M}^2$ and $a,b \in \reals$ we have $\langle aM + bN, R \rangle = \langle aM_\infty + bN_\infty, R_\infty \rangle = a \langle M, R \rangle + b \langle N, R \rangle$. 

We now show that $\mathcal{M}^2$ is complete.  Suppose $M^1, M^2, \dotsc$ is Cauchy in $\mathcal{M}^2$, then $M^1_\infty, M^2_\infty, \dotsc$ is Cauchy in $L^2$ and therefore has a limit $\xi$ in $L^2$ that is $\mathcal{F}_\infty$-measurable.  Define $M_t = \cexpectationlong{\mathcal{F}_t}{\xi}$ so we know that $M_t$ is a martingale and $M_\infty = \xi$ a.s.  TODO: Do we know at this point that $M$ is $L^2$ bounded?  Now by the Doob $L^2$ inequality Lemma \ref{DoobLpInequalityContinuous} applied to the closed martingale $M_t$ on $[0,\infty]$ we have $\norm{\sup_{0 \leq s \leq \infty} (M_s^n - M_s)}_2 \leq 2 \norm{M^n_\infty - M_\infty}_2$.  From this we get that $\lim_{n \to \infty} \norm{\sup_{0 \leq s \leq \infty} (M_s^n - M_s)}_2 = 0$ hence $\sup_{0 \leq s \leq \infty} (M_s^n - M_s) \toprob 0$ (Lemma \ref{ConvergenceInMeanImpliesInProbability}) and therefore $\sup_{0 \leq s \leq \infty} (M_s^n - M_s) \toas 0$ along a subsequence (Lemma \ref{ConvergenceInProbabilityAlmostSureSubsequence}) which shows that $M$ is has almost surely continuous sample paths (Lemma \ref{UniformLimitContinuousFunctionsIsContinuous}).
\end{proof}

\subsection{Quadratic Variation}
The crux of the problem in defining stochastic integrals is the fact that sample paths of continuous martingales almost surely have infinite total variation and therefore Lebesgue-Stieltjes integrals cannot be defined.   

\begin{lem}Let $M$ and $N$ be continuous local martingales and let $\tau$ be an optional time, then $M^\tau \left( N - N^\tau \right)$ is a local martingale.
\end{lem}
\begin{proof}
First let us assume that $N$ is a martingale, $\tau$ is an optional time and $\eta$ is an $\mathcal{F}_\tau$-measurable bounded random variable.  We claim that $\eta(N_t - N^\tau_t)$ is a martingale.  By the Optional Sampling Theorem \ref{OptionalSamplingContinuous} we know that $\tau \wedge t$ is a bounded optional time hence $N_{\tau \wedge t}$ is integrable and therefore by boundedness of $\eta$ we know that $\eta(N_t - N_{\tau \wedge t})$ is integrable.  To see adaptedness, note that $\eta(N_t - N_{\tau \wedge t}) = \eta \characteristic{\tau \leq t} (N_t - N_{\tau \wedge t})$ so that by $\mathcal{F}_\tau$-measurability of $\eta$ we also have $\mathcal{F}_t$-measurability of $\eta \characteristic{\tau \leq t}$.  Furthermore, $N_{\tau \wedge t}$ is $\mathcal{F}_{\tau \wedge t}$-measurable and since $\tau \wedge t \leq t$ we see that it is also $\mathcal{F}_t$-measurable.  To see that $\eta (N_t - N_{\tau \wedge t})$ is a martingale we let $\sigma$ be any bounded optional time and then note by Optional Sampling, the tower and pullout properties of conditional expectation 
\begin{align*}
\expectation{\eta (N_\sigma - N_{\tau \wedge \sigma})} &= \expectation{\eta \cexpectationlong{\mathcal{F}_\tau}{ (N_\sigma - N_{\tau \wedge \sigma})}} = \expectation{\eta  (N_{\tau \wedge \sigma} - N_{\tau \wedge \sigma})} = 0
\end{align*}
which independent of $\sigma$ hence we can apply Lemma \ref{MartingaleOptionalTimeCriterion} to conclude that $\eta(N - N^\tau)$ is a martingale.

TODO: Finish
\end{proof}

TODO: We used continuity of the local martingale $M$ to reduce ourselves to the case of bounded martingales which was used to obtain integrability.  Do general local martingales localize to $L^2$ bounded martingales or something else that would allow us to get integrability?

\begin{thm}[Quadratic Covariation]\label{OptionalQuadraticCovariation}Let $M$ and $N$ be continuous local martingales, there exists an almost surely unique continuous process $[M,N]$ of locally finite variation such that $[M,N]_0 = 0$ and $MN - [M,N]$ is a local martingale.  The pairing $[M,N]$ is bilinear and symmetric and for every optional time $\tau$ satisfies
\begin{align*}
[M,N]^\tau = [M^\tau, N^\tau] = [M^\tau, N] \text{ a.s.}
\end{align*}
If we define $[M] = [M,M]$ then it is the case that $[M]$ is almost surely non-decreasing.  The process $[M,N]$ is called the \emph{quadratic covariation} of $M$ and $N$ and the process $[M]$ is called the \emph{quadratic variation} of $M$.
\end{thm}
\begin{proof}
We first consider the case when $M=N$ and we first assume that $M$ is a bounded martingale such that $M_0=0$ and $\abs{M_t} \leq C$ for some deterministic constant $C > 0$.  To motivate the construction recall the basic fact that for a function $f$ of bounded variation we have the Lebesgue-Stieltjes integral $f^2 = 2 \int f \, df$.  We suspect that in a stochastic setting such an identity won't quite work (because the Stieltjes integral doesn't work).  That suspicion is correct and what does turn out to be true is that once we have defined a stochastic integral, $M^2 - 2 \int M \, dM = [M]$.  Of course our plan is to use the quadratic variation to define stochastic integrals so this reasoning is getting pretty circular here; nonetheless if we suspend belief for moment and define something that \emph{looks like} it could be $\int M \, dM$ then we might get the right definition for quadratic variation.  Motivated by these observations, our first step is to come up with an approximation of $M$ by predictable step processes so we can create an approximation of $\int M \, dM$.  For each $n > 0$ define the sequence of optional times $\tau^n_0, \tau^n_1, \dotsc$ by $\tau^n_0 = 0$ and 
\begin{align*} 
\tau^n_k &= \inf \lbrace t > \tau^n_{k-1} \mid \abs{M_t - M_{\tau^n_{k-1}}} = 2^{-n} \rbrace \text{ for $k > 0$} \\
&=\tau^n_{k-1} + \tau^n_1 \circ \theta_{\tau^n_{k-1}}
\end{align*}

Claim: Either $\tau^n_k = \infty$ or $M_{\tau^n_k} = j/2^n$ for some random $j \in \integers$.

This is a simple induction on $k$ for each $n$ using continuity of $M_t$, the fact that $M_0 = 0$ and $\tau^n_0=0$.

Claim: Suppose $M_t = j/2^n$ for some $n \geq 0$ and $k \geq 0$ and let $K = \max \lbrace k \mid \tau^n_k \leq t$, then $M_{\tau^n_K} = j/2^n$.

The claim is trivially true if $\tau^n_K = t$.  If this is not true by the previous claim we have $i \in \integers$ such that $M_{\tau^n_k} = i/2^n$.  By definition of $K$, we have $\tau^n_K < t < \tau^n_{K+1}$ and by definition of $\tau^n_{K+1}$, continuity of $M_t$ and the intermediate value theorem we know that $\abs{M_t - M_{\tau^n_K}} = \abs{i-j}2^{-n}< 2^{-n}$.  Thus $i=j$ and the claim is verified.

Claim: For every $n \geq 0$ and $k \geq 0$ there exists a random integer $l \geq 0$ such that $\tau^n_k = \tau^{n+1}_l$.

We proceed by induction on $k$ with the case $k=0$ being true because $\tau^n_0 = 0$ for all $n \geq 0$.  Having assumed $M_0 = 0$ we see that we can pick $0 \leq j < \infty$ such that $M_{\tau^n_k} = j/2^n$. (TODO: what to say when $\tau^n_k = \infty$; not clear that we can assert some $\tau^{n+1}_l=\infty$ since we may be oscillating with small enough amplitude?)  Let $i$ be the largest index such that $\tau^{n+1}_i \leq \tau^n_k$.  Since $M_{\tau^n_k} = j/2^n = 2j/2^{n+1}$ we can apply the previous claim to see that $M_{\tau^{n+1}_i}  = M_{\tau^n_k}$.  By the intermediate value theorem we know that $M_{\tau^n_{k-1}} < M_{\tau^{n+1}_i}  \leq M_{\tau^n_k}$. Because $\abs{M_{\tau^{n+1}_i} - M_{\tau^n_{k-1}}} = \abs{M_{\tau^{n}_k} - M_{\tau^n_{k-1}}} = 2^{-n}$ by definition of $\tau^n_k$ we know that $\tau^{n+1}_i \geq \tau^n_k$ and therefore $\tau^{n+1}_i = \tau^n_k$.

Define 
\begin{align*}
V^n_t &= \sum_{k=0}^\infty M_{\tau^n_{k}} \characteristic{(\tau^n_{k}, \tau^n_{k+1}]}(t)
\end{align*}
where despite the fact that we have written an infinite sum we don't have to worry about convergence since for any fixed $t$ the sum is finite.  Clearly, each $V^n$ is a bounded predictable step process and it is also clear that $V^n$ is an approximation of $M$ (though we won't yet belabor the exact sense in which this is true).  Pick $t \geq 0$ and let $K$ be the random index such that $\tau^n_{K} < t \leq \tau^n_{K+1}$ then we can compute using high school algebra and the fact that $M_{\tau^n_0} = M_0 = 0$
\begin{align*}
2 \int_0^t V^n \, dM &= 2 \sum_{k=0}^\infty M_{\tau^n_{k}} \left ( M_{t \wedge \tau^n_{k+1}} - M_{t \wedge \tau^n_k} \right ) \\
&=2 M_{\tau^n_K} M_t - 2 M_{\tau^n_K}^2 + 2 \sum_{k=0}^{K-1} M_{\tau^n_k} M_{\tau^n_{k+1}} - 2 \sum_{k=0}^{K-1} M^2_{\tau^n_k} \\
&=2 M_{\tau^n_K} M_t -  M_{\tau^n_K}^2 + 2 \sum_{k=0}^{K-1} M_{\tau^n_k} M_{\tau^n_{k+1}} - \sum_{k=0}^{K-1} M^2_{\tau^n_k}  - \sum_{k=0}^{K-1} M^2_{\tau^n_{k+1}} \\
&=2 M_{\tau^n_K} M_t -  M_{\tau^n_K}^2 - \sum_{k=0}^{K-1} \left (M_{\tau^n_{k+1}}  - M_{\tau^n_k}\right)^2 \\
&=M_t^2 - \left( M_t - M_{\tau^n_K} \right)^2 - \sum_{k=0}^{K-1} \left (M_{t \wedge \tau^n_{k+1}}  - M_{t \wedge \tau^n_k}\right)^2 \\
&=M_t^2 - \sum_{k=0}^\infty \left (M_{t \wedge \tau^n_{k+1}}  - M_{t \wedge \tau^n_k}\right)^2 \\
\end{align*}
So if we define 
\begin{align*}
Q^n_t = \sum_{k=0}^\infty \left (M_{t \wedge \tau^n_{k+1}}  - M_{t \wedge \tau^n_k}\right)^2 
\end{align*}
we have the identity
\begin{align*}
M^2_t &= 2 \int_0^t V^n \, dM + Q^n_t
\end{align*}
Since $V^n$ is a bounded predictable step process and $M$ is an $L^2$ continuous martingale, we know that $\int V^n \, dM$ is a continuous $L^2$ martingale (Lemma \ref{StochasticIntegralPredictableStepProcess}).   Furthermore by construction we have $\sup_{0 \leq t < \infty} \abs{V^n_t - M_t} < 2^{-n}$ and therefore $\sup_{0 \leq t < \infty} \abs{V^n_t - V^m_t} < 2^{-n+1}$ for all $n \leq m$.  
\begin{align*}
\norm{\int V^n \, dM - \int V^m \, dM}_2 &=\norm{\int (V^n - V^m )\, dM }_2 \\
&=\norm{\lim_{t \to \infty} \int_0^t (V^n - V^m )\, dM }_2\\
&\leq \lim_{t \to \infty} \norm{\int_0^t (V^n - V^m )\, dM }_2 && \text{ by Fatou's Lemma}\\
&\leq \lim_{t \to \infty} 2^{-n+1} \norm{M_t}_2 && \text{ by Lemma \ref{StochasticIntegralPredictableStepProcess}}\\
&= 2^{-n + 1} \norm{M_\infty}_2 = 2^{-n+1} \norm{M}_2 && \text{ since $M_t \tolp{2} M_\infty$}
\end{align*}
which shows that $\int V^n \, dM_s$ is a Cauchy sequence in $\mathcal{M}^2$.  (TODO: I am almost certain that we know $\int (V^n - V^m) \, dM$ is a bounded $L^2$ martingale so that in fact we have $\int_0^t (V^n - V^m) \, dM \tolp{2} \int_0^\infty (V^n - V^m) \, dM$ and we don't need Fatou above, we have equality).  By completeness of $\mathcal{M}^2$ (Lemma \ref{ContinuousL2MartingalesHilbert}) there is $N \in \mathcal{M}^2$ such that $\int V_s^n \, dM_s$ converges to $N$.  Define $[M] = M^2 - 2N$ and use the Doob $L^2$ inequality $\sup_{0 \leq t \leq \infty} \abs{N_t - \int_0^t V^n\, dM} \leq 2 \norm{N - \int V^n\, dM} \to 0$ to get
\begin{align*}
\sup_{0 \leq t < \infty} \abs{Q^n_t - [M]_t} &=\sup_{0 \leq t < \infty} \abs{Q^n_t - M^2_t + 2N_t} \\
&= 2 \sup_{0 \leq t < \infty} \abs{N_t - \int_0^t V^n \, dM} \toprob 0
\end{align*}
Therefore $\sup_{0 \leq t < \infty} \abs{Q^n_t - [M]_t} \toas 0$ along a subsequence (Lemma \ref{ConvergenceInProbabilityAlmostSureSubsequence}).  Define the random set $T = \lbrace \tau^n_k \mid n,k \in \naturals \rbrace$.  We have shown above that for any two elements $s < t \in T$ for sufficiently large $n$ such that $s = \tau^n_k$ and $t = \tau^n_j$ for appropriate $k,j \in \integers_+$ (where $k$ and $j$ depend on $n$ of course).  From the definition of $Q^n_t$ it follows that $Q^n_s \leq Q^n_t$ for all such $n$; thus $[M]$ is almost surely non-decreasing on $T$.  By continuity of $[M]$ we can extend this to conclude that almost surely $[M]$ is non-decreasing on the closure $\overline{T}$.  To see that $[M]$ is non-decreasing everywhere, we know that $\reals_+ \setminus \overline{T}$ is a countable union of open intervals so it suffices to show that $[M]$ is constant on any open interval $(a,b) \subset \reals_+ \setminus \overline{T}$.  If $[M]$ is not constant on $(a,b)$ then we can find suitable $s,t$ such that $a < s < t < b$ and $X_s = k/2^n$ and $X_t = (k+1)/2^n$ or $X_t = (k-1)/2^n$ for some $k,n \in \integers$.  Pick the largest $i$ such that $\tau^n_i \leq s$.  As $(a,b) \cap \overline{T} = \emptyset$ we know that $\tau^n_i < s$.  By our previous claim we know that $X_{\tau^n_i} = X_s$ and therefore $\abs{X_t - X_{\tau^n_i}} = \abs{X_t - X_s} = 2^{-n}$ which implies $\tau^n_i < s < \tau^n_{i+1} \leq t$ which contradicts $(a,b) \cap \overline{T} = \emptyset$.

Now we need to extend the definition of the quadratic variation to unbounded martingales $M$.  Let $\tau_n = \inf \lbrace t \geq 0 \mid \abs{M_t} = n \rbrace$ which is an optional time by continuity of sample paths of $M$ and Lemma \ref{HittingTimesContinuous}.  By what we have proven, we know that $[M^{\tau_n}]$ exists and is almost surely non-decreasing.
TODO: Finish the result by extending to the unbounded case.

Having defined the quadratic variation $[M]$, we now extend it to the quadratic covariation $[M,N]$ for general local martingales $M$ and $N$.  First we establish the uniqueness.  Note that if we are given processes of locally bounded variation $Q$ and $R$ such that $Q_0 = R_0 = 0$ and $MN - Q$ and $MN - R$ are local martingales, then $Q-R = (MN -R) - (MN -Q)$ is a local martingale of locally bounded variation and Lemma \ref{ContinuousLocalMartingaleBoundedVariation} implies that $Q - R = Q_0 - R_0 = 0$ a.s.  From the uniqueness we immediately see that $[M,N]$ is bilinear and symmetric.

Now we reduce the definition of $[M,N]$ to the case $[M]$ with $M_0 = 0$ by a pair of reductions.

Claim: $[M-M_0,N-N_0] = [M,N]$ a.s.

Simply note that 
\begin{align*}
MN - (M-M_0)(N-N_0) &= M_0 N_0 + M_0 N + N_0 M
\end{align*}
is a local martingale and therefore $(M-M_0)(N-N_0) - [M,N] = -(MN - (M-M_0)(N-N_0) ) + MN - [M,N]$ is a local martingale.

Claim: $[M,N] = \frac{1}{4} ([M+N] - [M-N])$

Note that 
\begin{align*}
4 MN - [M+N] + [M-N] &= ((M+N)^2 - [M+N]) - ((M-N)^2 - [M-N])
\end{align*}
is a local martingale.

Lastly we prove the behavior of localization under optional times.
Claim: Let $\tau$ be an optional time, then $[M,N]^\tau = [M^\tau, N^\tau] = [M^\tau,N]$ a.s.

For the first reduction, suppose that $\tau$ is an optional time then we know that
\begin{align*}
(MN - [M,N])^\tau &= M^\tau N^\tau - [M,N]^\tau
\end{align*}
which is a local martingale by Lemma \ref{LocalMartingaleLocalProperty} and moreover $[M,N]^\tau$ is of locally finite variation therefore we see $[M^\tau,N^\tau] = [M,N]^\tau$ a.s.  We also know that $M^\tau (N^\tau -N)$ is a local martingale (TODO:this is supposed to follow for martingales from optional sampling (doesn't that actually show $M_\tau(N - N^\tau)$ is a martingale) then given a localizing sequence $\tau_n$ for $M$ and $\sigma_n$ for $N$ we know that $\tau_n \wedge \sigma_n$ is a localizing sequence for both $M$ and $N$) and therefore 
\begin{align*}
M^\tau N - [M,N]^\tau &= M^\tau (N - N^\tau)  + M^\tau N^\tau - [M,N]^\tau
\end{align*}
is a local martingale which shows that $[M,N]= [M^\tau,N]$ a.s.

TODO: This proof does not make it clear that when $M$ is a martingale we know $M^2 - [M]$ is in fact a martingale (is that always true?  According to Rogers and Williams we know that $[M]$ is a uniformly integrable martingale whenever $M$ is $L^2$-bounded).  Here is an argument that may too complicated but shows that if $[M]$ exists for bounded martingales $M$ then $[M]$ exists for $L^2$ bounded martingales $M$ and $M^2 - [M]$ is a uniformly integrable martingale.

By $L^2$ boundedness we know that for every optional time $\tau$ we have $\abs{M^2_\tau} \leq (M^*)^2$ and moreover by Doob's inequality $\expectation{(M^*)^2} \leq 2 \norm{M} < \infty$ so $\lbrace M^2_\tau \mid \tau \text{ is an optional time}\rbrace$ is a uniformly integrable family (Example \ref{DominatedImpliesUniformlyIntegrable}).  Therefore by Lemma \ref{LpConvergenceUniformIntegrability} for any sequence of optional times $\tau_n$ such that $\tau_n \uparrow \infty$ a.s. we have not only $M^2_{\tau_n} \toas M^2_\infty$ but also $M_{\tau_n} \tolp{2} M_\infty$.

Now, define $\tau_n = \inf \lbrace t \geq 0 \mid M_t = n \rbrace$ which is an optional time by Lemma \ref{HittingTimesContinuous}.  TODO: Show $\tau_n \uparrow \infty$ a.s.  As in the proof above we know that $[M^{\tau_m}] = [M^{\tau_n}]$ on $[0,\tau_m]$ for any $m \leq n$ and therefore we can define $[M] = \lim_{n \to \infty} [M^{\tau_n}]$ and we have $[M]^{\tau_n} = [M^{\tau_n}]$ on $[0,\tau_n]$.  Moreover, since each $[M^{\tau_n}]$ is increasing we know that $[M^{\tau_n}]_\infty = [M^{\tau_n}]_{\tau_n} \uparrow [M]_\infty$ and therefore we can apply Monotone Convergence to conclude $[M^{\tau_n}]_\infty \tolp{1} [M]_\infty$.  TODO: Finish.
\end{proof}

\begin{lem}\label{QuadraticCovariationAndContinuity}Let $M_n$ be a sequence of continuous local martingales, then $M_n^* \toprob 0$ if and only if $[M_n]_\infty \toprob 0$.
\end{lem}
\begin{proof}
First we assume that $M_n^* \toprob 0$.  Let $\epsilon > 0$ be given and define $\tau_n = \inf \lbrace t \geq 0 \mid (M_n)_t > \epsilon \rbrace$ which is an optional time because of the continuity of $M_n$.  Moreover, we know that $M^{\tau_n}_n$ is a bounded continuous martingale and therefore $(M^2_n - [M_n])^{\tau_n} = (M^{\tau_n}_n)^2 - [M^{\tau_n}_n]$ is a martingale starting at zero which shows that for all $t \geq 0$,
\begin{align*}
\expectation{[M^{\tau_n}_n]_t} &=  \expectation{(M^{\tau_n}_n)_t^2} \leq \epsilon^2
\end{align*}
Now we can use a Markov bound to see that
\begin{align*}
\probability{[M_n]_\infty > \epsilon} &\leq \probability{[M_n]_\infty > \epsilon; \tau_n < \infty } + \probability{[M_n]_\infty > \epsilon; \tau_n = \infty } \\
&\leq \probability{ \tau_n < \infty } + \probability{[M_n]_{\tau_n} > \epsilon} \\
&\leq \probability{ M^*_n > \epsilon } + \epsilon^{-1} \expectation{[M_n]_{\tau_n}} \\
&\leq \probability{ M^*_n > \epsilon } + \epsilon
\end{align*}
To see that this shows convergence in probability, first note that by our assumption that $M^*_n \toprob 0$ we have $\lim_{n \to \infty} \probability{[M_n]_\infty > \epsilon} \leq \epsilon$.  But now note that the left hand limit is a decreasing function of $\epsilon$ and therefore 
\begin{align*}
\lim_{n \to \infty} \probability{[M_n]_\infty > \epsilon} &\leq \lim_{\epsilon \to 0^+} \lim_{n \to \infty} \probability{[M_n]_\infty > \epsilon} \leq \lim_{\epsilon \to 0^+} \epsilon
\end{align*}
thus as $\epsilon > 0$ was arbitrary we have shown $[M_n]_\infty \toprob 0$.

Now we assume that $[M_n]_\infty \toprob 0$.  As before let $\epsilon > 0$ be given and this time define $\tau_n = \inf \lbrace t \geq 0 \mid [M_n]_t > \epsilon^2$ which is an optional time by continuity of $[M_n]$.  

Claim: Let $N$ be a continuous local martingale with $N_0 = 0$ and $\expectation{[N]_\infty} < \infty$, the $N$ is in fact an $L^2$ bounded martingale.

To see the claim, pick $\sigma_n = \inf \lbrace t \geq 0 \mid \abs{N_t} > n \rbrace$ and we have seen that $\sigma_n$ is a localizing sequence for $N$ such that $N^{\sigma_n}$ is a bounded martingale.  Therefore $(N^{\sigma_n})_t^2 - [N^{\sigma_n}]$ is a martingale starting at zero and for all $t \geq 0$ because $[N]_t$ is increasing
\begin{align*}
\expectation {(N^{\sigma_n})_t^2} &= \expectation{[N^{\sigma_n}]_t} \leq \expectation{[N]_\infty} < \infty
\end{align*}
Therefore for fixed $t \geq 0$, the sequence $(N^{\sigma_n})^2_t$ is $L^2$ bounded and therefore the sequence $N^{\sigma_n}_t$ is uniformly integrable (Lemma \ref{BoundedLpImpliesUniformlyIntegrable}) which shows us that 
\begin{align*}
\cexpectation{\mathcal{F}_s}{N_t}   &= \lim_{n \to \infty} \cexpectation{\mathcal{F}_s}{N^{\sigma_n}_t} = \lim_{n \to \infty} N^{\sigma_n}_s = N_s
\end{align*}
and by Fatou's Lemma we have 
\begin{align*}
\expectation{N_t^2} &\leq \liminf_{n \to \infty} \expectation { (N^{\sigma_n})_t^2} \leq \expectation{[N]_\infty}
\end{align*}
which shows that $N$ is an $L^2$ bounded martingale.

Now we can apply the claim to the local martingale $M^{\tau_n}_n$ for which by definition of $\tau_n$ we have $[M^{\tau_n}_n]_\infty = [M_n]_{\tau_n} \leq \epsilon^2$ and therefore a fortiori $\expectation {[M^{\tau_n}_n]_\infty} < \infty$.  Thus we conclude that $M^{\tau_n}_n$ is an $L^2$-bounded martingale and therefore $(M^{\tau_n}_n)^2 - [M^{\tau_n}_n]$ is a uniformly integrable martingale starting at zero.  We are now in a position to mimic the first part of the proof.   By the martingale property and the defintion of $\tau_n$ we have for all $0 \leq t \leq \infty$,
\begin{align*}
\expectation{(M^{\tau_n}_n)_t^2} &= \expectation{ [M^{\tau_n}_n]_t } = \expectation{ [M_n]_{\tau_n \wedge t} } \leq \epsilon^2
\end{align*}
and by a Markov bound and Doob's $L^2$ inequality applied to the $L^2$ bounded martingale $M^{\tau_n}_n$ we get,
\begin{align*}
\probability{M^*_n \geq \epsilon} &\leq \probability{M^*_n \geq \epsilon; \tau_n < \infty }  + \probability{M^*_n \geq \epsilon ; \tau_n = \infty} \\
&\leq \probability{\tau_n < \infty }  + \probability{(M^{\tau_n}_n)^* \geq \epsilon} \\
&\leq \probability{\tau_n < \infty }  + \epsilon^{-1} \expectation{(M^{\tau_n}_n)^*} \\
&\leq \probability{[M_n]_\infty > \epsilon^2 }  + 2 \epsilon^{-1} \expectation{(M^{\tau_n}_n)^2_\infty} \\
&\leq \probability{[M_n]_\infty > \epsilon^2 }  + 2 \epsilon \\
\end{align*}
and as before take the limit as $n \to \infty$ and then as $\epsilon \to 0$ to see that $M^*_n \toprob 0$.
\end{proof}

Because the covariation process $[M,N]$ is of finite variation we can define a pointwise Lebesgue-Stieltjes integral $\int f(\omega,s) \, d[M,N]_s$ for any progressive process $f(\omega,t)$ (TODO: is jointly measurable enough?  If we assume progressive then I guess we get a local martingale out of this).  Note that there is the potential for ambiguity in interpreting an integral with respect to a process of finite variation when the integrand is a step process as we could also consider using the definition as an elementary stochastic integral.  It does turn out that these two possible definitions agree but we'll defer addressing the question and instead we will always explicitly denote the integration variable when considering a pointwise Stieltjes integral as in the expression $\int_0^t U_s \, dM_s$.  TODO: Validate that the elementary stochastic integral defined above is consistent with the definition of the pointwise Stieltjes integral; it is worth understand the point at which we need this fact as Rogers and Williams indicate that they don't require it for quite some time.  Actually the consistency when integrands are step processes is trivial to see.  The fact that Rogers and Williams defer is the deeper fact that once one has defined a stochastic integral for not necessarily continuous local martingales one has the possibility for a stochastic integral with an integrator of finite variation.  This integral can be shown to agree with the pointwise Stieltjes integral.

Before we begin we record the following simple fact about Lebesgue-Stieltjes integrals.

TODO: Remove as we moved this into a separate section.
\begin{lem}\label{StoppingStieltjesIntegrals}Let $F$ be a function of finite variation and for each $t \geq 0$ define $F^t(s) = F(t \wedge s)$, then for all measurable $g$ we have $\int_0^t g \, dF = \int g \, dF^t$.
\end{lem}
\begin{proof}
First suppose that $F$ is a non-decreasing right continuous function, and consider the Stieltjes measure $\mu_t$ defined by $F^t$ (see \ref{LebesgueStieltjesMeasure}).  For any interval $[a,b]$ we have
\begin{align*}
\mu_t([a,b]) &= F^t(b) - F^t(a) = F(b \wedge t) - F(a \wedge t) = \int_0^\infty \characteristic{[a,b]} \characteristic{[0,t]} \, dF 
\end{align*}
and therefore $\mu_t$ obtained by applying the density function $\characteristic{[0,t]}$ to the Stieltjes measure for $F$.  Now by Lemma \ref{ChainRuleDensity} we see that $\int g \, dF^t = \int g \characteristic{[0,t]} \, dF = \int_0^t g \, dF$.  To finish the result, write a function of finite variation as a difference of two montone functions.
\end{proof}

\begin{lem}\label{CovariationOfPredictableStepProcessesContinuousLocalMartingale}Let $M$ and $N$ be continuous local martingales and let $U$ and $V$ be finite predictable step processes with deterministic jump times, then 
\begin{align*}
[ \int U \, dM, \int V \, dN] &= \int U_s V_s \, d[M,N]_s \text{ a.s.}
\end{align*}
\end{lem}
\begin{proof}
We know that each of $\int U \, dM$ and $\int U \, dM$ is a continuous local martingale by Lemma \ref{StochasticIntegralPredictableStepProcess}.  In addition each of the expressions in the results is invariant under centering thus we may assume $M_0 = N_0 = 0$.  Furthermore for any optional time $\tau$ we have by Theorem \ref{OptionalQuadraticCovariation}
\begin{align*}
[ \int U \, dM, \int V \, dN]^\tau &= [ \left(\int U \, dM \right)^\tau, \left(\int V \, dN \right)^\tau] = [ \int U \, dM^\tau, \int V \, dN^\tau]
\intertext{and  by Lemma \ref{StoppingStieltjes}}
\left(\int U_s V_s \, d[M,N]_s\right)^\tau &= \int U_s V_s \, d[M,N]^\tau_s
\end{align*}
so if we choose a common localizing sequence $\tau_n \uparrow \infty$ it suffices prove the result for $M^{\tau_n}$, $N^{\tau_n}$ and $[M,N]^{\tau_n}$.  Thus, we may assume that $M$, $N$ and $[M,N]$ is each bounded.  Thus each of $M$, $N$ and $MN - [M,N]$ is a bounded martingale hence each is closable and we may in fact assume each is a bounded martingale on $[0,\infty]$.

Now we first assume that $V = 1$ and let $U = \sum_{k=1}^n \eta_k \characteristic{(t_{k-1}, t_k]}$.  By appending an extra term with $\eta_n = 0$ we may assume that $t_n=\infty$.  Now we compute using the definitions and the martingale property of $M$, $N$ and $MN-[M,N]$ to see
\begin{align*}
\expectation{N_\infty \int_0^\infty U \, dM } &= \expectation{\sum_{k=1}^n \eta_k \left(M_{t_k} - M_{t_{k-1}}\right) \sum_{k=1}^n \left( N_{t_k} - N_{t_{k-1}}\right)} \\
&= \expectation{\sum_{k=1}^n \eta_k \left(M_{t_k} N_{t_k} - M_{t_{k-1}} N_{t_{k-1}}\right)} \\
&= \expectation{\sum_{k=1}^n \eta_k \left([M, N]_{t_k} - [M, N]_{t_{k-1}}\right)} \\
&= \expectation{\int_0^\infty U_s \, d[M,N]_s}
\end{align*}
For an arbitrary optional time $\tau$ we can also apply this argument to $M^\tau$ and $N^\tau$ to see that
\begin{align*}
\expectation{N_\tau \int_0^\tau U \, dM } &= \expectation{N^\tau_\infty\int_0^\infty U \, dM^\tau } \\
&= \expectation{\int_0^\infty U_s \, d[M^\tau,N^\tau]_s} = \expectation{\int_0^\tau U_s \, d[M,N]_s}
\end{align*}
From Lemma \ref{MartingaleOptionalTimeCriterion} we see that $N_t \int_0^t U \, dM - \int_0^t U_s \, d[M,N]_s$ is a martingale
and therefore $[\int U \, dM, N] = \int_0^t U_s \, d[M,N]_s$ a.s. by uniqueness of the quadratic covariation.

Now we finish by assuming a general $V = \sum_{k=1}^n \xi_k \characteristic{(t_{k-1},t_k]}$.  Note that we can assume by redefining $\xi_k$ and $\eta_k$ appropriately that both $U$ and $V$ are defined with respect to the same sequence of deterministic jump times $0=t_0 < t_1 < \dotsb < t_n$ so in particular $UV = \sum_{k=1}^n \eta_k \xi_k \characteristic{(t_{k-1},t_k]}$.  We can compute directly twice using the special case just proven
\begin{align*}
[\int U \, dM , \int V \, dN]_t &= \int_0^t U_s \, d[M, \int V \, dN]_s \\
&= \sum_{k=1}^n \eta_k \left( [M, \int V \, dN]_{t_k \wedge t} - [M, \int V \, dN]_{t_{k-1} \wedge t} \right) \\
&= \sum_{k=1}^n \eta_k \left( \int_0 ^{t_k \wedge t} V_u \, d [M, N]_u - \int_0^{t_{k-1} \wedge t} V_u \,d [M, N]_u \right) \\
&= \sum_{k=1}^n \eta_k \sum_{j=0}^n  \xi_j \left( [M, N]_{t_j \wedge t_k\wedge t} - [M, N]_{t_{j-1} \wedge t_k \wedge t} - [M, N]_{t_j \wedge t_{k-1}\wedge t} + [M, N]_{t_{j-1} \wedge t_{k-1} \wedge t} \right) \\
&= \sum_{k=1}^n \eta_k  \xi_k \left( [M, N]_{t_k\wedge t} - [M, N]_{t_{k-1} \wedge t} \right) \\
&= \int_0^t U_s V_s \, d[M,N]_s
\end{align*}
and the full result is proven.
\end{proof}

We have the following bounds on ruin probabilities as a corollary of Optional Sampling for continuous martingales.
\begin{lem}\label{GamblersRuinContinuousMartingale}Let $M$ be a continuous martingale with $M_0 = 0$ and such that $\probability{M^* > 0} > 0$.  If we define $\tau_x = \inf \lbrace t>0 \mid M_t = x\rbrace$ then for every $a < 0 < b$ we have
\begin{align*}
\cprobability{M^* > 0}{\tau_a < \tau_b} &\leq \frac{b}{b-a} \leq \cprobability{M^* > 0}{\tau_a \leq \tau_b} 
\end{align*}
\end{lem}
\begin{proof}We know that $\tau_a$ and $\tau_b$ are optional by continuity of $M$ and Lemma \ref{HittingTimesContinuous}.  Define $\tau = \tau_a \wedge \tau_b$ which we know is optional as well.  For every $t \geq 0$, by Optional Sampling we know that $\expectation{M_{\tau \wedge t}} = M_0 = 0$.  Clearly $\lim_{t \to \infty} M_{\tau \wedge t} = M_\tau$ and by the definition of $\tau$ we know that $\abs{M_{\tau \wedge t}} \leq -a \vee b < \infty$ and therefore we can apply Dominated Convergence to conclude that $\expectation{M_\tau} = 0$.  Now we can establish bounds using two simple facts.   First by continuity of $M$, we know that $\tau_a = \tau_b$ if and only if $\tau_a = \tau_b = \tau = \infty$.   Secondly $\tau_a \neq \tau_b$ implies $M^* > 0$.  With these observations in hand,
\begin{align*}
0 &=\expectation{M_\tau; \tau_a < \tau_b} + \expectation{M_\tau; \tau_b < \tau_a} + \expectation{M_\infty; \tau_a = \tau_b =\infty} \\
&\leq a \probability{\tau_a < \tau_b} + b \probability{\tau_b < \tau_a} + b \probability{M^* > 0 ; \tau_a = \tau_b =\infty} \\
&= a \probability{\tau_a < \tau_b} + b \probability{M^* > 0 ; \tau_b \leq \tau_a} \\
&= a \probability{\tau_a < \tau_b} + b \probability{M^* > 0} - b \probability{M^* > 0 ; \tau_a < \tau_b} \\
&= b \probability{M^* > 0} - (b - a)\probability{M^* > 0 ; \tau_a < \tau_b} \\
\end{align*}
which gives the first inequality. The second inequality is demonstrated in the same way but using a lower bound for $M_\infty$ on $\tau_a = \tau_b = \infty$, 
\begin{align*}
0 &\geq a \probability{\tau_a < \tau_b} + b \probability{\tau_b < \tau_a} + a \probability{M^* > 0 ; \tau_a = \tau_b =\infty} \\
&= a \probability{M^* > 0; \tau_a \leq \tau_b} + b \probability{M^* > 0 ; \tau_b < \tau_a} \\
&= a \probability{M^* > 0; \tau_a \leq \tau_b} + b \probability{M^* > 0} - b \probability{M^* > 0 ; \tau_a \leq \tau_b} \\
&= b \probability{M^* > 0} - (b - a)\probability{M^* > 0; \tau_a \leq \tau_b} \\
\end{align*}
\end{proof}

\begin{thm}[Burkholder-Davis-Gundy Inequalities]\label{BDGInequalities}For every $p > 0$ there exist a constant $0 < c_p < \infty$ such that for every continuous local martingale $M$ with $M_0 = 0$ we have
\begin{align*}
c_p^{-1} \expectation{[M]^{p/2}_\infty} &\leq \expectation{(M^*)^p} \leq c_p \expectation{[M]^{p/2}_\infty}
\end{align*}
\end{thm}
\begin{proof}
TODO: Perform reduction to the bounded martingale case via localization and optional sampling (Kallenberg indicates that we may also assume $[M]$ is bounded).

The following argument is quite elementary in each of its steps but is not entirely obvious so we spell it out in great detail.  To derive the inequalities for expectations we'll use Lemma \ref{TailsAndExpectations} and therefore we proceed by creating tail bounds for the random variables in question.  We first work on the right hand inequality of the result.  Let $r > 0$ be fixed and define $\tau = \inf \lbrace t \geq 0 \mid M_t^2 = r \rbrace$ (which is an optional time by continuity and Lemma \ref{HittingTimesContinuous}) and define $\tilde{M} = M - M^\tau$ and $N = \tilde{M}^2 - [\tilde{M}]$.  Pick any $0 < c < 1$ (we'll later refine the required bounds on $c$) and write
\begin{align*}
\probability{(M^*)^2 \geq 4r} &= \probability{(M^*)^2 \geq 4r ; [M]_\infty \geq cr} + \probability{(M^*)^2 \geq 4r; [M]_\infty < cr} \\
&\leq \probability{[M]_\infty \geq cr} + \probability{(M^*)^2 \geq 4r; [M]_\infty < cr} 
\end{align*}
we get
\begin{align*}
\probability{(M^*)^2 \geq 4r} - \probability{[M]_\infty \geq cr} &\leq \probability{(M^*)^2 \geq 4r; [M]_\infty < cr} 
\end{align*}
Since $[\tilde{M}] = [M] - [M]^\tau$ and $[M]$ is non-decreasing it follows that $[\tilde{M}] \leq [M]$ and therefore $[M]_\infty < cr$ implies $[\tilde{M}]_\infty < cr$.  Since trivially $\tilde{M}^2 \geq 0$ we know that $[M]_\infty < cr$ implies $N > -cr$.  On $\lbrace (M^*)^2 \geq 4r \rbrace$ we know that $\tau < \infty$ and therefore $\abs{M_\tau} = \sqrt{r}$ and for any $\epsilon > 0$ we can find $t \geq 0$ such that $\abs{M_t} \geq 2\sqrt{r} - \epsilon$, thus $\abs{M_t - M_\tau} > \sqrt{r} - \epsilon$ which implies $\sup_t \tilde{M}^2_t \geq r$.  Putting these observations together we see that $\lbrace (M^*)^2 \geq 4r; [M]_\infty < cr \rbrace \subset \lbrace N > -cr; \sup_t N_t > r - cr \rbrace$ and we get 
\begin{align*}
\probability{(M^*)^2 \geq 4r} - \probability{[M]_\infty \geq cr} &\leq \probability{(M^*)^2 \geq 4r; [M]_\infty < cr} \\
&\leq \probability{N > -cr; \sup_t N_t > r - cr}
\end{align*}
Now since $N$ is a martingale with $N_0 = 0$, we can apply the Gambler's Ruin Lemma \ref{GamblersRuinContinuousMartingale} with $-cr < 0 < r - cr$ to and use the fact that $\lbrace N > -cr; \sup_t N_t > r - cr \rbrace \subset \lbrace \tau_{-cr} > \tau_{r - cr} ; N^* > 0 \rbrace$ to conclude that 
\begin{align*}
\probability {N > -cr; \sup_t N_t > r - cr} &\leq \probability{\tau_{-cr} > \tau_{r - cr} ; N^* > 0} \\
&= 1 - \probability{\tau_{-cr} \leq \tau_{r - cr} ; N^* > 0} \\
&\leq (1 - \frac{r - cr}{r -cr + cr})\probability{N^* > 0} = c \probability{N^* > 0}
\end{align*}
It is clear from the nonnegativity of $[\tilde{M}]$ and the definition of $N$ that $N^* > 0$ implies $\tilde{M}^*= (M-M^\tau)^* > 0$ which $\tau < \infty$ and therefore $(M^*)^2 > r$.  Thus
\begin{align*}
\probability{(M^*)^2 \geq 4r} - \probability{[M]_\infty \geq cr} &\leq \probability{(M^*)^2 \geq 4r; [M]_\infty < cr} \\
&\leq \probability{N > -cr; \sup_t N_t > r - cr} \\
&\leq c \probability{N^* > 0} \leq c \probability{(M^*)^2 > r}
\end{align*}

Now we multiply by $\frac{p}{2}r^{p/2-1}$ and integrate to get 
\begin{align*}
&\frac{p}{2}\int_0^\infty r^{p/2-1}\probability{(M^*)^2 \geq 4r} \, dr - \frac{p}{2}\int_0^\infty r^{p/2-1}\probability{[M]_\infty \geq cr} \, dr \\
&\leq \frac{cp}{2}\int_0^\infty r^{p/2-1} \probability{(M^*)^2 > r} \, dr \\
\end{align*}
which yields upon changing integration variables and applying Lemma \ref{TailsAndExpectations}
\begin{align*}
2^{-p} \expectation{\abs{M^*}^{p}} - c^{-p/2} \expectation{\abs{[M]_\infty}^{p/2}} &\leq c \expectation{\abs{M^*}^{p}}
\end{align*}
Thus we get the right hand inequality for $c_p = c^{-p/2}/(2^{-p} -c)$ which is a positive constant for any $0 < c < 2^{-p}$.

The proof of the left hand inequality follows the same pattern but this time we define the optional time $\tau = \inf \lbrace t \geq 0 \mid [M_t] = r \rbrace$ and as before $\tilde{M} = M - M^\tau$ and $N = \tilde{M}^2 - [\tilde{M}]$.  We let $r > 0$ be arbitrary, assuming that $0 < c < 1/4$.  We give the entire computation at once and them make some comments about the details of the justification:
\begin{align*}
\probability{[M]_\infty \geq 2r } - \probability{(M^*)^2 \geq cr} &\leq \probability{[M]_\infty \geq 2r ; (M^*)^2 < cr} \\
&\leq \probability{N < 4cr ; \inf_t N_t < 4cr -r} \\
&\leq 4c \probability{N^* > 0} \\
&\leq 4c \probability{[M]_\infty \geq r}
\end{align*}
The first inequality follows as before by a simple union bound.  To see the second inequality, note first that on $\lbrace (M^*)^2 < cr \rbrace$ by non-negativity of $[\tilde{M}]$ we have 
\begin{align*}
N &\leq \tilde{M}^2 \leq (\abs{M} + \abs{M^\tau})^2 \leq (2 M^*)^2 < 4 cr
\end{align*}
and also on $\lbrace [M]_\infty \geq 2r \rbrace$ we have $\tau < \infty$ and 
\begin{align*}
[\tilde{M}]_\infty = [M]_\infty - [M]_\tau \geq 2r - r = r
\end{align*}
To see the third inequality we again apply Gambler's Ruin Lemma \ref{GamblersRuinContinuousMartingale} to $N$  this time on $4cr - r < 0 < 4 cr$ noting that $\probability{N < 4cr ; \inf_t N_t < 4cr -r} \leq \probability{ \tau_{4cr - r} <  \tau_{4c} ; N^* > 0} \leq 4c \probability{N^* > 0}$.  The final inequality again follows from noting that $\tau < \infty$ on $N^* > 0$ and therefore because $[M]$ is non-decreasing we have $[M]_\infty \geq [M]_\tau = r$.

Again we multiply by $(p/2) r^{p/2 -1}$ and integrate to get 
\begin{align*}
&\frac{p}{2} \int_0^\infty r^{p/2 -1} \probability{[M]_\infty \geq 2r } \, dr - \frac{p}{2} \int_0^\infty r^{p/2 -1} \probability{(M^*)^2 \geq cr} \, dr\\
&\leq 4c \frac{p}{2} \int_0^\infty r^{p/2 -1} \probability{[M]_\infty \geq r} \, dr
\end{align*}
which upon changing variables and applying Lemma \ref{TailsAndExpectations}
\begin{align*}
2^{p/2} \expectation{\abs{[M]_\infty}^{p/2}} - c^{-p/2} \expectation{\abs{M^*}^p} &\leq 4c \expectation{\abs{[M]_\infty}^{p/2}} 
\end{align*}
which yields the left hand inequality with $c_p = c^{-p/2}/(2^{-p/2} -4c)$ which is positive for any $0 < c < 2^{-p/2 - 2}$.
\end{proof}

In the following Lemma we remind the reader of the notation $\int g \, \abs{dF}$ to denote integration with respect to the Lebesgue-Stieltjes measure determined by the total variation function of $F$.
\begin{lem}\label{CourregeCauchySchwartz}Let $M$ and $N$ be continuous local martingales, then almost surely for every $t \geq 0$, 
\begin{align}
\abs{[M,N]_t} &\leq \int_0^t \abs{d[M,N]}_s \leq [M]_t^{1/2} [N]_t^{1/2}
\label{CourregeCauchySchwartz1}\end{align}

Furthermore almost surely for any jointly measurable processes $U$ and $V$ we have 
\begin{align*}
\int_0^t \abs{U_sV_s} \, \abs{d[M,N]}_s &\leq \left(\int_0^t U_s^2 \, d[M]_s\right)^{1/2} \left(\int_0^t V_s^2 \, d[N]_s\right)^{1/2} 
\end{align*}
(TODO: Confirm that almost sure this holds for all $U,V$ not that for each pair $U,V$ this holds a.s.)
\end{lem}
\begin{proof}
First we can use positivity and bilinearity of quadratic covariation to see that for a fixed $t \geq 0$ and $\lambda \in \reals$ we have 
\begin{align*}
0 &\leq [M + \lambda N]_t = [M]_t + 2\lambda[M,N]_t + \lambda^2[N]_t \text{ a.s.}
\end{align*}
It follows that $\probability {\cap_{\lambda \in \rationals} \lbrace 0 \leq  [M]_t + 2\lambda[M,N]_t + \lambda^2[N]_t  \rbrace} = 1$ and by continuity of the quadratic polynomial we get that for fixed $t \geq 0$, almost surely $0 \leq [M]_t + 2\lambda[M,N]_t + \lambda^2[N]_t$ for all $\lambda \in \reals$.  Taking the discriminant of the quadratic polynomial and using the fact that it must be non-negative we see that for every $t \geq 0$ we have $[M,N]_t^2 \leq [M]_t [N]_t$ almost surely.  Again, taking the intersection of a countable number of almost sure events we see that almost surely we have $[M,N]_q^2 \leq [M]_q [N]_q$ for all $q \in \rationals$ with $q \geq 0$ and by continuity of the quadratic variation this implies that almost surely $[M,N]_t^2 \leq [M]_t [N]_t$ for all $t \geq 0$.

Now fix an $s \geq 0$ and consider the processes $M - M^s$ and $N - N^s$.  Replaying our continuity argument once more we see that almost surely the inequality just proven will hold almost surely over all the processes $M - M^s$, $N - N^s$ and all $t \geq 0$.   Using this fact and Theorem \ref{OptionalQuadraticCovariation} we conclude that almost surely for all $s \geq 0$ and $s < t$ we have
\begin{align*}
\abs{[M,N]_t - [M,N]_s } &=\abs{ [M - M^s, N-N^s]_t} \leq \left([M]_t - [M]_s\right)^{1/2} \left([N]_t - [N]_s\right)^{1/2}
\end{align*}
Suppose we are given a partition $s=t_0 < \dotsb < t_n=t$ and use the triangle inequality, the Cauchy-Schwartz inequality for sequences and the above inequality gives us 
\begin{align*}
\abs{[M,N]_t - [M,N]_s} &\leq \sum_{j=1}^n \abs{[M,N]_{t_j} - [M,N]_{t_{j-1}}} \\
&\leq \sum_{j=1}^n \left([M]_{t_j} - [M]_{t_{j-1}}\right)^{1/2} \left([N]_{t_j} - [N]_{t_{j-1}}\right)^{1/2} \\
&\leq  \left(\sum_{j=1}^n [M]_{t_j} - [M]_{t_{j-1}}\right)^{1/2} \left(\sum_{j=1}^n [N]_{t_j} - [N]_{t_{j-1}}\right)^{1/2} \\
&= ([M]_t - [M]_s)^{1/2} ([N]_t-[N]_s)^{1/2}
\end{align*}
Again, note that this holds almost sure simultaneously for all $0 \leq s < t$, all $n \geq 0$ and all partitions $s=t_0 < \dotsb < t_n=t$.  We may then take the supremum over all partitions to get 
\begin{align*}
\abs{[M,N]_t - [M,N]_s} &\leq \int_s^t \abs{d[M,N]_s} \leq ([M]_t - [M]_s)^{1/2} ([N]_t-[N]_s)^{1/2}
\end{align*}
and substituting $s=0$ we get \eqref{CourregeCauchySchwartz1}.

Before proceeding further it is helpful to name all of the random Lebesgue-Stieltjes measures floating around: let $\mu = d[M]$, $\nu = d[N]$ and $\rho = \abs{d[M,N]}$.  Note that we have shown that almost surely for every closed interval $I \subset \reals$ we have $\rho(I)^2 \leq \mu(I) \nu(I)$.  By continuity of $[M]$, $[N]$ and $[M,N]$ the measures above have no atoms and therefore this inequality also holds for open intervals.  Now if we let $G$ be an arbitrary open set then we can write it as a disjoint union of open intervals (Lemma \ref{OpenSetsOfReals}) $G = \cup_{n=1}^\infty I_n$.  Then by countable additivity and Cauchy-Schwartz for sequences
\begin{align*}
\rho(G) &= \sum_{n=1}^\infty \rho(I_n) \leq \sum_{n=1}^\infty \mu(I_n)^{1/2}\nu(I_n)^{1/2} \\
&\leq \left( \sum_{n=1}^\infty \mu(I_n) \right)^{1/2}\left( \sum_{n=1}^\infty \nu(I_n) \right)^{1/2} = \mu(G)^{1/2}\nu(G)^{1/2}
\end{align*}

TODO: Extend to general Borel sets by monotone classes: I think we needed boundedness of the measures here.

Now let $f = \sum_{i=1}^n a_i \characteristic{A_i}$ and $g = \sum_{i=1}^n b_i \characteristic{A_i}$ be positive simple functions.  Then once again applying Cauchy-Schwartz for sequences we get
\begin{align*}
\int f(s) g(s) \, \abs{d[M,N]_s} &= \sum_{i=1}^n a_i b_i \rho(A_i) \\
&\leq \sum_{i=1}^n a_ib_i \mu(A_i)^{1/2} \nu(A_i)^{1/2} \\
&\leq \left(\sum_{i=1}^n a_i^2 \mu(A_i) \right)^{1/2} \left( \sum_{i=1}^n b_i^2 \nu(A_i)\right)^{1/2} \\
&= \left( \int f^2(s) \, d[M]_s \right)^{1/2} \left( \int g^2(s) \, d[N]_s \right)^{1/2} 
\end{align*}


For general positive measurable functions $f$ and $g$ we take positive simple approximations $f_n \uparrow f$ and $g_n \uparrow g$ and we get by Monotone Convergence
\begin{align*}
\int f(s) g(s) \, \abs{d[M,N]_s} &= \lim_{n \to \infty} \int f_n(s) g_n(s) \, \abs{d[M,N]_s} \\
&\leq \lim_{n \to \infty}  \left( \int f_n^2(s) \, d[M]_s \right)^{1/2} \lim_{n \to \infty}\left( \int g_n^2(s) \, d[N]_s \right)^{1/2} \\
&= \left( \int f^2(s) \, d[M]_s \right)^{1/2} \left( \int g^2(s) \, d[N]_s \right)^{1/2} 
\end{align*}
noting that this holds almost surely for all $f$ and $g$ positive and measurable.

TODO: Finish, is there anything subtle about applying to the processes?
\end{proof}

\begin{defn}Given a continuous local martingale $M$ we let $L(M)$ denote the set of processes that are progressively measurable and for which $\int_0^t V^2_s \, d[M]_s < \infty$ almost surely for all $t \geq 0$.
\end{defn}
The space $L(M)$ gives the integrands for the extension of the stochastic integral with respect to the integrator $M$.
\begin{thm}\label{StochasticIntegralContinuousLocalMartingaleIntegrator}Let $M$ be a continuous local martingale and $V \in L(M)$, there exists an almost surely unique continuous local martingale $\int V \, dM$ starting at zero and for which for every continuous local martingale $N$ almost surely
$[\int V \, dM, N]_t = \int_0^t V_s \, d[M,N]_s$ for all $t \geq 0$.
\end{thm}
\begin{proof}
First we show uniqueness as we shall use it during the existence argument.  Suppose that $M^\prime$ and $M^{\prime \prime}$ are continuous local martingales starting at zero for which for every continuous local martingale $[M^\prime, N] = [M^{\prime \prime},N] = \int V_s \, d[M,N]_s$ almost surely.  By linearity of quadratic covariation, this tell us that for all $N$ we have $[M^{\prime} - M^{\prime \prime} ,N]=0$ almost surely.  In particular this will be true if we pick $N = M^{\prime} - M^{\prime \prime}$ so we know that $[M^{\prime} - M^{\prime \prime}] = 0$ almost surely.  By definition of the quadratic variation this implies that $(M^{\prime} - M^{\prime \prime})^2$ is a continuous local martingale starting at zero.  Picking a localizing sequence $\tau_n$ and using the martingale property we see that $\expectation{(M_{t \wedge \tau_n}^{\prime} - M_{t \wedge \tau_n}^{\prime \prime})^2} =0$ which shows us that $(M_{t \wedge \tau_n}^{\prime} - M_{t \wedge \tau_n}^{\prime \prime})^2$ almost surely.  Taking the limit as $n \to \infty$ we get that $(M_{t}^{\prime} - M_{t}^{\prime \prime})^2=0$ almost surely for each $t \geq 0$ hence simultaneously for all $t \in \rationals_+$ and then by continuity for all $t \geq 0$ almost surely.

We first assume that $\int_0^\infty V^2_s \, d[M]_s < \infty$ almost surely and we use the notation $\norm{V}^2_M = \int_0^\infty V^2_s \, d[M]_s$ to denote the corresponding value.  Then if $N \in \mathcal{M}^2$ we have
\begin{align*}
\abs{\expectation{\int_0^\infty V_s \, d[M,N]_s}} &\leq \expectation{\abs{\int_0^\infty V_s \, d[M,N]_s}} \\
&\leq  \expectation{\int_0^\infty \abs{V_s} \, \abs{ d[M,N]_s}} && \text{by Lemma \ref{AbsoluteValueOfStieltjes}} \\
&\leq  \expectation{\left(\int_0^\infty V^2_s \, d[M]_s\right)^{1/2}\left(\int_0^\infty d[N]_s\right)^{1/2}} && \text{by Lemma \ref{CourregeCauchySchwartz}} \\
&=  \expectation{\left(\int_0^\infty V^2_s \, d[M]_s\right)^{1/2}[N]_\infty^{1/2}} \\
&\leq \expectation{\int_0^\infty V^2_s \, d[M]_s}^{1/2}\expectation{[N]_\infty}^{1/2} && \text{by Cauchy Schwartz} \\
&=\norm{V}_M \expectation{N^2_\infty}^{1/2} = \norm{V}_M \norm{N}
\end{align*}
which shows that $N \mapsto \expectation{\int_0^\infty V_s \, d[M,N]_s}$ is a continuous linear functional on $\mathcal{M}^2$.  Thus since $\mathcal{M}^2$ is a Hilbert space with inner product given by $\langle M,N \rangle = \expectation{M_\infty N_\infty}$ (Lemma \ref{ContinuousL2MartingalesHilbert}) we know that there exists an $L^2$-bounded martingale $\int V \, dM \in \mathcal{M}^2$ such that $\expectation{\int_0^\infty V_s \, d[M,N]_s } = \expectation{N_\infty \cdot \int_0^\infty V \, dM}$ for all $N \in \mathcal{M}^2$ (we emphasize that the use of the integral sign in the name $\int V \, dM$ we give to this martingale is only meant to be suggestive and the reader should not get confused trying to figure out that this element can be constructed by some kind of generalized sum; at this point it is no more and no less than the element of the Hilbert space corresponding to the linear functional we've defined).

Since $V$ is progressive we know that $\int V_s \, d[M,N]_s$ is $\mathcal{F}$-adapted (Lemma \ref{StochasticStieltjesIntegral}) and we have just shown that it is integrable.  Now let $\tau$ be an arbitrary optional time and apply the above construction to $N^\tau$ (TODO: Remind why $N^\tau \in \mathcal{M}^2$).  in the following computation
\begin{align*}
\expectation{\int_0^\tau V_s \, d[M,N]_s}  &=\expectation{ \int_0^\infty V_s \, d[M,N]^\tau_s} && \text{Lemma \ref{StoppingStieltjes}}\\
&=\expectation{ \int_0^\infty V_s \, d[M,N ^\tau]_s} && \text{Lemma \ref{OptionalQuadraticCovariation}}\\
&=\expectation{ N^\tau_\infty \cdot \int_0^\infty V \, dM} && \text{definition of $\int V \, dM$}\\
&=\expectation{ N_\tau \cdot \int_0^\infty V \, dM} \\
&=\expectation{ N_\tau \cexpectationlong{\mathcal{F}_\tau}{ \int_0^\infty V \, dM} } && \text{Tower Property}\\
&=\expectation{ N_\tau \int_0^\tau V \, dM}  && \text{Optional Sampling}\\
\end{align*}
We apply Lemma \ref{MartingaleOptionalTimeCriterion} to conclude that $N_t \int_0^t V \, dM - \int_0^t V_s \, d[M,N]_s$ is a martingale.  By the continuity of $[M,N]$ we know that $\int_0^t V_s \, d[M,N]_s$ is continuous and has locally finite variation (Corollary \ref{StieltjesIntegralBoundedVariationAndContinuous}); thus uniqueness and the defining property of quadratic covariation implies $\int V_s \, d[M,N]_s = [N, \int V \, dM]$ almost surely.

The next step is to extend the defining property of the integral to arbitrary continuous local martingales.  For this we take a localizing sequence $\tau_n$ such that $N^{\tau_n}$ is bounded (hence in $\mathcal{M}^2$).  Let $A$ be the event that $\tau_n \uparrow \infty$ and for each $n$, let $A_n$ be the event that $[N^{\tau_n}, \int V \, dM] = \int V_s \, d[M,N]_s$.  For all  $\omega \in A \cap \left(\cap_{n=1}^\infty A_n \right)$ and $t \geq 0$ we have 
\begin{align*}
[N, \int V \, dM]_t(\omega) &= \lim_{n \to \infty} [N, \int V \, dM]^{\tau_n}_t(\omega) \\
&= \lim_{n \to \infty} [N ^{\tau_n}, \int V \, dM]_t(\omega) \\
&= \lim_{n \to \infty} \int_0^t V_s(\omega) \, d[M,N^{\tau_n}]_s(\omega) \\
&= \lim_{n \to \infty} \int_0^{t \wedge \tau_n} V_s(\omega) \, d[M,N]_s(\omega) \\
&= \int_0^{t} V_s(\omega) \, d[M,N]_s(\omega) \\
\end{align*}
and as $\probability{A \cap \left(\cap_{n=1}^\infty A_n \right)} = 1$ we have $[N, \int V \, dM] = \int V_s \, d[M,N]_s$ almost surely.

Lastly we must remove the assumption that $\int_0^\infty V_s^2 \, d[M]_s < \infty$.  We know that $\int_0^t V_s^2 \, d[M]_s$ is a continuous process (Lemma \ref{StieltjesIntegralBoundedVariationAndContinuous}) and therefore for every $n >0$ we can define an optional time $\tau_n = \inf \lbrace t \geq 0 \mid \int_0^t V_s^2 \, d[M]_s = n \rbrace$.  We have
\begin{align*}
\int_0^\infty V_s^2 \, d[M^{\tau_n}]_s &= \int_0^{\tau_n} V^2_s \, d[M]_s = n < \infty
\end{align*}
and by our assumption that $\int_0^t V_s^2 \, d[M]_s < \infty$ for all $t \geq 0$ we know that $\tau_n \uparrow \infty$.  We apply the existing construction to define $\int V \, dM^{\tau_n}$ and it satisfies 
\begin{align*}
[N, \int V \, dM^{\tau_n}]_t  &= \int_0^t V_s \, d[M,N]^{\tau_n}_s = \int_0^{t\wedge\tau_n} V_s \, d[M,N]_s
\end{align*}
for every continuous local martingale $N$.  Moreover for $m < n$, from the above fact and Lemma \ref{OptionalQuadraticCovariation} we have
\begin{align*}
[N, \left(\int V \, dM^{\tau_n}\right)^{\tau_m}]_t &= [N, \int V \, dM^{\tau_n}]_{t \wedge \tau_m} =\int_0^{t\wedge\tau_m} V_s \, d[M,N]_s
\end{align*}
for all continuous local martingales $N$ which by uniqueness of the stochastic integral shows $\left(\int V \, dM^{\tau_n}\right)^{\tau_m} = \int V \, dM^{\tau_m}$ so that $\int V \, dM^{\tau_m}$ and $\int V \, dM^{\tau_n}$ agree on the interval $[0,\tau_m]$.  Therefore we can define $\int_0^t V \, dM$ as the limit of $\int_0^t V \, dM^{\tau_n}$ for any $\tau_n \geq t$.  The fact that this defines an adapted process follows from writing $\int_0^t V \, dM = \sum_{n=1}^\infty \characteristic{\lbrace \tau_{n-1} \leq t < \tau_n\rbrace} \int_0^t V \, dM^{\tau_n}$ together with the facts that $\tau_n$ is optional and $\int V \, dM^{\tau_n}$ is adapted.  Continuity at $t \geq 0$ follows by picking $\tau_n > t$ and noting that $\int_0^t V \, dM = \int_0^t V \, dM^{\tau_n}$ and continuity of $\int_0^t V \, dM^{\tau_n}$ at $t$.  By Lemma \ref{LocalMartingaleLocalProperty} we know that $\int V \, dM$ is a continuous local martingale.  Lastly by construction, for all $n \geq 0$ and each continuous local martingale there is a set $A_n$ with $\probability{A_n} =1$ such that
\begin{align*}
[N, \int V \, dM]_t &=[N, \int V \, dM]^{\tau_n}_t = [N, \left(\int V \, dM\right)^{\tau_n}]_t = [N, \int V \, dM^{\tau_n}]_t \\
&= \int_0^t V_s \, d[M^{\tau_n}, N]_s = \int_0^{t\wedge \tau_n} V_s \, d[M, N]_s = \int_0^{t} V_s \, d[M, N]_s
\end{align*}
for all $0 \leq t \leq \tau_n$ on $A_n$.  Thus taking the intersection of $A_n$ we see that $[N, \int V \, dM] = \int V_s \, d[M,N]_s$ almost surely.
\end{proof}

With this Theorem proven we know that the following defintion makes sense.
\begin{defn}Given a continuous local martingale $M$ and a progressive process $V$ such that $\int_0^t V^2_s \, d[M]_s < \infty$ for all $t \geq 0$, the \emph{stochastic integral} $\int V \, dM$ is the almost surely unique continuous local martingale for which $[\int V \, dM, N]_t = \int_0^t V_s \, d[M,N]_s$ for all $t \geq 0$ almost surely for every continuous local martingale $N$.
\end{defn}

Here we collect a few of the most elementary facts about the stochastic integral.  In particular we call attention to the Ito Isometry which doesn't figure prominently in our presentation but is a critical step in others; we shall have more to say about this later.  
\begin{lem}\label{BasicPropertiesStochasticIntegralContinuousMartingale}Let $M$ be a continuous local martingales.  If $U,V \in L(M)$ such that $U_t=V_t$ for all $t \geq 0$ almost surely then $\int U \, dM = \int V \, dM$.  The stochastic integral is bilinear in both the integrand and integrator.
TODO: Be very precise about assumptions here!  E.g. is $V \in L(aM + bN)$ equivalent to $V \in L(M)$ and $V \in L(N)$?  Clearly the latter is at least as strong as the former.
If $M$ is a continuous local martingale and $V \in L(M)$ then we have $[\int V \, dM]_t = \int_0^t V^2_s \, d[M]_s$ for all $t \geq 0$ almost surely.  In particular, if $M$ is a continuous martingale then we have the \emph{Ito Isometry} 
\begin{align*}
\expectation{\left(\int_0^t V \, dM\right)^2} &= \int_0^t V_s^2 \, d[M]_s \text{ for all $t \geq 0$}
\end{align*}
\end{lem}
\begin{proof}
With the assumption that $U = V$ almost surely we see that for all continuous local martingales $N$ we have
\begin{align*}
[\int U \, dM, N]_t &= \int_0^t U_s \, d[M,N]_s = \int_0^t V_s \, d[M,N]_s = [\int V \, dM, N]_t
\end{align*}
for all $t \geq 0$ almost surely.  By the uniqueness property of the stochastic integral we have $\int U \, dM = \int V \, dM$.

Bilinearity boils down to a couple of simple computations using bilinearity of the Lebesgue-Stieltjes integral and the quadratic covariation
\begin{align*}
[\int (aV + bU) \, dM, N]_t &= \int_0^t (aV_s + bW_s) \, d[M,N]_s \\
&= a\int_0^t V_s \, d[M,N]_s + b\int_0^t W_s \, d[M,N]_s \\
&= a[\int V \, dM, N]_t + b[\int W \, dM, N]_t \\
&= [a\int V \, dM + b\int W \, dM, N]_t
\end{align*}
and
\begin{align*}
[\int V \, d[aM + bN], R]_t &= \int_0^t V_s \, d[aM+bN, R]_s \\
&= a\int_0^t V_s \, d[M, R]_s + b\int_0^t V_s \, d[N, R]_s \\
&= a[\int V \, dM, R] + b[\int V \, dN, R]_t \\
&= [a\int V \, dM + b\int V \, dN, R]_t
\end{align*}
Now apply the uniqueness criteria for stochastic integrals.

Using the defining property of the stochastic integral twice and Lemma \ref{ChainRuleStieltjes} once we see
\begin{align*}
[\int V \, dM]_t &= \int_0^t V_s \, d[M, \int V \, dM]_s = \int_0^t V_s \, d\int_0^s V_u \, d[M](u) \\
&= \int_0^t V^2(s) \, d[M]_s
\end{align*}
In the special case that $M$ is a martingale we know that $\left(\int V \, dM\right)^2 - [\int V \, dM]$ is a martingale starting at zero and thus taking expectations we get
\begin{align*}
\expectation{\left( \int_0^t V \, dM\right)^2} &= \expectation{[\int V \, dM]_t} = \expectation{\int_0^t V^2(s) \, d[M]_s}
\end{align*}
\end{proof}

It is common for the details of defining the stochastic integral to unfold a bit differently than our presentation.  The alternative presentation begins just as we have defines the stochastic integral for predictable step process integrands but then notes the property of the Ito Isometry holds for such integrands.  The basic idea is to show that predictable step processes are dense in an $L^2$ space and the use the Ito isometry to extend the definition of the stochastic integral by a completion argument.  There is a subtlety to deal with.  We note that the isometry holds for every \emph{fixed} $t \geq 0$ and thus is a family of isometries between an $L^2$ space of integrands (predictable step processes on $[0,t]$) and an $L^2$ space of random variables; it is not a single isometry between a spaces of processes.  There are two ways to proceed.  In the first case (Steele, Peres and Morters, others) one stays with the \emph{one $t$ at a time} approach and shows that step processes are dense in the progressive processes in $L^2(\Omega \times [0,t])$ and then extends the stochastic integral pointwise in $t \geq 0$.  An extra step is necessary at this point to show that one may find a version of the resulting stochastic integral process that is indeed a continuous martingale.  In the second case (e.g. Karatzas and Shreve), one defines a norm on the space of $L^2$ continuous martingales (different from the Hilbert space structure we have used) and shows that the Ito isometries can be assembled into a single isometry between the space of integrands and this space of martingales; again one extends by completion.  What about Rogers and Williams; they use the Ito isometry approach but I think the details are slightly different.
 
The basic continuity property of the stochastic integral is
\begin{lem}\label{LimitsOfStochasticIntegralContinuousLocalMartingale}Let $M_n$ be a sequence of continuous local martingales and let $V_n \in L(M_n)$, then 
$\left( \int V_n \, dM_n \right)^* \toprob 0$ if and only if $\int_0^\infty V_n^2(s) \, d[M_n](s) \toprob 0$.
\end{lem}
\begin{proof}
Lemma \ref{QuadraticCovariationAndContinuity} says that $\left( \int V_n \, dM_n \right)^* \toprob 0$ if and only if $[\int V_n \, dM_n]_\infty \toprob 0$ but Lemma \ref{BasicPropertiesStochasticIntegralContinuousMartingale} tells us that $[\int V_n \, dM_n]_\infty = \int_0^\infty V^2_n(s) \, d[M_n](s)$. 
\end{proof}

Before proceeding further we extend the class of integrators in what initially seems like a very ad-hoc manner.  Indeed this extension follows the historical path of the development of stochastic integration which broadened the scope of definitions in exactly these ways.  The reader is encouraged not to spend too much time trying to find the method in the madness as later we will prove a theorem that shows that the only continuous stochastic processes that make sense as integrators are the ones we define here.
\begin{defn}A \emph{continuous  semimartingale} $X$ is a cadlag adapted process in $\reals$ such that there is a continuous local martingale $M$ and a continuous, adapted process of locally finite variation $A$ with $A_0 = 0$ such that $X = M + A$.  A cadlag adapted process $X=(X_1, \dotsc, X_d)$ in $\reals^d$ is said to be a continuous semimartingale if and only each $X_i$ is.  Given a continuous semimartingale $X = M + A$ we let 
\begin{align*}
L(X) &= \lbrace V \mid V^2 \in L([M]) \text{ and } V \in L(A) \rbrace
\end{align*}
that is to say $L(X) = L(M) \cap L(A)$ and for any $V \in L(X)$ we define $\int V \, dX = \int V \, dM + \int V_s \, dA_s$.
\end{defn} 
Note that the decomposition $X = M + A$ is almost surely unique as if $M + A = \tilde{M} + \tilde{A}$ then $M - \tilde{M} = \tilde{A} - A$ is a continuous local martingale of locally finite variation and is therefore $0$ almost surely by Lemma \ref{ContinuousLocalMartingaleBoundedVariation}.  As such, we refer to this as the \emph{canonical decomposition}.

We want to develop the primary properties of the stochastic integral with a continuous semimartingale integrator.  Note that by the definition $\int V \, dX = \int V \, dM + \int V \, dA$ we can see that a stochastic integral with respect to a continuous semimartingale integrator is itself a continuous semimartingale.  Thus we can consider a stochastic integral as an integrator and the first result is a generalization of the ``chain rule'' proven in Lemma \ref{ChainRuleStieltjes}.
\begin{lem}\label{ChainRuleContinuousSemimartingale}Let $X$ be a continuous semimartingale and let $V \in L(X)$ then $U \in L(\int V \, dX)$ if and only if $UV \in L(X)$ and $\int U \, d\int V \, dX = \int UV \, dX$ a.s.
\end{lem}
\begin{proof}
For $X$ an adapted process of locally finite variation, this is proven in Lemma \ref{ChainRuleStieltjes}.  Now suppose that $X =M$ is a continuous local martingale.  In this case from the proof of Lemma \ref{LimitsOfStochasticIntegralContinuousLocalMartingale} and Lemma \ref{ChainRuleStieltjes} we have 
\begin{align*}
\int_0^t U_s^2 \, d[\int V \, dM]_s &= \int_0^t U_s^2 d \int_0^s V_u^2 d[M]_u = \int_0^t U_s^2 V_s^2 \, d[M]_s
\end{align*}
which shows us that $U \in L(\int V \, dM)$ if and only if $UV \in L(M)$.  Moreover, for any continuous local martingale $N$, we have
\begin{align*}
[\int U \, d\int V \, dM, N]_t &= \int_0^t U_s \, d[\int V \, dM, N]_s = \int_0^t U_s \, d \int_0^s V_u \, d[M, N]_u \\
&= \int_0^t U_s V_s \, d[M,N]_s = [\int UV \, dM, N]_t
\end{align*}
almost surely.  Thus by the defining property of stochastic integrals with a continuous local martingale integrator we know that $\int U \, d\int V \, dM = \int UV \, dM$.

Lastly let $X$ be a continuous semimartingale and let $X = M + A$ be the canonical decompostion of $X$.  Since the canonical decomposition of $\int V \, dX$ is $\int V \, dM + \int V_s \, dA_s$ we have 
\begin{align*}
L(\int V \, dX) &= L(\int V \, dM) \cap L(\int V_s \, dA_s) 
\end{align*}
hence combining results for Stieltjes integrals and continuous local martingale we have $U \in L(\int V \, dX)$ if and only if $UV \in L(M)$ and $UV \in L(A)$ (i.e. $UV \in L(X)$).  Furthermore,
\begin{align*}
\int_0^t U \, d\int V \, dX &= \int_0^t U \, d\int V \, dM + \int_0^t U \, d \int V_s \, dA_s \\
&= \int_0^t UV \, dM + \int_0^t U_s V_s \, dA_s = \int_0^t UV \, dX
\end{align*}
and the result is proven.
\end{proof}

The other useful result is the behavior of stochastic integrals under stopping (a generalizaiton of Lemma \ref{StoppingStieltjes}).
\begin{lem}\label{StoppingIntegralsContinuousSemimartingale}Let $X$ be a continuous semimartingale, $V \in L(X)$ and $\tau$ an optional time then
\begin{align*}
\left(\int V \, dX \right)^\tau &= \int V \, dX^\tau = \int \characteristic{[0,\tau]} V \, dX
\end{align*}
\end{lem}
\begin{proof}
The result is proven for Stieltjes integrals in Lemma \ref{StoppingStieltjes}, so consider next the case in which $X = M$ is a continuous local martingale.  Suppose $N$ is another continuous local martingale and compute
\begin{align*}
[\left(\int V \, dM \right)^\tau, N]_t &= [\int V \, dM, N]^\tau_t = \int_0^{t \wedge \tau} V_s \, d[M,N]_s = \int_0^t V_s \, d[M,N]^\tau_s \\
&= \int_0^t V_s \, d[M^\tau,N]_s = [\int V \, M^\tau, N]_t
\end{align*}
and similarly
\begin{align*}
[\left(\int V \, dM \right)^\tau, N]_t &= [\int V \, dM, N]^\tau_t = \int_0^t \characteristic{[0,\tau]} V_s \, d[M,N]_s = [\int \characteristic{[0,\tau]} V \, dM, N]_t
\end{align*}
and we appeal to the defining property of stochastic integrals with a continuous local martingale integrator.

For a general continuous semimartingale $X$, let $X = M + A$ be the canonical decomposition and then the fact that $M^\tau$ is a continuous local martingale and $A^\tau$ has locally finite variation to conclude that the canonical decomposition of $X^\tau$ is $M^\tau + A^\tau$ and use the results for the continuous local martingale case and the Stieltjes integral case to see
\begin{align*}
\left(\int V \, dX \right)^\tau &= \left(\int V \, dM\right)^\tau + \left(\int V_s \, dA_s\right)^\tau = \int V \, dM^\tau + \int V_s \, dA^\tau_s = \int V \, dX^\tau
\end{align*}
The second equality is equally trivial.
\end{proof}

The following Lemma will be a useful for exchanging limits and stochastic integrals and represents the fundamental continuity property of stochastic integrals.
\begin{lem}\label{DominatedConvergenceContinuousSemimartingale}Let $X$ be a continuous semimartingale and let $U, V, V_1, V_2, \dotsc \in L(X)$ with $\abs{V_n} \leq U$ and $V_n \toas V$ (TODO: Make precise what this means) then $\sup_{0 \leq s \leq t} \abs{\int_0^s V_n \, dX - \int_0^s V \, dX} \toprob 0$ for all $t \geq 0$.
\end{lem}
\begin{proof}
Write $X = M + A$ so that $U^2 \in L([M])$ and $U \in L(A)$.  By ordinary Dominated Convergence applied pointwise in $\Omega$ we know that almost surely $\int_0^t V_n(u) \, dA(u) \to \int_0^t V(u) \, dA(u)$ and $\int_0^t V_n^2(u) \, d[M](u) \to \int_0^t V^2(u) \, d[M](u)$ for every $t \geq 0$.  Because $\abs{V_n} \leq U$ we have
\begin{align*}
\abs{\int_0^t V_n(u) \, dA(u)} &\leq \int_0^t \abs{V_n(u)} \, d\abs{A}(u) \leq \int_0^t U(u) \, d\abs{A}(u)
\end{align*}
and the uniform continuity of $\int_0^t U(u) \, d\abs{A}(u)$ on every bounded interval we know that the family $\int_0^t V_n(u) \, dA(u)$ is uniformly equicontinuous on every bounded interval.  Therefore the pointwise convergence $\int_0^t V_n(u) \, dA(u) \to \int_0^t V(u) \, dA(u)$ can be extended to uniform convergence on bounded intervals $\sup_{0 \leq s \leq t} \abs{\int_0^s V_n(u) \, dA(u) - \int_0^s V(u) \, dA(u)} \toas 0$ and so it follows that $\sup_{0 \leq s \leq t} \abs{\int_0^s V_n(u) \, dA(u) - \int_0^s V(u) \, dA(u)} \toprob 0$.

From $\int_0^t V_n^2(u) \, d[M](u) \toas \int_0^t V^2(u) \, d[M](u)$ we get $\int_0^\infty V_n^2(u) \, d[M^t](u) \toas \int_0^\infty V^2(u) \, d[M^t](u)$ (Lemma \ref{StoppingStieltjes}).  By Lemma \ref{LimitsOfStochasticIntegralContinuousLocalMartingale} the latter convergence statement implies $\left(\int V_n \, dM^t  - \int V \, dM^t\right)^* \toprob 0$ and the Lemma follows since $\left(\int V_n \, dM^t  - \int V \, dM^t\right)^* = \left(\int V_n \, dM  - \int V \, dM\right)_t^*$ (TODO: Is this obvious from earlier or do we need to reference the general stopping property of stochastic integral) from Lemma \ref{StoppingIntegralsContinuousSemimartingale}.
\end{proof}

Recall that in the proof of Theorem \ref{OptionalQuadraticCovariation} we motivated the construction of the quadratic variation $[M]$ by pointing out that in the case of a bounded martingale starting at zero what we were doing was defining $[M] = M^2 - \int M \, dM$; the stochastic integral had not been defined at that point so the comment served the pedagogical purpose of motivating the formulae but wasn't mathematically justified.  Now that we have defined the stochastic integral are in a position to state and prove a proper Theorem.
\begin{thm}[Integration by parts]\label{IntegrationByPartsContinuousSemimartingale}Let $X$ and $Y$ be continuous semimartingales then 
\begin{align*}
X Y &= X_0 Y_0 + \int X \, dY + \int Y \, dX + [X,Y]
\end{align*}
\end{thm}
\begin{proof}
First let us assume $X =Y$ (we will later use polarization to extend to the general case).  Furthermore, let us assume that $X = M$ where $M \in \mathcal{M}^2$ is bounded and starts at zero.  Recall that from the proof of Theorem \ref{OptionalQuadraticCovariation}, if we define for $n \geq 0$,
\begin{align*} 
\tau^n_k &= \inf \lbrace t > \tau^n_{k-1} \mid \abs{M_t - M_{\tau^n_{k-1}}} = 2^{-n} \rbrace \text{ for $k > 0$} \\
V^n_t &= \sum_{k=0}^\infty M_{\tau^n_{k}} \characteristic{(\tau^n_{k}, \tau^n_{k+1}]}(t) \\ 
Q^n_t &= \sum_{k=0}^\infty \left (M_{t \wedge \tau^n_{k+1}}  - M_{t \wedge \tau^n_k}\right)^2 
\end{align*}
then we have the identity
\begin{align*}
M^2_t &= 2 \int_0^t V^n \, dM + Q^n_t
\end{align*}
and the convergence results that $V^n \toas M$ and $\sup_{0 \leq t < \infty} \abs{Q^n_t - [M]_t} \toprob 0$.  While in the proof of Theorem \ref{OptionalQuadraticCovariation} we weren't in a position to discuss the convergence of $\int_0^t V^n \, dM$ we now note that in addition we have $\abs{V^n_t} \leq \sup_{0 \leq s \leq t} \abs{M_s} < \infty$ so we can apply Lemma \ref{DominatedConvergenceContinuousSemimartingale} to conclude that 
\begin{align*}
\sup_{0 \leq s \leq t} \abs{\int_0^s V^n \, dM - \int_0^t M \, dM} \toprob 0
\end{align*}
for all $t \geq 0$.  So we have $Q^n_t \toas [M]_t$ and $\int_0^s V^n \, dM  \toas \int_0^t M \, dM$ along a common subsequence and therefore $M^2_t = 2 \int_0^t M \, dM + [M]_t$ almost surely.  For an arbitrary continuous local martingale $M$ we take a localizing sequence $\tau_n$ such that each $M^{\tau_n}$ is bounded (Lemma \ref{ContinuousLocalMartingaleLocalizeToBounded}) then using the result for bounded $M$, Lemma \ref{StoppingIntegralsContinuousSemimartingale} and Theorem \ref{OptionalQuadraticCovariation} we have for each $t \geq 0$, almost surely
\begin{align*}
M_t^2 &= \lim_{n \to \infty} M^2_{t \wedge \tau_n} = \lim_{n \to \infty} 2 \int_0^{t} M^{\tau_n} \, dM^{\tau_n} + [M^{\tau_n}]_t \\
&=\lim_{n \to \infty} \left( 2 \int_0^{t \wedge \tau_n} M \, dM + [M]_{t \wedge \tau_n} \right) = 2 \int_0^{t} M \, dM + [M]_{t} 
\end{align*}

Note that by Tonelli's Theorem we know that for any measurable space $S$, any $\sigma$-finite measure $\mu$ and any positive measurable function $f : S \times S \to \reals_+$ we have
\begin{align*}
\iint f(x,y) \, d\mu(x) \otimes d\mu(y) &= \int \left [ \int f(x,y) \, d\mu(y) \right] \, d\mu(x) \\
&= \int \left [ \int f(y,x) \, d\mu(x) \right] \, d\mu(y)  = \iint f(y,x) \, d\mu(x) \otimes d\mu(y)
\end{align*}
so in particular the product measure is invariant under reflection along the diagonal. Using this fact, for $X = A$ with $A$ of locally finite variation and $A_0 = 0$, we have by definition $[A] = 0$ and 
\begin{align*}
A_t^2 &= \int_0^t \int_0^t dA(u) \otimes dA(v) =2\int_0^t \left[\int_0^u \, dA(v)\right] \, dA(u) \ = 2 \int_0^t A(u) \, dA(u)
\end{align*}
so the result holds for Stieltjes integrals.

Now assume that $X = M+A$ is a continuous semimartingale with $X_0 = 0$.  Using the results for the continuous local martingale case and the Stieltjes integral case we have
\begin{align*}
X^2 &= M^2 + A^2 + 2MA = 2 \int M \, dM + 2 \int A_s \, dA_s + [M] + 2MA\\
&=2 \int X \, dX - 2 \int A \, dM - 2 \int M_s \, dA_s + [X] + 2MA 
\end{align*}
so the result will follow if we can show that $MA = \int A \, dM + \int M_s \, dA_s$ almost surely.  For this we can proceed by defining approximations.  Fix a $t \geq 0$ and for each $n > 0$ define processes $A^n_s = A_{(k-1)t/n}$ and $M^n_s = M_{tk/n}$ for $s \in (t(k-1)/n, tk/n]$.  Note $A^n$ is a predictable step process by construction and that
\begin{align*}
&\int_0^t A^n \, dM + \int_0^t M^n_s \, dA_s \\
&= \sum_{k=1}^n A_{t(k-1)/n} \left(M_{tk/n} - M_{t(k-1)/n}\right) + \sum_{k=1}^n M_{kt/n} \left(A_{tk/n} - A_{t(k-1)/n}\right) \\
&=A_t M_t
\end{align*}
for every $n > 0$.  We have $A^n \toas A$ by continuity of $A$ and therefore $\sup_{0 \leq s \leq t} \abs{\int_0^s A^n \, dM - \int A \, dM} \toprob 0$ by Lemma \ref{LimitsOfStochasticIntegralContinuousLocalMartingale} (TODO: we need domination!) and $M^n \toas M$ and therefore $\int_0^t M^n_s \, dA_s \to \int_0^t M_s \, dA_s$ by Dominated Convergence applied pointwise (TODO: We need domination!).

Now we remove the assumption $X_0 = 0$.  Applying the result proven to $X - X_0$, we have
\begin{align*}
X^2 &= (X-X_0)^2 + 2X_0X - X_0^2 = 2 \int (X - X_0) \, d(X - X_0) + [X-X_0] + 2X_0 X - X_0^2 \\
&= 2 \int X \, dX - 2X_0 (X-X_0) + [X] + 2X_0 X - X_0^2 = X_0^2 + 2 \int X \, dX + [X]
\end{align*}

Lastly, we perform the polarization to extend to general $X$ and $Y$, using bilinearity of the stochastic integral and bilinearity and symmetry of the quadratic covariation,
\begin{align*}
XY &= \frac{1}{4} \left( (X + Y)^2 - (X-Y)^2 \right) \\
&=\frac{1}{4} \bigl( (X_0 + Y_0)^2 + 2\int (X+Y) \, d(X+Y) + [X+Y] \\
&- (X_0-Y_0)^2 - 2\int (X-Y) d(X-Y) - [X-Y] \bigr) \\
&=X_0 Y_0 + \int X \, dY + \int Y \, dX + [X,Y]
\end{align*}
\end{proof}

We have mentioned that our introduction of the concept of semimartingales was not well motivated.  Though there is a much deeper justification for the relevance of the concept to be provided later, note that the integration by parts formula gives us an inkling that the concept is robust.  Even if we started with just local martingales, multiplying them together would not result in a local martingale but only a semimartingale.  The integration by parts formula shows us that the space of semimartingales forms an algebra.  Even more is true however.
The following Theorem shows that the class of continuous semimartingales is closed under composition sufficiently smooth functions and provides a means of computing many stochastic integrals.  It is probably the most important theorem in stochastic calculus.
\begin{thm}[Ito's Lemma]\label{ItoLemmaContinuousSemimartingale}Let $X$ be a continuous semimartingale and let $f \in C^2(\reals^d)$ then almost surely
\begin{align*}
f(X) &= f(X_0) + \int f^\prime(X) \, dX + \frac{1}{2} \int f^{\prime \prime}(X) (s) \, d[X](s)
\end{align*}
\end{thm}
\begin{proof}
Let $\mathcal{C}$ be set of all functions for which the result holds.
First we show that $\mathcal{C}$ contains all polynomials and then extend to smooth functions via an approximation argument.  It is trivial that it is true for $f=c$ a constant and for $f(x) = x$ the result is simply the fact that $\int_0^t dX = X - X_0$.  To see that $\mathcal{C}$ contains all polynomials, if suffices to show that $\mathcal{C}$ is an algebra.  Suppose that $f,g \in \mathcal{C}$, using integration by parts Theorem \ref{IntegrationByPartsContinuousSemimartingale}, the Chain Rule Lemma \ref{ChainRuleContinuousSemimartingale} and the defining property of stochastic integrals, we get almost surely
\begin{align*}
&f (X) g(X) - f(X_0) g(X_0) = \int f(X) \, dg(X) + \int g(X) \, df(X) + [f(X), g(X)] \\
&=\int f(X) \, d\int g^{\prime}(X) \, dX + \frac{1}{2} \int f(X) \, d\int g^{\prime \prime}(X)(s) \, d[X](s) \\
&+ \int g(X) \, d\int f^{\prime}(X) \, dX + \frac{1}{2} \int g(X) \, d\int f^{\prime \prime}(X)(s) \, d[X](s) \\
&+ [\int f^{\prime}(X)\, dX + \frac{1}{2}\int f^{\prime \prime}(X)(s) \, d[X](s) , \int g^{\prime} (X) \, dX + \frac{1}{2} \int g^{\prime\prime}(X)(s) \, d[X](s)] \\
&=\int f(X)  g^{\prime}(X) \, dX + \frac{1}{2} \int f(X) g^{\prime \prime}(X) (s) \, d[X](s) + \int g(X)  f^{\prime}(X) \, dX\\
&+ \frac{1}{2} \int g(X) f^{\prime \prime}(X)(s) \, d[X](s) + [\int f^{\prime}(X)\, dX , \int g^{\prime} (X) \, dX] \\
&=\int (fg)^{\prime}(X) \, dX + \frac{1}{2} \int f(X) g^{\prime \prime}(X) (s) \, d[X](s) + \frac{1}{2} \int g(X) f^{\prime \prime}(X)(s) \, d[X](s) \\
&+ \int f^{\prime}(X) g^{\prime} (X) (s) \, d[X](s) \\
&=\int (fg)^{\prime}(X) \, dX  + \frac{1}{2} \int (fg)^{\prime \prime}(X)(s) \, d[X](s) 
\end{align*}

Now suppose that we have $f \in C^2(\reals)$.  Let $t \geq 0$ be fixed and by the Weierstrass Approximation Theorem (Corollary \ref{WeierstrassApproximation}) (TODO: We actually need approximation in $C((-\infty, \infty); \reals)$; i.e. uniform approximation on compact sets) find a polynomials $q_n(x)$ such that $q_n$ uniformly approximates $f^{\prime \prime}(x)$ on every interval $[-c,c]$.  Taking two antiderivatives of each $q_n(x)$ we get polynomials $p_n(x)$ such that 
\begin{align*}
\lim_{n \to \infty} \sup_{-c \leq x \leq c} \abs{f(x) - p_n(x)} \vee \abs{f^{\prime}(x) - p^{\prime}_n(x)}  \vee \abs{f^{\prime\prime}(x) - p^{\prime\prime}_n(x)} &= 0
\end{align*}
for every $t \geq 0$.  In particular, $p_n(X_t(\omega)) \to f(X_t(\omega))$ for every $t \geq 0$ and $\omega \in \Omega$.
TODO: Finish
\end{proof}

It can be notationally convenient to have a complex variables version of Ito's Lemma.  We say that $Z$ is a complex semimartingale if $Z = X + iY$ where each of $X$ and $Y$ is a real semimartingale.  
\begin{cor}\label{ItoLemmaContinuousSemimartingaleEntireFunction}Let $Z$ be a complex semimartingale and let $f$ be an entire function then
\begin{align*}
f(Z_t) &= f(Z_0) + \int_0^t f^{\prime}(Z) \, dZ + \frac{1}{2} \int_0^t f^{\prime \prime}(Z) \, d[Z]
\end{align*}
\end{cor}
\begin{proof}
Write $Z = X + iY$ where $X$ and $Y$ are each real valued semimartingales.  If we write $f = g + ih$ then we know that $g$ and $h$ are analytic but in particular are in $C^2(\reals^2)$ so we may apply Ito's Lemma to them.  Unfolding our notation and using the Cauchy-Riemann equations $\frac{\partial g}{\partial x} = \frac{\partial h}{\partial y}$ and  $\frac{\partial g}{\partial y} =  -\frac{\partial h}{\partial x}$  we get 
\begin{align*}
&\int_0^t f^{\prime}(Z) \, dZ = \int_0^t (\frac {\partial g}{\partial x}(X,Y) + i \frac {\partial h}{\partial x}(X,Y) ) \, d(X + iY) \\
&= \int_0^t \frac {\partial g}{\partial x}(X,Y) \, dX - \int_0^t \frac {\partial h}{\partial x}(X,Y)  \, dY + i \int_0^t \frac {\partial h}{\partial x}(X,Y) \, dX + i \int_0^t \frac {\partial g}{\partial x}(X,Y) \, dY \\
&= \int_0^t \frac {\partial g}{\partial x}(X,Y) \, dX + \int_0^t \frac {\partial g}{\partial y}(X,Y)  \, dY + i \int_0^t \frac {\partial h}{\partial x}(X,Y) \, dX + i \int_0^t \frac {\partial h}{\partial y}(X,Y) \, dY \\
\end{align*}
Similarly recall that two application of Cauchy-Riemann implies $\frac{\partial^2 g}{\partial x^2} = \frac{\partial^2 h}{\partial x \partial y} = -\frac{\partial^2 g}{\partial x^2}$ and similarly with $h$ to get
\begin{align*}
&\int_0^t f^{\prime \prime}(Z) \, d[Z] = \int_0^t (\frac {\partial^2 g}{\partial x^2}(X,Y) + i \frac {\partial^2 h}{\partial x^2}(X,Y) ) \, d[X + iY] \\
&= \int_0^t \frac {\partial^2 g}{\partial x^2}(X,Y) \, d[X] - 2 \int_0^t \frac {\partial^2 h}{\partial x^2}(X,Y)  \, d[X,Y] -\int_0^t \frac {\partial^2 g}{\partial x^2}(X,Y) \, d[Y] \\ 
&+i \int_0^t \frac {\partial^2 h}{\partial x^2}(X,Y) \, d[X] + 2 i \int_0^t \frac {\partial^2 g}{\partial x^2}(X,Y)  \, d[X,Y] - i \int_0^t \frac {\partial^2 h}{\partial x^2}(X,Y) \, d[Y] \\
&= \int_0^t \frac {\partial^2 g}{\partial x^2}(X,Y) \, d[X] + 2\int_0^t \frac {\partial^2 g}{\partial x \partial y}(X,Y)  \, d[X,Y] + \int_0^t \frac {\partial^2 g}{\partial y^2}(X,Y) \, d[Y] \\ 
&+i \int_0^t \frac {\partial^2 h}{\partial x^2}(X,Y) \, d[X] + 2 i \int_0^t \frac {\partial^2 h}{\partial x \partial y}(X,Y)  \, d[X,Y] + i \int_0^t \frac {\partial^2 h}{\partial y^2}(X,Y) \, d[Y] \\
\end{align*}
Applying Ito's Lemma to each of $g$ and $h$ separately and using the above formulae we conclude that 
\begin{align*}
f(Z) &= g(X,Y) + ih(X,Y) = f(Z_0) + \int_0^t f^{\prime}(Z) \, dZ + \frac{1}{2}\int_0^t f^{\prime \prime}(Z) \, d[Z]
\end{align*}
\end{proof}

The following lemma provides an intuitively appealing interpretation of the quadratic covariation that ties it together with the traditional notion of variation in measure theory.  Note that the convergence of the approximation is in probability and not almost sure convergence.  By making more assumptions about the underlying partitions one can prove an almost sure approximation (or simply pass to an appropriate subsequence).
\begin{lem}\label{ApproximateOptionalQuadraticCovariationContinuousSemimartingale}Let $X$ and $Y$ be continuous semimartingales, let $t \geq 0$ be fixed and suppose that we have a sequence of partitions $0=t_{n,0} < t_{n,1} < \dotsb < t_{n, k_n}=t$ such that $\lim_{n \to \infty} \max_{1 \leq k \leq k_n} (t_{n,k} - t_{n, k-1}) = 0$, then 
\begin{align*}
\sum_{k=1}^{k_n} (X_{n,k} - X_{n,k-1}) (Y_{n,k} - Y_{n,k-1}) \toprob [X,Y]_t
\end{align*}
\end{lem}
\begin{proof}
Using $[X,Y] = [X-X_0, Y-Y_0]$ it is immediate that we may assume $X_0 = Y_0 = 0$.  Given the partition $0=t_{n,0} < t_{n,1} < \dotsb < t_{n, k_n}=t$ we define predictable step processes $X^n_s = \sum_{k=1}^{k_n} X_{t_{k-1}} \characteristic{(t_{k-1}, t_k]}(s)$ and $Y^n_s = \sum_{k=1}^{k_n} Y_{t_{k-1}} \characteristic{(t_{k-1}, t_k]}(s)$.  By a little algebra using the fact that the integrals $\int X^n \, dY$ and $\int Y^n \, dX$ are given by Riemann sums we see
\begin{align*}
&\sum_{k=1}^{k_n} (X_{n,k} - X_{n,k-1}) (Y_{n,k} - Y_{n,k-1}) \\
&= \sum_{k=1}^{k_n} X_{n,k} (Y_{n,k} - Y_{n,k-1})  - \int_0^t X^n \, dY \\
&= \sum_{k=1}^{k_n}( X_{n,k} Y_{n,k} - X_{n,k-1} Y_{n,k-1})  - \int_0^t X^n \, dY - \int_0^t Y^n \, dX \\
&= X_t Y_t - \int_0^t X^n \, dY - \int_0^t Y^n \, dX \\
\end{align*}
By continuity of $X$ and $Y$ we see that $X^n \toas X$ and $X^n_t\leq X^*_t< \infty$.  Since $X$ is continuous, $X^*$ is also continuous hence $X^* \in L(Y)$, therefore we may apply Lemma \ref{DominatedConvergenceContinuousSemimartingale} to conclude that $\int_0^t X^n \, dY \toprob \int_0^t X \, dY$.  In exactly the same way we see that $\int_0^t Y^n \, dX \toprob \int_0^t Y \, dX$.  Now we can apply integration by parts Lemma \ref{IntegrationByPartsContinuousSemimartingale} to conclude that 
\begin{align*}
\sum_{k=1}^{k_n} (X_{n,k} - X_{n,k-1}) (Y_{n,k} - Y_{n,k-1})  \toprob X_t Y_t - \int_0^t X \, dY - \int_0^t Y \, dX = [X,Y]_t
\end{align*}
\end{proof}

\subsection{Approximation By Step Processes}

We defined the stochastic integral in an elegant but somewhat abstract way as the representative of a linear functional on a Hilbert space.  The uniqueness property of the integral showed us that this definition was consistent with intuitively clear defintion of the stochastic integral for step process integrands as Riemann sums.  The uniqueness property of the stochastic integral has shown itself to be a very useful technical tool but is lacking somewhat in intuitive appeal.  We repair this deficiency by showing that the continuity properties of the stochastic integral also characterize the extension from step process integrands.  To see this requires that we understand the approximation by step processes in the spaces $L(M)$.  We note that these approximation results also lead to an alternative path to defining the stochastic integral in the first place.  

\begin{lem}Let $X$ be a continuous semimartingale with canonical decomposition $X = M+A$ and let $V \in L(X)$.  Then there exists processes $V_1, V_2, \dotsc \in \mathcal{E}$ such that almost surely $\lim_{n \to \infty} \int_0^t (V_n -V)^2(s) \, d[M](s) = 0$ and $\lim_{n \to \infty} \sup_{0 \leq s \leq t} \abs{\int_0^s (V_n-V)(u) \, dA(u) } = 0$ for all $t \geq 0$.
\end{lem}
\begin{proof}
TODO: A bunch of stuff

Now suppose that $A$ is a strictly increasing, continuous and adapted process with $A_0=0$.  If one thinks for a moment about the case in which $A_t = t$ then it is more or less clear how to approximate any integrable function $f$ by a continuous one: just define $f^h(t) = \frac{1}{h}\int_{t-h}^t f(s) \, ds$ for $h > 0$ and note that by the Fundamental Theorem of Calculus for almost all $t$ we have $\lim_{h \to 0^+} f^h(t) = f(t)$.  If we treat a general Stieltjes integral then we just have to use the fact that every Lebesgue-Stieltjes measure is of the form $\pushforward{G}{\lambda}$.  Specifically, from the proof of Lemma \ref{LebesgueStieltjesMeasure} recall that if $F$ is nondecreasing and right continuous then the Lebesgue-Stieltjes measure associated with $F$ is given by $\pushforward{G}{\lambda}$ where
$G(t) = \sup\lbrace s \mid F(s) < t \rbrace$.
Let us apply this to our process $A$ pointwise by defining the process, $T_t = \sup \lbrace s \geq 0 \mid A_s < t \rbrace$ for $t \geq 0$.  TODO: Show $T$ is a process.  Because we have assumed that $A$ is strictly increasing, $T$ is strictly increasing and is an actual inverse satisfying $T(A(t)) = A(T(t)) = t$.  We can now define the approximation for $h > 0$ and $t > 0$,
\begin{align*}
V^h_t &= \frac{1}{h} \int_{T((A_t-h) \vee 0)}^t V(s) \, dA(s) = \frac{1}{h} \int_{(A_t-h) \vee 0}^{A_t} V(T(s)) \, ds
\end{align*}
where we have used the change of variables Lemma \ref{ChangeOfVariables} and the fact that $T((A_t - h) \vee 0)\leq T(s) \leq t$ if and only if $(A_t - h) \vee 0 \leq s \leq A(t)$.  TODO: What about $t=0$?  Having expressed the definition of $V^h_t$ in terms of an ordinary Lebesgue integral, we can apply the Fundamental Theorem of Calculus to see that 
\begin{align*}
\lim_{h \to 0} V^h (T(t)) &= \lim_{h \to 0} \frac{1}{h} \int_{(t-h) \vee 0}^{t} V(T(s)) \, ds = V(T(t))
\end{align*} 
for almost all $0 \leq t \leq A_1$.  Now we can apply the Dominated Convergence Theorem to conclude 
\begin{align*}
\lim_{h \to 0} \int_0^1 \abs{V^h_s - V_s} \, dA_s &= \lim_{h \to 0} \int_0^{A_1} \abs{V^h(T(s)) - V(T(s))} \, ds = 0
\end{align*}
\end{proof}

\begin{lem}\label{SimpleProcessApproximationBoundedLebesgue}Let $V$ be a bounded $\mathcal{F}$-adapted process then there exist $V^n \in \mathcal{E}$ such that 
\begin{align*}
\sup_{0 \leq T < \infty} \lim_{n \to \infty} \expectation{\int_0^T \abs{V_s - V_s^n}^2 \, ds} = 0
\end{align*}
\end{lem}
\begin{proof}
First fix a $T \geq 0$ and we will approximate on the interval $[0,T]$.  It is also notationally convenient to set $V_t = 0$ for all $t < 0$ in what follows.  Set up the following family of approximations; for every $s \geq 0$ and $n \in \naturals$ define
\begin{align*}
V^{(n,s)}_t(\omega)  &= \sum_{j=0}^{\ceil{2^n T}} V_{j/2^n + s}(\omega) \characteristic{(j/2^n + s, (j+1)/2^n + s]}(t) \characteristic{[0,T]}(t)
\end{align*}
Note that $V^{(n,s)} \in \mathcal{E}$ and moreover it is jointly measurable in $(s,t,\omega)$.  Also note that $V^{(n,s)}_t = V^{(n, s+1/2^n)}_t$ for all $s \geq 0$ and all $t \geq 0$.

Claim: Let $f \in L^2([0,T])$ then $\lim_{h \downarrow 0} \int_0^T (f(s) - f((s - h) \vee 0))^2 \, ds= 0$.

By Lemma \ref{LpApproximationByContinuous} we can find bounded continuous $f_n$ such that $f_n \tolp{2} f$.  By the triangle inequality, continuity of $f_n$, Dominated Convergence and the translation invariance of Lebesgue measure we get for every $n$
\begin{align*}
&\lim_{h \downarrow 0} \left(\int_0^T (f(s) - f((s - h) \vee 0))^2 \, ds\right)^{1/2} \\
&\leq \left(\int_0^T (f(s) -f_n(s))^2 \, ds\right)^{1/2} + \\
&\lim_{h \downarrow 0} \left(\int_0^T (f_n(s) - f_n((s - h) \vee 0))^2 \, ds\right)^{1/2} + \\
&\lim_{h \downarrow 0} \left(\int_0^T (f_n((s-h) \vee 0) - f((s - h) \vee 0))^2 \, ds\right)^{1/2} \\
&=\left(\int_0^T (f(s) -f_n(s))^2 \, ds\right)^{1/2} + \lim_{h \downarrow 0} \left(\int_0^{T-h} (f_n(s) - f(s))^2 \, ds\right)^{1/2}\\
&\leq 2 \norm{f - f_n}_2
\end{align*}
so we now take the limit as $n \to \infty$.

It is a simple matter to extend this result to a bounded adapted process $V$.  In this case we know that $\int_0^T (V_s - V_{(s-h) \vee 0})^2 \, ds$ is bounded and therefore we conclude from Dominated Convergence and the result on $L^2([0,T])$ that
\begin{align*}
\lim_{h \downarrow 0} \expectation{\int_0^T (V_s - V_{(s-h) \vee 0})^2 \, ds} &= \expectation{\lim_{h \downarrow 0} \int_0^T (V_s - V_{(s-h) \vee 0})^2 \, ds} = 0
\end{align*}

Claim: $\lim_{n \to \infty } \expectation {\int_0^T \int_0^1 (V^{(n,s)}_t - V_t)^2 \, ds \, dt} = 0$.

First off, from $V^{(n,s)}_t = V^{(n, s+1/2^n)}_t$, the definition of $V^{(n,s)}_t$ and a change of integration variable we write
\begin{align*}
\int_0^1 (V^{(n,s)}_t - V_t)^2 \, ds &= 2^n \int_0^{2^{-n}}  (V^{(n,s)}_t - V_t)^2 \, ds = 2^n \int_{t-2^{-n}}^t  (V_s - V_t)^2 \, ds = 2^n \int_0^{2^{-n}}  (V_t - V_{t - h})^2 \, dh
\end{align*}
Now using this fact and Tonelli's Theorem
\begin{align*}
\expectation {\int_0^T \int_0^1 (V^{(n,s)}_t - V_t)^2 \, ds \, dt}  &= 2^n \int_0^{2^{-n}} \expectation {\int_0^T (V_t - V_{t -h})^2 \, dt}  \, dh
\end{align*}
By the previous claim for any $\epsilon > 0$ we can find $N>0$ such that $\expectation {\int_0^T (V_t - V_{t -h})^2 \, dt} < \epsilon$ for all $0 \leq h \leq 2^{-N}$ and therefore for all $0 \leq h \leq 2^{-n}$ for any $n \geq N$.  Thus for any $n \geq N$ we have $\expectation {\int_0^T \int_0^1 (V^{(n,s)}_t - V_t)^2 \, ds \, dt} < \epsilon$ and the claim is shown by letting $\epsilon \to 0$.

TODO: Make sure we deal with the boundary at $0$ consistently (we're not at the moment).

Viewing $\expectation {\int_0^n (V^{n}_t - V_t)^2 \, dt}$ as a random variable on the probability space $([0,1], \mathcal{B}([0,1]), \lambda)$  and applying Tonelli's Theorem to previous claim, conclude $\expectation {\int_0^T (V^{(n,s)}_t - V_t)^2 \, dt} \tolp{1} 0$ which implies $\expectation {\int_0^T (V^{(n,s)}_t - V_t)^2 \, dt} \toas 0$ along some subsequence $N \subset \naturals$ (Lemma \ref{ConvergenceInMeanImpliesInProbability}  and Lemma \ref{ConvergenceInProbabilityAlmostSureSubsequence}).  Pick any $s \in [0,1]$ where the subsequence converges.

To finish the proof, for each $n \in \naturals$ we apply the result for fixed $T=n$ and find an element $V^n \in \mathcal{E}$ such that $\expectation {\int_0^n (V^{n}_t - V_t)^2 \, dt} < 1/n$.  Then given $T > 0$ and any $\epsilon > 0$ it holds for any $n > \epsilon^{-1} \vee T$ that 
\begin{align*}
\expectation {\int_0^T (V^{n}_t - V_t)^2 \, dt} &< \expectation {\int_0^n (V^{n}_t - V_t)^2 \, dt} < 1/n < \epsilon
\end{align*}
and the result is proven.
\end{proof}

\begin{lem}\label{SimpleProcessApproximationPredictableStepProcess}Let $A$ be a non-decreasing, continuous and $\mathcal{F}$-adapted process such that $A_0 = 0$ and $\expectation{A_t} < \infty$ for all $t \geq 0$.  Let $\sigma$ and $\tau$ be bounded $\mathcal{F}$-optional times such that $\sigma \leq \tau$ and $\xi$ be an $\mathcal{F}_\sigma$-measurable bounded random variable.
Then there exist $V^n \in \mathcal{E}$ such that 
\begin{align*}
\sup_{0 \leq T < \infty} \lim_{n \to \infty} \expectation{\int_0^T \abs{\xi \characteristic{(\sigma, \tau]}(s) - V^n(s)}^2 \, dA(s)} = 0
\end{align*}
\end{lem}
\begin{proof}
Take the standard discrete approximation of optional times $\tau_n = \frac{1}{2^n} \floor{2^n \tau + 1}$ and $\sigma_n = \frac{1}{2^n} \floor{2^n \sigma + 1}$ (Lemma \ref{DiscreteApproximationOptionalTimes}) so that $\tau_n \downarrow \tau$ and $\sigma_n \downarrow \sigma$.  Note that $s \in (\sigma_n, \tau_n]$ if and only if there exists a $k$ such that $\tau_n \geq k/2^n$, $\sigma_n \leq (k-1)/2^n$ and $(k-1)/2^n < s \leq k/2^n$.  As $\tau_n = k/2^n$ when $(k-1)/2^n \leq \tau < k/2^n$ and likewise for $\sigma_n$ we see that $\tau_n \geq k/2^n$ is equivalent to $\tau_n \geq (k-1)/2^n$ and $\sigma_n \leq (k-1)/2^n$ is equivalent to $\sigma < (k-1)/2^n$. From these facts and the boundedness of $\tau$ we see that 
\begin{align*}
\characteristic{(\sigma_n, \tau_n]}(s) &= \sum_{k=1}^N \characteristic{\lbrace \sigma < (k-1)/2^n \leq \tau \rbrace} \characteristic{ ((k-1)/2^n, k/2^n]}(s)
\end{align*}
for some large $N$.  Now we define 
\begin{align*}
V^n &= \xi \characteristic{(\sigma_n, \tau_n]}(s) = \sum_{k=1}^N \xi \characteristic{\lbrace \sigma < (k-1)/2^n \leq \tau \rbrace} \characteristic{ ((k-1)/2^n, k/2^n]}(s)
\end{align*}
and claim that $V^n \in \mathcal{E}$.  

Lastly we note that because $\sigma < \sigma_n \leq \tau < \tau_n$ and $\xi$ is bounded (say $\abs{\xi} \leq K$) we get
\begin{align*}
\expectation{\int_0^T \abs{\xi \characteristic{(\sigma, \tau]}(s) - V^n(s)}^2 \, dA(s)} &=\expectation{\xi^2 \int_0^T (\characteristic{(\sigma, \tau]}(s) - \characteristic{(\sigma_n, \tau_n]}(s))^2 \, dA(s)} \\
&=\expectation{\xi^2 (A_{\tau_n} - A_\tau)} + \expectation{\xi^2 (A_{\sigma_n} - A_\sigma)} \\
&\leq K^2 \expectation{ (A_{\tau_n} - A_\tau)} + \expectation{(A_{\sigma_n} - A_\sigma)} \\
\end{align*}
If we let $C$ be a bound for $\tau$, it follows that $\tau_n$ is bounded by $C+1$ for all $n$ and by the non-decreasingness of $A$ we have $\abs{A_{\tau_n} - A_\tau} \leq 2A_{C+1}$ and similarly with $\sigma$, therefore by Dominated Convergence we get $\lim_{n \to \infty} \expectation{\int_0^T \abs{\xi \characteristic{(\sigma, \tau]}(s) - V^n(s)}^2 \, dA(s)}=0$.

TODO: If we need the sup over $T$ then we have that argument elsewhere; check if we really use it.
\end{proof}

\begin{lem}Let $A$ be a non-decreasing, continuous and $\mathcal{F}$-adapted process with $A_0=0$ and $\expectation{A_t} < \infty$ for all $t \geq 0$.  Let $V$ be an $\mathcal{F}$-progressively measurable process such that 
\begin{align*}
\expectation{\int_0^t V^2_s \, dA_s} < \infty
\end{align*}
for every $t \geq 0$, then 
then there exist $V^n \in \mathcal{E}$ such that 
\begin{align*}
\sup_{0 \leq T < \infty} \lim_{m \to \infty} \expectation{\int_0^T \abs{V(s) - V^n(s)}^2 \, dA(s)} = 0
\end{align*}
\end{lem}
\begin{proof}
Pick a $T \geq 0$ fixed and assume that $V_t = 0$ for all $t > T$ and that $V_t(\omega) \leq C$ for all $t \geq 0$ and $\omega \in \Omega$.  Now we want to use the fact that a Lebesgue-Stieltjes integral can be reduced to an ordinary Lebesgue integral via change of variables: this will allow us to use Lemma \ref{SimpleProcessApproximationBoundedLebesgue}.  To make dealing with the change of variables a bit easier, consider $A_s + s$ which is a strictly increasing function; in this case we have genuine inverse $T_s$ that is increasing.  Moreover since $A_{T_s}+T_s = s$ and $A_s \geq 0$ we have $T_s \leq s$ and from the increasingness of $T_s$ we have $\lbrace T_s \leq t \rbrace = \lbrace s \leq A_t + t\rbrace \in \mathcal{F}_t$; so in particular, each $T_s$ is a bounded $\mathcal{F}$-optional time.  Now define the process $W_s = V_{T_s}$ and the filtration $\mathcal{G}_s = \mathcal{F}_{T_s}$ and note that by $\mathcal{F}$-progressive measurability of $V$ and Lemma \ref{StoppedProgressivelyMeasurableProcess} we know that $W_s$ is $\mathcal{G}$-adapted.  Also we compute
\begin{align*}
\expectation { \int_0^\infty W_s^2 \, ds} &= \expectation { \int_0^\infty \characteristic{T_s \leq T} (s) V_{T_s}^2 \, ds} = \expectation { \int_0^{A_T + T} V_{T_s}^2 \, ds} \leq C ( \expectation{A_T}+T) < \infty
\end{align*}
so that in particular $\lim_{R \to \infty} \expectation { \int_R^\infty W_s^2 \, ds} = 0$.  
By our boundedness assumption and Lemma  \ref{SimpleProcessApproximationBoundedLebesgue} we know that we can approximate $W$ by $\mathcal{G}$-predictable step processes with deterministic jump times.  Thus if we let $\epsilon > 0$ then we can find $R>0$ such that $\expectation { \int_R^\infty W_s^2 \, ds} < \epsilon/2$ and $W^\epsilon_s = \xi_0 \characteristic{\lbrace 0 \rbrace}(s) + \sum_{j=1}^n \xi_j \characteristic{(s_{j-1}, s_j]}(s)$ such that 
$\expectation{\int_0^R \abs{W_s - W_s^\epsilon}^2 \, ds} < \epsilon/2$ and by defining $W^\epsilon_s = 0$ for $s > R$ we have
\begin{align*}
\expectation{\int_0^\infty \abs{W_s - W_s^\epsilon}^2 \, ds} &= \expectation{\int_0^R \abs{W_s - W_s^\epsilon}^2 \, ds}  + \expectation{\int_R^\infty W_s^2 \, ds} < \epsilon
\end{align*}
Now we undo our change of variables to see what type of approximation we have of $V$.  Let 
\begin{align*}
V^\epsilon_s &= W^\epsilon_{A_s+s} = \xi_0 \characteristic{\lbrace 0 \rbrace}(A_s+s) + \sum_{j=1}^n \xi_j \characteristic{(s_{j-1}, s_j]}(A_s+s) \\
&=\xi_0 \characteristic{\lbrace 0 \rbrace}(s) + \sum_{j=1}^n \xi_j \characteristic{(T_{s_{j-1}}, T_{s_j}]}(s) 
\end{align*}
we claim that $V^\epsilon$ is $\mathcal{F}$-adapted.  This follows from the fact that $\xi_j$ is $\mathcal{F}_{s_{j-1}}$-measurable and for any $u > 0$ and $j \geq 1$, 
\begin{align*}
\lbrace \xi_j \characteristic{(T_{s_{j-1}}, T_{s_j}]}(s) \leq u \rbrace &= \lbrace \xi_j \leq u \rbrace \cap \lbrace T_{s_{j-1}} < s \rbrace \cap \lbrace s \leq T_{s_j}  \rbrace \in \mathcal{F}_s
\end{align*}
TODO: Why is $\lbrace s \leq T_{s_j}  \rbrace \in \mathcal{F}_s$?
Moreover, by the construction of Stieltjes integral we have
\begin{align*}
\expectation{\int_0^T \abs{V_s - V^\epsilon_s}^2 \, dA_s} &\leq \expectation{\int_0^\infty \abs{V_s - V^\epsilon_s}^2 \, d(A_s + s)} \\
&=\expectation{\int_0^\infty \abs{W_s - W^\epsilon_s}^2 \, ds} < \epsilon
\end{align*}
 We are not quite done as $V^\epsilon$ has random jump times.  However, we can apply Lemma \ref{SimpleProcessApproximationPredictableStepProcess} to find $V^{(m,n)} \in \mathcal{E}$ such that $\lim_{m \to \infty} \expectation{\int_0^T \abs{V^{1/n}_s -V^{(m.n)}_s}^2 \, dA_s} = 0$ and then we find a $V^{(m_n,n)}$ such that $\lim_{n \to \infty} \expectation{\int_0^T \abs{V_s -V^{(m_n.n)}_s}^2 \, dA_s} = 0$.

Now we remove the assumption that $V$ is bounded.  For a general $V_s$ such that $\expectation { \int_0^T V_s^2 \, dA_s} < \infty$, let $V^n_s = V_s \characteristic{\abs{V_s} \leq n}$ where by the Dominated Convergence Theorem we know that $\expectation { \int_0^T \abs{V_s - V_s^n}^2 \, dA_s} = 0$.  Since each $V_s^n$ is bounded we can find a sequence $V^{(n,m)}_s$ such that $\lim_{m \to \infty} \expectation { \int_0^T \abs{V^n_s - V_s^{(n,m)}}^2 \, dA_s} = 0$ and now an array argument shows we get a subsequence $V^{(n,m_n)}$ such that $\lim_{n \to \infty} \expectation { \int_0^T \abs{V^{(n,m_n)}_s - V_s}^2 \, dA_s} = 0$.


Lastly it remains to remove the assumption that we are dealing with a fixed $T \geq 0$.  By what we have proven thus far, if $V$ is such that $\expectation { \int_0^t V_s^2 \, dA_s} < \infty$ for all $t \geq 0$, then for each $m > 0$ we have a sequence $V^{(n,m)} \in \mathcal{E}$ such that $\lim_{n \to \infty} \expectation{\int_0^m \abs{V_s - V^{(n,m)}_s}^2 \, dA_s} = 0$, so in particular there is $n_m$ such that $\expectation{\int_0^m \abs{V_s - V^{(n_m,m)}_s}^2 \, dA_s} < \frac{1}{m}$.  If we let $V_s^m = V_s^{(n_m,m)}$ then
for every $T > 0$, 
\begin{align*}
\lim_{m \to \infty} \expectation{\int_0^T \abs{V_s - V^{(n_m,m)}_s}^2 \, dA_s} &\leq \lim_{m \to \infty} \expectation{\int_0^m \abs{V_s - V^{(n_m,m)}_s}^2 \, dA_s} = 0
\end{align*}
and thus $\sup_{0 \leq T < \infty} \lim_{m \to \infty} \expectation{\int_0^T \abs{V_s - V^{(n_m,m)}_s}^2 \, dA_s} = 0$ and we are finally done.
\end{proof}

\subsection{Brownian Motion and Continuous Martingales}

The theme of this section is the centrality of Brownian motion in the universe of continuous martingales.  We demonstrate through several different constructions that all continuous martingales can be derived from (or transformed into) a suitable Brownian motion.  We start with a slightly different problem.  Suppose we are given a Brownian motion, we ask whether we can identify a class of continuous martingales that can be constructed from it.

\begin{defn}Let $B_t=(B^1_t, \dotsc, B^d_t)$ be a $d$-dimensional Brownian motion and let $\mathcal{F}_t$ be completion of the filtration generated by $B$, we say that a cadlag (local) martingale that is adapted to $\mathcal{F}$ is a \emph{Brownian (local) martingale}.
\end{defn}

Note that we have not assumed that a Brownian martingale is continuous but only cadlag.  Our goals is to show that all Brownian martingales may be constructed as stochastic integrals of suitable progressively measurable integrands.  One corollary of this fact is that Brownian martingales are in fact continuous (even though the definition only assumes that they are cadlag).  We start out working with $L^2$ continuous martingales so that we may leverage Hilbert space structures to assist in the analysis.  First a basic decomposition result.


OOPS!  Here we are using the covariation of not necessarily continuous martingales which we haven't defined!  Better go back to Kallenberg to understand his proof that doesn't use this idea.  Note that the result goes though with additional assumption of continuity but the results are actually strong enough that continuity of Brownian martingales is part of the conclusion.
\begin{lem}Let $B$ be a one-dimensional Brownian motion and let $M$ be a bounded $L^2$ Brownian martingale then there is $V \in L(B)$ and a bounded $L^2$ martingale $Z$ such that $M = \int V \, dB + Z$ and $[Z, \int U \, dB] = 0$ for all $U \in L(B)$.  Moreover such a decomposition is unique up to indistinguishability.
\end{lem}
\begin{proof}
We first show the uniqueness part of the claim.  Suppose that we have $M = \int V \, dB + Z = \int \tilde{V} \, dB + \tilde{Z}$.  It then follows by linearity of the stochastic integral that $Z - \tilde{Z} = \int (\tilde{V} - V) \, dB$ is an $L^2$ bounded continuous martingale and therefore $[Z-\tilde{Z}] = [Z, \int (\tilde{V} - V) \,dB] -  [\tilde{Z}, \int (\tilde{V} - V) \,dB] = 0$.  Therefore $Z$ and $\tilde{Z}$ are indistinguishable and it follows that $\int V \, dB$ and $\int \tilde{V} \, dB$ are indistinguishable which implies $V$ and $\tilde{V}$ are indistinguishable by the Ito Isometry.

Now we reduce to demonstrating the decomposition for the stopped process $M^t$.  To that end, suppose that we have $M^t = \int V \, dB + Z$, then clearly for $s < t$ we have 
\begin{align*}
M^s & = (M^t)^s = \left( \int V \, dB + Z \right)^s = \int \characteristic{[0,s]} V \, dB + Z^s
\end{align*}
so we can define $V$ and $Z$ by extending from each interval $[0,t]$.  It is clear that $M = \int V \, dB + Z$ and moreover for every $0 \leq t < \infty$ we have $M^t = \int \characteristic{[0,t]} V \, dB + Z^t$ with $[Z^t, \int U \, dB] = 0$ for all $U \in L(B)$.  From this for every $U \in L(B)$, we have $[Z, \int U \, dB]^n = [Z^n, \int U \, dB] = 0$ for every $n \in \naturals$ and therefore it follows that $[Z, \int U \, dB] = 0$.

So we now fix $t > 0$ and suppose that $M_t = M_s$ for all $s \geq t$.  We consider $M_t$ as an element of $L^2(\Omega, \mathcal{F}_t)$. 

Claim: The subspace of elements of the form $\int_0^t V \, dB$ is closed.  

This follows from the Ito isometry as if $\int V^n \, dB$ is a convergent sequence in $L^2(\Omega, \mathcal{F}_t)$ then by the Ito Isometry we know that $V^n$ is Cauchy in $L^2(\Omega \times[0,t])$ hence converges to a progressive process $V \in L^2(\Omega \times [0,t])$ (note it follows from Lemma \ref{ProgressivelyMeasurableProcesses} that the limit of progressive processes is progressive).  Again by the Ito Isometry, it follows that $\int V^n \, dB \tolp{2} \int V \, dB$.  

From the claim we can write $M_t = \int_0^t V \, dB + Z_t$ where $\expectation{Z_t \int_0^t U \, dB} = 0$ for all progressive $U \in L^2(\Omega \times [0,t])$.  Now let $Z_s$ be a cadlag version of the martingale $\cexpectationlong{\mathcal{F}_s}{Z_t}$ (noting that $Z_s = Z_t$ for all $s \geq t$) and by Jensen's inequality for conditional expectations 
\begin{align*}
\expectation{Z_s^2} \leq \expectation{\cexpectationlong{\mathcal{F}_s}{Z^2_t}} = \expectation{Z_t^2} < \infty
\end{align*}
which shows that $Z_t$ is $L^2$-bounded.
\end{proof}

\begin{lem}\label{ComplexExponentialMartingale}Let $M_t$ be a real continuous local martingale such that $M_0=0$ then $Z_t = e^{iM_t + \frac{1}{2}[M]_t}$ is a complex local martingale satisfying $Z_t = 1 + i \int_0^t Z dM$.
\end{lem}
\begin{proof}
Apply Ito's Lemma Corollary \ref{ItoLemmaContinuousSemimartingaleEntireFunction} to the complex semimartingale $X_t = iM_t + \frac{1}{2}[M]_t$ and the entire function $f(z) = e^z$ to see that
\begin{align*}
Z_t &= 1 + \int_0^t Z \, dX + \frac{1}{2} \int_0^t Z_s \, d[X]_s \\
&= 1 + i \int_0^t Z \, dM - \frac{1}{2} \int_0^t Z_s \, d[M]_s +\frac{1}{2}\int_0^t Z_s \, d[M]_s = 1 + i \int_0^t Z\, dM
\end{align*}
The fact that $Z_t$ is a complex local martingale follows from the fact that it is a stochastic integral.
\end{proof}

\begin{lem}\label{RepresentationOfBrownianFunctionals}Let $B_t=(B^1_t, \dotsc, B^d_t)$ be a $d$-dimensional Brownian motion and let $\xi$ be a $B$-measurable random variable with $\expectation{\xi} = 0$ and $\expectation{\xi^2} < \infty$.  There exists $\probabilityop \times \lambda$ almost everywhere unique processes $V^1, \dotsc, V^d$ such that $\expectation{\int_0^\infty (V^j(s))^2 ds} < \infty$ for each $j=1, \dotsc, d$ and $\xi = \sum_{j=1}^d \int_0^\infty V^j \, dB^j$ almost surely.
\end{lem}
\begin{proof}
Let $H$ be the subspace of $L^2(\Omega, \mathcal{A}, \probabilityop)$ such that $\expectation{\xi} = 0$.  Note that if $\xi_1, \xi_2, \dots$ is a sequence in $H$ and $\xi \in L^2$ such that $\xi_n \tolp{2} \xi$ then by Jensen's Inequality, $\expectation{\xi}^2 = \lim_{j \to \infty} \expectation{\xi - \xi_j}^2 \leq \lim_{j \to \infty} \expectation{(\xi - \xi_j)^2}  = 0$ and therefore $H$ is closed hence a Hilbert space.  Now let $K$ be the subspace of elements of the form $\sum_{j=1}^d \int_0^\infty V^j \, dB^j$ where $V^j$ are progressive processes with $\expectation{\int_0^\infty (V^j(s))^2 \, ds } < \infty$.  

Claim: $K \subset H$ is a closed subspace

First focus on a single $B^j$.  Note that by the Ito Isometry we have
\begin{align*}
\expectation{\left(\int_0^t V^j \, dB^j\right)^2} &= \expectation {\int_0^t (V^j(s))^2 \, ds } \leq \expectation{\int_0^\infty (V^j(s))^2 \, ds } < \infty
\end{align*}
and therefore each $\int V^j \, dB^j$ is $L^2$-bounded and $\int_0^\infty V^j \, dB^j$ is defined and in $L^2$ (hence in $H$).  Moreover we have the limit of the Ito Isometry $\expectation{\left(\int_0^\infty V^j \, dB^j\right)^2} = \expectation{\int_0^\infty (V^j(s))^2 \, ds}$.  Thus if $\int V^{n,j} \, dB^j$ converges in $L^2$ then it is Cauchy which implies $V^{n,j}$ is Cauchy and thus $V^{n,j}$ converges to some $V^{j}$ by completeness of $L^2$.  Another application of the Ito Isometry shows that $\int_0^\infty V^{n,j} \, dB^j \tolp{2} \int_0^\infty V^j \, dB^j$ hence the space of $\int_0^\infty V^j \, dB^j$ is a closed subspace of $H$ for each $j=1, \dotsc, d$.  Lastly note that for $i \neq j$ we have $\expectation{\int_0^\infty V^i \, dB^i \int_0^\infty V^j \, dB^j} = \expectation{[\int V^i \, dB^i \int V^j \, dB^j]_\infty} = \expectation{\int_0^\infty V_s^i V_s^j \, d[B^i,B^j]_s} = 0$ since $[B^i,B^j] = 0$.  Thus the space of $\sum_{j=1}^d \int_0^\infty V^j \, dB^j$ is the orthogonal sum of closed subspaces and is therefore closed.

The uniqueness claim of the Lemma also follows from the argument above since we have shown that $K$ is an orthogonal sum of subspaces each of which is isometric to $L^2(\Omega \times \reals_+, \mathcal{A} \otimes \mathcal{B}(\reals_+), P \times \lambda)$.

Now for the existence portion of the argument, for any $\xi \in H$ we can write $\xi = \eta + \sum_{j=1}^d \int_0^\infty V^j \, dB^j$ with $\eta$ orthogonal to $K$.  It suffices to show that if $\eta$ is $B$-measurable then $\eta = 0$; so let $\eta \in H \ominus K$.  Suppose that $u^1, \dotsc, u^d$ are deterministic functions in $L^2(\reals)$, $M_t = \sum_{j=1}^d \int_0^t u^j \, dB^j$ and $Z_t = e^{iM_t + \frac{1}{2}[M]_t}$.  By Lemma \ref{ComplexExponentialMartingale} and Lemma \ref{ChainRuleContinuousSemimartingale} we know that 
\begin{align*}
Z_t - 1 &= \int_0^t Z \, dM = \sum_{j=1}^d \int_0^t Z \, d\int u^j \, dB^j = \sum_{j=1}^d \int_0^t Z u^j \, dB^j 
\end{align*} 
Moreover we have $[M]_t = \sum_{j=1}^d \int_0^t (u^j(s))^2 ds$ is deterministic and therefore 
\begin{align*}
\expectation{\abs{Z_t}^2} = \expectation{e^{[M]_t}} = e^{\sum_{j=1}^d \int_0^t (u^j(s))^2 ds} \leq e^{\sum_{j=1}^d \norm{u^j}_2} < \infty
\end{align*}
so $Z_t$ is $L^2$ bounded.  Therefore $Z_\infty - 1 \in K$ and from $\eta \in H \ominus K$ we have 
\begin{align*}
0 &= \expectation{\eta (Z_\infty - 1)} = \expectation{\eta Z_\infty} = e^{\frac{1}{2} \sum_{j=1}^d \int_0^\infty (u^j(s))^2 ds} \expectation{\eta e^{i\sum_{j=1}^d \int_0^\infty u^j \, dB^j}}
\end{align*}
and therefore $\expectation{\eta e^{i\sum_{j=1}^d \int_0^\infty u^j \, dB^j}} = 0$.

This expression looks quite a bit like a charactersitic function; we proceed to pick some strategic $u^j$ so that it really becomes an honest one.  Fix an arbitrary $n \in \naturals$ and let $(t_1, \dotsc, t_n) \in \reals^n_+$ and $\theta^1, \dotsc, \theta^n \in \reals^d$ be given.  Define the step functions $u^j = \sum_{k=1}^n \theta^k_j \characteristic{[0,t_k]}$ for $j=1, \dotsc, d$. Then $\sum_{j=1}^d \int_0^\infty u^j \, dB^j = \sum_{j=1}^d\sum_{k=1}^n \theta^k_j B^j_{t_k} = \sum_{k=1}^n \langle \theta^k , B_{t_k}\rangle$ and therefore we get
\begin{align*}
\expectation{\eta e^{\sum_{k=1}^n \langle \theta^k , B_{t_k}\rangle}} = 0
\end{align*}
Writing $\eta = \eta_+ - \eta_-$ with $\eta_\pm \geq 0$ we note that by Lemma \ref{ChainRuleDensity} and Lemma \ref{ChangeOfVariables}
\begin{align*}
\expectation{\eta_\pm e^{\sum_{k=1}^n \langle \theta^k , B_{t_k}\rangle}} &= \int e^{\sum_{k=1}^n \langle \theta^k , B_{t_k}\rangle} \, d (\eta_\pm \cdot \probabilityop) \\
&= \int e^{\sum_{k=1}^n \langle \theta^k , x_k \rangle} \, d \pushforward{(B_{t_1}, \dotsc , B_{t_n})}{\eta_\pm \cdot \probabilityop}(x_1, \dotsc, x_n)
\end{align*} 
is the Fourier transform of the measure $\pushforward{(B_{t_1}, \dotsc , B_{t_n})}{\eta_\pm \cdot \probabilityop}$ on $\reals^{nd}$.  By uniqueness of the Fourier transform or measures we conclude that $\expectation{\eta ; (B_{t_1}, \dotsc B_{t_n}) \in A} = 0$ for all $A \in \mathcal{B}(\reals^{nd})$.  

Since it is trivial that $\lbrace (B_{t_1}, \dotsc B_{t_n}) \in A \rbrace \cap  \lbrace (B_{s_1}, \dotsc B_{s_m}) \in C \rbrace =  \lbrace (B_{t_1}, \dotsc B_{t_n}, B_{s_1}, \dotsc B_{s_m}) \in A \times C \rbrace$ we see that sets of the form $\lbrace (B_{t_1}, \dotsc B_{t_n}) \in A$ are a $\pi$-system and we know they generate $\vee_{t \geq 0} \sigma(B_t)$.  Moreover if we let $\mathcal{C} = \lbrace A \in \mathcal{A} \mid \expectation{\eta ; A} = 0\rbrace$ 
we have $A \subset C$ then $\expectation{\eta ; C \setminus A}  = \expectation{\eta ; C} - \expectation{\eta ; A} = 0$ and if $A_1 \subset A_2 \subset \dotsb$ then by Dominated Convergence, 
$\expectation{\eta ; \cup_{j=1}^\infty A_j} = \lim_{j \to \infty} \expectation{\eta ; A_j} = 0$ which shows $\mathcal{C}$ is a $\lambda$-system.  Now by the $\pi$-$\lambda$ Theorem \ref{MonotoneClassTheorem} we see that $\expectation{\eta ; A }=0$ for all $A \in \vee_{t \geq 0} \sigma(B_t)$ which shows that $\cexpectationlong{\vee_{t \geq 0} \sigma(B_t)}{\eta} = 0$.  If we also assume that $\eta$ is $B$-measurable then we know that $\eta = \cexpectationlong{\vee_{t \geq 0} \sigma(B_t)}{\eta} $ and we are done.
\end{proof}

\begin{thm}[Martingale Representation Theorem]\label{MartingaleRepresentationTheorem}Let $B=(B_1, \dotsc, B_d)$ be a $d$-dimensional Brownian motion and let $\mathcal{F}_t$ be the complete filtration generated by $B$.  Let $M$ be a cadlag local $\mathcal{F}$-martingale.  Then $M$ is continuous and moreover there exists $\probabilityop \times \lambda$-almost everywhere unique progressive processes $V^1, \dotsc , V^d$ such that 
\begin{align*}
M &= M_0 + \sum_{j=1}^d \int V^j \, dB^j
\end{align*}
\end{thm}
\begin{proof}
By applying the result to $M-M_0$ it is clear that we may assume that $M_0=0$.  We first show that $M$ is continuous.  By Lemma \ref{LocalMartingaleLocalizeToUniformlyIntegrable} we may pick a localizing sequence $\tau_n$ such that $M^{\tau_n}$ is a uniformly integrable cadlag martingale.  If we can show that every $M^{\tau_n}$ is almost surely continuous then it follows that $M$ is almost surely continuous.  To be precise, if we let $A = \lbrace \tau_n \uparrow \infty \rbrace \cap \cap_{n=1}^\infty \lbrace M^{\tau_n} \text{ is continuous}\rbrace$ then for all $\omega \in A$ and for every $t \geq 0$ there exists an $N$ such that $\tau_n(\omega) \geq t+1$ for all $n \geq N$ and therefore $M_s(\omega) = M^{\tau_n}_s(\omega)$ for all $0 \vee t-1 < s < t+1$ and therefore $M$ is continuous at $t$. 

Thus we may assume that $M$ is a uniformly integrable martingale starting at zero.  By the Martingale Convergence Theorem \ref{L1MartingaleConvergenceTheoremContinuous} we have $\mathcal{F}_\infty$-measurable $M_\infty$ such that $M \tolp{1} M_\infty$.  Since $L^2$ is dense in $L^1$ we may find $\xi^n \in L^2(\Omega, \mathcal{F}_\infty)$ such that $\xi^n \tolp{1} M_\infty$.  By $\mathcal{F}_\infty$-measurability of $\xi^n$ and Lemma \ref{RepresentationOfBrownianFunctionals} we know that the martingale $M^n_t = \cexpectationlong{\mathcal{F}_t}{\xi^n}$ is almost surely continuous.  Denote $\Delta M_t = M_t - \lim_{s \uparrow t}M_s$ to be jump process associated with $M$ and note that by the Doob Maximal Inequality (Lemma \ref{DoobMaximalInequalityContinuous}) we have for each $\epsilon > 0$ and $n \in \naturals$
\begin{align*}
\probability{ \sup_{0 \leq t < \infty} \abs{\Delta M_t} > 2 \epsilon} &\leq \probability{\sup_{0 \leq t < \infty} \abs{M^n_t - M_t} > \epsilon} \leq \epsilon^{-1} \expectation{\abs{\xi^n - M_\infty}}
\end{align*}
and taking the limit as $n \to \infty$ we see that $\probability{ \sup_{0 \leq t < \infty} \abs{\Delta M_t} > 2 \epsilon} = 0$ for every $\epsilon > 0$ and thus $\probability{ \sup_{0 \leq t < \infty} \abs{\Delta M_t} \neq 0} \leq \cup_{n=1}^\infty \probability{ \sup_{0 \leq t < \infty} \abs{\Delta M_t} > 1/n} = 0$ which shows us that $M_t$ is almost surely continous.

By the above argument we may now assume that $M$ is a continuous local $\mathcal{F}$-martingale.  By Lemma \ref{ContinuousLocalMartingaleLocalizeToBounded} we may assume that we have a localizing sequence $\tau_n$ such each $M^{\tau_n}$ is bounded (in particular $L^2$ bounded).  Therefore $M^{\tau_n}_\infty$ exists and is in $L^2$.  Since $M^{\tau_n}_\infty$ is $\mathcal{F}_\infty$-measurable and $\expectation{M^{\tau_n}_\infty}=0$ we may apply Lemma \ref{RepresentationOfBrownianFunctionals} to conclude there are $V^{j,n} \in L(B^1)$ such that $M^{\tau_n}_\infty = \sum_{j=1}^d \int_0^\infty V^{j,n} \, dB^j$.  Therefore from the fact that $M^{\tau_n}_t$ is a closable martingale we have $M^{\tau_n}_t = \cexpectationlong{\mathcal{F}_t}{M^{\tau_n}_\infty} = \sum_{j=1}^d \int_0^t V^{j,n} \, dB^j$.

For $m < n$ we have $\tau_m \leq \tau_n$ and therefore using Lemma \ref{StoppingIntegralsContinuousSemimartingale}
\begin{align*}
\sum_{j=1}^d \int V^{j,m} \, dB^j &= M^{\tau_m} = (M^{\tau_n})^{\tau_m} = \sum_{j=1}^d \int \characteristic{[0,\tau_m]} V^{j,n} \, dB^j
\end{align*}
and by the almost sure uniqueness of the $V^{j,m}$ we conclude that $V^{j,n}\mid_{[0,\tau_m]} = V^{j,m}$.  Therefore there exists $V^j$ such that $V^j \mid_{[0,\tau_n]} = V^{j,n}$ and for any $t \geq 0$ using Lemma \ref{StoppingIntegralsContinuousSemimartingale}
\begin{align*}
M_t &= \lim_{n \to \infty} M^{\tau_n}_t =  \lim_{n \to \infty} \sum_{j=1}^d \int_0^t V^{j,n} \, dB^j \\
&= \lim_{n \to \infty}  \sum_{j=1}^d \int_0^t \characteristic{[0,\tau_n]} V^{j} \, dB^j \\
&=  \lim_{n \to \infty} \sum_{j=1}^d \int_0^{\tau_n \wedge t} V^{j} \, dB^j = \sum_{j=1}^d \int_0^{t} V^{j} \, dB^j 
\end{align*}
almost surely.
TODO: Show $V^j \in L(B^1)$ and show a.s. uniqueness of $V^j$.
\end{proof}

Another result that is important is the Levy's characterization of Brownian motion in terms of its covariance structure.
\begin{thm}[Levy's Theorem]\label{LevyCharacterizationOfBrownianMotion}Let $B_t=(B^1_t, \dotsc, B^d_t)$ be a process in $\reals^d$ such that $B_0=0$, then $B$ is an $\mathcal{F}$-Brownian motion if and only if $B$ is a continuous local $\mathcal{F}$-martingale with $[B^i,B^j]_t = \delta_{ij} t$.
\end{thm}
\begin{proof}
Suppose that $B$ is a continuous local  $\mathcal{F}$-martingale with $[B^i,B^j]_t = \delta_{ij} t$ and $B_0 = 0$.  We need to show that for each $s < t$,  $B_t - B_s$  is independent of $\mathcal{F}_s$ and Gaussian with covariance matrix $t -s$ times the identity.   Fix $S < T$ and define  the filtration $\tilde{\mathcal{F}}_t = \mathcal{F}_{t+S}$,  $\tilde{B}^i_t = B_{t+S} - B_S$ for each $i=1, \dotsc, d$ and $\tilde{B} = (\tilde{B}^1, \dotsc, \tilde{B}^d)$.  Clearly, $\tilde{B}$ is a continuous local $\tilde{\mathcal{F}}$-martingale and moreover note that $[\tilde{B}^i,\tilde{B}^j]_t = [B^i,B^j]_{t+S} - [B^i,B^j]_S = t \delta_{ij}$.  Let $u=(u_1, \dotsc, u_d) \in \reals^d$ be given and define 
\begin{align*}
N_t &= \langle u , \tilde{B}_t^{T-S} \rangle = u_1 (B_{(t+S) \wedge T}^1 - B_S^1) + \dotsb + u_d (B_{(t + S) \wedge T}^d - B_S^d)
\end{align*}  
Clearly, $N_t$ is a continuous local $\tilde{\mathcal{F}}$-martingale such that $N_0 = 0$ and also has the quadratic variation
\begin{align*}
[N]_t &= \sum_{i=1}^d \sum_{j=1}^du_iu_j [(\tilde{B}^i)^{T-S},  (\tilde{B}^j)^{T-S}]_t \\
&= \sum_{i=1}^d \sum_{j=1}^du_iu_j ([B^i, B^j ]_{t+S} -  [B^i, B^j ]_{S})^{T-S}=  (t \wedge (T-S)) \norm{u}_2^2\\
\end{align*}
By Lemma \ref{ComplexExponentialMartingale} we know that $Z_t = \exp(iN_t + \frac{1}{2} [N]_t)$ is a continuous local $\tilde{\mathcal{F}}$-martingale that satisfies $Z_0 = 1$.  Since $[N]_\infty = [N]_{T-S} = (T-S) \norm{u}_2^2< \infty$ we know that $Z_t$ is bounded and therefore we can apply Lemma \ref{BoundedLocalMartingaleIsMartingale} to see that is a uniformly integrable martingale.
For any $A \in \tilde{\mathcal{F}}_0 = \mathcal{F}_S$ we have by the martingale property of $Z$ and the definition of $N$
\begin{align*}
\probability{A} &= \expectation{Z_0 ; A} = \expectation{Z_\infty ; A} = \expectation{ \exp(iN_\infty + \frac{1}{2} [N]_\infty); A} \\
&=  \expectation{ e^{i\langle u, \tilde{B}_{T-S} \rangle}; A} e^{\frac{1}{2}(T-S) \norm{u}_2^2}
\end{align*}
which shows us that $\cexpectationlong{\mathcal{F}_S}{e^{i \langle u, \tilde{B}_{T-S} \rangle}} = e^{-\frac{1}{2} (T-S) \langle u, u \rangle}$.  We may now apply uniqueness of conditional characteristic functions Lemma \ref{ConditionalCharacteristicFunctions} and Theorem \ref{GaussianVectorCharacteristicFunction} to conclude that the conditional probability distribution $\cprobability{\mathcal{F}_S}{ (B_{T}^1 - B_S^1,\dotsc, B_{T}^d - B_S^d) \in \cdot}$ is centered Gaussian with covariance matrix $(T-S)$ times the identity. As the conditional probability distribution is deterministic and we see that $B_T -B_S$ is independent of $\mathcal{F}_S$ as well.
\end{proof}

\subsubsection{Girsanov Theory}

We now begin an investigation of how continuous local martingales behave as the underlying probability measure is changed.  

\begin{defn}Let $P$ and $Q$ be probability measures on a measure space $(\Omega, \mathcal{A})$ with a filtration $\mathcal{F}_t$ with index set $T$.  We say that $Q$ is \emph{locally absolutely continuous} with respect to $P$ is for each $t \in T$ we have $Q \ll P$ on $\mathcal{F}_t$.  If in addition $P$ is locally absolutely continuous with respect to $Q$ then say that $P$ and $Q$ are \emph{locally equivalent}.
\end{defn}

TODO:  Are there any subtleties about the usual conditions?  The fact that the usual conditions are tied to the probability measure by the assumption that each $\mathcal{F}_t$ contains all subsets of the null sets of the probability measure.  When we pass to a new probability measure (even with the absolute continuity assumptions) we may be introducing new null sets and the filtration may no longer satisfy the usual conditions right?  Most presentations restrict to the situation of equivalent probability measures so this problem doesn't arise but Kallenberg clearly doesn't intend to make this restriction at the outset.  There is indeed something subtle about the usual conditions (see Bichteler).  In the first place it is observed (and noted elsewhere) that the usual conditions mean that if $Q \ll P$ on $\mathcal{F}_0$ then $Q \ll P$ on all of $\mathcal{F}_\infty$ (i.e. there isn't a useful notion of being only locally absolutely continuous).  On the other hand, Kallenberg uses the usual conditions to assume a cadlag version of the likelihood ratio process.  Moreover, it seems that some of the more useful variants of Girsanov are not compatible with the usual conditions since they require that the change of measure is not absolutely continuous but merely locally so.  It seems like Kallenberg has made a bit of a muddle of this.  I'm still trying to distill the core issues.  In the Brownian motion case a constant drift term illustrates the problem.  Let $B_t$ be a standard Brownian motion, let $\mu > 0$ be a constant and consider $\tilde{B}_t = B_t - \mu t$.  Let $\mathcal{F}_t$ be the filtration generated by $B_t$ and let $\tilde{\mathcal{F}}_t$ be the usual augmentation.  If we define $Z_t = \exp\left [ \mu B_t - \frac{1}{2} \mu^2 t\right]$ then we can define a new probability measure $\tilde{P}$ on each $\tilde{\mathcal{F}}_t$ by $\tilde{P}(A) = \expectation{Z_t ; A}$.  What is true is that
\begin{itemize}
\item[(i)]$\tilde{B}_t$ is a $\tilde{\mathcal{F}}$-Brownian motion on $[0,t]$ with respect to $\tilde{P}$ for all $0 \leq t < \infty$.
\item[(ii)]$P$ and $\tilde{P}$ are mutually absolutely continous on $\tilde{\mathcal{F}}_t$ for all $0 \leq t < \infty$.
\item[(iii)]There is an extension of $\tilde{P}$ to all of $\mathcal{F}_\infty$ such that $\tilde{B}_t$ is an $\mathcal{F}$-Brownian motion
\end{itemize}
what is not true is that
\begin{itemize}
\item[(i)]The extension of $\tilde{P}$ to all of $\mathcal{F}_\infty$ is not equal to $\expectation{Z_t; A}$ on all of $\tilde{\mathcal{F}}_t$ but only on $\mathcal{F}_t$.  In particular $\lbrace \lim_{t \to \infty} B_t/t = \mu \rbrace$ is a $P$ - null set but is $\tilde{P}$ almost sure.
\item[(ii)]The extension $\tilde{P}$ and $P$ are not mutually absolutely continuous on $\mathcal{F}_\infty$.
\item[(iii)]$\tilde{B}_t$ is not a $\tilde{\mathcal{F}}$-Brownian motion on $[0,\infty)$ with respect to $\tilde{P}$. 
\end{itemize}

TODO: Is there an obstruction to extending $\tilde{P}$ to a probability measure on $\tilde{\mathcal{F}}_\infty$ or is it just that $\tilde{B}_t$ will not be a Brownian motion on $[0,\infty)$ with respect to such an extension?  From van der Vaart's notes, the Brownian motion example above shows that there is an obstruction.  The key seems to be that if such an extension did exist then Girsanov would apply and we'd be able to conclude that $\tilde{B}_t$ would be a Brownian motion on it.  That would cause a contradiction because the Brownian-ness of $\tilde{B}_t$ would imply that $\tilde{P}\lbrace \lim_{t \to \infty} B_t/t = \mu \rbrace = 1$ but the completeness of $\mathcal{F}_t$ would imply that $\tilde{P}\lbrace \lim_{t \to \infty} B_t/t = \mu \rbrace = P\lbrace \lim_{t \to \infty} B_t/t = \mu \rbrace = 0$.  Does this mean that Kallenberg's Lemma 18.18 is incorrect?

TODO: Is it also that case that $\tilde{B}_t$ is not a continuous local $\tilde{P}$-martingale on $[0,\infty)$?  This question only makes sense if the answer to the previous question is that there is such an extension to $\tilde{P}$ on all of $\tilde{\mathcal{F}}_\infty$.

One of the nasty things about developing the theory in this way (and having a result that doesn't hold for filtrations that satisfy the usual conditions) is the fact that we have all sorts of results that do assume the usual conditions and it is not terribly clear what the ramifications of losing the assumption are.  Bichteler has identified an extension procedure that is more conservative than the imposition of the usual conditions (his \emph{natural} conditions) that seems to preserve all the important results but also allows an extension of $\tilde{P}$ such that $\tilde{B}_t$ will be a Brownian motion on all of $[0,\infty)$ with respect to $\tilde{P}$.  Note that the example of the event $\lbrace \lim_{t \to \infty} B_t/t = \mu \rbrace$ shows that such an extension will not be mutually absolutely continuous but it will be locally so.

We need a preliminary result that says when a non-negative cadlag supermartingale hits $0$ it is absorbed.  The reader should convince herself that this is expected: since a supermartingle is non-increasing on average, once it hits zero any attempt to return to a positive value would have to be offset by corresponding negative value.  Making sense of the intuition requires an argument using optional times.  
\begin{lem}\label{PositiveSupermartingaleAbsorption}Let $X \geq 0$ be a cadlag $\mathcal{F}$-supermartingale (not necessarily satisfying the usual conditions) and let $\tau = \inf \lbrace t \mid X_{t-} \wedge X_t =0 \rbrace$, then $X \equiv 0$ a.s. on $[\tau, \infty)$.
\end{lem}
\begin{proof}
First note that $X$ is also an $\mathcal{F}^+$-supermartingale.  To see this, pick for any $s < t$ pick a sequence $s_m \downarrow s$ and use the Levy Downward Theorem
\ref{JessenConditioningLimits}  to conclude $\cexpectationlong{\mathcal{F}_s^+}{X_t} = \lim_{n \to \infty} \cexpectationlong{\mathcal{F}_{s_n}^+}{X_t} \leq X_t$.  Now we use an approximation to the optional time $\tau$.  The idea the approximation is that $X_{t-} \wedge X_t=0$ if and only if $X_{t-} \wedge X_t < 1/n$ for all $n \in \naturals$ so we look for the first point $t$ for which $X_{t-} \wedge X_t < 1/n$, that is to say we consider the hitting time $\tau_n = \lbrace t \mid X_t < 1/n \rbrace$ and use the fact that $\tau_n \uparrow \tau$ (we'll actually show this carefully later in the proof but it isn't hard to believe).  Note that $\tau_n$ is an $\mathcal{F}^+$-optional time due to the openness of $(-\infty, 1/n)$ and the right continuity of $X$ (Lemma \ref{HittingTimesContinuous}).  Note that for all $n \in \naturals$ the right continuity of $X$ implies that $X_{\tau_n} \leq 1/n$ (just pick a random sequence $t_m \downarrow \tau_n$ such that $X_{t_m} < 1/n$).  Now pick $t \geq 0$ and $n \in \naturals$ and use the supermartingale property, the $\mathcal{F}^+_{\tau_n \wedge t}$-measurability of $\lbrace \tau_n \leq t \rbrace$ (TODO: where  do we show this) and Optional Sampling Theorem \ref{OptionalSamplingContinuous}
\begin{align*}
\expectation{ X_t ; \tau_n \leq t } &=\expectation{ \cexpectationlong{\mathcal{F}^+_{\tau_n \wedge t}}{X_t} ; \tau_n \leq t }  \leq \expectation{ X_{\tau_n \wedge t} ; \tau_n \leq t }  \\
&=\expectation{ X_{\tau_n} ; \tau_n \leq t } \leq 1/n
\end{align*}
Using the non-negativity and integrability of $X_t$, the fact that $\tau_n \uparrow \tau$ and Dominated Convergence, we get 
\begin{align*}
0 &\leq \expectation{ X_t ; \tau \leq t } \leq \lim_{n \to \infty} 1/n = 0
\end{align*}
and therefore $X_t = 0$ a.s. on the set $\lbrace \tau \leq t\rbrace$.  Taking a countable union of null events we see that almost surely for all $q \in \rationals_+$,  $X_q = 0$ on the set $\lbrace \tau \leq q\rbrace$ and by right continuity we get that 
\begin{align*}
\cap_{q \in \rationals_+} \lbrace X_q = 0 \text{ on } \lbrace \tau \leq q \rbrace \rbrace &=
\cap_{0 \leq t < \infty} \lbrace X_t = 0 \text{ on } \lbrace \tau \leq t \rbrace \rbrace
\end{align*}
(for $\omega \in \cap_{q \in \rationals_+} \lbrace X_q = 0 \text{ on } \lbrace \tau \leq q \rbrace \rbrace$ and $t \geq \tau(\omega)$, pick $q_n \downarrow t$ and note that $X_{q_n}(\omega) = 0$ so by right continuity $X_t(\omega) = 0$). This yields the final result.

We now return to the deferred justification of the claim that $\tau_n \uparrow \tau$.  It is clear that $\tau_n$ is non-decreasing.  To see that $\tau_n \leq \tau$ note that if $X_{t-} \wedge X_t = 0$ the either $X_t = 0$ or we may find $s < t$ such that $X_s < 1/n$ and therefore $\tau_n \leq t$.  In the opposite direction, we give ourselves an $\epsilon$ of room and pick an arbitrary $\epsilon > 0$.  Pick a random $N > 0$ such that $\lim_{n \to \infty} \tau_n - \epsilon/2 \leq \tau_n$ for all $n \geq N$ and then for each $n \geq N$ we pick a $t_n$ such that $X_{t_n} < 1/n$ and $t_n \leq \tau_n + \epsilon/2$.  In this way we construct a sequence $t_n$ in $[0,\lim_{n \to \infty} \tau_n+\epsilon]$ such that $X_{t_n} < 1/n$.  By compactness we get a $t \in [0,\lim_{n \to \infty} \tau_n+\epsilon]$ and a convergent subsequence $N^\prime$ such that $t_n \to t$ along $N^\prime$  which by passing to another subsequence we may assume is either increasing or decreasing.  From this and right continuity of $X$ we conclude that $X_{t-} \wedge X_t = 0$ and therefore $\tau \leq \lim_{n \to \infty} \tau_n+\epsilon$ and since $\epsilon > 0$ was arbitrary we are done.
\end{proof}

\begin{lem}\label{LikelihoodRatioProcesses}Let $P$ and $Q$ be probability measures on a measure space $(\Omega, \mathcal{A})$ with a filtration $\mathcal{F}$ (not necessarily satisfying the usual conditions).   Suppose that $Q$ is locally absolutely continuous with respect to $P$ and let $Z_t$ be an $\mathcal{F}$-adapted process such that $Q = Z_t \cdot P$ on $\mathcal{F}_t$ for all $t \geq 0$, then 
\begin{itemize}
\item[(i)] An adapted process $X$ is a $Q$-martingale if and only if $X Z$ is a $P$-martingale.  In particular, $Z_t$ is a $P$-martingale.  Moreover, $Z$ is uniformly integrable if and only if $Q \ll P$ on $\mathcal{F}_\infty$.  
\item[(ii)] If $Z$ is a cadlag version then for any optional time
\begin{align*}
Q &= Z_\tau \cdot P \text{ on $\mathcal{F}_\tau \cap \lbrace \tau < \infty \rbrace$ }
\end{align*}
and an adapted cadlag process $X$ is a local $Q$-martingale if and only if $X Z$ is a local $P$-martingale.
\item[(iii)] If $\tau_n$ is a sequence of optional times such that $\tau_n \uparrow \infty$ $P$-almost surely then $\tau_n \uparrow \infty$ $Q$-almost surely.
\item[(iv)] An adapted cadlag process $X$ is a local $Q$-martingale if and only if $X Z$ is a local $P$-martingale.  
\item[(v)] If $Z$ is a cadlag version then $Q$-almost surely for every $t > 0$ we have $\inf_{0\leq s \leq t} Z_s > 0$.  If $Q$ and $P$ are locally equivalent then this is true $P$-almost surely as well.
\end{itemize}
\end{lem}
\begin{proof}
We start with proving (i).  Note that since $X_t$ is $\mathcal{F}_t$-measurable by Lemma \ref{ChainRuleDensity} and non-negativity of $Z_t$ we have $\sexpectation{\abs{X_t}}{Q} = \sexpectation{Z_t \abs{X_t}}{P}= \sexpectation{ \abs{Z_t X_t}}{P}$ and therefore $X_t$ is $Q$-integrable if and only if $Z_t X_t$ is $P$-integrable.  If we let $A \in \mathcal{F}_s$ then if we assume $Z_t X_t$ is a $P$-martingale then for $t \geq s$,
\begin{align*}
\sexpectation{X_t ; A}{Q} = \sexpectation{Z_t X_t ; A}{P} = \sexpectation{Z_s X_s ; A}{P}  = \sexpectation{X_s ; A}{Q}
\end{align*}
and similarly if we assume that $X_t$ is a $Q$-martingale then we just the run the logic in a slightly different order
\begin{align*}
\sexpectation{Z_t X_t ; A}{P} = \sexpectation{X_t ; A}{Q} = \sexpectation{X_s ; A}{Q}  = \sexpectation{Z_s X_s ; A}{P}
\end{align*}
Thus we see that $X_t$ is a $Q$-martingale if and only if $Z_t X_t$ is a $P$-martingale.  Since $X_t \equiv 1$ is obviously a $Q$-martingale we see that $Z_t$ is a $P$-martingale.  If we assume that $Z$ is a uniformly integrable $Q$-martingale then by the Martingale Convergence Theorem \ref{L1MartingaleConvergenceTheoremContinuous} there exists $Z_\infty$ such that $Z_t = \cexpectationlong{\mathcal{F}_t}{Z_\infty}$ a.s.  Therefore if we assume that $A \in \mathcal{F}_t$ we have 
\begin{align*}
(Z_\infty \cdot P)(A) &= \expectation{Z_\infty ; A} = \expectation{\cexpectationlong{\mathcal{F}_t}{Z_\infty} ; A} = \expectation{Z_t; A} = Q(A)
\end{align*}
and since $\cup_{t \geq 0} \mathcal{F}_t$ is a $\pi$-system generating $\mathcal{F}_\infty$ we know that $Z_\infty \cdot P = Q$ on $\mathcal{F}_\infty$ by monotone classes (specifically Lemma \ref{UniquenessOfMeasure}).  On the other hand suppose that $Q \ll P$ on $\mathcal{F}_\infty$ and write $Q = \xi \cdot P$.  Then since $\mathcal{F}_t \subset \mathcal{F}_\infty$ we know that for all $t \geq 0$ and $A \in \mathcal{F}_t$ we have $Q(A) = \expectation{\xi ; A} = \expectation{\cexpectationlong{\mathcal{F}_t}{\xi} ; A}$ which shows $Z_t = \cexpectationlong{\mathcal{F}_t}{\xi}$ by the $P$-almost sure uniqueness of the Radon-Nikodym derivative.  This shows that $Z_t$ is uniformly integrable.

We now show (ii).  Note that since we know $Z_t$ is a martingale and we have assumed $\mathcal{F}$ is right continuous we can apply Theorem \ref{CadlagModificationContinuousMartingale} we may assume that we have taken a cadlag version of $Z_t$.  If we let $\tau$ be an optional time then we fix $t \geq 0$ and assume $A \in \mathcal{F}_{\tau \wedge t} \subset \mathcal{F}_t$ and apply Optional Sampling to see that 
\begin{align*}
Q(A) = \expectation{Z_t ; A} = \expectation{\cexpectationlong{\mathcal{F}_{\tau\wedge t}}{Z_t} ; A} =  \expectation{Z_{\tau \wedge t}; A} 
\end{align*}
Given an arbitrary $A \in \mathcal{F}_\tau$ we know from Proposition \ref{StoppedAlgebraMinOfOptionalTimes} that for all $t \geq 0$ we have $A \cap \lbrace \tau \leq t \rbrace \in \mathcal{F}_{\tau \wedge t}$.  Therefore $Q(A ; \tau \leq t) = \expectation{Z_{\tau}; A; \tau \leq t}$ and by continuity of measure and Monotone Convergence we get 
\begin{align*}
Q(A ; \tau < \infty) &= \lim_{n \to \infty} Q(A ; \tau \leq n) = \lim_{n \to \infty} \expectation{Z_{\tau}; A; \tau \leq n}= \expectation{Z_{\tau}; A; \tau < \infty}
\end{align*}

To see (iii), we let $\tau = \sup_n \tau_n$ and note that $\tau$ is an optional time by Lemma \ref{InfSupStoppedFiltration}.  Therefore we may apply (ii) and the fact that $\probability{\tau < \infty} = 0$ to conclude that $\sprobability{\tau < \infty}{Q} = \sexpectation{Z_\tau ; \tau < \infty}{P} = 0$.  Note that the above argument is necessary since we don't necessarily have $Q \ll P$ on $\mathcal{F}_\infty$.

To see (iv), suppose that $X$ is a local $P$-martingale.  Let $\tau_n \uparrow \infty$ $P$-a.s. be a localizing sequence for $X$ so that $X^{\tau_n}$ is a $P$-martingale for every $n \in \naturals$.  By (i) we conclude that $Z X^{\tau_n}$ is a $Q$-martingale.  It follows that $(Z X^{\tau_n})^{\tau_n} = Z^{\tau_n} X^{\tau_n} = (Z X)^{\tau_n}$ is a $Q$-martingale for every $n \in \naturals$.  Since by (iii) it follows that $\tau_n \uparrow \infty$ $Q$-almost surely we conclude that $ZX$ is a local $Q$-martingale.  

To see (v), by the $\mathcal{F}_t$-measurability of $Z_t$ we have $\sprobability{Z_t = 0}{Q} = \expectation{Z_t ; Z_t = 0} = 0$ and therefore $Z_t > 0$ $Q$-almost surely for each $t \geq 0$.  Since $Z_t$ is a cadlag $P$-martingale we let $\tau = \inf \lbrace t \mid Z_{t-} \wedge Z_t = 0 \rbrace$ and apply Lemma \ref{PositiveSupermartingaleAbsorption} to conclude that $Z_t \equiv 0$ $P$-almost surely on $[\tau,\infty)$: more formally written as $\probability{\cap_{0 \leq t < \infty} \lbrace Z_t \characteristic{\tau \leq t} = 0 \rbrace}=1$.  In particular, we have $\probability{Z_t \characteristic{\tau \leq t} = 0}=1$ for all $t \geq 0$.  Since $\lbrace Z_t \characteristic{\tau \leq t} = 0\rbrace \in \mathcal{F}_{t}$-measurable and  $Q \ll P$ on $\mathcal{F}_{t}$, by taking complements we see that $\sprobability{Z_t > 0 ; \tau \leq t}{Q}=0$ for all $t \geq 0$..  Putting these two facts together we conclude for all $t \geq 0$ that
\begin{align*}
\sprobability{\tau \leq t}{Q} &= \sprobability{Z_t = 0 ; \tau \leq t}{Q} + \sprobability{Z_t > 0 ; \tau \leq t}{Q}  \\
&\leq \sprobability{Z_t = 0}{Q} + \sprobability{Z_t > 0 ; \tau \leq t}{Q}  = 0
\end{align*}
By continuity of measure $\sprobability{\tau < \infty}{Q} = \lim_{t \to \infty} \sprobability{\tau \leq t}{Q} = 0$ and therefore
$\tau = \infty$ $Q$-almost surely. and therefore $Z_{t-} \wedge Z_t > 0$ for all $t \geq 0$ $Q$-almost surely which shows the result.  If we now assume that $P$ and $Q$ are locally equivalent, we also have $Z_t > 0$ $P$-almost surely for each $t \geq 0$.  We have already shown that, indepedent of the assumption of local equivalence, we have $Z_t \equiv 0$ $P$-almost surely on $[\tau,\infty)$.  By exactly the same argument as above, these two facts imply the result with respect to $P$.
\end{proof}

TODO: Van der Vaart claims that it is not true that $X$ is a local $Q$-martingale if and only if $ZX$ is a local $P$-martingale unless we assume that $P$ and $Q$ are locally equivalent.  Understand whether that is true and if so produce a counterexample and find the flaw in the proof from Kallenberg (which is very brief).

\begin{thm}\label{AbstractTransformationOfDriftContinuousLocalMartingale}Let $P$ and $Q$ be locally equivalent probability measures on a measure space $(\Omega, \mathcal{A})$ with a filtration $\mathcal{F}$.   Let $Z_t$ be an $\mathcal{F}$-adapted process such that $Q = Z_t \cdot P$ on $\mathcal{F}_t$ for all $t \geq 0$ and assume that $Z_t$ is almost surely continuous.  Then if $M$ is a local $P$-martingale, the process $\tilde{M}_t = M_t - \int_0^t Z^{-1}_s d[Z,M]_s$ is a local $Q$-martingale.
\end{thm}
\begin{proof}
The first thing to note is that the process $M_t - \int_0^t Z^{-1}_s d[Z,M]_s$ is well defined.  From Lemma \ref{LikelihoodRatioProcesses} we know that  $Q$-almost surely and $P$-almost surely, for all $t \geq 0$ $\inf_{0 \leq s \leq t} \lbrace t \mid Z_s \rbrace > 0$; thus $Q$-a.s. and $P$-a.s. the process $Z^{-1}$ is bounded on every $[0,t]$ and therefore $\int_0^t Z^{-1}_s d[Z,M]_s$ exists.


For each $n \in \naturals$, let $\tau_n = \inf \lbrace t \mid Z_t < 1/n \rbrace$ and define $\tilde{M}^n_t = M_t^{\tau_n} - \int_0^t \characteristic{[0,\tau_n]} Z_s^{-1} \, d[Z, M^{\tau_n}]_s$.  Note that from the definition of $\tau_n$, $\characteristic{[0,\tau_n]} Z_s^{-1}$ is bounded and therefore $\tilde{M}^n_t$ is well defined and moreover is a continuous $\mathcal{F}$-semimartingale.  By  integration by parts (Lemma \ref{IntegrationByPartsContinuousSemimartingale}) and the Chain Rule (Lemma \ref{ChainRuleContinuousSemimartingale})  we get
\begin{align*}
\tilde{M}^n_t Z_t - \tilde{M}^n_0 Z_0 &= \int_0^t \tilde{M}^n \, dZ + \int_0^t Z \, d\tilde{M}^n + [\tilde{M}^n, Z]_t \\
&= \int_0^t \tilde{M}^n \, dZ + \int_0^t Z \, d M^{\tau_n} -  \int_0^t Z_s \, d \int_0^s Z^{-1}_u [Z, M^{\tau_n}]_u + [M^{\tau_n}, Z]_t \\
&= \int_0^t \tilde{M}^n \, dZ + \int_0^t Z \, d M^{\tau_n} -  \int_0^t d[Z, M]_s + [M^{\tau_n}, Z]_t \\
&= \int_0^t \tilde{M}^n \, dZ + \int_0^t Z \, d M^{\tau_n}
\end{align*}
which shows that $\tilde{M}^n_t Z_t$ is a local $P$-martingale and therefore $\tilde{M}^n_t$ is a local $Q$-martingale by Lemma \ref{LikelihoodRatioProcesses}.

TODO: Kallenberg states this Lemma without the assumption of local equivalence of $P$ and $Q$ (i.e. only assuming that $Q$ is locally absolutely continuous with respect to $P$).  The main point that I don't understand is showing that $M$ is well defined both $P$-a.s. as well as $Q$-a.s.

TODO: How do we see that $\int_0^t Z^{-1}_s  d[Z,M]_s$ is well defined in general?  Perhaps there is an argument that shows that if we let $\tau = \inf \lbrace t \mid Z_t = 0 \rbrace$ then $0 < Z^{-1} < \infty$ on $[0,\tau)$ and  by  Lemma \ref{PositiveSupermartingaleAbsorption}  $Z^{-1} = \infty$ on $[\tau, \infty)$ $P$-a.s.  If we can then show that $[Z,M]=0$ on $[\tau,\infty)$.  By  then perhaps we can conclude that $\int_0^t Z^{-1}_s d[Z,M]_s$ is well defined for all $t \geq 0$ (the only issue is $t \geq \tau$.  This is a question of Stieltjes integrals but seems possible as it is a $\infty \cdot 0$ type of situation.  Perhaps this argument won't work either as we still would have to control the rate that $Z^{-1}_t \uparrow \infty$ as $t \to \tau$.

TODO: As an alternative to showing that Here I try to show that $\tilde{M}$ is well defined by defining it as a limit of the $\tilde{M}^n$; is this any better because we only know that $\tau_n \uparrow \tau$ $Q$-a.s?  Now note that since $\inf_{0 \leq s \leq t} Z_t > 0$ $Q$-almost surely (Lemma \ref{LikelihoodRatioProcesses}) we know that $\tau_n \uparrow \tau$ $Q$-almost surely.  We know that for any fixed $t \geq 0$ the sequence $\tilde{M}^n_t$ is $Q$-almost surely constant and therefore we can define $\tilde{M}_t = \lim_{n \to \infty} \tilde{M}^n_t$ and it follows that $\tilde{M}^n = (\tilde{M})^{\tau_n}$.  Now we can apply Lemma \ref{LocalMartingaleLocalProperty} to conclude that $\tilde{M}$ is a local $Q$-martingale.
\end{proof}

Note that in the result above the quadratic covariation $[Z,M]$ is taken with respect to the measure $P$.  However we know from Lemma \ref{ApproximateOptionalQuadraticCovariationContinuousSemimartingale} that for each $t \geq 0$ we can find a sequence of partitions $0=t_{n,0} < \dotsb < t_{n,k_n} = t$ such that $\sum_{j=1}^{k_n} (Z_{t_{n, j}} - Z_{t_{n, j-1}})  (M_{t_{n, j}} - M_{t_{n, j-1}}) \toprob [Z,M]_t$ with respect to $P$.  Since $P$ and $Q$ are locally equivalent this implies that in fact $[Z,M]$ is the quadratic covariation of $Z$ and $M$ under $Q$ as well.

If we specialize the previous result to the case in which $M$ is a Brownian motion then it is easy to see that $\tilde{M}$ is also a Brownian motion; thus the family of Brownian motions is invariant under locally equivalent changes of measure.
\begin{cor}Let $P$ and $Q$ be locally equivalent probability measures on a measure space $(\Omega, \mathcal{A})$ with a filtration $\mathcal{F}$.   Let $Z_t$ be an $\mathcal{F}$-adapted process such that $Q = Z_t \cdot P$ on $\mathcal{F}_t$ for all $t \geq 0$ and assume that $Z_t$ is almost surely continuous.  Then if $M$ is a $P$-Brownian motion, then process $M_t - \int_0^t Z^{-1}_s d[Z,M]_s$ is a $Q$-Brownian motion.
\end{cor}
\begin{proof}
Since $\int_0^t Z^{-1}_s d[Z,M]_s$ has finite variation we know that $[M - \int Z^{-1}_s d[Z,M]_s]_t = [M]_t = t$ where we have used fact that $M$ is a $P$-Brownian motion and the discussion preceeding the corollary to note that the quadratic variation of $M$ with respect to $Q$ is the same as the quadratic variation with respect to $P$.  Since $M$ is continuous local $Q$-martingale, it follows from Levy's Theorem \ref{LevyCharacterizationOfBrownianMotion} that $M - \int Z^{-1}_s d[Z,M]_s$ is in fact a $Q$-Brownian motion.
\end{proof}

In some ways Theorem \ref{AbstractTransformationOfDriftContinuousLocalMartingale} is a deceptively clean result.  In applications it is common that one is not given the measure $Q$ rather one starts with a nonnegative process $Z_t$.  Two things need to be addressed. First is that it is often easy to see that $Z$ is a local martingale (e.g. by expressing $Z$ as a stochastic integral) but the hypotheses of the theorem require that it is a true martingale.  Therefore one should spend some time developing conditions that allow one to conclude that a nonnegative local martingale is a martingale; there are no necessary and sufficient conditions known but there are some useful sufficient conditions.  The second, more subtle, issue is constructing the measure $Q$ from the given nonnegative martingale $Z_t$.  As in Lemma \ref{LikelihoodRatioProcesses} this is easy if $Z_t$ is uniformly integrable but in many important applications uniform integrability of $Z_t$ will not hold.  As it turns out, the existence of a $Q$ such that $Q=Z_t \cdot P$ on every $\mathcal{F}_t$ is not guaranteed and depends on the properties of the underlying filtration $\mathcal{F}$.  In particular, the usual conditions on $\mathcal{F}$ may be incompatible with the existence of $Q$.  Theorem \ref{AbstractTransformationOfDriftContinuousLocalMartingale} has become such an important tool that this phenomenon is viewed as a deficiency of the usual conditions and has led some authors to propose that the usual conditions be replaced by a different extension procedure that is compatible with Theorem \ref{AbstractTransformationOfDriftContinuousLocalMartingale}.


We examine conditions under which a positive local martingale is a martingale.  First, we note that every positive continuous local martingale has a logarithm that is a continuous local martingale.
\begin{lem}\label{RealExponentialContinuousMartingale}A continuous process $Z > 0$ is a local martingale if and only if there exists a continuous local martingale $M$ such that
\begin{align*}
Z_t &= \mathcal{E}(M)_t \equiv e^{M_t - \frac{1}{2}[M]_t} \text{ for all $t \geq 0$}
\end{align*}
Such an $M$ is almost surely unique and satifies $[M,N]_t = \int_0^t Z^{-1}_s \, [Z,N]_s$ for any continuous local martingale $N$.
\end{lem}
\begin{proof}
Suppose that $M$ is a continuous local martingale then apply It\^{o}'s Lemma to the continuous semimartingale $M - \frac{1}{2}[M]$ to see
\begin{align*}
\mathcal{E}(M)_t &= e^{M_0} + \int_0^t \mathcal{E}(M) \, d(M - \frac{1}{2}M) + \frac{1}{2} \int_0^t \mathcal{E}(M)_s \, d[M]_s =  e^{M_0} + \int_0^t \mathcal{E}(M) \, dM
\end{align*}
which shows that $\mathcal{E}(M)$ is a stochastic integral hence a continuous local martingale.

If we assume that $Z>0$ is a continuous local martingale then again apply It\^{o}'s Lemma, Lemma \ref{ChainRuleStieltjes} and the defining property of stochastic integrals to see (TODO: We need the extension to functions defined on an open subset of $\reals^d$) to see
\begin{align*}
\log(Z)_t - \log(Z)_0 &= \int_0^t Z^{-1} \, dZ - \frac{1}{2} \int_0^t Z^{-2}_s \, d[Z]_s \\
&= \int_0^t Z^{-1} \, dZ - \frac{1}{2} \int_0^t Z_s^{-1} d \int_0^s Z_s^{-1} d[Z]_s \\
&= \int_0^t Z^{-1} dZ - \frac{1}{2} \int_0^t Z_s^{-1}  d [ \int Z^{-1} \, dZ, Z]_s \\
&=  \int_0^t Z^{-1} dZ - \frac{1}{2}[ \int Z^{-1} \, dZ]_t
\end{align*}
so the result holds with $M_t = \int_0^t Z^{-1} \, dZ$.  From this expression for $M$ it follows that for any continuous local martingale $N$, we have $[M,N]_t = [\int Z^{-1} \, dZ, N]_t = \int_0^t Z^{-1}_s \, d[Z,N]_s$.  Uniqueness follows that if $M$ and $N$ are continuous local martingales with $M - \frac{1}{2} [M] = N - \frac{1}{2} [N]$ then we have $M -N = \frac{1}{2} [N] - \frac{1}{2} [M]$ is a continuous local martingale of finite variation hence is almost surely zero by Lemma \ref{ContinuousLocalMartingaleBoundedVariation}.
\end{proof}

As a result of Lemma \ref{RealExponentialContinuousMartingale} we look for conditions on a continuous local martingale $M$ that guarantee that $\mathcal{E}(M)$ is a continuous martingale.  The following is a commonly used condition.
\begin{lem}[Novikov's Condition]\label{NovikovsCondition}Let $M$ be a continuous local martingale with $M_0 = 0$ such that $\expectation{e^{\frac{1}{2}[M]_t}} < \infty$ for all $t \geq 0$ then $\mathcal{E}(M)$ is a martingale.  If in addition $\expectation{e^{\frac{1}{2}[M]_\infty}} < \infty$ then $\mathcal{E}(M)$ is a uniformly integrable martingale.
\end{lem}
\begin{proof}
TODO
\end{proof}

\begin{examp}Constructing an example of a Brownian motion with a drift term that can be removed with respect to a filtration generated by the Brownian motion but cannot be removed with respect to the completion of that filtration turns out not to be too hard. Let $B_t$ be a standard Brownian motion 
\end{examp}

The following theorm clarifies when a deterministic drift term can be removed from a Brownian motion.  Historically, this is one of the first results dealing with change of measure.  TODO: The result as specified in Kallenberg states that we use the augmented filtration; I'm not sure if the result is true under these circumstances (just consider the limit event $B_t/t \to \mu$ as in the example).  I suspect it is true if the filtration is that generated by $B$ (or restricting to an arbitrary finite interval $[0,T]$).
\begin{thm}[Cameron-Martin Theorem]\label{CameronMartin}Let $B=(B_1, \dotsc, B_d)$ be a $d$-dimensional Brownian motion and let $\mathcal{F}_t$ be the complete filtration generated by $B$. Let $h: \reals_+ \to \reals^d$ be a continuous function with $h(0) = 0$ and let $P_h$ be distribution of $B + h$.  Then $P_0 \sim P_h$ on $\mathcal{F}_t$ for all $t \geq 0$ if and only $h(t) = \int_0^t f(s) \, ds$ for some $f \in L^2_{loc}$.  Moreover, in this case we have $P_h = \mathcal{E}(f \cdot B)_t \cdot P_0$ on $\mathcal{F}_t$.
\end{thm}
\begin{proof}
First assume that $P_0 \sim P_h$ on $\mathcal{F}_t$ for all $t \geq 0$.  By Lemma \ref{LikelihoodRatioProcesses} we know that there exists a $P_0$-martingale $Z$ with $Z_t > 0$ and $P_h = Z_t \cdot P_0$ on $\mathcal{F}_t$ for all $t \geq 0$.  Since $Z$  is a Brownian martingale we may apply the Martingale Representation Theorem \ref{MartingaleRepresentationTheorem} to conclude that $Z$ is almost surely continuous and therefore by Lemma \ref{RealExponentialContinuousMartingale} we may write $Z = \mathcal{E}(M)$ for some continuous local $P_0$-martingale $M$.  Applying the Martingale Representation Theorem to $M$ we know there are almost surely unique processes $V^j \in L(B^j)$ such that $M = M_0 + \sum_{j=1}^d \int V^j \, dB^j$.  Note that $V^j \in L(B^j)$ implies $\int_0^t (V^j(s))^2 \, ds < \infty$ for all $t \geq 0$ and therefore we have $V^j \in L^2_{loc}$.
TODO: Finish
\end{proof}
