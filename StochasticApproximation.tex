\chapter{Stochastic Approximation}

This chapter covers some of the basic results in the theory of stochastic approximation and in doing so provides some applications of discrete time martingale theory and weak convergence theory to optimization problems.  The statement of the stochastic approximation problem that one often encounters is so abstract and general that it can be difficult to understand how it could be relevant to any particular problem.  Indeed it is common to see stochastic approximation defined as the study of discrete time stochastic processes of the form 
\begin{align*}
\theta_{n+1} &= \theta_n + \epsilon_n Y_n
\end{align*} 
where $Y_n$ is a random vector.

To motivate the form of the problem statement, let us tie this into the problem of optimization specifically gradient descent.  Given a function $f$ we have a globally convergent algorithm for minimization given by $x_{n+1} = x_n - \alpha_n \nabla f(x_n)$ where $\alpha_n$ is a sequence of real numbers that satisfies Armijo conditions.  Now suppose that we don't have the ability to measure $-\nabla f(x_n)$ exactly but that we have some noise corrupted  version thereof.  If we call the observed approximate gradient $Y_n$, then the gradient descent algorithm has the form of a stochastic approximation problem and we can ask whether we still have convergence in a appropriate stochastic sense (e.g. almost sure).  In line with this specific case, we often think of the process $Y_n$ as being a sequence of observations and though it doesn't have any real mathematical meaning, we shall use the terminology in what follows.

As we've mentioned in our discussion of optimization, in practice constrained optimization is at least as important as unconstrained optimization and therefore we should look for how to incorporate constraints into stochastic approximation.  The way we shall do this at this point is to assume that the sequence $\theta_n$ is constrained to lie in some closed set $F$ and to maintain the constraint at each iteration by a brute force projection (say in $L^2$ norm) onto the set $F$.   Thus in the constrained case we are considering a stochastic process 
\begin{align*}
\theta_{n+1} &= \Pi_F \left[ \theta_n + \epsilon_n Y_n\right ]
\end{align*} 
where $Y_n$ is a random vector and $\Pi_F$ represents projection onto $F$.  It is common to define the projection correction term $Z_n =  \epsilon_n^{-1} \lbrace \Pi_F \left[ \theta_n + \epsilon_n Y_n\right ] -   \theta_n - \epsilon_n Y_n \rbrace$ so that we may write 
\begin{align*}
\theta_{n+1} &= \theta_n + \epsilon_n Y_n + \epsilon_n Z_n
\end{align*} 

In order to discuss the hypotheses that one might need to make on the stochastic process $Y_n$, it is convenient to assume a structural form for $Y_n$.  Let $\mathcal{F}_n = \sigma(\theta_0, Y_j ; j<n)$ be a filtration $\mathcal{F}$.  For our first results we shall assume that there exists functions $g_n$, an $\mathcal{F}$-martingale difference sequence $\delta M_n$ and a stochastic process $\beta_n$ such that $Y_n = g_n(\theta_n) + \delta M_n + \beta_n$.  The reader should think of these terms in the following way.  The term $g_n(\theta_n)$ represents the mean/true value of the process (e.g. the value of the gradient in the steepest descent case), the term $\delta M_n$ represents a noise term and $\beta_n$ represents a bias term in the observation.  The reason why the bias term $\beta_n$ is called out as being different from $g_n(\theta_n)$ is that we shall be assuming that it becomes asymptotically small.

One of the key techniques in proving theorems in stochastic approximation is the ODE method.  The basic idea is that one can view the process $\theta_n$ as a discretization of an ordinary differential equation 
that is described by the conditional means of $Y_n$.  

In many of the proofs we will considering the continuous time limits of discrete time processes.  To do this we be making interpolations of the discrete time processes and want to have recourse to compactness results that will give conditions under which limits exist.  A natural tool for this would be to use the Arzela-Ascoli Theorem for continuous functions or the Skorohod topology versions of that for cadlag functions.  As it turns out neither of these is the exact fit for what we do since we'll be considering cadlag processes that converge to continuous functions and we want uniform convergence on compact sets.  So what we want is a slight extension of Arzela-Ascoli Theorem.

The following is a version of the Arzela Ascoli Theorem \ref{ExtendedArzelaAscoliTheorem} that gives a
sufficent criteria for a sequence of possibly discontinuous functions
to have a continuous limit.  The referenced version of the Arzela Ascoli Theorem
doesn't apply in a non-trivial way since the criterion for
equicontinuity $\lim_{\delta \to 0} \sup_{f \in A} m(T, f, \delta) = 0$ implies that every $f$ is continuous.  However, note that if $f_n$ is a sequence of
functions in $C([0,\infty); \reals^d)$ then $\lbrace f_n \rbrace$ is equicontinuous if
and only if $\lim_{\delta \to 0} \limsup_{n \to \infty} m(T, f_n,
\delta) = 0$.  This criterion does not imply that each $f_n$ is
continuous and turns out to be a useful extension of equicontinuity
for sequences of non-continuous functions.
\begin{thm}[Extended Arzela-Ascoli
  Theorem]\label{ExtendedArzelaAscoliTheorem}Let $f_n : [0,\infty) \to
 \reals^d$ be measurable functions such that  
\begin{itemize}
\item[(i)]$\sup_{n} \abs{f_n(0)} < \infty$
\item[(ii)] $\lim_{\delta \to 0} \limsup_{n \to \infty} m(T, f_n,
  \delta) = 0$ for all $T > 0$.
\end{itemize}
then there exists $f \in C([0,\infty), \reals^d)$ such that $f_n$
converges to $f$ uniformly on compact sets.
\end{thm}
\begin{proof}
First note that if (i) and (ii) hold for the sequence $f_n$ then the conditions also hold for any subsequence of $f_n$.  
Suppose that $f_n$ satisfy (i) and (ii) and let $T > 0$.  By (ii) there exists a $\delta > 0$ such that 
\begin{align*}
\limsup_{n \to \infty} m(T, f_n,  \delta)  < 1
\end{align*}
hence there exists an $N \in \naturals$ such that $m(T, f_n,  \delta)  < 1$ for all $n \geq N$.  Now pick $m \in \naturals$ such 
that $m \delta < T \leq (m+1)\delta$ and just as in Theorem \ref{ArzelaAscoliTheorem}  by considering  the grid $0, \delta, 2\delta, \dotsc,
m\delta, T$ we can write the telescoping sum
\begin{align*}
f_n(T) - f_n(0) = f_n(T) - f_n(m\delta) + \sum_{k=1}^m f_n(k \delta) - f_n((k-1)\delta)
\end{align*}
and use the triangle inequality to conclude that $\abs{f_n(T)}
\leq \abs{f_n(0)} + m+1$ for every $n \geq N$.  Coupled with (i) this shows that
$\sup_{n \geq N} \abs{f_n(T)} < \infty$.  By local compactness of $\reals$ we see that  $f_n(T)$ converges 
along a subsequence of $\lbrace n, n+1, \dotsc \rbrace$.  Using the observation that $f_n$ along this subsequence still satisfies
(i) and (ii) we see that we can enumerate $T \in \rationals_+$ and use induction and a diagonal subsequence argument to get a 
single subsequence of $f_n$ that converges for all $T \in \rationals_+$.  Define $f(T)$ for $T \in \rationals_+$ as the limit of this subsequence of $f_n$.  

The proof that $f \in C([0,\infty); \reals^d)$ and that $f_n$ converges to $f$ uniformly on compact sets is almost exactly the same as the proof in Theorem
\ref{ArzelaAscoliTheorem}.  The only difference is that the condition (ii) applied the sequence $f_n$ only constrains terms $\abs{f_n(s) - f_n(t)}$ for $n$ sufficiently large.  Examining the proof of Theorem \ref{ArzelaAscoliTheorem} one will see that this is all that is required.
\end{proof}

We also need a partial converse, namely that a convergence sequence of functions is equicontinuous in the extended sense.
\begin{prop}\label{PartialConverseExtendedArzelaAscoli}Let $f_n : \reals \to \reals^d$ be a sequence of measurable functions that converges to a continuous function with convergence uniform on compact sets, then $f_n$ is equicontinuous in the extended sense.
\end{prop}
\begin{proof}
Let $f$ be the limit of $f_n$.  Pick an $N$ such that $\norm{f_n(0) - f(0)} < 1$ for all $n \geq N$ then it follows that $\norm{f_n(0)} \leq \norm{f_0(0)} \vee \dotsb \vee \norm{f_{N-1}(0)} \vee \norm{f_N(0)} + 1$ for all $n \in \naturals$.  Now let $T > 0$ and $\epsilon > 0$ be given and use the fact that $f$ is uniformly continuous on $[-T,T]$ to pick 
a $\delta > 0$ such that $m(T, f, \delta) < \epsilon/3$.  Now by uniform convergence of $f_n$ to $f$ on $[-T,T]$ we pick $N > 0$ such that $\sup_{-T \leq t \leq T} \norm{f_n(t) - f(t)} < \epsilon/3$ for $n \geq N$.  Thus for $n \geq N$,
\begin{align*}
m(T,f_n,\delta) &= \sup_{\substack{
\abs{s -t} < \delta \\
-T \leq s,t \leq T}} \norm{f_n(s) - f_n(t)} \\
&\leq \sup_{\substack{
\abs{s -t} < \delta \\
-T \leq s,t \leq T}} \norm{f_n(s) - f(s)} + \norm{f(s) - f(t)} + \norm{f_n(t) - f(t)} < \epsilon
\end{align*}
and it follows that $\limsup_{n \to \infty} m(T, f_n, \delta) < \epsilon$ and thus $\lim_{\delta \to 0} \limsup_{n \to \infty} m(T, f_n, \delta) = 0$.
\end{proof}

We shall now assume that we are in the situation of having a constraint set $F$ defined by continuously differentiable function $c_i(x)$ which satisfy the LICQ.  

TODO:  Use the KKT  conditions applied to $\min_{x \in F} \norm{x - (\theta_n + \epsilon_n Y_n)}^2$ to show that $Z_n$ is in the normal cone 


\begin{thm}Suppose we are given a process $Y_n$, a constraint set $F$, a random variable $\theta_0$ and a deterministic sequence $\epsilon_n$.  Define the process 
\begin{align*}
\theta_{n+1} &= \Pi_F \left[\theta_{n} + \epsilon_n Y_n \right]
\end{align*}
and suppose that there are measurable functions $g_n(\theta)$ such that if we write $\cexpectationlong{\theta_0, Y_i; 0 \leq i \leq n-1}{Y_n} = g_n(\theta_n) + \beta_n$ such that
\begin{itemize}
\item[(i)] $\sup_n \expectation{Y_n^2} < \infty$
\item[(ii)] $\epsilon_n$ for $n \in \integers$ is a sequence with $\epsilon_n = 0$ for $n < 0$, $\epsilon_n \geq 0$ for $n \geq 0$, $\lim_{n \to \infty} \epsilon_n = 0$,  $\sum_{n=0}^\infty \epsilon_n = \infty$ and $\sum_{n=0}^\infty \epsilon^2_n < \infty$.  
\item[(iii)] Suppose the $g_n(\theta)$ are uniformly continuous in $n$ and there is a continuous function $\overline{g}(\theta)$ such that for each $\theta \in F$ we have
\begin{align*}
\lim_{n \to \infty} \abs{\sum_{i=n}^{m(t_n+t)} \epsilon_i \lbrace g_i(\theta) - \overline{g}(\theta) \rbrace }&= 0
\end{align*}
\item[(iv)] $\beta_n \toas 0$
\end{itemize}
Then there is a set $A$ of probability zero such that for $\omega \notin A$ the set of functions $\lbrace \theta^n(\omega, \cdot), Z^n(\omega, \cdot); n < \infty \rbrace$ is equicontinuous.  If $(\theta(\omega, \cdot), Z(\omega, \cdot))$ is the limit of some convergent subsequence then the pair satisfies the projected ODE 
\begin{align*}
\dot{\theta} &= \overline{g}(\theta) + z \text{, $z \in \mathcal{N}(\theta)$}
\end{align*}
and $\theta_n(\omega)$ converges to a limit set of the projected ODE in $F$.  
\end{thm}
\begin{proof}
Let $\mathcal{F}_n = \sigma(\theta_0, Y_i; 0 \leq i \leq n)$ be the filtration defined by $Y_n$ and the initial condition $\theta_0$.
We write
\begin{align*}
\theta_{n+1} &= \Pi_F \left [ \theta_n + \epsilon_n Y_n \right ] \\
&=\theta_n + \epsilon_n Y_n + \epsilon_n Z_n \\
&=\theta_n + \epsilon_n \cexpectationlong{\mathcal{F}_{n-1}}{Y_n} + \epsilon_n \left( Y_n -  \cexpectationlong{\mathcal{F}_{n-1}}{Y_n} \right) + \epsilon_n Z_n \\
&=\theta_n + \epsilon_n g_n(\theta_n) + \epsilon_n \beta_n + \epsilon_n \delta M_n + \epsilon_n Z_n \\
\end{align*}
where we have defined $\delta M_n = Y_n -  \cexpectationlong{\mathcal{F}_{n-1}}{Y_n}$.  Note that $\epsilon_n \delta M_n$ is an
$\mathcal{F}$-martingale difference sequence and therefore by Proposition \ref{MartingaleDifferenceSequence} the process $M_n = \sum_{j=0}^n \epsilon_j \delta M_j$ is an 
$\mathcal{F}$-martingale.  Furthermore by Jensen's Inequality for conditional expectations (Theorem \ref{JensenConditionalExpectation}) and the fact that $Y_n$ is $L^2$-bounded
we also know that $\delta M_n$ is an $L^2$ martingale difference sequence hence by Proposition \ref{SquareIntegrableMartingaleDifferenceWhiteNoise} we know that $\expectation{\delta M_n  \delta M_m} = 0$.  For every fixed $m \in \integers_+$ we know that the process $(M_{n+m} - M_m)^2$ is a submartingale with respect to the shifted
filtration $\tilde{\mathcal{F}}_n = \mathcal{F}_{n+m}$ and if we apply Doob's Maximal Inequality (Lemma \ref{DoobMaximalInequalityDiscrete}) we get for every $\lambda > 0$ and $m < n$,
\begin{align*}
\probability{\sup_{m \leq j \leq n} \abs{M_j - M_m} \geq \lambda} &= \probability{\sup_{m \leq j \leq n} (M_j - M_m)^2 \geq \lambda^2} \\
&\leq \lambda^{-2} \expectation{(M_n - M_m)^2} \\
&=\lambda^{-2}\sum_{i=m+1}^n \sum_{j=m+1}^n  \epsilon_i \epsilon_j \expectation{\delta M_i \delta M_j} \\
&=\lambda^{-2}\sum_{j=m+1}^n \epsilon_j^2  \expectation{\delta M_j^2} \\
&\leq 2 \lambda^{-2} \sup_{n} \expectation{Y_n^2} \sum_{j=m+1}^\infty \epsilon_j^2  \\
\end{align*}
By continuity of measure, we can let $n \to \infty$ and then $m \to \infty$ and use the hypothesis that $\sum_{n=0}^\infty \epsilon^2_n < \infty$ to conclude that for every $\lambda >0$ we have 
\begin{align}\label{SASimpleMartingaleNoiseConvergence}
\lim_{m \to \infty} \probability{\sup_{m \leq j} \abs{M_j - M_m} \geq \lambda} &= 0
\end{align}
TODO: Could we have just appealed to an off the shelf Martingale Convergence theorem here; not sure this is a interesting kind of convergence because are looking at a subsequence that starts at a point that goes to infinity????  We have just proven that $\sup_{m \leq j} \abs{M_j - M_m} \toprob 0$.

Now we move to the interpolated process.  Recall that we define $t_0 = 0$ and $t_n = \sum_{i=0}^{n-1} \epsilon_i$ for $n \in \naturals$.  We define $m(t) = n$ for $t_n \leq t < t_{n-1}$.  Using $m(t)$ we define the interpolated processes for $t \geq 0$,
\begin{align*}
M^n(t) &= \sum_{i=n}^{m(t_n + t)-1} \epsilon_i \delta M_i &  
B^n(t) &=\sum_{i=n}^{m(t_n + t)-1} \epsilon_i \beta_i &
Z^n(t) &= \sum_{i=n}^{m(t_n + t)-1} \epsilon_i Z_i
\end{align*}
and for $t < 0$,
\begin{align*}
M^n(t) &= - \sum_{i=m(t_n + t)}^{n-1} \epsilon_i \delta M_i &  
B^n(t) &= - \sum_{i=m(t_n + t)}^{n-1} \epsilon_i \beta_i &  
Z^n(t) &= - \sum_{i=m(t_n + t)}^{n-1} \epsilon_i Z_i &  
\end{align*}
and note that $M^n(t) = M^0(t_n+t) - M^0(t_n)$ and similarly with $B^n$ and $Z^n$.  Moreover
$M^0(t) = M_{m(t) -1}$.
Furthermore we define
\begin{align*}
\overline{G}^n(t) &= \sum_{i=n}^{m(t_n + t)-1} \epsilon_i \overline{g}(\theta_n) &  
\tilde{G}^n(t) &=\sum_{i=n}^{m(t_n + t)-1} \epsilon_i \left( g_n(\theta_n) - \overline{g}(\theta) \right )&
\end{align*}
so that we have 
\begin{align*}
\theta^n(t) &= \theta_n + \overline{G}^n(t)  + \tilde{G}^n(t) + M^n(t) + Z^n(t)  + B^n(t) 
\end{align*}

\begin{clm}Almost surely for all $T > 0$, $\lim_{n \to \infty} \sup_{-T \leq t \leq T} M^n(t) = 0$.
\end{clm}

Let $T > 0$ be given.  By the definition of $M^n$ and the triangle inequality we get for every $n \in \naturals$ and $m < m(t_n -T)$ we have
\begin{align*}
\sup_{-T \leq t \leq T} \abs{M^n (t)}  
&=\sup_{-T \leq t \leq T} \abs{M^0 (t_n + t) - M^0(t_n)}  \\
&=2 \sup_{-T \leq t \leq T} \abs{M^0 (t_n + t) - M^0(t_n -T )} \\  
&\leq 2 \sup_{m(t_n-T) - 1 \leq j}  \abs{M_j - M_{m(t_n -T )-1}}\\
&\leq 4 \sup_{m \leq j}  \abs{M_j - M_{m}}\\
\end{align*}
If $\lim_{n \to \infty} \sup_{-T \leq t \leq T} M^n (t) != 0$ then there is a $\lambda>0$ and a subsequence $n_j$such that $\sup_{-T \leq t \leq T} \abs{M^{n_j} (t)} \geq \lambda$ for all $j \in \naturals$.  Since we know that $\lim_{j \to \infty} m(t_{n_j} -T) = \infty$, we know $\sup_{m \leq j}  \abs{M_j - M_{m}} \geq \lambda/4$ for all $m$ and therefore the claim follows from Equation \eqref{SASimpleMartingaleNoiseConvergence}.  


\begin{clm}Almost surely for all $T > 0$, $\lim_{n \to \infty} \sup_{-T \leq t \leq T} B^n(t) = 0$.
\end{clm}

We actually want this under a couple of different hypotheses: $\beta_n \toas 0$ and $\sum_{n=0}^\infty \epsilon_n \abs{\beta_n} < \infty$ a.s.  In the latter case 
\begin{align*}
\sup_{-T \leq t \leq T} \abs{B^n(t)} &\leq \sum_{i=m(t_n -T)}^{m(t_n+T) -1} \epsilon_i \abs{\beta_i} \leq \sum_{i=m(t_n -T)}^{\infty} \epsilon_i \abs{\beta_i} 
\end{align*}
so the result follows from the fact that $\lim_{n \to \infty} m(t_n -T) = \infty$.  TODO: What about the former case?

\begin{clm}$\theta_{n+1} - \theta_n \toas 0$.
\end{clm}
Using the Markov Inequality, $\sup_n \expectation{Y_n^2} < \infty$ and $\sum_{n=0}^\infty \epsilon_n^2 <\infty$ we get for any $\lambda > 0$,
\begin{align*}
\sum_{n=0}^\infty \probability{\epsilon_n \abs{Y_n} \geq \lambda} &\leq 
\sum_{n=0}^\infty \frac{\epsilon_n^2 \expectation{Y_n^2}}{\lambda^2} \\
&\leq \frac{\sup_n \expectation{Y_n^2}}{\lambda^2} \sum_{n=0}^\infty \epsilon_n^2  < \infty
\end{align*}
and therefore the Borel Cantelli Theorem \ref{BorelCantelli} implies that $\probability{\epsilon_n \abs{Y_n} \geq \lambda \text{ i.o.}} = 0$ and therefore by Lemma \ref{ConvergenceAlmostSureByInfinitelyOften} we conclude that $\epsilon_n \abs{Y_n} \toas 0$.  Thus the definition of $\Pi_F$ and the fact that $\theta_n \in F$, we see that (we are using the argument that $\Pi_F(\theta_n + \epsilon_n Y_n) = \argmin_{x \in F} \abs{\theta_n + \epsilon_n Y_n - x}$ hence since $\theta_n \in F$,
\begin{align*}
\abs{\theta_n + \epsilon_n Y_n - \Pi_F(\theta_n + \epsilon_n Y_n)} &\leq \abs{\theta_n + \epsilon_n Y_n - \theta_n} = \epsilon_n \abs{Y_n}
\end{align*}
is this always true or does it require some assumption like prox-regularity????)
\begin{align*}
\lim_{n \to \infty} \abs{\theta_{n+1} - \theta_n } &= \lim_{n \to \infty} \abs{ \Pi_F \left[ \theta_n + \epsilon_n Y_n \right] - \theta_n } \\
&\leq  \lim_{n \to \infty} \lbrace \abs{ \Pi_F \left[ \theta_n + \epsilon_n Y_n \right] - \theta_n - \epsilon_n Y_n} + \epsilon_n \abs{Y_n} \rbrace \\
&\leq 2 \lim_{n \to \infty} \epsilon_n \abs{ Y_n}  = 0
\end{align*}
almost surely.


By prior claims and Proposition \ref{PartialConverseExtendedArzelaAscoli} we know that almost surely, $M^n(t)$ and $B^n(t)$ are each equicontinuous in the extended sense. 

\begin{clm}$Z^n$ is almost surely equicontinuous in the extended sense.
\end{clm}
Here is the hyperrectangle case.  We work pathwise so lets assume that $\omega \in \Omega$ is fixed.  

\begin{clm}If $Z^n(\omega)$ is not equicontinuous in the extended sense then there is an $\epsilon > 0$, a sequence $n_k \in \naturals$ with $\lim_{k \to \infty} n_k = \infty$ and a sequence $\delta_k > 0$ with $\lim_{k \to \infty} \delta_k = 0$ such that $\abs{Z^{n_k}(\omega, \delta_k)} \geq \epsilon$.
\end{clm}
 If $Z^n$ is not equicontinuous in the extended sense then there is a $T > 0$ such that $\lim_{\delta \to 0} \limsup_{n \to \infty} m(T, Z^n, \delta) > 0$ i.e. an $\epsilon > 0$, a sequence $\delta_k$ with $\delta_k \to 0$, a sequence $n_k$ with $n_k \to \infty$ and $s_k, u_k$ with $-T \leq s_k < u_k \leq T$ and $u_k - s_k < \delta_k$ such that $\abs{Z^{n_k}(u_k) - Z^{n_k}(s_k)} \geq \epsilon$.  Recalling $Z^n(t) = Z^0(t_n+t) - Z^0(t_n)$  we see that $\abs{Z^{n_k}(u_k) - Z^{n_k}(s_k)} \geq \epsilon$ is equivalent to $\abs{Z^{0}(t_{n_k} + u_k) - Z^0(t_{n_k} + s_k)} \geq \epsilon$ and therefore if we define $m_k$ such that .  By redefining the sequence $n_k$ to be $\tilde{n}_k = m(t_{n_k} + s_k)$, $\tilde{s}_k = s_k  + t_{n_k} - t_{\tilde{n}_k}$  and $\tilde{u}_k = u_k + t_{n_k} - t_{\tilde{n}_k}$ we get $\abs{Z^{\tilde{n}_k}(\tilde{u}_k) - Z^{\tilde{n}_k}(\tilde{s}_k)} \geq \epsilon$ where by definition $t_{\tilde{n}_k} \leq s_k < t_{\tilde{n}_k + 1} = t_{\tilde{n}_k } + \epsilon_{\tilde{n}_k+1}$ and thus $0 \leq \tilde{s}_k < \epsilon_{\tilde{n}_k+1}$   but still 
\begin{align*}
\lim_{k \to \infty} \tilde{n}_k &= \lim_{k \to \infty} m(t_{\tilde{n}_k}) \geq \lim_{k \to \infty} m(t_{n_k} - T) = \infty
\end{align*} 
from which it also follows that $\lim_{k \to \infty} \tilde{s}_k = 0$ and therefore $\lim_{k \to \infty} \tilde{u}_k \leq  \lim_{k \to \infty} \tilde{s}_k + \delta_k = 0$.  Since $0 \leq \tilde{s}_k < \epsilon_{\tilde{n}_k+1}$ we also have $Z^{\tilde{n}_k}(\tilde{s}_k) = Z^{\tilde{n}_k}(0) = 0$ so the sequences $\tilde{n}_k$ and $\tilde{u}_k$ satisfy the claim.

TODO: Note that $\delta_k$ doesn't necessarily go to 0 faster than $\epsilon_{n_k} + \epsilon_{n_k + 1}$ so $Z^{n_k}(\delta_k)$ may be a sum of multiple jumps $\epsilon_{n_k} Z_{n_k} + \dotsb + \epsilon_{n_k + m_k} Z_{n_k + m_k}$.  This makes the geometry a tad confusing for me.  Can we reduce to a case in which $0 \leq \delta_k + \epsilon_{n_k} < \epsilon_{n_k+1}$?


TODO: I still don't see the geometry here.  Relevant facts: 
\begin{itemize}
\item $\theta_{n+1} \in \interior(F)$ implies $Z_n = 0$ (this follows from the next item and the fact that $N_F(\theta_{n+1}) = \lbrace 0 \rbrace$ if $\theta_{n+1} \in \interior(F)$
\item $Z_n \in N_F(\theta_{n+1})$
\item $\epsilon_n Z_n \toas 0$
\item $M^n(t) \toas 0$ uniformly on compacts
\item $B^n(t) \toas 0$ uniformly on compacts
\end{itemize}
In prose, a asymptotic jump in $Z^{n_k}$ cannot be into the interior because that would imply that $Z_{n_k} = 0$.  The claim is that in the limit, the jump therefore must be from the boundary of $F$ (I don't see this yet but I suspect it follows from $\theta_{n+1} - \theta_n \toas 0$) to another point on the boundary of $F$ (I get this follows from $Z_n \in N_F(\theta_{n+1})$) and that this contradicts the fact that $Z_n \in N_F(\theta_{n+1})$ (I don't see this yet).  The basic intuition is this: 
\begin{itemize}
\item asymptotically since $\epsilon_n Z_n \to 0$ a.s. we know that any jump $Z^n(0)$ in the limit must be from the boundary of $F$
\item jumps in $Z_n$ are always to the boundary and not to the interior thus in the limit this is true and therefore the asymptotic jump is from a boundary point to another boundary point
\item since $Z_n \in N_F(\theta_{n+1})$ and the normal cone points \emph{into} $F$ it is impossible for the jump to be between boundary points
\end{itemize}
So the relevant geometry here just precludes some kind of limit of $Z_n$ pointing out of the normal cone (this seems like it will be true with regularity assumptions for then we know that any limit of proximal normals is a limiting normal but with appropriate regularity assumptions we know that proximal normals are the same thing as limiting normals).

TODO: Finish
\end{proof}

\section{Dynamical Systems Approach}

This section follows Benaim's notes (which in turn summarize a bunch of the Benaim and Hirsch work).

\begin{defn}Let $(S,r)$ be a metric space a continuous map $\Phi : [0,\infty) \times S \to S$. We write $\Phi_t(x) = \Phi(t,x)$ for $0 \leq t < \infty$ and $\Phi^x(t) = \Phi(t,x)$ for $x \in S$.  $\Phi$ is said to be a \emph{semiflow} if 
\begin{itemize}
\item[(i)] $\Phi_0 = \IdentityMatrix$
\item[(ii)] $\Phi_s \circ \Phi_t = \Phi_{t+s}$ for all $0 \leq t,s < \infty$.
\end{itemize}
A continuous map $\Phi : (-\infty, \infty) \times S \to S$ with properties (i) and (ii) is called a \emph{flow}.  Given $x \in S$ we often refer to $\Phi^x$ as an \emph{orbit}.
\end{defn}

\begin{defn}Let $(S,r)$ be a metric space and let $\Phi$ be a semiflow on $S$, then $X \in C([0,\infty) ; S)$ is said to be an \emph{asymptotic pseudotrajectory} of $\Phi$ if for all $T > 0$ we have 
\begin{align*}
\lim_{t \to \infty} \sup_{0 \leq h \leq T} r(X(t+h), \Phi_h(X(t))) = 0
\end{align*}
If the image $X([0,\infty))$ has compact closure we often say that $X$ is \emph{precompact}.
\end{defn}

Our first goal is formulate some conditions that are equivalent to an $X$ being an asymptotic pseudotrajectory.  The idea is to investigate the orbits of $X$ under a canonical flow on the space $C((-\infty, \infty); S)$.  The first step is to define the \emph{translation flow} on $C((-\infty, \infty); S)$.  Recall that $C((-\infty, \infty); S)$ is a metric space with metric
\begin{align*}
d(f,g) &= \sum_{n=1}^\infty 2^{-n} (\sup_{-n \leq t \leq n} r(f(t), g(t)) \wedge 1)
\end{align*}

\begin{prop}Let $(S,r)$ be a metric space define $\Theta : (-\infty, \infty) \times C((-\infty, \infty); S) \to C (-\infty, \infty); S)$ by $\Theta(t,f)(s) =f(t+s)$, then $\Theta$ is a flow.
\end{prop}
\begin{proof}
The only non-trivial part is the proof that $\Theta$ is continuous.  Suppose that $(t, f), (t_n,f_n) \in  (-\infty, \infty) \times C((-\infty, \infty); S)$ and $\lim_{n \to \infty} (t_n, f_n) = (t,f)$.  Let $\epsilon > 0$ and $T > 0$ be given.  $f$ is uniformly continuous on $[-T-\abs{t}-1,T+\abs{t}+1]$ so we may pick a $\delta > 0$ such that 
\begin{align*}
\sup_{\substack{\abs{u}, \abs{v} \leq T + \abs{t}+1 \\ \abs{u-v} < \delta}} r(f(u), f(v)) < \epsilon/2
\end{align*}
Since $\lim_{n \to \infty} t_n = t$ and $\lim_{n \to \infty} f_n = f$ we may pick $N > 0$ such that $\abs{t_n - t} < \delta \wedge 1$ and $\sup_{\abs{s} \leq T + \abs{t} + 1} r(f_n(s), f(s)) < \epsilon/2$ for all $n \geq N$
Therefore for all $n \geq N$
\begin{align*}
\sup_{-T \leq s \leq T} r(\Theta(t_n, f_n)(s), \Theta(t,f)(s)) &= \sup_{-T \leq s \leq T} r(f_n(t_n +s), f(t+s)) \\
&\leq \sup_{-T \leq s \leq T} r(f_n(t_n +s), f(t_n+s)) + \sup_{-T \leq s \leq T} r(f(t_n +s), f(t+s)) \\
&\leq \sup_{\abs{s} \leq T+\abs{t}+1} r(f_n(s), f(s)) + \sup_{\abs{s} \leq T+\abs{t}+1} r(f(s), f(s)) < \epsilon \\
\end{align*}
\end{proof}

TODO: Is there a translation flow on $D((-\infty, \infty); S)$ in the uniform topology?  in the Skorohod topology?

If we are given a semiflow $\Phi : [0,\infty) \times S \to S$ it is often convenient to consider it as a flow $\Phi : (-\infty,\infty) \times S \to S$ by defining $\Phi(-t,x) = \Phi(0,x) = x$.  TODO: Exercise to show this is a flow (specifically continuity)

In order to compare the flow $\Phi$ with the flow $\Theta$ we use the following
\begin{prop}Let $(S,r)$ be a metric space and let $\Phi$ be a semiflow or flow on $S$.  If we define $H(x) = \Phi^x \in C((-\infty, \infty); S)$ and $S_\Phi = \range{H}$ then $H$ is a homeomorphism of $S$ and $S_\Phi$ and moreover
\begin{align*}
\Theta_t (H(x)) &= H(\Phi_t(x))
\end{align*}
where we assume that $t \geq 0$ if $\Phi$ is a semiflow and $-\infty < t < \infty$ is $\Phi$ is a flow.  Therefore $S_\Phi$ is a closed subset of $C((-\infty, \infty); S)$ and is invariant under $\Theta$.  The map $\hat{\Phi} : C((-\infty, \infty);S) \to S_\Phi$ defined by $\hat{\Phi}(X) = H(X(0)) = \Phi^{X(0)}$ is continuous retraction.
\end{prop}
\begin{proof}
TODO
\end{proof}

We can now reformulate the definition of asymptotic pseudotrajectory in terms of behavior under the translation flow.
\begin{lem}Let $(S,r)$ be a metric space, $X \in C([0,\infty); S)$ and $\Phi$ a semiflow on $S$ then $X$ is an asymptotic pseudotrajectory of $\Phi$ if and only if
\begin{align*}
\lim_{t \to \infty} d(\Theta_t(X), \hat{\Phi}(\Theta_t(X))) &= 0
\end{align*}
\end{lem}
\begin{proof}
By Lemma \ref{UniformConvergenceOnCompacts} we know that $\lim_{t \to \infty} d(\Theta_t(X), \hat{\Phi}(\Theta_t(X))) = 0$ if and only if $\lim_{t \to \infty} \sup_{0 \leq s \leq T} r(\Theta_t(X)(s), \hat{\Phi}(\Theta_t(X))(s)) $ for every $T>0$.  Substituting definitions of $\Theta$ and $\hat{\Phi}$ we see that this is equivalent to
$\lim_{t \to \infty} \sup_{0 \leq s \leq T} r(X(t+s), \Phi_s(X_t)) $ for every $T>0$.  
\end{proof}

\begin{thm}Let $(S,r)$ be a metric space, $X \in C((-\infty, \infty); S)$ such that $\range{X}$ has compact closure in $S$ and $\Phi : [0,\infty) \times S \to S$ be a semiflow.  The $X$ is an asymptotic pseudotrajectory of $\Phi$ is and only if $X$ is uniformly continuous and every limit point of $\Theta_t(X)$ is in $S_\Phi$.  In either case $\lbrace \Theta_t(X) \rbrace$ is relatively compact in $C((-\infty, \infty); S)$.  
\end{thm}
\begin{proof}
To see (i) implies (ii) first assume that $X$ is an asymptotic pseudotrajectory; for every $T>0$ we have $\lim_{t \to \infty} \sup_{0 \leq s \leq T} r(X_{t+s}, \Phi_s(X_t)) =0$.  Let $K$ be the closure of $\range{X}$ so that $K$ is compact in $S$.  
\begin{clm}Let $\epsilon > 0$ be given and then there exists $\delta>0$ such that $r(\Phi_t(y), y)<\epsilon$ for all $\abs{t} < \delta$
\end{clm}
Let $x \in S$ be arbitrary.  Consider 
\begin{align*}
\Phi^{-1}( B(x, \epsilon/2)) &= \lbrace (t,y) \mid r(\Phi_t(y), x) < \epsilon/2 \rbrace
\end{align*}
Since $\Phi_0$ is identity, we know that $(0,x) \in \Phi^{-1}( B(x, \epsilon/2)) $ and therefore we can find a $0 < \delta_x < \epsilon/2$ such that $B((0,x), \delta_x) \subset \Phi^{-1}( B(x, \epsilon/2))$.  Moreover we assume are using the metric $\abs{\cdot} \vee r(\cdot, \cdot)$ on $[0,\infty) \times S$ so that  $B((0,x), \delta_x) = (-\delta_x, \delta_x) \times B(x,\delta_x)$ From these two facts it follows that for all $y \in B(x, \delta_x)$ then for all $\abs{t} < \delta_x$
\begin{align*}
r(\Phi_t(y), y) &\leq r(\Phi_t(y), x) + r(x,y) < \epsilon
\end{align*}
Now the set of $B(x, \delta_x)$ with $x \in K$ is an open cover of $K$ hence there is a finite subcover $B(x_1, \delta_{x_1}), \dotsc, B(x_n, \delta_{x_n})$.  Let $\delta = \delta_{x_1} \wedge \dotsb \wedge \delta_{x_n}$.  Every $y \in K$ belongs to some $B(x_j, \delta_{x_j})$ and therefore $r(\Phi_t(y), y) < \epsilon$ for $\abs{t} < \delta_{x_j}$ an a fortiori for $\abs{t} < \delta$.

\begin{clm}$X$ is uniformly continuous
\end{clm}
Let $\epsilon>0$ be given and pick $\delta>0$ as in the previous claim.  Because $X$ is an asymptotic pseudotrajectory we may pick a $t_0>0$ such that for all $t \geq t_0$ we have $\sup_{0 \leq h \leq \delta} r(X_{t+h}, \Phi_h(X_t)) < \epsilon$.  Thus
\begin{align*}
\sup_{0 \leq h \leq \delta} r(X_{t+h}, X_t) &\leq \sup_{0 \leq h \leq \delta} r(X_{t+h}, \Phi_h(X_t)) + \sup_{0 \leq h \leq \delta} r(\Phi_h(X_t), X_t) < 2\epsilon
\end{align*}
and uniformly continuity follows. 

Suppose that $Y$ is a limit point of $\Theta_t(X)$ and $Y \notin S_\Phi$.  Since $S_\Phi$ is closed it follows that $d(Y, S_\Phi) > 0$.  Let $n_k$ be a subsequence such that $\lim_{k \to \infty} \Theta_{n_k}(X) = Y$ and observe
\begin{align*}
\lim_{k \to \infty} d(\Theta_{t_k}(X), \hat{\Phi}(\Theta_{t_k}(X))) &\geq \lim_{k \to \infty} \lbrace d(Y, \hat{\Phi}(\Theta_{t_k}(X))) - d(\Theta_{t_k}(X),Y)\rbrace \geq d(Y, S_\Phi) > 0
\end{align*}
which is a contradiction.

Now to see that (ii) implies (i) suppose that $X$ is uniformly continuous and all the limit points of $\Theta_t(X)$ are in $S_\Phi$.  First we show that (ii) implies (iii).
\begin{clm}The family $\lbrace \Theta_t(X) \rbrace$ is relatively compact.
\end{clm}
First we establish equicontinuity of $\lbrace \Theta_t(X) \rbrace$.  Let $T > 0$ and $\epsilon > 0$ be given.  By uniform continuity there exists $\delta > 0$ such that $r(X(t), X(s)) < \epsilon$ for all $\abs{t-s}<\delta$. Therefore
\begin{align*}
\sup_{\substack{-T \leq u<v \leq T \\ v-u<\delta}} r(\Theta_t(X)(u), \Theta_t(X)(v)) &= \sup_{\substack{-T \leq u<v \leq T \\ v-u<\delta}} r(X(t+u), X(t+v)) \leq \sup_{\substack{-\infty <  u<v <\infty\\ v-u<\delta}} r(X(u), X(v)) < \epsilon
\end{align*}
Since $\range{X}$ is relatively compact it follows that $\lbrace \Theta_t(X)(s) \mid t \geq 0 \rbrace$ is relatively compact for every $s$ and therefore we may apply the Arzela-Ascoli Theorem \ref{ArzelaAscoliTheorem} to conclude that $\lbrace \Theta_t(X) \rbrace$ is relatively compact in $C((-\infty,\infty); S)$.  

Now suppose that $\lim_{t \to \infty} d(\Theta_t(X), \hat{\Phi}(\Theta_t(X))) \neq 0$ then there exists an $\epsilon > 0$ and a sequence $t_k$ such that $d(\Theta_{t_k}(X), \hat{\Phi}(\Theta_{t_k}(X))) \geq \epsilon$.  By relative compactness by passing to a subsequence and by using the assumption that every limit point of $\Theta_t(X)$ is in $S_\Phi$  we may assume that there exists $Y \in S_\Phi$ such that $\lim_{k \to \infty} \Theta_{t_k}(X) = Y$.  Since $\hat{\Phi}$ is a continuous retraction onto $S_\Phi$ it follows that 
\begin{align*}
\lim_{k \to \infty} \hat{\Phi}(\Theta_{t_k}(X)) &= \hat{\Phi}( Y) = Y = \lim_{k \to \infty} \Theta_{t_k}(X) 
\end{align*}
which is a contradiction.
\end{proof}

\begin{defn}Suppose we have a sequence of vectors $x_n \in \reals^d$ for $n \in \integers_+$, $U_n \in \reals^d$ for $n \in \naturals$ and $\gamma_n \in \reals$ for $n \in \naturals$ such that $\sum_{n=1}^\infty \gamma_n = \infty$ and $\lim_{n \to \infty} \gamma_n = 0$ satisfying
\begin{align*}
x_{n+1} - x_n &= \gamma_{n+1} ( F(x_n) + U_{n+1})
\end{align*}
\end{defn} 