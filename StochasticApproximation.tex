\chapter{Stochastic Approximation}

This chapter covers some of the basic results in the theory of stochastic approximation and in doing so provides some applications of discrete time martingale theory and weak convergence theory to optimization problems.  The statement of the stochastic approximation problem that one often encounters is so abstract and general that it can be difficult to understand how it could be relevant to any particular problem.  Indeed it is common to see stochastic approximation defined as the study of discrete time stochastic processes of the form 
\begin{align*}
\theta_{n+1} &= \theta_n + \epsilon_n Y_n
\end{align*} 
where $Y_n$ is a random vector.

To motivate the form of the problem statement, let us tie this into the problem of optimization specifically gradient descent.  Given a function $f$ we have a globally convergent algorithm for minimization given by $x_{n+1} = x_n - \alpha_n \nabla f(x_n)$ where $\alpha_n$ is a sequence of real numbers that satisfies Armijo conditions.  Now suppose that we don't have the ability to measure $-\nabla f(x_n)$ exactly but that we have some noise corrupted  version thereof.  If we call the observed approximate gradient $Y_n$, then the gradient descent algorithm has the form of a stochastic approximation problem and we can ask whether we still have convergence in a appropriate stochastic sense (e.g. almost sure).  In line with this specific case, we often think of the process $Y_n$ as being a sequence of observations and though it doesn't have any real mathematical meaning, we shall use the terminology in what follows.

As we've mentioned in our discussion of optimization, in practice constrained optimization is at least as important as unconstrained optimization and therefore we should look for how to incorporate constraints into stochastic approximation.  The way we shall do this at this point is to assume that the sequence $\theta_n$ is constrained to lie in some closed set $F$ and to maintain the constraint at each iteration by a brute force projection (say in $L^2$ norm) onto the set $F$.   Thus in the constrained case we are considering a stochastic process 
\begin{align*}
\theta_{n+1} &= \Pi_F \left[ \theta_n + \epsilon_n Y_n\right ]
\end{align*} 
where $Y_n$ is a random vector and $\Pi_F$ represents projection onto $F$.  It is common to define the projection correction term $Z_n =  \epsilon_n^{-1} \lbrace \Pi_F \left[ \theta_n + \epsilon_n Y_n\right ] -   \theta_n - \epsilon_n Y_n \rbrace$ so that we may write 
\begin{align*}
\theta_{n+1} &= \theta_n + \epsilon_n Y_n + \epsilon_n Z_n
\end{align*} 

In order to discuss the hypotheses that one might need to make on the stochastic process $Y_n$, it is convenient to assume a structural form for $Y_n$.  Let $\mathcal{F}_n = \sigma(\theta_0, Y_j ; j<n)$ be a filtration $\mathcal{F}$.  For our first results we shall assume that there exists functions $g_n$, an $\mathcal{F}$-martingale difference sequence $\delta M_n$ and a stochastic process $\beta_n$ such that $Y_n = g_n(\theta_n) + \delta M_n + \beta_n$.  The reader should think of these terms in the following way.  The term $g_n(\theta_n)$ represents the mean/true value of the process (e.g. the value of the gradient in the steepest descent case), the term $\delta M_n$ represents a noise term and $\beta_n$ represents a bias term in the observation.  The reason why the bias term $\beta_n$ is called out as being different from $g_n(\theta_n)$ is that we shall be assuming that it becomes asymptotically small.

One of the key techniques in proving theorems in stochastic approximation is the ODE method.  The idea is that one can view the process $\theta_n$ as a discretization of an ordinary differential equation that is described by the conditional means of $Y_n$.  

In many of the proofs we will considering the continuous time limits of discrete time processes.  To do this we be making interpolations of the discrete time processes and want to have recourse to compactness results that will give conditions under which limits exist.  A natural tool for this would be to use the Arzela-Ascoli Theorem for continuous functions or the Skorohod topology versions of that for cadlag functions.  As it turns out neither of these is the exact fit for what we do since we'll be considering cadlag processes that converge to continuous functions and we want uniform convergence on compact sets.  So what we want is a slight extension of Arzela-Ascoli Theorem.

The following is a version of the Arzela Ascoli Theorem \ref{ExtendedArzelaAscoliTheorem} that gives a
sufficent criteria for a sequence of possibly discontinuous functions
to have a continuous limit.  The referenced version of the Arzela Ascoli Theorem
doesn't apply in a non-trivial way since the criterion for
equicontinuity $\lim_{\delta \to 0} \sup_{f \in A} m(T, f, \delta) = 0$ implies that every $f$ is continuous.  However, note that if $f_n$ is a sequence of
functions in $C([0,\infty); \reals^d)$ then $\lbrace f_n \rbrace$ is equicontinuous if
and only if $\lim_{\delta \to 0} \limsup_{n \to \infty} m(T, f_n,
\delta) = 0$.  This criterion does not imply that each $f_n$ is
continuous and turns out to be a useful extension of equicontinuity
for sequences of non-continuous functions.
\begin{thm}[Extended Arzela-Ascoli
  Theorem]\label{ExtendedArzelaAscoliTheorem}Let $f_n : [0,\infty) \to
 \reals^d$ be measurable functions such that  
\begin{itemize}
\item[(i)]$\sup_{n} \abs{f_n(0)} < \infty$
\item[(ii)] $\lim_{\delta \to 0} \limsup_{n \to \infty} m(T, f_n,
  \delta) = 0$ for all $T > 0$.
\end{itemize}
then there exists $f \in C([0,\infty), \reals^d)$ such that $f_n$
converges to $f$ uniformly on compact sets.
\end{thm}
\begin{proof}
First note that if (i) and (ii) hold for the sequence $f_n$ then the conditions also hold for any subsequence of $f_n$.  
Suppose that $f_n$ satisfy (i) and (ii) and let $T > 0$.  By (ii) there exists a $\delta > 0$ such that 
\begin{align*}
\limsup_{n \to \infty} m(T, f_n,  \delta)  < 1
\end{align*}
hence there exists an $N \in \naturals$ such that $m(T, f_n,  \delta)  < 1$ for all $n \geq N$.  Now pick $m \in \naturals$ such 
that $m \delta < T \leq (m+1)\delta$ and just as in Theorem \ref{ArzelaAscoliTheorem}  by considering  the grid $0, \delta, 2\delta, \dotsc,
m\delta, T$ we can write the telescoping sum
\begin{align*}
f_n(T) - f_n(0) = f_n(T) - f_n(m\delta) + \sum_{k=1}^m f_n(k \delta) - f_n((k-1)\delta)
\end{align*}
and use the triangle inequality to conclude that $\abs{f_n(T)}
\leq \abs{f_n(0)} + m+1$ for every $n \geq N$.  Coupled with (i) this shows that
$\sup_{n \geq N} \abs{f_n(T)} < \infty$.  By local compactness of $\reals$ we see that  $f_n(T)$ converges 
along a subsequence of $\lbrace n, n+1, \dotsc \rbrace$.  Using the observation that $f_n$ along this subsequence still satisfies
(i) and (ii) we see that we can enumerate $T \in \rationals_+$ and use induction and a diagonal subsequence argument to get a 
single subsequence of $f_n$ that converges for all $T \in \rationals_+$.  Define $f(T)$ for $T \in \rationals_+$ as the limit of this subsequence of $f_n$.  

The proof that $f \in C([0,\infty); \reals^d)$ and that $f_n$ converges to $f$ uniformly on compact sets is almost exactly the same as the proof in Theorem
\ref{ArzelaAscoliTheorem}.  The only difference is that the condition (ii) applied the sequence $f_n$ only constrains terms $\abs{f_n(s) - f_n(t)}$ for $n$ sufficiently large.  Examining the proof of Theorem \ref{ArzelaAscoliTheorem} one will see that this is all that is required.
\end{proof}

We also need a partial converse, namely that a convergence sequence of functions is equicontinuous in the extended sense.
\begin{prop}\label{PartialConverseExtendedArzelaAscoli}Let $f_n : \reals \to \reals^d$ be a sequence of measurable functions that converges to a continuous function with convergence uniform on compact sets, then $f_n$ is equicontinuous in the extended sense.
\end{prop}
\begin{proof}
Let $f$ be the limit of $f_n$.  Pick an $N$ such that $\norm{f_n(0) - f(0)} < 1$ for all $n \geq N$ then it follows that $\norm{f_n(0)} \leq \norm{f_0(0)} \vee \dotsb \vee \norm{f_{N-1}(0)} \vee \norm{f_N(0)} + 1$ for all $n \in \naturals$.  Now let $T > 0$ and $\epsilon > 0$ be given and use the fact that $f$ is uniformly continuous on $[-T,T]$ to pick 
a $\delta > 0$ such that $m(T, f, \delta) < \epsilon/3$.  Now by uniform convergence of $f_n$ to $f$ on $[-T,T]$ we pick $N > 0$ such that $\sup_{-T \leq t \leq T} \norm{f_n(t) - f(t)} < \epsilon/3$ for $n \geq N$.  Thus for $n \geq N$,
\begin{align*}
m(T,f_n,\delta) &= \sup_{\substack{
\abs{s -t} < \delta \\
-T \leq s,t \leq T}} \norm{f_n(s) - f_n(t)} \\
&\leq \sup_{\substack{
\abs{s -t} < \delta \\
-T \leq s,t \leq T}} \norm{f_n(s) - f(s)} + \norm{f(s) - f(t)} + \norm{f_n(t) - f(t)} < \epsilon
\end{align*}
and it follows that $\limsup_{n \to \infty} m(T, f_n, \delta) < \epsilon$ and thus $\lim_{\delta \to 0} \limsup_{n \to \infty} m(T, f_n, \delta) = 0$.
\end{proof}

We shall now assume that we are in the situation of having a constraint set $F$ defined by continuously differentiable function $c_i(x)$ which satisfy the LICQ.  

TODO:  Use the KKT  conditions applied to $\min_{x \in F} \norm{x - (\theta_n + \epsilon_n Y_n)}^2$ to show that $Z_n$ is in the normal cone 


\begin{thm}Suppose we are given a process $Y_n$, a constraint set $F$, a random variable $\theta_0$ and a deterministic sequence $\epsilon_n$.  Define the process 
\begin{align*}
\theta_{n+1} &= \Pi_F \left[\theta_{n} + \epsilon_n Y_n \right]
\end{align*}
and suppose that there are measurable functions $g_n(\theta)$ such that if we write $\cexpectationlong{\theta_0, Y_i; 0 \leq i \leq n-1}{Y_n} = g_n(\theta_n) + \beta_n$ such that
\begin{itemize}
\item[(i)] $\sup_n \expectation{Y_n^2} < \infty$
\item[(ii)] $\epsilon_n$ for $n \in \integers$ is a sequence with $\epsilon_n = 0$ for $n < 0$, $\epsilon_n \geq 0$ for $n \geq 0$, $\lim_{n \to \infty} \epsilon_n = 0$,  $\sum_{n=0}^\infty \epsilon_n = \infty$ and $\sum_{n=0}^\infty \epsilon^2_n < \infty$.  
\item[(iii)] Suppose the $g_n(\theta)$ are uniformly continuous in $n$ and there is a continuous function $\overline{g}(\theta)$ such that for each $\theta \in F$ we have
\begin{align*}
\lim_{n \to \infty} \abs{\sum_{i=n}^{m(t_n+t)} \epsilon_i \lbrace g_i(\theta) - \overline{g}(\theta) \rbrace }&= 0
\end{align*}
\item[(iv)] $\beta_n \toas 0$
\end{itemize}
Then there is a set $A$ of probability zero such that for $\omega \notin A$ the set of functions $\lbrace \theta^n(\omega, \cdot), Z^n(\omega, \cdot); n < \infty \rbrace$ is equicontinuous.  If $(\theta(\omega, \cdot), Z(\omega, \cdot))$ is the limit of some convergent subsequence then the pair satisfies the projected ODE 
\begin{align*}
\dot{\theta} &= \overline{g}(\theta) + z \text{, $z \in \mathcal{N}(\theta)$}
\end{align*}
and $\theta_n(\omega)$ converges to a limit set of the projected ODE in $F$.  
\end{thm}
\begin{proof}
Let $\mathcal{F}_n = \sigma(\theta_0, Y_i; 0 \leq i \leq n)$ be the filtration defined by $Y_n$ and the initial condition $\theta_0$.
We write
\begin{align*}
\theta_{n+1} &= \Pi_F \left [ \theta_n + \epsilon_n Y_n \right ] \\
&=\theta_n + \epsilon_n Y_n + \epsilon_n Z_n \\
&=\theta_n + \epsilon_n \cexpectationlong{\mathcal{F}_{n-1}}{Y_n} + \epsilon_n \left( Y_n -  \cexpectationlong{\mathcal{F}_{n-1}}{Y_n} \right) + \epsilon_n Z_n \\
&=\theta_n + \epsilon_n g_n(\theta_n) + \epsilon_n \beta_n + \epsilon_n \delta M_n + \epsilon_n Z_n \\
\end{align*}
where we have defined $\delta M_n = Y_n -  \cexpectationlong{\mathcal{F}_{n-1}}{Y_n}$.  Note that $\epsilon_n \delta M_n$ is an
$\mathcal{F}$-martingale difference sequence and therefore by Proposition \ref{MartingaleDifferenceSequence} the process $M_n = \sum_{j=0}^n \epsilon_j \delta M_j$ is an 
$\mathcal{F}$-martingale.  Furthermore by Jensen's Inequality for conditional expectations (Theorem \ref{JensenConditionalExpectation}) and the fact that $Y_n$ is $L^2$-bounded
we also know that $\delta M_n$ is an $L^2$ martingale difference sequence hence by Proposition \ref{SquareIntegrableMartingaleDifferenceWhiteNoise} we know that $\expectation{\delta M_n  \delta M_m} = 0$.  For every fixed $m \in \integers_+$ we know that the process $(M_{n+m} - M_m)^2$ is a submartingale with respect to the shifted
filtration $\tilde{\mathcal{F}}_n = \mathcal{F}_{n+m}$ and if we apply Doob's Maximal Inequality (Lemma \ref{DoobMaximalInequalityDiscrete}) we get for every $\lambda > 0$ and $m < n$,
\begin{align*}
\probability{\sup_{m \leq j \leq n} \abs{M_j - M_m} \geq \lambda} &= \probability{\sup_{m \leq j \leq n} (M_j - M_m)^2 \geq \lambda^2} \\
&\leq \lambda^{-2} \expectation{(M_n - M_m)^2} \\
&=\lambda^{-2}\sum_{i=m+1}^n \sum_{j=m+1}^n  \epsilon_i \epsilon_j \expectation{\delta M_i \delta M_j} \\
&=\lambda^{-2}\sum_{j=m+1}^n \epsilon_j^2  \expectation{\delta M_j^2} \\
&\leq 2 \lambda^{-2} \sup_{n} \expectation{Y_n^2} \sum_{j=m+1}^\infty \epsilon_j^2  \\
\end{align*}
By continuity of measure, we can let $n \to \infty$ and then $m \to \infty$ and use the hypothesis that $\sum_{n=0}^\infty \epsilon^2_n < \infty$ to conclude that for every $\lambda >0$ we have 
\begin{align}\label{SASimpleMartingaleNoiseConvergence}
\lim_{m \to \infty} \probability{\sup_{m \leq j} \abs{M_j - M_m} \geq \lambda} &= 0
\end{align}
TODO: Could we have just appealed to an off the shelf Martingale Convergence theorem here; not sure this is a interesting kind of convergence because are looking at a subsequence that starts at a point that goes to infinity????  We have just proven that $\sup_{m \leq j} \abs{M_j - M_m} \toprob 0$.

Now we move to the interpolated process.  Recall that we define $t_0 = 0$ and $t_n = \sum_{i=0}^{n-1} \epsilon_i$ for $n \in \naturals$.  We define $m(t) = n$ for $t_n \leq t < t_{n-1}$.  Using $m(t)$ we define the interpolated processes for $t \geq 0$,
\begin{align*}
M^n(t) &= \sum_{i=n}^{m(t_n + t)-1} \epsilon_i \delta M_i &  
B^n(t) &=\sum_{i=n}^{m(t_n + t)-1} \epsilon_i \beta_i &
Z^n(t) &= \sum_{i=n}^{m(t_n + t)-1} \epsilon_i Z_i
\end{align*}
and for $t < 0$,
\begin{align*}
M^n(t) &= - \sum_{i=m(t_n + t)}^{n-1} \epsilon_i \delta M_i &  
B^n(t) &= - \sum_{i=m(t_n + t)}^{n-1} \epsilon_i \beta_i &  
Z^n(t) &= - \sum_{i=m(t_n + t)}^{n-1} \epsilon_i Z_i &  
\end{align*}
and note that $M^n(t) = M^0(t_n+t) - M^0(t_n)$ and similarly with $B^n$ and $Z^n$.  Moreover
$M^0(t) = M_{m(t) -1}$.
Furthermore we define
\begin{align*}
\overline{G}^n(t) &= \sum_{i=n}^{m(t_n + t)-1} \epsilon_i \overline{g}(\theta_n) &  
\tilde{G}^n(t) &=\sum_{i=n}^{m(t_n + t)-1} \epsilon_i \left( g_n(\theta_n) - \overline{g}(\theta) \right )&
\end{align*}
so that we have 
\begin{align*}
\theta^n(t) &= \theta_n + \overline{G}^n(t)  + \tilde{G}^n(t) + M^n(t) + Z^n(t)  + B^n(t) 
\end{align*}

\begin{clm}Almost surely for all $T > 0$, $\lim_{n \to \infty} \sup_{-T \leq t \leq T} M^n(t) = 0$.
\end{clm}

Let $T > 0$ be given.  By the definition of $M^n$ and the triangle inequality we get for every $n \in \naturals$ and $m < m(t_n -T)$ we have
\begin{align*}
\sup_{-T \leq t \leq T} \abs{M^n (t)}  
&=\sup_{-T \leq t \leq T} \abs{M^0 (t_n + t) - M^0(t_n)}  \\
&=2 \sup_{-T \leq t \leq T} \abs{M^0 (t_n + t) - M^0(t_n -T )} \\  
&\leq 2 \sup_{m(t_n-T) - 1 \leq j}  \abs{M_j - M_{m(t_n -T )-1}}\\
&\leq 4 \sup_{m \leq j}  \abs{M_j - M_{m}}\\
\end{align*}
If $\lim_{n \to \infty} \sup_{-T \leq t \leq T} M^n (t) != 0$ then there is a $\lambda>0$ and a subsequence $n_j$such that $\sup_{-T \leq t \leq T} \abs{M^{n_j} (t)} \geq \lambda$ for all $j \in \naturals$.  Since we know that $\lim_{j \to \infty} m(t_{n_j} -T) = \infty$, we know $\sup_{m \leq j}  \abs{M_j - M_{m}} \geq \lambda/4$ for all $m$ and therefore the claim follows from Equation \eqref{SASimpleMartingaleNoiseConvergence}.  


\begin{clm}Almost surely for all $T > 0$, $\lim_{n \to \infty} \sup_{-T \leq t \leq T} B^n(t) = 0$.
\end{clm}

We actually want this under a couple of different hypotheses: $\beta_n \toas 0$ and $\sum_{n=0}^\infty \epsilon_n \abs{\beta_n} < \infty$ a.s.  In the latter case 
\begin{align*}
\sup_{-T \leq t \leq T} \abs{B^n(t)} &\leq \sum_{i=m(t_n -T)}^{m(t_n+T) -1} \epsilon_i \abs{\beta_i} \leq \sum_{i=m(t_n -T)}^{\infty} \epsilon_i \abs{\beta_i} 
\end{align*}
so the result follows from the fact that $\lim_{n \to \infty} m(t_n -T) = \infty$.  TODO: What about the former case?

\begin{clm}$\theta_{n+1} - \theta_n \toas 0$.
\end{clm}
Using the Markov Inequality, $\sup_n \expectation{Y_n^2} < \infty$ and $\sum_{n=0}^\infty \epsilon_n^2 <\infty$ we get for any $\lambda > 0$,
\begin{align*}
\sum_{n=0}^\infty \probability{\epsilon_n \abs{Y_n} \geq \lambda} &\leq 
\sum_{n=0}^\infty \frac{\epsilon_n^2 \expectation{Y_n^2}}{\lambda^2} \\
&\leq \frac{\sup_n \expectation{Y_n^2}}{\lambda^2} \sum_{n=0}^\infty \epsilon_n^2  < \infty
\end{align*}
and therefore the Borel Cantelli Theorem \ref{BorelCantelli} implies that $\probability{\epsilon_n \abs{Y_n} \geq \lambda \text{ i.o.}} = 0$ and therefore by Lemma \ref{ConvergenceAlmostSureByInfinitelyOften} we conclude that $\epsilon_n \abs{Y_n} \toas 0$.  Thus the definition of $\Pi_F$ and the fact that $\theta_n \in F$, we see that (we are using the argument that $\Pi_F(\theta_n + \epsilon_n Y_n) = \argmin_{x \in F} \abs{\theta_n + \epsilon_n Y_n - x}$ hence since $\theta_n \in F$,
\begin{align*}
\abs{\theta_n + \epsilon_n Y_n - \Pi_F(\theta_n + \epsilon_n Y_n)} &\leq \abs{\theta_n + \epsilon_n Y_n - \theta_n} = \epsilon_n \abs{Y_n}
\end{align*}
is this always true or does it require some assumption like prox-regularity????)
\begin{align*}
\lim_{n \to \infty} \abs{\theta_{n+1} - \theta_n } &= \lim_{n \to \infty} \abs{ \Pi_F \left[ \theta_n + \epsilon_n Y_n \right] - \theta_n } \\
&\leq  \lim_{n \to \infty} \lbrace \abs{ \Pi_F \left[ \theta_n + \epsilon_n Y_n \right] - \theta_n - \epsilon_n Y_n} + \epsilon_n \abs{Y_n} \rbrace \\
&\leq 2 \lim_{n \to \infty} \epsilon_n \abs{ Y_n}  = 0
\end{align*}
almost surely.


By prior claims and Proposition \ref{PartialConverseExtendedArzelaAscoli} we know that almost surely, $M^n(t)$ and $B^n(t)$ are each equicontinuous in the extended sense. 

\begin{clm}$Z^n$ is almost surely equicontinuous in the extended sense.
\end{clm}
Here is the hyperrectangle case.  We work pathwise so lets assume that $\omega \in \Omega$ is fixed.  

\begin{clm}If $Z^n(\omega)$ is not equicontinuous in the extended sense then there is an $\epsilon > 0$, a sequence $n_k \in \naturals$ with $\lim_{k \to \infty} n_k = \infty$ and a sequence $\delta_k > 0$ with $\lim_{k \to \infty} \delta_k = 0$ such that $\abs{Z^{n_k}(\omega, \delta_k)} \geq \epsilon$.
\end{clm}
 If $Z^n$ is not equicontinuous in the extended sense then there is a $T > 0$ such that $\lim_{\delta \to 0} \limsup_{n \to \infty} m(T, Z^n, \delta) > 0$ i.e. an $\epsilon > 0$, a sequence $\delta_k$ with $\delta_k \to 0$, a sequence $n_k$ with $n_k \to \infty$ and $s_k, u_k$ with $-T \leq s_k < u_k \leq T$ and $u_k - s_k < \delta_k$ such that $\abs{Z^{n_k}(u_k) - Z^{n_k}(s_k)} \geq \epsilon$.  Recalling $Z^n(t) = Z^0(t_n+t) - Z^0(t_n)$  we see that $\abs{Z^{n_k}(u_k) - Z^{n_k}(s_k)} \geq \epsilon$ is equivalent to $\abs{Z^{0}(t_{n_k} + u_k) - Z^0(t_{n_k} + s_k)} \geq \epsilon$ and therefore if we define $m_k$ such that .  By redefining the sequence $n_k$ to be $\tilde{n}_k = m(t_{n_k} + s_k)$, $\tilde{s}_k = s_k  + t_{n_k} - t_{\tilde{n}_k}$  and $\tilde{u}_k = u_k + t_{n_k} - t_{\tilde{n}_k}$ we get $\abs{Z^{\tilde{n}_k}(\tilde{u}_k) - Z^{\tilde{n}_k}(\tilde{s}_k)} \geq \epsilon$ where by definition $t_{\tilde{n}_k} \leq s_k < t_{\tilde{n}_k + 1} = t_{\tilde{n}_k } + \epsilon_{\tilde{n}_k+1}$ and thus $0 \leq \tilde{s}_k < \epsilon_{\tilde{n}_k+1}$   but still 
\begin{align*}
\lim_{k \to \infty} \tilde{n}_k &= \lim_{k \to \infty} m(t_{\tilde{n}_k}) \geq \lim_{k \to \infty} m(t_{n_k} - T) = \infty
\end{align*} 
from which it also follows that $\lim_{k \to \infty} \tilde{s}_k = 0$ and therefore $\lim_{k \to \infty} \tilde{u}_k \leq  \lim_{k \to \infty} \tilde{s}_k + \delta_k = 0$.  Since $0 \leq \tilde{s}_k < \epsilon_{\tilde{n}_k+1}$ we also have $Z^{\tilde{n}_k}(\tilde{s}_k) = Z^{\tilde{n}_k}(0) = 0$ so the sequences $\tilde{n}_k$ and $\tilde{u}_k$ satisfy the claim.

TODO: Note that $\delta_k$ doesn't necessarily go to 0 faster than $\epsilon_{n_k} + \epsilon_{n_k + 1}$ so $Z^{n_k}(\delta_k)$ may be a sum of multiple jumps $\epsilon_{n_k} Z_{n_k} + \dotsb + \epsilon_{n_k + m_k} Z_{n_k + m_k}$.  This makes the geometry a tad confusing for me.  Can we reduce to a case in which $0 \leq \delta_k + \epsilon_{n_k} < \epsilon_{n_k+1}$?


TODO: I still don't see the geometry here.  Relevant facts: 
\begin{itemize}
\item $\theta_{n+1} \in \interior{F}$ implies $Z_n = 0$ (this follows from the next item and the fact that $N_F(\theta_{n+1}) = \lbrace 0 \rbrace$ if $\theta_{n+1} \in \interior{F}$
\item $Z_n \in N_F(\theta_{n+1})$
\item $\epsilon_n Z_n \toas 0$
\item $M^n(t) \toas 0$ uniformly on compacts
\item $B^n(t) \toas 0$ uniformly on compacts
\end{itemize}
In prose, a asymptotic jump in $Z^{n_k}$ cannot be into the interior because that would imply that $Z_{n_k} = 0$.  The claim is that in the limit, the jump therefore must be from the boundary of $F$ (I don't see this yet but I suspect it follows from $\theta_{n+1} - \theta_n \toas 0$) to another point on the boundary of $F$ (I get this follows from $Z_n \in N_F(\theta_{n+1})$) and that this contradicts the fact that $Z_n \in N_F(\theta_{n+1})$ (I don't see this yet).  The basic intuition is this: 
\begin{itemize}
\item asymptotically since $\epsilon_n Z_n \to 0$ a.s. we know that any jump $Z^n(0)$ in the limit must be from the boundary of $F$
\item jumps in $Z_n$ are always to the boundary and not to the interior thus in the limit this is true and therefore the asymptotic jump is from a boundary point to another boundary point
\item since $Z_n \in N_F(\theta_{n+1})$ and the normal cone points \emph{into} $F$ it is impossible for the jump to be between boundary points
\end{itemize}
So the relevant geometry here just precludes some kind of limit of $Z_n$ pointing out of the normal cone (this seems like it will be true with regularity assumptions for then we know that any limit of proximal normals is a limiting normal but with appropriate regularity assumptions we know that proximal normals are the same thing as limiting normals).

Here is how the argument unfolds in Kushner-Clark
\begin{itemize}
\item First show that $\epsilon_n \norm{\delta M_n} \toas 0$ (Kushner-Clark show this from the asymptotic rate of change condition the Kushner-Clark criterion discussed below)
\item Pick a deterministic sequence $c_n \to 0$ and almost surely $\epsilon_n \norm{\delta M_n}\leq c_n/2$ for all but finitely many $n \in \naturals$ (follows from previous point); consider separately the cases of $\epsilon_n \norm{\delta M_n} \leq c_n/2$ for all $n \in \naturals$.
\item Handle the case in which a non-zero (but finite) number of $\epsilon_n \norm{\delta M_n} > c_n$
\item In the case that $\epsilon_n \norm{Z_n} \leq c_n/2$ for all $n \in \naturals$ observe there exists $K$ such that
\begin{itemize}
\item $Z_n \in N_F(\theta_{n+1})$
\item $\epsilon_n \norm{Z_n} \leq \epsilon_n(K + c_n)$ (do they really mean $\epsilon_n \norm{Z_n} \leq K(\epsilon_n + c_n)$ as below?)
\item $Z_n=0$ if $d(\partial F, \theta_n) > K(\epsilon_n + c_n)$.
\end{itemize}
\end{itemize}
I believe that the second two points above both follow from noting that that since $\theta_n \in F$, (and Kushner-Clark also assume that $g_n$ are uniformly bounded on $F$)
\begin{align*}
\epsilon_n \norm{Z_n} &= \norm{\Pi_F(\theta_n + \epsilon_n Y_n) - \theta_n - \epsilon_n Y_n} \\
&\leq \norm{\theta_n - \theta_n - \epsilon_n Y_n} \\
&= \norm{\epsilon_n g_n(\theta_n) + \epsilon_n \beta_n + \epsilon_n \delta M_n } \\
&\leq \epsilon_n \sup_{x \in F} \abs{g(x)} + \epsilon_n \sup_n\norm{\beta_n} + \epsilon_n c_n/2 \\
&\leq ((\sup_{x \in F} \abs{g(x)} + \sup_n\norm{\beta_n}) \maxop c_n/2) \epsilon_n
\end{align*}
and and if $Z_n \neq 0$ then it follows that $\theta_n + \epsilon_n Y_n \in F^c$ and  therefore as above
\begin{align*}
d(\partial F, \theta_n) &=d(\partial F^c, \theta_n) = d(\overline{F^c}, \theta_n) \leq \norm{\theta_n + \epsilon_n Y_n -\theta_n} \\
&\leq  ((\sup_{x \in F} \abs{g(x)} + \sup_n\norm{\beta_n}) \maxop c_n/2) \epsilon_n
\end{align*}
Note that Kushner and Clark assume that $F$ is the closure of its interior thus $\partial F = \partial F^c$; this follows since $F$ closed implies $F^c$ is open
hence $\partial F^c = \overline{F^c} \setminus F^c \subset F$ and if $x \in \partial F^c$ then for every open set $x \in U$ we must have $U \cap \int(F) \neq \emptyset$ for otherwise 
$x \notin \overline{\int(F)} = F$ which is a contradiction.  TODO: Under what circumstances is it true that $d(F,x) = d(\partial F, x)$ for $x \notin F$?  It is true in $\reals^d$.

Based on these properties Kushner and Clark claim that $Z^0(t)$ is uniformly continuous on $[0,\infty)$.  They say that if not then by the uniform continuity of $\theta^0(t) - Z^0(t)$ we can 
find an $\epsilon > 0$ and sequences $s_k \to \infty$, $\delta_k \to 0$ with $\delta_k > 0$ such that 
\begin{itemize}
\item[a)] $\lim_{k \to \infty} d(X^0(s_k), \partial F) = 0$
\item[b)] $\abs{X^0(s_k+\delta_k) - X^0(s_k)} \geq \epsilon$ for all $k \in \naturals$
\item[c)] $d(X^0(s_k+\delta_k), \partial F) \geq \epsilon/2$ for all $k \in \naturals$
\end{itemize}
Then they claim that this is a contradiction.  Here is where I stand on this ...

TODO: Finish
\end{proof}

\section{Dynamical Systems Approach}

This section follows Benaim's notes (which in turn summarize a bunch of the Benaim and Hirsch work).

\begin{defn}Let $(S,r)$ be a metric space a continuous map $\Phi : [0,\infty) \times S \to S$. We write $\Phi_t(x) = \Phi(t,x)$ for $0 \leq t < \infty$ and $\Phi^x(t) = \Phi(t,x)$ for $x \in S$.  $\Phi$ is said to be a \emph{semiflow} if 
\begin{itemize}
\item[(i)] $\Phi_0 = \IdentityMatrix$
\item[(ii)] $\Phi_s \circ \Phi_t = \Phi_{t+s}$ for all $0 \leq t,s < \infty$.
\end{itemize}
A continuous map $\Phi : (-\infty, \infty) \times S \to S$ with properties (i) and (ii) is called a \emph{flow}.  Given $x \in S$ we often refer to $\Phi^x$ as an \emph{orbit}.
\end{defn}

\begin{defn}Let $(S,r)$ be a metric space and let $\Phi$ be a semiflow on $S$, then $X \in C([0,\infty) ; S)$ is said to be an \emph{asymptotic pseudotrajectory} of $\Phi$ if for all $T > 0$ we have 
\begin{align*}
\lim_{t \to \infty} \sup_{0 \leq h \leq T} r(X(t+h), \Phi_h(X(t))) = 0
\end{align*}
If the image $X([0,\infty))$ has compact closure we often say that $X$ is \emph{precompact}.
\end{defn}

Our first goal is formulate some conditions that are equivalent to an $X$ being an asymptotic pseudotrajectory.  The idea is to investigate the orbits of $X$ under a canonical flow on the space $C((-\infty, \infty); S)$.  The first step is to define the \emph{translation flow} on $C((-\infty, \infty); S)$.  Recall that $C((-\infty, \infty); S)$ is a metric space with metric
\begin{align*}
d(f,g) &= \sum_{n=1}^\infty 2^{-n} (\sup_{-n \leq t \leq n} r(f(t), g(t)) \wedge 1)
\end{align*}

\begin{prop}Let $(S,r)$ be a metric space define $\Theta : (-\infty, \infty) \times C((-\infty, \infty); S) \to C (-\infty, \infty); S)$ by $\Theta(t,f)(s) =f(t+s)$, then $\Theta$ is a flow.
\end{prop}
\begin{proof}
The only non-trivial part is the proof that $\Theta$ is continuous.  Suppose that $(t, f), (t_n,f_n) \in  (-\infty, \infty) \times C((-\infty, \infty); S)$ and $\lim_{n \to \infty} (t_n, f_n) = (t,f)$.  Let $\epsilon > 0$ and $T > 0$ be given.  $f$ is uniformly continuous on $[-T-\abs{t}-1,T+\abs{t}+1]$ so we may pick a $\delta > 0$ such that 
\begin{align*}
\sup_{\substack{\abs{u}, \abs{v} \leq T + \abs{t}+1 \\ \abs{u-v} < \delta}} r(f(u), f(v)) < \epsilon/2
\end{align*}
Since $\lim_{n \to \infty} t_n = t$ and $\lim_{n \to \infty} f_n = f$ we may pick $N > 0$ such that $\abs{t_n - t} < \delta \wedge 1$ and $\sup_{\abs{s} \leq T + \abs{t} + 1} r(f_n(s), f(s)) < \epsilon/2$ for all $n \geq N$
Therefore for all $n \geq N$
\begin{align*}
\sup_{-T \leq s \leq T} r(\Theta(t_n, f_n)(s), \Theta(t,f)(s)) &= \sup_{-T \leq s \leq T} r(f_n(t_n +s), f(t+s)) \\
&\leq \sup_{-T \leq s \leq T} r(f_n(t_n +s), f(t_n+s)) + \sup_{-T \leq s \leq T} r(f(t_n +s), f(t+s)) \\
&\leq \sup_{\abs{s} \leq T+\abs{t}+1} r(f_n(s), f(s)) + \sup_{\abs{s} \leq T+\abs{t}+1} r(f(s), f(s)) < \epsilon \\
\end{align*}
\end{proof}

TODO: Is there a translation flow on $D((-\infty, \infty); S)$ in the uniform topology?  in the Skorohod topology?

If we are given a semiflow $\Phi : [0,\infty) \times S \to S$ it is often convenient to consider it as a flow $\Phi : (-\infty,\infty) \times S \to S$ by defining $\Phi(-t,x) = \Phi(0,x) = x$.  TODO: Exercise to show this is a flow (specifically continuity)

In order to compare the flow $\Phi$ with the flow $\Theta$ we use the following
\begin{prop}Let $(S,r)$ be a metric space and let $\Phi$ be a semiflow or flow on $S$.  If we define $H(x) = \Phi^x \in C((-\infty, \infty); S)$ and $S_\Phi = \range{H}$ then $H$ is a homeomorphism of $S$ and $S_\Phi$ and moreover
\begin{align*}
\Theta_t (H(x)) &= H(\Phi_t(x))
\end{align*}
where we assume that $t \geq 0$ if $\Phi$ is a semiflow and $-\infty < t < \infty$ is $\Phi$ is a flow.  Therefore $S_\Phi$ is a closed subset of $C((-\infty, \infty); S)$ and is invariant under $\Theta$.  The map $\hat{\Phi} : C((-\infty, \infty);S) \to S_\Phi$ defined by $\hat{\Phi}(X) = H(X(0)) = \Phi^{X(0)}$ is continuous retraction.
\end{prop}
\begin{proof}
TODO
\end{proof}

We can now reformulate the definition of asymptotic pseudotrajectory in terms of behavior under the translation flow.
\begin{lem}Let $(S,r)$ be a metric space, $X \in C([0,\infty); S)$ and $\Phi$ a semiflow on $S$ then $X$ is an asymptotic pseudotrajectory of $\Phi$ if and only if
\begin{align*}
\lim_{t \to \infty} d(\Theta_t(X), \hat{\Phi}(\Theta_t(X))) &= 0
\end{align*}
\end{lem}
\begin{proof}
By Lemma \ref{UniformConvergenceOnCompacts} we know that $\lim_{t \to \infty} d(\Theta_t(X), \hat{\Phi}(\Theta_t(X))) = 0$ if and only if $\lim_{t \to \infty} \sup_{0 \leq s \leq T} r(\Theta_t(X)(s), \hat{\Phi}(\Theta_t(X))(s)) $ for every $T>0$.  Substituting definitions of $\Theta$ and $\hat{\Phi}$ we see that this is equivalent to
$\lim_{t \to \infty} \sup_{0 \leq s \leq T} r(X(t+s), \Phi_s(X_t)) $ for every $T>0$.  
\end{proof}

\begin{thm}\label{AsymptoticPseudotrajectoryTranslationLimitPointCharacterization}Let $(S,r)$ be a metric space, $X \in C((-\infty, \infty); S)$ such that $\range{X}$ has compact closure in $S$ and $\Phi : [0,\infty) \times S \to S$ be a semiflow.  The $X$ is an asymptotic pseudotrajectory of $\Phi$ is and only if $X$ is uniformly continuous and every limit point of $\Theta_t(X)$ is in $S_\Phi$.  In either case $\lbrace \Theta_t(X) \rbrace$ is relatively compact in $C((-\infty, \infty); S)$.  
\end{thm}
\begin{proof}
To see (i) implies (ii) first assume that $X$ is an asymptotic pseudotrajectory; for every $T>0$ we have $\lim_{t \to \infty} \sup_{0 \leq s \leq T} r(X_{t+s}, \Phi_s(X_t)) =0$.  Let $K$ be the closure of $\range{X}$ so that $K$ is compact in $S$.  
\begin{clm}Let $\epsilon > 0$ be given and then there exists $\delta>0$ such that $r(\Phi_t(y), y)<\epsilon$ for all $\abs{t} < \delta$
\end{clm}
Let $x \in S$ be arbitrary.  Consider 
\begin{align*}
\Phi^{-1}( B(x, \epsilon/2)) &= \lbrace (t,y) \mid r(\Phi_t(y), x) < \epsilon/2 \rbrace
\end{align*}
Since $\Phi_0$ is identity, we know that $(0,x) \in \Phi^{-1}( B(x, \epsilon/2)) $ and therefore we can find a $0 < \delta_x < \epsilon/2$ such that $B((0,x), \delta_x) \subset \Phi^{-1}( B(x, \epsilon/2))$.  Moreover we assume are using the metric $\abs{\cdot} \vee r(\cdot, \cdot)$ on $[0,\infty) \times S$ so that  $B((0,x), \delta_x) = (-\delta_x, \delta_x) \times B(x,\delta_x)$ From these two facts it follows that for all $y \in B(x, \delta_x)$ then for all $\abs{t} < \delta_x$
\begin{align*}
r(\Phi_t(y), y) &\leq r(\Phi_t(y), x) + r(x,y) < \epsilon
\end{align*}
Now the set of $B(x, \delta_x)$ with $x \in K$ is an open cover of $K$ hence there is a finite subcover $B(x_1, \delta_{x_1}), \dotsc, B(x_n, \delta_{x_n})$.  Let $\delta = \delta_{x_1} \wedge \dotsb \wedge \delta_{x_n}$.  Every $y \in K$ belongs to some $B(x_j, \delta_{x_j})$ and therefore $r(\Phi_t(y), y) < \epsilon$ for $\abs{t} < \delta_{x_j}$ an a fortiori for $\abs{t} < \delta$.

\begin{clm}$X$ is uniformly continuous
\end{clm}
Let $\epsilon>0$ be given and pick $\delta>0$ as in the previous claim.  Because $X$ is an asymptotic pseudotrajectory we may pick a $t_0>0$ such that for all $t \geq t_0$ we have $\sup_{0 \leq h \leq \delta} r(X_{t+h}, \Phi_h(X_t)) < \epsilon$.  Thus
\begin{align*}
\sup_{0 \leq h \leq \delta} r(X_{t+h}, X_t) &\leq \sup_{0 \leq h \leq \delta} r(X_{t+h}, \Phi_h(X_t)) + \sup_{0 \leq h \leq \delta} r(\Phi_h(X_t), X_t) < 2\epsilon
\end{align*}
and uniformly continuity follows. 

Suppose that $Y$ is a limit point of $\Theta_t(X)$ and $Y \notin S_\Phi$.  Since $S_\Phi$ is closed it follows that $d(Y, S_\Phi) > 0$.  Let $n_k$ be a subsequence such that $\lim_{k \to \infty} \Theta_{n_k}(X) = Y$ and observe
\begin{align*}
\lim_{k \to \infty} d(\Theta_{t_k}(X), \hat{\Phi}(\Theta_{t_k}(X))) &\geq \lim_{k \to \infty} \lbrace d(Y, \hat{\Phi}(\Theta_{t_k}(X))) - d(\Theta_{t_k}(X),Y)\rbrace \geq d(Y, S_\Phi) > 0
\end{align*}
which is a contradiction.

Now to see that (ii) implies (i) suppose that $X$ is uniformly continuous and all the limit points of $\Theta_t(X)$ are in $S_\Phi$.  First we show that (ii) implies (iii).
\begin{clm}The family $\lbrace \Theta_t(X) \rbrace$ is relatively compact.
\end{clm}
First we establish equicontinuity of $\lbrace \Theta_t(X) \rbrace$.  Let $T > 0$ and $\epsilon > 0$ be given.  By uniform continuity there exists $\delta > 0$ such that $r(X(t), X(s)) < \epsilon$ for all $\abs{t-s}<\delta$. Therefore
\begin{align*}
\sup_{\substack{-T \leq u<v \leq T \\ v-u<\delta}} r(\Theta_t(X)(u), \Theta_t(X)(v)) &= \sup_{\substack{-T \leq u<v \leq T \\ v-u<\delta}} r(X(t+u), X(t+v)) \leq \sup_{\substack{-\infty <  u<v <\infty\\ v-u<\delta}} r(X(u), X(v)) < \epsilon
\end{align*}
Since $\range{X}$ is relatively compact it follows that $\lbrace \Theta_t(X)(s) \mid t \geq 0 \rbrace$ is relatively compact for every $s$ and therefore we may apply the Arzela-Ascoli Theorem \ref{ArzelaAscoliTheorem} to conclude that $\lbrace \Theta_t(X) \rbrace$ is relatively compact in $C((-\infty,\infty); S)$.  

Now suppose that $\lim_{t \to \infty} d(\Theta_t(X), \hat{\Phi}(\Theta_t(X))) \neq 0$ then there exists an $\epsilon > 0$ and a sequence $t_k$ such that $d(\Theta_{t_k}(X), \hat{\Phi}(\Theta_{t_k}(X))) \geq \epsilon$.  By relative compactness by passing to a subsequence and by using the assumption that every limit point of $\Theta_t(X)$ is in $S_\Phi$  we may assume that there exists $Y \in S_\Phi$ such that $\lim_{k \to \infty} \Theta_{t_k}(X) = Y$.  Since $\hat{\Phi}$ is a continuous retraction onto $S_\Phi$ it follows that 
\begin{align*}
\lim_{k \to \infty} \hat{\Phi}(\Theta_{t_k}(X)) &= \hat{\Phi}( Y) = Y = \lim_{k \to \infty} \Theta_{t_k}(X) 
\end{align*}
which is a contradiction.
\end{proof}

\begin{defn}Suppose we have a continuous function $F : \reals^d \to \reals^d$, a sequence of vectors $x_n \in \reals^d$ for $n \in \integers_+$, $U_n \in \reals^d$ for $n \in \naturals$ and $\gamma_n \in \reals_+$ for $n \in \naturals$ such that $\sum_{n=1}^\infty \gamma_n = \infty$ and $\lim_{n \to \infty} \gamma_n = 0$ satisfying
\begin{align*}
x_{n+1} - x_n &= \gamma_{n+1} ( F(x_n) + U_{n+1})
\end{align*}
we define a sequence 
\begin{align*}
\tau_0 = 0 \text{ and } \tau_n = \sum_{j=1}^n \gamma_j \text{ for $n \geq 1$}
\end{align*}
interpolations of $x_n$ $X, \overline{X} : \reals_+ \to \reals^d$
\begin{align*}
X(\tau_n + s) &= x_n + s\frac{x_{n+1} - x_n}{\tau_{n+1} - \tau_n} = x_n + s \gamma_{n+1}^{-1} (x_{n+1} - x_n) \text{, and } \overline{X}(\tau_n + s) = x_n \text{ for $n \in \naturals$ and $0 \leq s < \gamma_{n+1}$}
\end{align*}
the ``inverse'' of the sequence $\tau_n$, $m : \reals_+ \to \integers_+$
\begin{align*}
m(t) &= \sup \lbrace k \geq 0 \mid t \geq \tau_k \rbrace
\end{align*}
and continuous time interpolations $\overline{U} : \reals_+ \to \reals^d$ and $\overline{\gamma}: \reals_+ \to \reals_+$ defined by
\begin{align*}
\overline{U}(\tau_n + s) = U_{n+1} \text{, and } \overline{\gamma}(\tau_n +s) = \gamma_{n+1}  \text{ for $n \in \naturals$ and $0 \leq s < \gamma_{n+1}$}
\end{align*}
\end{defn} 

We note the following simple facts
\begin{prop}\label{StochasticApproximationInterpolations}For all $t \in \reals_+$, $\overline{X}(t) = X(\tau_{m(t)}) = x_{m(t)}$, $\overline{U}(t) = U_{m(t) + 1}$, $\overline{\gamma}(t) = \gamma_{m(t)+1}$, $\tau_{m(t)} \leq t < \tau_{m(t)+1}$
and 
\begin{align*}
X(t) &= X(0) + \int_0^t (F(\overline{X}(s)) + \overline{U}(s)) \, ds
\end{align*}
\end{prop}
\begin{proof}
By the assumption $\sum_{n=1}^\infty \gamma_n$ we know that $\lim_{n \to \infty} \tau_n = \infty$.  By non-negativity of $\gamma_n$ we know that the sequence $\tau_n$ is non-decreasing.  Therefore by definition we know that $m(t)$ is the unique non-negative integer such that $\tau_{m(t)} \leq t < \tau_{m(t)+1}$.  Note that it may be that for some $n < m(t)$ we also have $\tau_n = \tau_{m(t)}$ but then $\tau_n=\tau_{n+1}$ and it is not the case that $\tau_n \leq t < \tau_{n+1}$.  For such $n$ is also true that $\gamma_{n+1} = 0$ and thus the values at $n$ are not used in any of the interpolations.  In particular writing $t = \tau_{m(t)} +s$ with $0 \leq s < \gamma_{m(t)+1}$ by the definitions of $X$ and $\overline{X}$ we see that
\begin{align*}
\overline{X}(t) &= \overline{X}(\tau_{m(t)} + s) = x_{m(t)} = X(\tau_{m(t)})
\end{align*}
$\overline{U}(t) \overline{U}(\tau_{m(t)} + s) = U_{m(t)+1}$ and $\overline{\gamma}(t) \overline{\gamma}(\tau_{m(t)} + s) = \gamma_{m(t)+1}$.

To see the last equality we express the integral using the fact that the integrand is piecewise constant and that from the discussion above we may write $t = \tau_{m(t)} + s$ with $0 \leq s < \gamma_{m(t)+1}$ and use the recursion defining $x_n$
\begin{align*}
\int_0^t (F(\overline{X}(s)) + \overline{U}(s)) \, ds &= \sum_{n=0}^{m(t)-1} \gamma_{n+1} (F(x_n) + U_{n+1}) + s (F(x_{m(t)}) + U_{m(t)+1}) \\
&=\sum_{n=0}^{m(t)-1} (x_{n+1} - x_n)  + s (F(x_{m(t)}) + U_{m(t)+1}) \\
&= x_{m(t)} - x_0 + s \frac{x_{m(t) + 1} - x_{m(t)}}{\gamma_{m(t)+1}} \\
&= X(t) - X(0)
\end{align*}
\end{proof}

Thinking of the recursion $x_{n+1} = x_n + \gamma_{n+1} (F(x_n) + U_{n+1})$ as a perturbed version of the Euler method for solving an ordinary differential equation we pose the question of how well the interpolations $X(t)$ approximate solutions to the differential equation $\frac{dX}{dt} = F(X(t))$.  In particular we seek asymptotic decay conditions of the sequence of perturbations $\gamma_{n+1} U_{n+1}$ that allow us to prove that $X(t)$ is an asymptotic pseudotrajectory.  The following condition on the noise sequence is due to Kushner and Clark (TODO: Reference).
\begin{defn}We say that $\gamma_{n+1}$ and $U_{n+1}$ satisfy the \emph{Kushner-Clark criterion} if for every $T > 0$ we have
\begin{align*}
\lim_{n \to \infty} \sup \lbrace \norm{\sum_{i=n}^{k-1} \gamma_{i+1} U_{i + 1}} \mid k=n+1, \dotsc, m(\tau_n+T) \rbrace &= 0
\end{align*}
\end{defn}

\begin{prop}\label{KushnerClarkCriterionIntegralForm}Let $\gamma_{n+1}$ and $U_{n+1}$ be given and define $\tau_0=0$, $\tau_n = \sum_{i=1}^n \gamma_i$ for $n \in \naturals$ and
\begin{align*}
\Delta(t, T) = \sup_{0 \leq h \leq T} \norm {\int_t^{t+h} \overline{U}(s) \, ds}
\end{align*} 
then 
\begin{itemize}
\item[(i)] for every $\delta>0$ we have $\Delta(t,T) \leq 2 \Delta(t-\delta, T+\delta)$
\item[(ii)] for every $n \in \integers_+$ we have 
\begin{align*}
\sup \left \lbrace \norm{\sum_{i=n}^{k-1} \gamma_{i+1} U_{i + 1}} \mid k=n+1, \dotsc, m(\tau_n+T) \right \rbrace 
&= \Delta(\tau_n, m(\tau_n+T) - \tau_n) 
\leq \Delta(\tau_n, T)
\end{align*}
\end{itemize}
If $\lim_{n \to \infty} \gamma_n =0$ and $\sum_{n=1}^\infty \gamma_n = \infty$ then the Kushner-Clark condition is equivalent to $\lim_{t \to \infty} \Delta(t,T) = 0$ for all $T > 0$.
\end{prop}
\begin{proof}
To see (i) let $t \geq 0$, $T, \delta>0$ and $0 \leq h \leq T$ be given then
\begin{align*}
\norm{\int_t^{t+h}  \overline{U}(s) \, ds} &=  \norm{\int_{t-\delta}^{t+h}\overline{U}(s) \, ds - \int_{t-\delta}^t \overline{U}(s) \, ds} \\
&\leq \norm{\int_{t-\delta}^{(t-\delta)+h+\delta}\overline{U}(s) \, ds} + \norm{\int_{t-\delta}^t \overline{U}(s) \, ds} \\
&\leq 2 \sup_{0 \leq h \leq T+\delta} \norm{\int_{t-\delta}^{(t-\delta)+h}\overline{U}(s) \, ds} = 2 \Delta(t-\delta, T+\delta)
\end{align*}
To see (ii) we write
\begin{align*}
\sum_{i=n}^{k-1} \gamma_{i+1} U_{i + 1} &= \sum_{i=n}^{k-1} (\tau_{i+1} - \tau_i) \overline{U}(\tau_i) = \int_{\tau_n}^{\tau_k} \overline{U}(s) \, ds
\end{align*}
We note that $\int_{\tau_n}^{t} \overline{U}(s) \, ds$ is linear for $\tau_k \leq t < \tau_{k+1}$ so by the convexity of norms we see that 
\begin{align*}
\sup_{\tau_k \leq t < \tau_{k+1}} \norm{\int_{\tau_n}^{t} \overline{U}(s) \, ds } = \norm{\int_{\tau_n}^{\tau_k} \overline{U}(s) \, ds} \maxop \norm{\int_{\tau_n}^{\tau_{k+1}} \overline{U}(s) \, ds}
\end{align*}
Combining this fact with the fact that $m(\tau_n+T) \leq \tau_n+T$ we get
\begin{align*}
&\sup \left \lbrace \norm{\sum_{i=n}^{k-1} \gamma_{i+1} U_{i + 1}} \mid k=n+1, \dotsc, m(\tau_n+T) \right \rbrace  \\
&= \sup \left \lbrace \norm{\int_{\tau_n}^{\tau_k} \overline{U}(s) \, ds} \mid k=n+1, \dotsc, m(\tau_n+T) \right \rbrace \\
&= \sup \left \lbrace \norm{\int_{\tau_n}^{\tau_k} \overline{U}(s) \, ds} \mid k=n, \dotsc, m(\tau_n+T) \right \rbrace \\
&= \sup \left \lbrace \norm{\int_{\tau_n}^{t} \overline{U}(s) \, ds} \mid \tau_{n} \leq t \leq \tau_{m(\tau_n+T)} \right \rbrace \\
&= \Delta(\tau_n, m(\tau_n+T) - \tau_n) \\
&\leq \Delta(\tau_n, T)
\end{align*}

If $\tau_n \to \infty$ then the condition $\lim_{t \to \infty} \Delta(t,T) = 0$ for all $T > 0$ clearly implies $\lim_{n \to \infty} \Delta(\tau_n,T) = 0$ for all $T > 0$; by (ii) this implies the Kushner-Clark 
criterion.  On the other hand assume the Kushner-Clark criterion, let  $T>0$ and $\epsilon>0$ be given and pick $N_T>0$ such that  $\Delta(\tau_n, m(\tau_n+T+2) - \tau_n) < \epsilon/2$ for all $n \geq N_T$.  Because $\gamma_n \to 0$ we may pick $N_1$ large enough that $\gamma_{n+1} < 1$ for all $n \geq N_1$ from which it follows that for such $n$
\begin{align*}
T + 1 &< \tau_n + T + 2 - \gamma_{m(\tau_n+T+2)+1} - \tau_n \leq \tau_{m(\tau_n + T + 2)+1} - \gamma_{m(\tau_n+T+2)+1} - \tau_n = \tau_{m(\tau_n + T + 2)} - \tau_n
\end{align*}
For $n \geq N_1 \maxop N_T$ we have $\Delta(\tau_n, T+1) \leq \Delta(\tau_n, m(\tau_n+T+2) - \tau_n) < \epsilon/2$.  
Then for all $t \geq N_1 \maxop \tau{N_T}$ there is a unique $n \geq N_1 \maxop N_T$ such that $\tau_n \leq t < \tau_{n+1}$ which implies $t - \tau_{n} < 1$ so
\begin{align*}
\Delta(t,T) &= \Delta(t - \tau_n + \tau_n, T) \leq 2\Delta(\tau_n, T+ (t-\tau_n)) \leq 2 \Delta(\tau_n, T+1) < \epsilon
\end{align*}
which shows $\lim_{t \to \infty} \Delta(t,T) = 0$.
\end{proof}

\begin{prop}\label{KushnerClarkImpliesPseudotrajectory}Let $F : \reals^d \to \reals^d$ be a continuous vector field such that the differential equation $\dot{x} = F(x)$ has unique integral curves.  Assume that $\gamma_{n+1}$ and $U_{n+1}$ satisfy the Kushner-Clark criterion and either
\begin{itemize}
\item[(i)] $\sup_{n} \norm{x_n} < \infty$ 
\item[(ii)] $F$ is Lipschitz and bounded on a (convex?) neighborhod of $\lbrace x_n \rbrace$.
\end{itemize}
then $X$ is an asymptotic pseudotrajectory of the flow induced by $F$.  Furthermore if (ii) is true then
\begin{align*}
\sup_{0 \leq h \leq T} \norm{X(t+h) - \Phi_h(X(t))} &\leq C \left( \Delta(t-1, T+1) + \sup_{t \leq s \leq t+T} \norm{\overline{\gamma}(s)} \right)
\end{align*}
where the constant $C$ only depends on $T$ and $F$.
\end{prop}
\begin{proof}
\begin{clm}\label{KushnerClarkImpliesPseudotrajectory:UniformContinuity} $X$ is uniformly continuous.
\end{clm}
If (i) holds then by continuity of $F$ we know that $\sup_n \norm{F(x_n)} < \infty$.  If (ii) holds then  $\sup_n \norm{F(x_n)} < \infty$  because $F$ is assumed bounded on a neighborhood of $\lbrace x_n \rbrace$.  In either case there exists a constant $K$ such that $\sup_t \norm{F(X(t))} \leq K$.  From Proposition \ref{StochasticApproximationInterpolations} and Proposition \ref{KushnerClarkCriterionIntegralForm} we get
\begin{align*}
&\limsup_{t \to \infty} \sup_{0 \leq h \leq T} \norm{X(t+h) - X(t)} = \limsup_{t \to \infty}\sup_{0 \leq h \leq T} \norm{\int_t^{t+h} (F(\overline{X})(u) + \overline{U}(u)) \, du} \\
&\leq \limsup_{t \to \infty}\sup_{0 \leq h \leq T} \norm{\int_t^{t+h} F(\overline{X})(u) \, du} + \limsup_{t \to \infty} \sup_{0 \leq h \leq T}\norm{\int_t^{t+h} \overline{U}(u)) \, du} \leq KT
\end{align*}

From this it follows that $X$ is uniformly continuous (given $\epsilon > 0$ let $\delta_1 = \epsilon/2K  \minop 1$ and pick $s>0$ such that $\sup_{t \geq s} \sup_{0 \leq h \leq \delta_1} \norm{X(t+h) - X(t)} < K\delta_1 + \epsilon/2 = \epsilon$ for all $t\geq s$.  Since $[0,s+1]$ is compact we know that $X$ is uniformly continuous on $[0,s+1]$ (Theorem \ref{UniformContinuityOnCompactSets}) and  therefore there exists $\delta_2>0$ such that $\sup{0 \leq t \leq s} \sup_{0 \leq h \leq \delta_2} \norm{X(t+h) - X(t)} < \epsilon$.  Let $\delta= \delta_1 \minop \delta_2$ and it follows that $\sup_{0 \leq t < \infty} \sup_{0 \leq h \leq \delta} \norm{X(t+h) - X(t)} < \epsilon$).

Now lets break $\Theta_t (X)$ up into some terms that we'll examine individually.
\begin{align}\label{KushnerClarkImpliesPseudotrajectory:TranslationTerms}
\Theta_t(X)(s) &= X(t+s) = X(0) + \int_0^{t+s} F(\overline{X}(u)) \, du + \int_0^{t+s} \overline{U}(u) \, du \\
&=X(0) + \int_0^{t} (F(\overline{X}(u)) + \overline{U}(u) ) \, du + \int_t^{t+s} F(\overline{X}(u)) \, du + \int_t^{t+s} \overline{U}(u) \, du \text{ ????} \\
&=X(t) + \int_0^s F(X(t+u)) \, du + \int_t^{t+s} (F(\overline{X})(u) - F(X(u))) \, du + \int_t^{t+s} \overline{U}(u) \, du \\
&=\Theta_t(X)(0) + \int_0^s F(\Theta_t(X)(u)) \, du + \int_t^{t+s} (F(\overline{X})(u) - F(X(u))) \, du + \int_t^{t+s} \overline{U}(u) \, du \\
&=L_F(\Theta_t(X))(s) + A_t(s) + B_t(s)
\end{align}
where have defined
\begin{align*}
L_F(X)(s) &= X(0) + \int_0^s F(X(u)) \, du \text{ for $X \in C([0,\infty); \reals^d)$} \\ 
A_t(s) &= \int_t^{t+s} (F(\overline{X}(u)) - F(X(u))) \, du \\
B_t(s) &=  \int_t^{t+s} \overline{U}(u) \, du 
\end{align*}

Note that $L_F(X) = X$ if and only if $X$ is an integral curve of the differential equation $\dot{x} = F(x)$ (TODO: Presumably this is the fixed point operator used in Picard iteration).

The last term is the easiest to handle;  by Lemma \ref{UniformConvergenceOnCompacts} the Kushner-Clark assumption is equivalent to the statement that $\lim_{t \to \infty} B_t = 0$ in $C([0,\infty); \reals^d)$ (i.e. uniformly on compact sets).

We now turn to estimates on the second term which addresses errors that arise as a result of mixing the linear and constant interpolations of the $x_n$.
\begin{clm} $\lim_{t \to \infty} A_t = 0$ in $C([0,\infty); \reals^d)$
\end{clm}
Fix $t,T > 0$  and consider the interval $t \leq u \leq t+T$.  From Proposition \ref{StochasticApproximationInterpolations}  we get
\begin{align*}
\norm{X(u) - \overline{X}(u)} &= \norm{X(u) - X(\tau_{m(u)})} = \norm{\int_{\tau_{m(u)}}^u (F(\overline{X}(s)) + \overline{U}(s)) \, ds} \\
&\leq K (u - \tau_{m(u)}) + \norm{\int_{\tau_{m(u)}}^u \overline{U}(s) \, ds} \leq K \overline{\gamma}(u) + \norm{\int_{\tau_{m(u)}}^u \overline{U}(s) \, ds} 
\end{align*}

Since $\lim_{n \to \infty} \gamma_n = 0$ and $t \leq u$ we know that for $t$ sufficiently large we have 
\begin{align*}
\tau_{m(u)+1} - \tau_{m(u)} &= \gamma_{m(u)+1} = \overline{\gamma}(u) < 1
\end{align*}
and therefore $t \leq u < \tau_{m(u)+1} < 1 + \tau_{m(u)}$; in particular $t-1 < \tau_{m(u)} \leq u$.  Therefore we may write
\begin{align*}
\norm{\int_{\tau_{m(u)}}^u \overline{U}(s) \, ds} &= \norm{\int_{t-1}^u \overline{U}(s) \, ds - \int_{t-1}^{\tau_{m(u)}} \overline{U}(s) \, ds} \\
&\leq \norm{\int_{t-1}^u \overline{U}(s) \, ds} + \norm{\int_{t-1}^{\tau_{m(u)}} \overline{U}(s) \, ds} \leq 2\Delta(t-1, T+1)
\end{align*}
and so we get
\begin{align}\label{KushnerClarkImpliesPseudotrajectory:InterpolationError}
\sup_{t \leq u \leq t+T} \norm{X(u) - \overline{X}(u)} &\leq 2\Delta(t-1, T+1) + K \sup_{t \leq u \leq t+T}\overline{\gamma}(u) 
\end{align}
Under either assumption (i) or (ii), $F$ is uniformly continuous on a neighborhood of the $\lbrace x_n \rbrace$.  Fix $T > 0$ then for any
$\epsilon > 0$ there exists a $\delta$ such that $\norm{X(u) - \overline{X}(u)} < \delta$ implies 
$\norm{F(X(u)) - F(\overline{X}(u))} < \epsilon/T$.  By \eqref{KushnerClarkImpliesPseudotrajectory:InterpolationError}, the Kushner-Clark condition 
and the fact that $\gamma_n \to 0$ we know that $\sup_{t \leq u \leq t+T} \norm{X(u) - \overline{X}(u)} < \delta$ for sufficiently large $t$.  Hence
\begin{align*}
\sup_{0 \leq s \leq T} \norm{A_t(s)} &= \sup_{0 \leq s \leq T} \norm{\int_t^{t+s} (F(\overline{X}(u)) - F(X(u))) \, du} \leq T (\epsilon/T) = \epsilon
\end{align*}
hence $\lim_{t \to \infty} \sup_{0 \leq s \leq T} \norm{A_t(s)} =0$.  Since $T>0$ we arbitrary we see that $A_t \to 0$ in $C([0,\infty), \reals^d)$.

We note that the operator $L_F : C([0,\infty), \reals^d) \to C([0,\infty), \reals^d)$ is continuous (TODO: This should be put somewhere that we discuss
Picard iteration).  This follows from the fact that $F$ is continuous and therefore $X \to F \circ X$ is continuous, evaluation $X \to X(0)$ is continuous and
$X \to \int_0 X(s) \, ds$ is continuous.

Now suppose that $X^*$ is a limit point of $\lbrace \Theta_t(X) \rbrace$.  Thus there exists a sequence $t_n$ with $t_n \to \infty$ and $\lim_{n \to \infty} \Theta_{t_n}(X) = X^*$
in $C([0,\infty), \reals^d$.  It follows from the prior claims and the continuity of $L_F$ that
\begin{align*}
X^* &= \lim_{n \to \infty} \Theta_{t_n}(X) = \lim_{n \to \infty} L_F(\Theta_{t_n}(X)) + A_{t_n} + B_{t_n} = \lim_{n \to \infty} L_F(\Theta_{t_n}(X)) \\
&= L_F(\lim_{n \to \infty}\Theta_{t_n}(X)) = L_F(X^*)
\end{align*}
and therefore $X^*$ is a solution of $\dot{x} = F(x)$.  Since we have assumed that $F$ has unique integral curves it follows that in fact $X^* = \Phi^{X^*(0)} \in S_\Phi$.
From Claim \ref{KushnerClarkImpliesPseudotrajectory:UniformContinuity} and  Theorem \ref{AsymptoticPseudotrajectoryTranslationLimitPointCharacterization} it follows that $X$ is an asymptotic pseudotrajectory.  (TODO: Don't we need relative compactness of $X([0,\infty))$ which only holds under (i)?)

Now suppose that (ii) holds and that $\norm{F(x) - F(y)} \leq L \norm{x - y}$ on a neighborhood of the $\lbrace x_n \rbrace$.  In this case from \eqref{KushnerClarkImpliesPseudotrajectory:InterpolationError} we get for $t$ sufficiently large and $T>0$
\begin{align*}
\norm{A_t(s)} &= \norm{\int_t^{t+s} (F(\overline{X}(u)) - F(X(u))) \, du} \leq T L (2\Delta(t-1, T+1) + K\sup_{t \leq u \leq t+T}\overline{\gamma}(u) ) \text{ for $0 \leq s \leq T$} \\
\end{align*}
and we also have
\begin{align*}
\norm{B_t(s)} &= \norm{\int_t^{t+s} \overline{U}(u) \, du} \leq \Delta(t,T) \leq 2 \Delta(t-1, T+1)
\end{align*}
By \eqref{KushnerClarkImpliesPseudotrajectory:TranslationTerms} and the fact that as an integral curve we have $\Phi_s(X(t)) = \Phi^{X(t)}(s)= L_F(\Phi^{X(t)})$ and Gronwall's Inequality (Proposition \ref{GronwallsInequality}) for sufficiently large $t$, all $T>0$ and $0 \leq s \leq T$
\begin{align*}
\norm{X(t+s) - \Phi_s(X(t))} &= \norm{\Theta_t(X)(s) - L_F(\Theta_t(X))(x) +  L_F(\Theta_t(X))(s) + L_F(\Phi^{X(t)})(s)} \\
&=\norm{A_t(s) + B_t(s) +  \int_0^s (F(\Theta_t(X)(u)) - F(\Phi_u(X(t)))) \, du} \\
&\leq \norm{A_t(s)} + \norm{B_t(s)} + L \int_0^s \norm{X(t+u) - \Phi_u(X(t))} \, du \\
&\leq 2(TL + 1) \Delta(t-1, T+1) + TLK \sup_{t \leq u \leq t+T}\overline{\gamma}(u) + L \int_0^s \norm{X(t+u) - \Phi_u(X(t))} \, du \\
&\leq (2(TL + 1) \Delta(t-1, T+1) + TLK \sup_{t \leq u \leq t+T}\overline{\gamma}(u)) (1 + \int_0^s e^{L(s-u)} \, du) \\
&\leq (2(TL + 1) \Delta(t-1, T+1) + TLK \sup_{t \leq u \leq t+T}\overline{\gamma}(u)) (1 + L^{-1} e^{TL}) \\
\end{align*}
Now take the supremeum over all $0 \leq s \leq T$.
\end{proof}

We now return to the realm of probability and we assume that the sequences $x_n$ and $U_n$ are now random processes.  The first question is whether we can find probabilistic hypotheses 
that guarantee the Kushner-Clark conditions hold almost surely.  

We need a small consequence of H\"{o}lder's inequality.
\begin{lem}\label{BMPHolderInequality}Let $a_i \geq 0$, $b_i \in \reals$, $p > 1$ and $0 < \delta < 1$ then 
\begin{align*}
\left(\sum_{i=n}^m \abs{ a_i b_i} \right)^p &\leq \left( \sum_{i=n}^m a_i^{\delta p/(p-1)} \right)^{p-1} \sum_{i=n}^m a_i^{1-\delta)p} \abs{b_i}^p
\end{align*}
\end{lem}
\begin{proof}
Noting that $p$ and $\frac{p}{p-1}$ are conjugate exponents we simply apply H\"{o}lder's inequality
\begin{align*}
\sum_{i=n}^m \abs{a_i b_i} &= \sum_{i=n}^m (a_i^\delta) (a_i^{1-\delta} \abs{b_i}) \\
&\leq \left(\sum_{i=n}^m a_i^{\delta p/p-1} \right )^{p-1/p} \left( \sum_{i=n}^m a_i^{1-\delta)p} \abs{b_i}^p \right){1/p}
\end{align*}
and take the $p^{th}$ power.  TODO: Why do we need $0 < \delta < 1$?
\end{proof}

TODO: Do we need a vector valued version of Burkholder or is the current proof provided valid in $\reals^d$???

\begin{prop}\label{AlmostSureKushnerClarkBoundedMoments}Let $(\Omega, \mathcal{A}, P)$ be a probability space with a filtration $\mathcal{F}_n$.  Suppose that $x_n$ and $U_n$ are adapted processes, $U_n$ is a martingale difference sequence (i.e. $\cexpectationlong{\mathcal{F}_n}{U_{n+1}}=0$ for all $n \in \integers_+$), $\gamma_n$ is a deterministic sequence such that $\lim_{n \to \infty} \gamma_n = 0$ and $\sum_{n=1}^\infty \gamma_n = \infty$ and 
\begin{align*}
x_{n+1} &= x_n + \gamma_{n+1} (F(x_n) + U_{n+1})
\end{align*}
If for some $q \geq 2$ we have $\sup_n \expectation{\norm{U_{n+1}}^q} < \infty$ and $\sum_{n=1}^\infty \gamma_n^{1+q/2} < \infty$ then the Kushner-Clark condition holds almost surely.
\end{prop}
\begin{proof}
\begin{clm}For all $T > 0$ and $t \geq 0$ there exists a constant $C_q$ such that
\begin{align*}
\expectation{\Delta(t, T)^q} &\leq C_q  T^{q/2 -1} \sup_m \norm{U_m}^q \int_{t}^{t+T} \overline{\gamma}^{q/2}(s) \, ds
\end{align*}
\end{clm}
The proof of the claim requires an inequality.  Let $\psi_1, \psi_2, \dotsc$ be a sequence of non-negative numbers and define $\sigma_0=0$ and $\sigma_n = \sum_{i=1}^n \psi_i$ for $n \geq 1$.   Since $\psi_{n+1} U_{n+1}$ is an $\mathcal{F}$ martingale difference sequence we know that $\sum_{i=1}^n \psi_{n+1} U_{n+1}$ is an $\mathcal{F}$-martingale and therefore by 
the right hand side of Burkholder's Inequality (Theorem \ref{BurkholderInequalities}) we conclude that for every $t \geq 0$ and $n \in \integers_+$
\begin{align*}
\expectation{\sup_{n < k \leq m(\sigma_n + T)} \norm{\sum_{i=n}^{k-1} \psi_{i+1} U_{i+1}}^q} &\leq C_q \expectation{\left(\sum_{i=n}^{m(\sigma_n + T)-1} \psi^2_{i+1}\norm{ U_{i+1}}^2\right)^{q/2}}
\end{align*}
If we suppose $q > 2$ then we can apply Lemma \ref{BMPHolderInequality} with $p=q/2$, $\delta=(q-2)/2q$, $a_i=\psi^2_{i+1}$ and $b_i = \norm{U_{i+1}}^2$ hence ($p/(p-1) = (q/2)/((q/2)-1) = q/(q-2)$ and $1-\delta = (q+2)/2q$)  to conclude
\begin{align*}
&\expectation{\sup_{n < k \leq m(\sigma_n + T)} \norm{\sum_{i=n}^{k-1} \psi_{i+1} U_{i+1}}^q} \\
&\leq C_q \expectation{\left(\sum_{i=n}^{m(\sigma_n + T)-1} \psi^{2\left( \frac{q-2}{2q} \right) \left ( \frac{q}{q-2} \right)}_{i+1}\right)^{q/2-1} \sum_{i=n}^{m(\sigma_n + T)-1} \psi_{i+1}^{2\left( \frac{q+2}{2q} \right) \left( \frac{q}{2} \right)} \norm{U_{i+1}}^{2 \left(\frac{q}{2} \right)}} \\
&= C_q \expectation{\left(\sum_{i=n}^{m(\sigma_n + T)-1} \psi_{i+1}\right)^{q/2-1} \sum_{i=n}^{m(\sigma_n + T)-1} \psi_{i+1}^{1+q/2} \norm{U_{i+1}}^{q}} \\
&= C_q \expectation{\left(\sigma_{m(\sigma_n+T)} - \sigma_n \right)^{q/2-1} \sum_{i=n}^{m(\sigma_n + T)-1} \psi_{i+1}^{1+q/2} \norm{U_{i+1}}^{q}} \\
&\leq C_q (\sigma_n + T - \sigma_n)^{q/2 -1} \expectation{\sum_{i=n}^{m(\sigma_n + T)-1} \psi_{i+1}^{1+q/2} \norm{U_{i+1}}^{q}} \\
&\leq C_q  T^{q/2 -1} \sup_m \norm{U_m}^q \sum_{i=n}^{m(\sigma_n + T)-1} \psi_{i+1}^{1+q/2} \\
\end{align*}
Now if we fix $t \geq 0$ and we consider $\Delta(t,T) = \sup_{t \leq u \leq t+T} \norm{\int_t^{u} \overline{U}(s) \, ds}$.  As in the proof of Proposition \ref{KushnerClarkCriterionIntegralForm} the piecewise linearity of the integral as a function of $u$ and the convexity of the norm implies that the supremum is attained at some $u \in \lbrace t, m(t)+1, \dotsc, m(t+T), t+T \rbrace$.
Define the sequence $\psi_{m(t)} = t$, $\psi_{m(t)+1} = \tau_{m(t)+1} - t = \gamma_{m(t)+1} - (t - \tau_{m(t)})$, $\psi_i = \gamma_i$ for $i=m(t)+1, \dotsc, m(t+T)$ and $\psi_{m(t+T)+1} = t+T - m(t+T)$.  Applying the above inequality (noting $\sigma_n = t$) we get (TODO: there is some ambiguity to clear up about $m$ defined by the $\gamma_i$ and $m$ defined by the $\psi$; the salient point is these two functions are equal on the interval $[t,t+T]$).
\begin{align*}
\expectation{\Delta(t,T)} &= \expectation{\norm{\sup_{m(t) < k \leq m(t+T)} \psi_{i+1} U_{i+1}}^q} \\
&\leq C_q  T^{q/2 -1} \sup_m \norm{U_m}^q \sum_{i=m(t)}^{m(t + T)-1} \psi_{i+1}^{1+q/2}  \\
&\leq C_q  T^{q/2 -1} \sup_m \norm{U_m}^q \sum_{i=m(t)}^{m(t + T)-1} \psi_{i+1} \gamma_{i+1}^{q/2}  \\
&=C_q  T^{q/2 -1} \sup_m \norm{U_m}^q \int_t^{t+T} \overline{\gamma}^{q/2}(s) \, ds
\end{align*}

TODO: Handle the case $q=2$ which is more direct.

For $q \geq 2$ and every $\epsilon > 0$
\begin{align*}
\sum_{k=0}^\infty \probability{\Delta(kT,T) > \epsilon} &\leq \epsilon^{-q} \sum_{k=0}^\infty \expectation{\Delta(kT, T)^q} \\
&\leq \epsilon^{-q} C_q T^{q/2 -1} \sup_m \norm{U_m}^q \sum_{k=0}^\infty \int_{kT}^{(k+1)T} \overline{\gamma}^{q/2}(s) \, ds \\
&= \epsilon^{-q} C_q T^{q/2 -1} \sup_m \norm{U_m}^q \int_0^\infty \overline{\gamma}^{q/2}(s) \, ds \\
&= \epsilon^{-q} \sum_{n=1}^\infty \gamma_{n+1}^{1 + q/2} < \infty
\end{align*}
and therefore by the Borel Cantelli Theorem \ref{BorelCantelli} we get $\probability{\Delta(kT,T) > \epsilon i.o.} =0$ and by Lemma \ref{ConvergenceAlmostSureByInfinitelyOften}
$\lim_{k \to \infty} \Delta(kT,T) = 0$ almost surely.

For an arbitrary $0 \leq t < \infty$ there exists a unique $k \in \integers_+$ such that $kT \leq t < (k+1)T$ and for such a $k$ we have for $0 \leq h \leq T$  
$\int_t^{(t+h) \minop (k+1)T} = \int_{kT}^{(t+h) \minop (k+1)T} - \int_{kT}^t$ and therefore $\norm{\int_t^{(t+h) \minop (k+1)T}} \leq \norm{\int_{kT}^{(t+h) \minop (k+1)T}} + \norm{\int_{kT}^t} \leq 2 \sup_{0 \leq h \leq T} \int_{kT}^{kT+h}$ also
$\norm{\int_{(t+h) \minop (k+1)T}^{t+h}} \leq sup_{0 \leq h \leq T}\norm{\int_{(k+1)T}^{(k+1)T+h}}$ and so $\Delta(t,T) \leq 2 \Delta(kT, T) + \Delta((k+1)T,T)$.  This shows that 
$\lim_{t \to \infty} \Delta(t,T) = 0$ almost surely.  Now we apply Proposition \ref{KushnerClarkCriterionIntegralForm}.
\end{proof}