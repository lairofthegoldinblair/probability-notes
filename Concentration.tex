\chapter{Concentration Inequalities}
\begin{lem}[Markov Inequality]\label{MarkovInequality}Let $\xi$ be a positive integrable random variable.  Then $\probability{\xi>t} \leq \frac{E(\xi)}{t}$
\end{lem}
\begin{proof}
$E(\xi) \geq E(\xi\characteristic{\{\xi>t\}}) \geq E(t\characteristic{\{\xi>t\}})=t\probability{\xi>t}$
\end{proof}

\begin{lem}[Chebeshev's Inequality]\label{ChebInequality}Let $\xi$ be a random variable with finite mean $\mu$ and finite variance $\sigma$.  Then $\probability{|\xi-\mu|>t} \leq \frac{\sigma^2}{t^2}$\end{lem}
\begin{proof}
$\probability{|\xi-\mu|>t} = \probability{(\xi-\mu)^2 > t^2} \leq \frac{\expectation{(\xi-\mu)^2}}{t^2}=\frac{\sigma^2}{t^2}$
\end{proof}

\begin{lem}[One Sided Chebeshev's Inequality]\label{OneSidedChebInequality}Let $\xi$ be a random variable with finite mean $\mu$ and
  finite variance $\sigma$.  Then $\probability{\xi-\mu>\lambda} \leq
  \frac{\sigma^2}{\sigma^2 + \lambda^2}$
\end{lem}
\begin{proof}
First we assume $\expectation{\xi}=0$.  We prove a family of
inequalities for a real parameter $c > 0$.
\begin{align*}
\probability{\xi>\lambda}  
&= \probability{\xi + c > \lambda + c} \\
& \leq \probability{(\xi + c)^2 > (\lambda + c)^2} & \textrm{ because }
\lambda+c>0 \\
& \leq \frac{\expectation{\xi^2} + c^2}{(\lambda + c)^2}
\end{align*}
Now we extract the best estimate by finding the minimum of the right
hand side with respect to $c$.  Differentiating we get a vanishing
first derivative when
$
(\lambda^2 + c^2) 2c = (\expectation{\xi^2} + c^2) 2(\lambda +
c)$.  Divide by $2(\lambda+ c)$  and subtract $c^2$ to get the
  minimum at $c=\expectation{\xi}/\lambda > 0$.  Plug this value in to
  get the final estimate.
\begin{align*}
\frac{
  \expectation{\xi^2} +
  (\frac{\expectation{\xi^2}}{\lambda})^2
}
{(\lambda +
    \frac{\expectation{\xi^2}}{\lambda})^2
} & =
    \frac{\expectation{\xi^2}(1 +
      \frac{\expectation{\xi^2}}{\lambda^2})}
{
  \lambda^2 (1 + \frac{\expectation{\xi^2}}{\lambda^2}
)^2
} \\
& = \frac{\expectation{\xi^2}}{\lambda^2 + \expectation{\xi^2}}
\end{align*}
Now apply the above inequality to the centered random variable $\xi -
\mu$ to get the general result.
\qedhere
\end{proof}

\begin{defn}
We say that a random variable $\xi$ is \emph{subgaussian} if and only if
there exist constants $c, C > 0$ such that $\probability { \abs{\xi}
  \geq \lambda} \leq C e^{-c \lambda^2}$ for all $\lambda > 0$.
\end{defn}

TODO: Show that any Gaussian is subgaussian (independent of its mean?).

TODO: Show any bounded (or almost surely bounded) random variable is
subgaussian.

\begin{examp}Given the nomenclature it isn't surprising that Gaussian
  random variables are subgaussian.  As it turns out it is useful to
  analyze the case of a $N(0,\sigma^2)$ random variable separately
  since it has slightly different behavior than the general $N(\mu,
  \sigma^2)$ case.  Let us assume that $\xi$ is a normal random
  variable with mean $0$ and variance $\sigma^2$.  We have a standard tail estimate for $\lambda \geq \sigma$
\begin{align*}
\probability{\xi \geq \lambda} &= 
\frac{1}{\sqrt{2\pi}\sigma} \int_\lambda^\infty e^{-x^2/2\sigma^2} \, dx \leq 
\frac{1}{\sqrt{2\pi}\sigma} \int_\lambda^\infty \frac{x}{\sigma} e^{-x^2/2\sigma^2} \, dx =
\frac{1}{\sqrt{2\pi}} e^{-\lambda^2/2\sigma^2}
\end{align*}
The $0 \leq \lambda \leq \sigma$ case can easily be handled with a constant
multiplier but we can actually find the constant that gives a tight
bound.  Note that $\frac{1}{\sqrt{2\pi}\sigma} \int_0^\infty e^{-x^2/2\sigma^2} =
\frac{1}{2}$ so we can't do any better than $\probability{\xi \geq
  \lambda} \leq \frac{1}{2} e^{-\lambda^2/2\sigma^2}$; in fact this bound
works for all $\lambda \geq 0$.  We've already shown this for $\lambda
\geq 1$ and $\lambda=0$.  To show the bound on $[0,1]$ we calculate
the derivative 
\begin{align*}
\frac{d}{d\lambda} \left( \frac{1}{2} e^{-\lambda^2/2\sigma^2} -
  \frac{1}{\sqrt{2\pi}\sigma} \int_\lambda^\infty e^{-x^2/2\sigma^2} \, dx \right)
&=\left( -\frac{\lambda}{2\sigma^2} + \frac{1}{\sqrt{2\pi}\sigma}\right) e^{-\lambda^2/2}
\end{align*}
from which we conclude there is a unique maximum of the function at
$\lambda=\sigma \sqrt{\frac{2}{\pi}} \in (0,\sigma)$.  We have already validated
that the function is nonnegative at the endpoints of $[0,\sigma]$ so it must
be nonnegative on the entire interval.  Now by symmetry of $\xi$, the
calculation also shows that $\probability{\xi \leq -\lambda} \leq
\frac{1}{2}e^{-\lambda^2/2\sigma^2}$ and therefore $\probability{\abs{\xi}
  \geq \lambda} \leq e^{-\lambda^2/2\sigma^2}$.

Now for a general $N(\mu, \sigma)$ normal random variable $\xi$ we
have by change of variables
\begin{align*}
\probability{\xi \geq \lambda} &= 
\frac{1}{\sqrt{2\pi} \sigma} \int_\lambda^\infty e^{-(x-\mu)^2/2\sigma^2} \, dx = 
\frac{1}{\sqrt{2\pi}} \int_{(\lambda-\mu)/\sigma}^\infty e^{-x^2/2} \,
dx \leq \frac{1}{\sqrt{2\pi}} e^{-(\lambda-\mu)^2/2\sigma^2}
\end{align*}

TODO: Finish
\end{examp}

\begin{lem}Let $\{\xi_i\}_{i=1}^m$ be jointly independent subgaussian random variables.  Then $\expectation{e^{\sum_{i=1}^m \xi}} =
  \prod_{i=1}^m \expectation{e^{\xi_i}}$.
\end{lem}
\begin{proof}
First show that for a subgaussian $\xi$, we have by dominated
convergence the Taylor expansion 
$$\expectation{e^{t\xi}} = 1 + \sum_{k=1}^\infty
\frac{t^k}{k!}\expectation{\xi^k}$$
The proof of this fact is to exhibit an integrable function that
dominates the sequence of partial sums $1+\sum_{k=1}^n
\frac{t^k\xi^k}{k!}$.  This is obvious if $\xi$ is almost surely
bounded but it's not obvious to me that this should be true for a
subgaussian $\xi$.  TODO: Perhaps we need to use uniform integrability or
something like that in the subgaussian/subexponential case.

In any case, assuming the validity of the above identity for each
$\xi$, we turn to the case of the sum.
\end{proof}

\begin{lem}\label{SubgaussianEquivalence}$\xi$ is subgaussian if and only if there exists $C$ such
  that $\expectation{e^{t\xi}} \leq C e^{Ct^2}$ and if and only if
    there exists $C$ such that $\expectation{|\xi|^k} \leq
    \left(Ck\right)^{\frac{k}{2}}$ for all $t \in \reals$.
\end{lem}
\begin{proof}
Suppose $\xi$ is subgaussian and calculate:
\begin{align*}
\expectation{e^{t\xi}} &= \int_0^\infty \probability{e^{t\xi} \geq
    \lambda} d\lambda 
= \int_{-\infty}^\infty \probability{e^{t\xi} \geq e^{t\eta}} t
e^{t\eta} d\eta \\
&= \int_{-\infty}^\infty \probability{\xi \geq \eta} t
e^{t\eta} d\eta 
\leq \int_{-\infty}^\infty C t e^{t\eta - c\eta^2} d\eta  
= C t e^{\frac{t^2}{4c}}\int_{-\infty}^\infty 
e^{-\left(\sqrt{c}\eta - \frac{t}{2\sqrt{c}}\right)^2} d\eta \\
&= C^\prime t e^{\frac{t^2}{4c}} 
\leq C^\prime e^{\frac{5c t^2}{4c}} 
\end{align*}

Now assume that we have $\expectation{e^{t\xi}} \leq C e^{Ct^2}$ for
all $t$.  Pick an arbitrary $t>0$ to be
chosen later and proceed by using first order
moment method:
\begin{align*}
\probability{\xi \geq \lambda} &= \probability{e^{t\xi} \geq
  e^{t\lambda}} 
\leq \frac{\expectation{e^{t\xi}}}{e^{t\lambda}} 
\leq C e^{Ct^2 - t\lambda} \\
\end{align*}
Now we pick $t$ to minimize the upper bound derived above; simple
calculus shows this occcurs at $t=\frac{\lambda}{2C}$.  Subtituting
yields the bound 
\begin{align*}
\probability{\xi \geq \lambda} \leq C e ^ {-\frac{\lambda^2}{4C}}
\end{align*}
For the other tail, we note that our assumption holds equally well for
$-\xi$.  Thus we can use the same method to bound 
\begin{align*}
\probability{\xi \leq -\lambda} = \probability{-\xi \geq \lambda} \leq C e ^ {-\frac{\lambda^2}{4C}}
\end{align*}
therefore taking the union bound we get
\begin{align*}
\probability{|\xi| \geq \lambda} \leq 2 C e ^ {-\frac{\lambda^2}{4C}}
\end{align*}

Now consider absolute moments of subgaussian variables.  We can assume
that $\xi \geq 0$ and calculate as before:
\begin{align*}
\expectation{\xi^k} 
&= \int_0^\infty \probability{\xi^k \geq x} dx 
= k\int_0^\infty \probability{\xi^k \geq y^k} y^{k-1} dy \\
&= k C \int_0^\infty y^{k-1} e^{-cy^2} dy 
= k C \frac{c^{k-3}}{2} \int_0^\infty x^{\frac{k}{2}-1} e^{-x} dx \\
&= k C \frac{c^{k-3}}{2} \Gamma\left(\frac{k}{2}\right) 
\leq k C \frac{c^{k-3}}{2} \left(\frac{k}{2}\right)^\frac{k}{2} \\
\end{align*}
To go the other direction, assume $\expectation{|\xi|^k} \leq
(Ck)^\frac{k}{2}$ and pick a constant $0 < c < \frac{e}{2C}$ 
\begin{align*}
\expectation{e^{K\xi^2}} &= 1 + \sum_{k=1}^\infty
\frac{t^k\expectation{\xi^{2k}}}{k!}\\
&\leq 1 + \sum_{k=1}^\infty
\frac{(2tCk)^k}{k!}\\
&\leq 1 + \sum_{k=1}^\infty
\left(\frac{2tC}{e}\right)^k < \infty \\
\end{align*}
Now use the elementatry bound $ab \leq \frac{(a^2 + b^2)}{2}$ so see
\begin{align*}
\expectation{e^{t\xi}} &\leq 
\end{align*}
\end{proof}

The definition of subgaussian random variables differs in a minor way
from another in common use in the literature.  In particular, in some descriptions a random
variable $\xi$ is called subgaussian if and only if
$\expectation{e^{t\xi}} \leq e^{\frac{c^2 t^2}{2}}$ for all $t \in \reals$.  The important
difference here compared with the characterization in Lemma
\ref{SubgaussianEquivalence} is that the constant on the right hand side is $1$.
With this definition, we must add the hypothesis $\expectation{\xi}=0$
to get equivalence with the other definition.

\begin{lem}Suppose $\xi$ is a random variable such that there exists
  $c>0$ for which
\begin{align*}
\expectation{e^{t\xi}} &\leq e^{\frac{c^2 t^2}{2}} \text{ for all $t \in \reals$}
\end{align*}
then $\expectation{\xi}=0$ and $\expectation{\xi^2} \leq c^2$.
\end{lem}
\begin{proof}
By Dominated Convergence and the hypothesis we get
\begin{align*}
\sum_{n=0}^\infty \frac{t^n}{n!}\expectation{\xi^n} &=
\expectation{e^{t\xi}} \leq e^{\frac{c^2 t^2}{2}} = \sum_{n=0}^\infty
\frac{c^{2n}}{2^n n!}t^{2n}
\end{align*}
so in particular by taking only terms up to order $t^2$ and using the
fact that the constant term in on both sides is $1$, we have
\begin{align*}
t \expectation{\xi} + \frac{t^2}{2} \expectation{\xi^2} &= \frac{c^2 t^2}{2}
+
o(t^2) \text { as $t \to 0$}
\end{align*}
If we divide both sides by $t > 0$ and take the limit as $t \to 0^+$ then we
get $\expectation{\xi} \leq 0$.  If we divide by $t < 0$ and take the
limit as $t \to 0^-$ then we get $\expectation{\xi} \geq 0$.  Thus we
can conclude $\expectation{\xi} = 0$.  If we plug that in and divide
by $t^2$ and take the limit as $t \to 0$ then see $\expectation{\xi^2}
\leq c^2$.
\end{proof}
Note that the argument in the proof above doesn't even get off the
ground unless the constant of the bounding exponential is assumed to
be $1$.

The following lemma is useful for the second moment method for
deriving tail bounds.
\begin{lem}Let $\{\xi_i\}_{i=1}^m$ be pairwise independent random
  variables and $c_i$ be scalars.  Then $\variance{\sum_{i=1}^m c_i \xi} =
  \sum_{i=1}^m |c_i|^2\variance{\xi_i}$.
\end{lem}
\begin{proof}
TODO
\end{proof}

\begin{lem}[Bennett's Inequality]\label{Bennett} Let $\{\xi_i\}_{i=1}^m$ be independent random variables with means $\mu_i$ and variances $\sigma_i$.  Set $\Sigma^2 = \sum_{i=1}^m \sigma_i^2$.  If for every $i$, $|\xi_i - \mu_i| \leq M$ almost everywhere then for every $\lambda > 0$ we have $$
\probability{\sum_{i=1}^m [\xi_i - \mu_i] > \lambda} \leq 
e^{
	-\frac{\lambda}{M}\{(1 + \frac{\Sigma^2}{M\lambda})\log(1+\frac{M\lambda}{\Sigma^2}) - 1\}
}
$$
\end{lem}
\begin{proof}
First it is easy to see that by subtracting means we may assume that
$\mu_i=0$.  Then we have $\sigma_i = \expectation{\xi_i^2}$.  We use
the exponential moment method.  We show a family of inequalities
depending on a real parameter $c > 0$ which we will pick later.  First we have 
\begin{align*}
\probability{\sum_{i=1}^m \xi_i > \lambda} & =
\probability{c\sum_{i=1}^m \xi_i > c\lambda}  && \textrm{since } c >0.\\
  & =\probability{e^{c\sum_{i=1}^m \xi_i} > e^{c\lambda}} &&
  \textrm{since } e^x \textrm{ is increasing}\\
  & \leq e^{-c\lambda} \expectation{e^{c\sum_{i=1}^m \xi_i}} && \textrm
  {by Markov's Inequality}\eqref{MarkovInequality}\\
  & = e^{-c\lambda} \prod_{i=1}^m \expectation{e^{c\xi_i}} &&
  \textrm{by independence and boundedness.  TODO: do we really need boundedness?}
\end{align*}
Now we consider an individual term $\expectation{e^{c\xi_i}}$ for an
almost surely bounded $\xi_i$ with zero mean.
\begin{align*}
\expectation{e^{c\xi_i}} &= \expectation{\sum_{k=0}^\infty
  \frac{c^k\xi_i^k}{k!}} = \sum_{k=0}^\infty
  \frac{c^k}{k!}\expectation{\xi_i^k} && \textrm{by dominated
    convergence} \\
& = 1  + \sum_{k=2}^\infty
  \frac{c^k}{k!}\expectation{\xi_i^k} && \textrm{by mean zero} \\
& \leq 1  + \sum_{k=2}^\infty
  \frac{c^k M^{k-2} \sigma_i^2}{k!} && \textrm{by boundedness and
    definition of variance} \\
& \leq e^{\sum_{k=2}^\infty  \frac{c^k M^{k-2} \sigma_i^2}{k!}} &&
\textrm{since } 1+x \leq e^x \eqref{BasicExponentialInequalities} \\
& = e^{\frac{(e^{cM} -1 - cM)\sigma_i^2}{M^2}}
\end{align*}
Therefore,
\begin{align*}
\probability{\sum_{i=1}^m \xi_i > \lambda} &\leq e^{-c\lambda} \prod_{i=1}^m
e^{\frac{(e^{cM} -1 - cM)\sigma_i^2}{M^2}} \\
& = e^{\frac{(e^{cM} -1 - cM)\Sigma^2}{M^2}}
\end{align*}
Now we pick $c>0$ to minimize the bound above ($e^{cM} -1 =
\frac{M\lambda}{\Sigma^2}$ or equivalently $c = \frac{1}{M}\ln(1 + \frac{M\lambda}{\Sigma^2})$).
Substituting yields the final bound
\begin{align*}
\probability{\sum_{i=1}^m \xi_i > \lambda} &\leq e^{-(\lambda + \frac{\Sigma^2}{M})
  \frac{1}{M}\ln(1 + \frac{M\lambda}{\Sigma^2}) + \frac{\lambda}{M}}
\\
& = e^{-\frac{\lambda}{M} \{
  (1+\frac{\Sigma^2}{\lambda M}) \ln(1 + \frac{M\lambda}{\Sigma^2}) -1
\}}
\end{align*}
\end{proof}

\begin{lem}[Bernstein's or Chernoff's Inequality]\label{Bernstein} Let
  $\{\xi_i\}_{i=1}^m$ be independent random variables with means
  $\mu_i$ and variances $\sigma_i$.  Set $\Sigma^2 = \sum_{i=1}^m
  \sigma_i^2$.  If for every $i$, $|\xi_i - \mu_i| \leq M$ almost
  everywhere then for every $\lambda > 0$ we have 
\begin{align*}
\probability{\sum_{i=1}^m [\xi_i - \mu_i] > \lambda} &\leq 
e^{
	-\{\frac{\lambda^2}{2(\Sigma^2 + \frac{1}{3}M\lambda)}\}
}
\end{align*}
\end{lem}
\begin{proof}
TODO
\end{proof}

The next inequality has a pleasing form because the resulting bound is
of the form of a Gaussian random variable.  Such bounds are
interesting enough that they warrant the following definition.
\begin{defn} Let $\xi$ be a real valued random variable with mean
  $\mu$.  We say that $\xi$ has a \emph{subgaussian upper tail} if there
  exists a  constants $C > 0$ and $c > 0$ such that for all $\lambda > 0$,
$$
\probability{[\xi-\mu] > \lambda} \leq Ce^{-c\lambda^2}.
$$
We say that $\xi$ has a \emph{subgaussian tail up to} $\lambda_0$ if the
above bound holds for $\lambda < \lambda_0$.  We say that $\xi$ has a
\emph{subgaussian tail} if both $\xi$ and $-\xi$ have subgaussian upper
tails (or equivalently if $|\xi|$ has a subgaussian tail.
\end{defn}

 The boundedness assumption on the individual random variables in the
above sums can be relaxed to an assumption that the individual random
variables has subgaussian tails.  Moreover, one can generalize the sum
of random variables to an arbitrary linear combination of random
variables on the unit sphere.

\begin{lem}\label{Matousek} Let $\{\xi_i\}_{i=1}^m$ be independent
  random variables with $E[\xi_i]=0$ and $E[\xi_i^2]=1$ and uniform
  subgaussian tails.  Let $\{\alpha_i\}_{i=1}^m$ be real coefficients
satisfying $\sum_{i=1}^m \alpha_i^2 = 1$.  The then random variable
$\eta=\sum_{i=1}^m \alpha_i\xi_i$ has $E[\eta]=0$, $E[\eta^2]=1$ and a
subgaussian tail.
\end{lem}
\begin{proof}
TODO
\end{proof}

\begin{lem}[Exercise 7 Lugosi] Let $\{\xi_i\}_{i=1}^n$ be independent
  random variables with values in $[0,1]$.  Let $S_n = \sum_{i=1}^n
  \xi_i$ and let $\mu=\expectation{S_n}$.  Show that for any
  $\lambda\geq \mu$,
\begin{align*}
\probability{S_n \geq \lambda} \leq
\left(\frac{\mu}{\lambda}\right)^\lambda \left(\frac{n-\mu}{n-\lambda}\right)^{n-\lambda}.
\end{align*}
\end{lem}
\begin{proof}
Use Chernoff bounding.  Looking at the solution, we can pattern match
that we may want to use the convexity of $e^x$ since the solution
seems to reference the endpoints of the interval $[0,n]$; indeed that
is the way to proceed.  TODO: convert the argument below for $n=1$ to
cover general $n$.
To estimate $\expectation{e^{s\xi_i}}$ we first use convexity of
$e^{x}$ on the interval $[0,s]$.  For $0 \leq \theta \leq 1$,
\begin{align*}
e^{s\theta} &= e^{\theta \cdot s + (1-\theta) \cdot 0}  \leq \theta e^s + (1-\theta)e^0 = \theta e^s + (1-\theta)
\end{align*}
Substituting $\theta=\xi_i$ and taking expectations we get
\begin{align*}
\expectation{e^{s\xi_i}} \leq \mu_i e^s + (1-\mu_i).
\end{align*}
So now we minimize the Chernoff bound by using elementary calculus
\begin{align*}
\frac{d}{ds} \mu_i e^{s(1-\lambda)} + (1-\mu_i)e^{-s\lambda} = \mu_i
(1-\lambda) e^{s(1-\lambda)} + \lambda (1-\mu_i)e^{-s\lambda} 
\end{align*}
which equals $0$ when
$s=\ln\left(\frac{\lambda(1-\mu_i)}{\mu_i(1-\lambda)}\right)$.  This
value is positive when $\lambda \geq \mu$.
Backsubstituing this value and doing some algebra shows
\begin{align*}
e^{-s\lambda}\expectation{e^{s\xi_i}} \leq
\left(\frac{\mu_i}{\lambda}\right)^\lambda
\left(\frac{1-\mu_i}{1-\lambda}\right)^{1-\lambda}
\end{align*}
Note also an argument for a related estimate (Exercise 8) that uses bounds similar to those in Bennett can be made
as follows.  Since $\xi_i \in [0,1]$, we have that $\xi_i^k \leq
\xi_i$.  With this observation, 
\begin{align*}
\expectation{e^{s\xi_i}} &= 1 + \sum_{k=1}^\infty \frac{s^k
  \expectation{\xi_i^k}}{k!} \\
& \leq 1 + \sum_{k=1}^\infty \frac{s^k \mu_i}{k!} \\
&= 1 + \mu_i (e^s -1) \\
&\leq e^{\mu_i(e^s - 1)}
\end{align*}
Now we select $s$ to minimize the Chernoff bound $e^{\mu_i(e^s - 1)
  -s\lambda}$ which simple calculus shows happens at
$s=\ln\left(\frac{\lambda}{\mu_i}\right)$; the location of the minimum
being positive precisely when $\lambda \geq \mu_i$.  Backsubstituting
yields a bound $\left(\frac{\mu_i}{\lambda}\right)^\lambda e^{\lambda
  - \mu_i}$.
\end{proof}

\section{Sanov's Theorem for Finite Alphabets}

In this section we develop the simplest version of a large deviation
principle for empirical measures.  We assume a finite alphabet
$\Sigma = \lbrace a_1, \dotsc, a_{\card{\Sigma}} \rbrace$ with the powerset
$\sigma$-algebra.  We investigate the concentration properties of 
sequences $\xi_1, \xi_2, \dotsc$ of random elements in $\Sigma$.  
The empirical measure of a sequence of random
elements in $\Sigma$ is often called the \emph{type} of the sequence.  

TODO: Topology on the space of probability measures on a finite space.  Identification with the induced topology from $\reals^n$.

\begin{defn}Given a finite measure space $(\Sigma, 2^\Sigma, \mu)$ we let 
\begin{align*}
\Sigma_\mu &= \lbrace a \in \Sigma \mid \mu(a) > 0 \rbrace
\end{align*}
\end{defn}

\begin{prop}\label{FiniteProbabilityMeasuresAbsoluteContinuity}Let $\mu$ and $\nu$ be probability measures on the finite measurable space $(\Sigma, 2^\Sigma, \mu)$ then
the following are equivalent
\begin{itemize}
\item[(i)]$\Sigma_\nu \subset \Sigma_\mu$
\item[(ii)] $\nu \ll \mu$
\item[(iii)] $\kldiv{\nu}{\mu} < \infty$
\end{itemize}
Moreover for each fixed $\mu$ the set of $\nu$ such that $\nu \ll \mu$ is compact and $\kldiv{\nu}{\mu}$ is finite and continuous on this set.
\end{prop}
\begin{proof}
Left to the reader.  TODO: Add to exercises and provide the simple solution.

To see the compactness, we only need to see that the set of $\nu$ with $\nu \ll \mu$ is closed (it is trivially bounded).  Letting $\nu = \lim_{n \to \infty} \nu_n$ with $\nu_n \ll \mu$ we see that if $\mu(a) = 0$ then it follows that $\nu_n(a) = 0$ for all $n \in \naturals$ and therefore $\nu(a) = \lim_{n \to \infty} \nu_n(a) = 0$.
\end{proof}

\begin{defn}Let $(\Sigma, 2^\Sigma)$ be a finite measurable space and let $y \in \Sigma^n$
\begin{align*}
\type{y} &= \frac{1}{n} \sum_{a \in \Sigma} \sum_{j=1}^n \characteristic{a}(y_j) \delta_a
\end{align*}
The range $\type{\Sigma^n}$ is denoted $\types{n}$.  Given a probability measure $\nu \in \types{n}$ the
\emph{type class} of $\nu$ is 
\begin{align*}
\typeclass{\nu} &= \typeop^{-1} y = \lbrace y \in \Sigma^n \mid \type{y} = \nu \rbrace
\end{align*}
\end{defn}

\begin{lem}\label{SizeOfAType}Let $(\Sigma, 2^{\Sigma})$ be a finite measure space then 
\begin{align*}
\card{\types{n}} \leq (n+1)^{\card{\Sigma}}
\end{align*}
\end{lem}
\begin{proof}
Each element $\nu \in \types{n}$ must have probabilities $\nu(a) = j_a/n$ for some $j_a \in \lbrace 0,1, \dotsc, n \rbrace$; therefore 
it is uniquely determined by a tuple in $\lbrace 0, 1, \dotsc, n \rbrace^\Sigma$.  The size of the latter set is $(n+1)^{\card{\Sigma}}$.
\end{proof}

\begin{lem}\label{ProbabilityOfAType}Let $\mu$ and $\nu$ be probability measures on the finite
  measure space $(\Sigma, 2^{\Sigma})$.  Let $\xi_1, \xi_2, \dotsc$ be i.i.d. random
  elements in $\Sigma$ with $\law{\xi_j} = \mu$ for $j \in \naturals$, then
\begin{align*}
\probability{(\xi_1, \dotsc, \xi_n) = y} &= e^{-n(\shannon{\type{y}} + \kldiv{\type{y}}{\mu})}
\end{align*}
\end{lem}
\begin{proof}
First suppose that $\Sigma_{\type{y}} \not\subset \Sigma_\mu$.  In this case there exists $1 \leq j \leq n$ such that $y_j \notin \Sigma_\mu$ and therefore
$\probability{(\xi_1, \dotsc, \xi_n) = y} =0$.  Also by Proposition \ref{FiniteProbabilityMeasuresAbsoluteContinuity}, $\kldiv{\type{y}}{\mu} = \infty$ which implies $ e^{-n(\shannon{\type{y}} + \kldiv{\type{y}}{\mu})}=0$ as well.

Thus we may assume that $\Sigma_{\type{y}} \subset \Sigma_\mu$.  By Lemma \ref{IndependenceProductMeasures} we
have
\begin{align*}
\probability{(\xi_1, \dotsc, \xi_n) = y} &= \prod_{j=1}^n \probability{\xi_j= y_j} = \prod_{a \in \Sigma_{\type{y}}} \probability{\xi_j= a_j}^{n \type{y}(a_j)} \\
&= \prod_{a \in \Sigma_{\type{y}}} \mu(a)^{n \type{y}(a)} = \prod_{a \in \Sigma_{\type{y}}} e^{n \type{y}(a) \log \mu(a)} \\
&= \exp \left(n \sum_{a \in \Sigma_{\type{y}}} \type{y}(a) \log \mu(a) \right) \\
&= \exp \left( -n \sum_{a \in \Sigma_{\type{y}}} - \type{y}(a) \log \type{y}(a) + \type{y}(a) \log \type{y}(a) - \type{y}(a) \log \mu(a) \right ) \\
&= \exp \left( -n \sum_{a \in \Sigma_{\type{y}}} - \type{y}(a) \log \type{y}(a) + \type{y}(a) \log( \frac{ \type{y}(a) }{\mu(a)} )\right) \\
&= \exp \left( -n \sum_{a \in \Sigma} - \type{y}(a) \log \type{y}(a) + \type{y}(a) \log( \frac{ \type{y}(a) }{\mu(a)} )\right) \\
&= e^{-n(\shannon{\type{y}} + \kldiv{\type{y}}{\mu})}
\end{align*}
The reader is invited to make careful note of where we used the assumption $\Sigma_{\type{y}} \subset \Sigma_\mu$ and our conventions $0 \cdot \log 0 = 0$ and $\log \frac{0}{0} = 0$ in the calculation above.
\end{proof}

If we consider a probability measure $\mu \in \types{n}$ then $(y_1, \dotsc, y_n) \in \typeclass{\mu}$ if and only if the number of $y_j$ equal to $a \in \Sigma$ is equal to $n \mu(a)$; thus 
\begin{align*}
\card{\typeclass{\mu}} &= \frac{n!}{(n \mu(a_1))! \dotsb (n \mu(a_{\card{\Sigma}}))!}
\end{align*}
is just a multinomial coefficient.  Our goal is to understand the asymptotic behavior of such a coefficient.  One way to approach this is to use Stirling's approximation.  We present a different approach here.
\begin{lem}\label{SizeOfATypeClass}For any $\nu \in \types{n}$ we have
\begin{align*}
\frac{1}{(n+1)^{\card{\Sigma}}} e^{n\shannon{\nu}} &\leq \card{\typeclass{\nu}} \leq e^{n\shannon{\nu}}
\end{align*}
\end{lem}
\begin{proof}
The upper bound is a straightforward corollary of Lemma \ref{ProbabilityOfAType} in the case $\mu=\nu$; let $\xi_1, \dotsc, \xi_n$ be i.i.d. with
$\law{\xi_j} = \nu$ then
\begin{align*}
1 &\geq \probability{(\xi_1, \dotsc, \xi_n) \in \typeclass{\nu}} = \sum_{y \in \typeclass{\nu}} \probability{(\xi_1, \dotsc, \xi_n) = y} = 
\card{\typeclass{\nu}} e^{-n \shannon{\nu}}
\end{align*}

\begin{clm}For all $\mu, \nu \in \types{n}$ we have 
\begin{align*}
\probability{(\xi_1, \dotsc, \xi_n) \in \typeclass{\mu}} \geq \probability{(\xi_1, \dotsc, \xi_n) \in \typeclass{\nu}}
\end{align*}
\end{clm}
If $\probability{(\xi_1, \dotsc, \xi_n) \in \typeclass{\nu}}=0$ there is nothing to prove. If $\probability{\type{(\xi_1, \dotsc, \xi_n)} = \mu}>0$ then it is clear that $\Sigma_\mu \subset \Sigma_\nu$.  Using this fact we can calculate
\begin{align*}
\frac{\probability{(\xi_1, \dotsc, \xi_n) \in \typeclass{\mu}}}{\probability{(\xi_1, \dotsc, \xi_n) \in \typeclass{\nu}}} 
&=\frac{\sum_{y \in \typeclass{\mu}} \probability{(\xi_1, \dotsc, \xi_n) = y}}{\sum_{y \in \typeclass{\nu}} \probability{(\xi_1, \dotsc, \xi_n) = y}} =\frac{\card{\typeclass{\mu}} \prod_{a \in \Sigma_{\mu}} \nu(a)^{n \mu(a)}}{\card{\typeclass{\nu}} \prod_{a \in \Sigma_{\nu}} \nu(a)^{n \nu(a)}} \\
&=\frac{\card{\typeclass{\mu}} \prod_{a \in \Sigma_{\nu}} \nu(a)^{n \mu(a)}}{\card{\typeclass{\nu}} \prod_{a \in \Sigma_{\nu}} \nu(a)^{n \nu(a)}} 
=\prod_{a \in \Sigma_{\nu}}  \frac{(n \nu(a))! }{(n \mu(a))!} \nu(a)^{n (\mu(a) - \nu(a))}\\
\end{align*}
For arbitrary $m,l \in \integers$ and $n \in \naturals$ we have $\frac{m!}{l!} \geq l^{m-l}$ (if $m =l$ then both sides are $1$, if $m>l$ then this follows from $m(m-1) \dotsb (m-l+1) \geq l^{m-l}$ and if $m < l$ this follows from $l^{l-m} \geq l(l-1) \dotsb (l-m+1)$).  Applying this inequality to the last product above 
yields
\begin{align*}
\frac{\probability{(\xi_1, \dotsc, \xi_n) \in \typeclass{\mu}}}{\probability{(\xi_1, \dotsc, \xi_n) \in \typeclass{\nu}}} 
&\geq \prod_{a \in \Sigma_{\nu}}  n^{n (\nu(a) - \mu(a))} = n^{n \sum_{a \in \Sigma_{\nu}} (\nu(a) - \mu(a))} = 1
\end{align*}
and the claim is proved.

From the claim, for $\nu \in \types{n}$ we have by Lemma \ref{ProbabilityOfAType} and Lemma \ref{SizeOfAType}
\begin{align*}
1 &= \sum_{\mu \in \types{n}} \probability{(\xi_1,\dotsc, \xi_n) \in \typeclass{\mu}} 
\leq \card{\types{n}}  \probability{(\xi_1,\dotsc, \xi_n) \in \typeclass{\nu}} \\
&=\card{\types{n}} \sum_{y \in \typeclass{\nu}} \probability{(\xi_1,\dotsc, \xi_n) = y} 
= \card{\types{n}} \card{\typeclass{\nu}} e^{-n \shannon{\nu}} \\
&\leq (n+1)^{\card{\Sigma}} \card{\typeclass{\nu}} e^{-n \shannon{\nu}}
\end{align*}
\end{proof}

TODO: Alternative proof using Stirling's formula

\begin{lem}\label{ProbabilityOfATypeClass}Let $\xi_1, \xi_2, \dotsc$ be i.i.d. random elements in $\Sigma$ with $\law{\xi_j} = \mu$ then for any $\nu \in \types{n}$ we have
\begin{align*}
(n+1)^{-\card{\Sigma}} e^{-n \kldiv{\nu}{\mu}} &\leq \probability{(\xi_1, \dotsc,\xi_n) \in \typeclass{\nu}} \leq e^{-n \kldiv{\nu}{\mu}}
\end{align*}
\end{lem}
\begin{proof}
From Lemma \ref{ProbabilityOfAType} we get
\begin{align*}
\probability{(\xi_1, \dotsc,\xi_n) \in \typeclass{\nu}} 
&= \sum_{y \in \typeclass{\nu}} \probability{(\xi_1, \dotsc,\xi_n) = y} 
= \card{\typeclass{\nu}} e^{-n(\shannon{\nu} +\kldiv{\nu}{\mu})}
\end{align*}
Now by Lemma \ref{SizeOfATypeClass} we conclude 
\begin{align*}
(n+1)^{-\card{\Sigma}} e^{-n\kldiv{\nu}{\mu}}  
&= (n+1)^{-\card{\Sigma}} e^{n \shannon{\nu}} e^{-n(\shannon{\nu} +\kldiv{\nu}{\mu})} 
\leq \probability{(\xi_1, \dotsc,\xi_n) \in \typeclass{\nu}} \\
&\leq e^{n \shannon{\nu}} e^{-n(\shannon{\nu} +\kldiv{\nu}{\mu})} = e^{-n\kldiv{\nu}{\mu}}
\end{align*}
\end{proof}

\begin{thm}\label{SanovFiniteAlphabets}Let $(\Sigma, 2^\Sigma)$ be a finite measure space, let $\xi_1, \xi_2, \dotsc$ be i.i.d. random elements in $\Sigma$ with $\law{\xi_j} = \mu$ for all $j \in \naturals$ and let $\Gamma$ be a set of probability measures on $\Sigma$ then 
\begin{align*}
- \inf_{\nu \in \interior(\Gamma)} \kldiv{\nu}{\mu} 
&\leq \liminf_{n \to \infty} \frac{1}{n} \log \probability{\type{(\xi_1, \dotsc, \xi_n)} \in \Gamma} \\
&\leq \limsup_{n \to \infty} \frac{1}{n} \log \probability{\type{(\xi_1, \dotsc, \xi_n)} \in \Gamma} 
\leq - \inf_{\nu \in \Gamma} \kldiv{\nu}{\mu} \\
\end{align*}
\end{thm}
\begin{proof}
First note that by  Lemma \ref{ProbabilityOfATypeClass} and Lemma \ref{SizeOfAType}
\begin{align*}
\probability{\type{(\xi_1, \dotsc, \xi_n)} \in \Gamma}
&= \sum_{\nu \in \Gamma \cap \types{n}} \probability{\type{(\xi_1, \dotsc, \xi_n)} = \nu }
\leq \sum_{\nu \in \Gamma \cap \types{n}} e^{-n \kldiv{\nu}{\mu}} \\
&\leq \card{\Gamma \cap \types{n}} e^{-n \inf_{\nu \in \Gamma \cap \types{n}} \kldiv{\nu}{\mu}} \\
&\leq (n+1)^{\card{\Sigma}} e^{-n \inf_{\nu \in \Gamma \cap \types{n}} \kldiv{\nu}{\mu}} \\
\end{align*}
and by  Lemma \ref{ProbabilityOfATypeClass} and the non-negativity of the terms $ e^{-n \kldiv{\nu}{\mu}}$
we get
\begin{align*}
\probability{\type{(\xi_1, \dotsc, \xi_n)} \in \Gamma}
&= \sum_{\nu \in \Gamma \cap \types{n}} \probability{\type{(\xi_1, \dotsc, \xi_n)} = \nu }
\geq \sum_{\nu \in \Gamma \cap \types{n}} (n+1)^{-\card{\Sigma}} e^{-n \kldiv{\nu}{\mu}} \\
&\geq (n+1)^{-\card{\Sigma}} e^{-n \inf_{\nu \in \Gamma \cap \types{n}} \kldiv{\nu}{\mu}}
 \end{align*}
By L'Hopital's rule
\begin{align*}
\lim_{n \to \infty} \frac{\log (n+1)^{\pm\card{\Sigma}}}{n} = \pm \card{\Sigma} \lim_{n \to \infty} \frac{1}{n+1} =0
\end{align*}
and therefore
\begin{align*}
\limsup_{n \to \infty} \frac{1}{n} \log \probability{\type{(\xi_1, \dotsc, \xi_n)} \in \Gamma} 
&\leq \limsup_{n \to \infty} (-\inf_{\nu \in \Gamma \cap \types{n}} \kldiv{\nu}{\mu}) \\
&\leq \limsup_{n \to \infty} (-\inf_{\nu \in \Gamma} \kldiv{\nu}{\mu})
=-\inf_{\nu \in \Gamma} \kldiv{\nu}{\mu}
\end{align*}
which gives the upper bound.

For the lower bound we start with
\begin{align*}
\liminf_{n \to \infty} \frac{1}{n} \log \probability{\type{(\xi_1, \dotsc, \xi_n)} \in \Gamma} 
&\geq \liminf_{n \to \infty} (-\inf_{\nu \in \Gamma \cap \types{n}} \kldiv{\nu}{\mu}) \\
&= -\limsup_{n \to \infty} (\inf_{\nu \in \Gamma \cap \types{n}} \kldiv{\nu}{\mu})
\end{align*}
Note that if there is no $\nu \in \interior(\Gamma)$ such that $\nu \ll \mu$ then $-\inf_{\nu \in \interior(\Gamma)} \kldiv{\nu}{\mu}= -\infty$
and the lower bound is trivially true.  Thus we may assume that such a $\nu$ exists and in this case
$-\inf_{\nu \in \interior(\Gamma)} \kldiv{\nu}{\mu} = -\inf_{\substack{\nu \in \interior(\Gamma) \\ \nu \ll \mu}} \kldiv{\nu}{\mu}$.
Suppose we are given a $\nu \in \interior(\Gamma)$ such that $\nu \ll \mu$.
There exists a $\delta >0$ such that $\norm{\nu^\prime - \nu} < \delta$ implies $\nu^\prime \in \Gamma$ (TODO: We need the fact that
we are using the total variation metric on the space of probability measures; for finite alphabets it shouldn't matter).  By Lemma ??? we know that we
can find a sequence $\nu_n$ with $\nu_n \in \types{n}$ and $\lim_{n \to \infty} \norm{\nu_n - \nu} = 0$.  In particular, eventually $\nu_n \in \interior(\Gamma)$.  Furthermore since $\kldiv{\nu}{\mu} < \infty$ and the relative entropy is continuous with respect to the total variation we know that 
eventually $\kldiv{\nu_n}{\mu} < \infty$ as well.
\begin{align*}
- \limsup_{n \to \infty} \inf_{\nu^\prime \in \Gamma \cap \types{n}} \kldiv{\nu^\prime}{\mu} 
&\geq - \limsup_{n \to \infty} \kldiv{\nu_n}{\mu} = -\kldiv{\nu}{\mu}
\end{align*}
from which it follows that
\begin{align*}
\liminf_{n \to \infty} \frac{1}{n} \log \probability{\type{(\xi_1, \dotsc, \xi_n)} \in \Gamma} 
&\geq \sup_{\substack{\nu \in \interior(\Gamma)\\ \nu \ll \mu}} -\kldiv{\nu}{\mu} = -\inf_{\nu \in \interior(\Gamma)} \kldiv{\nu}{\mu}
\end{align*}
\end{proof}

With Sanov's Theorem in hand we can derive a large deviation principle for empirical means.  We reiterate that we are deviating from the historical order in
that large deviation principles for empirical means we developed prior to large deviation principles for empirical measures.

\begin{thm}\label{CramersFiniteAlphabet}Let $(\Sigma,2^\Sigma)$ be a finite measure space and let $\xi_1, \xi_2, \dotsc$ be an i.i.d. sequence of random elements
in $\Sigma$ with $\law{\xi_j} = \mu$ for all $j \in \naturals$.  Let $f : \Sigma \to \reals$ be a function, let $\eta_j = f(\xi_j)$ for all $j \in \naturals$ and
\begin{align*}
\hat{S}_n &= \frac{1}{n} \sum_{j=1}^n \eta_j
\end{align*}
If 
\begin{align*}
I(x) &= \inf \lbrace \kldiv{\nu}{\mu} \mid \sexpectation{f}{\nu}=x \rbrace
\end{align*}
then for every $A \subset \reals$
\begin{align*}
-\inf_{x \in \interior(A)} I(x) 
&\leq \liminf_{n \to \infty} \frac{1}{n} \log \probability{\hat{S}_n \in A} \\
&\leq \limsup_{n \to \infty} \frac{1}{n} \log \probability{\hat{S}_n \in A} 
\leq -\inf_{x \in A} I(x)\\
\end{align*}
Moreover, $I(x)$ is continuous on the interval $K=[\min_{a \in \Sigma}f(a), \max_{a \in \Sigma} f(a)]$ and 
\begin{align*}
I(x) &= \sup_{-\infty < \lambda < \infty} (\lambda x - \Lambda(\lambda)) \text{ for $x \in K$}
\end{align*}
where 
\begin{align*}
\Lambda(\lambda) &= \log \sexpectation{e^{\lambda f}}{\mu}
\end{align*}
\end{thm}
\begin{proof}
Given $\sexpectation{f}{\cdot}$ is a continuous map from the 

\begin{clm}For every $\lambda, x \in \reals$
\begin{align*}
\lambda x - \Lambda(\lambda) &\leq \inf_{\lbrace \nu \mid \sexpectation{f}{\nu}=x \rbrace} \kldiv{\nu}{\mu} = I(x)
\end{align*}
and equality holds when $x = \sexpectation{f}{\nu_\lambda}$ where  $\nu_\lambda(a) = \mu(a) e^{\lambda f(a) - \Lambda(\lambda)}$. 
\end{clm}
Now by the strict concavity of $\log$ and Jensen's Inequality
\begin{align*}
\log \sum_{a \in \Sigma} \mu(a) e^{\lambda f(a)} 
&=\log \sum_{a \in \Sigma} \nu(a) \frac{\mu(a)}{\nu(a)} e^{\lambda f(a)} 
\geq \sum_{a \in \Sigma} \nu(a) \log \frac{\mu(a)}{\nu(a)} e^{\lambda f(a)} \\
&=\lambda \sexpectation{f}{\nu} - \kldiv{\nu}{\mu}
\end{align*}
and equality holds if and only if $\nu(a) > 0$ implies
\begin{align*}
\frac{\mu(a)}{\nu(a)} e^{\lambda f(a)} &= \sexpectation{\frac{\mu(a)}{\nu(a)} e^{\lambda f(a)}}{\nu}
=\sexpectation{e^{\lambda f(a)}}{\mu} = e^{\Lambda(\lambda)}
\end{align*}
which is to say we have equality precisely at the probability measure $\nu_\lambda(a) = \mu(a) e^{\lambda f(a) - \Lambda(\lambda)}$. 
If we pick $-\infty < x < \infty$ then by considering the above inequality for all $\nu$ with $\sexpectation{f}{\nu}=x$ the
claim follows.

Now we need to get a handle on the conditions under which
$x = \sexpectation{f}{\nu_\lambda}$.  First note that
\begin{align*}
\Lambda^\prime(\lambda) &=\frac{d}{d\lambda} \log \sum_{a \in \Sigma} \mu(a) e^{\lambda f(a)} 
=\frac{\sum_{a \in \Sigma} \mu(a) f(a) e^{\lambda f(a)}}{\sum_{a \in \Sigma} \mu(a) e^{\lambda f(a)}}
=\sum_{a \in \Sigma} \mu(a) f(a) e^{\lambda f(a) - \Lambda(\lambda)} 
= \sexpectation{f}{\nu_\lambda}
\end{align*}
so by the prior claim we have $\lambda x - \Lambda(\lambda) = I(x)$ for all $x \in \lbrace \Lambda^{\prime}(\lambda) \mid -\infty < \lambda < \infty \rbrace$.
Observe that 
\begin{align*}
\Lambda^{\prime \prime}(\lambda) &= 
\frac{\sexpectation{e^{\lambda f}}{\mu}\sexpectation{f^2 e^{\lambda f}}{\mu} - \sexpectation{f e^{\lambda f}}{\mu}^2}{\sexpectation{e^{\lambda f}}{\mu}^2}
\end{align*}
and by Cauchy-Schwartz
\begin{align*}
\sexpectation{f e^{\lambda f}}{\mu}^2 &\leq \sexpectation{\abs{f} e^{\lambda f}}{\mu}^2 \leq \sexpectation{f^2 e^{\lambda f}}{\mu}\sexpectation{e^{\lambda f}}{\mu}
\end{align*}
which shows that $\Lambda^\prime(\lambda)$ is non-decreasing.  Let $a_- = \argmin_{a \in \Sigma} f(a)$ and $a_+ = \argmax_{a \in \Sigma} f(a)$ and that
\begin{align*}
\lim_{\lambda \to \infty} \nu_\lambda(a) &= \mu(a) \lim_{\lambda \to \infty} \frac{1}{\sum_{a^\prime \in \Sigma}\mu(a^\prime) e^{\lambda(f(a^\prime) - f(a))} }= \delta_{a_+}
\end{align*}
and
\begin{align*}
\lim_{\lambda \to -\infty} \nu_\lambda(a) &= \mu(a) \lim_{\lambda \to -\infty} \frac{1}{\sum_{a^\prime \in \Sigma} \mu(a^\prime) e^{\lambda(f(a^\prime) - f(a))} } = \delta_{a_-}
\end{align*}
hence $\lim_{\lambda \to \pm \infty} \Lambda^\prime(\lambda) = \lim_{\lambda \to \pm \infty}  \sexpectation{f}{\nu_\lambda} = f(a_\pm)$.  Since $\Lambda^\prime$ is continuous and non-decreasing it follows that $(f(a_-), f(a_+)) \subset \lbrace \Lambda^{\prime}(\lambda) \mid -\infty < \lambda < \infty \rbrace$ and thus
$\lambda x - \Lambda(\lambda) = I(x)$ on $(f(a_-), f(a_+))$.  Consider the endpoints $a_\pm$ and compute using $\sexpectation{f}{\delta_{a_\pm}} = f(a_\pm)$
and the claim to see
\begin{align*}
-\log \mu(a_\pm) &= \kldiv{\delta_{a_\pm}}{\mu} \geq I(f(a_\pm)) \geq \sup_{-\infty < \lambda < \infty} \lambda f(a_\pm) - \Lambda(\lambda) \\
&\geq \lim_{\lambda \to \pm\infty} \lambda f(a_\pm) - \Lambda(\lambda) = \lim_{\lambda \to \pm\infty} \log \frac{e^{\lambda f(a_\pm)}}{\sexpectation{e^{\lambda f}}{\mu}}\\
&= \lim_{\lambda \to \pm\infty} \log \frac{1}{\sum_{a\in \Sigma} \mu(a) e^{\lambda(f(a) - f(a_\pm))} }= - \log \mu(a_\pm)
\end{align*}
\end{proof}

\section{Cramer's Theorem}
We now extend large deviations to random vectors.  We first handle the case of large deviations of empirical means of a random variable: a result known as Cramer's Theorem.

\begin{lem}\label{CumulantGeneratingFunctionReals}Let $\xi$ be a random variable, let $\Lambda_\xi(\lambda) = \log \expectation{e^{\lambda \xi}}$ be the cumulant generating function and let $\Lambda_\xi^*(x) = \sup_{\lambda \in \reals} \lbrace \lambda x - \Lambda_\xi(\lambda) \rbrace$ be the Legendre-Fenchel transform of the cumulant generating function.  The
\begin{itemize}
\item[(i)] The range of $\Lambda_\xi$ is $(-\infty, \infty]$, $\Lambda_\xi(0) = 0$ and $\Lambda_\xi$ is a convex function.  Moreover $\Lambda_\xi(\lambda) = \Lambda_{-\xi}(-\lambda)$.
\item[(ii)] $\Lambda_\xi^*$ is convex, $0 \leq \Lambda_\xi^*(x) \leq \infty$ and $\Lambda_\xi^*$ is lower semicontinuous.  Moreover $\Lambda_\xi^*(x) = \Lambda_{-\xi}^*(-x)$.
\item[(iii)] If $\Lambda_\xi(\lambda) = \infty$ for all $\lambda \neq 0$ then $\Lambda_\xi^* \equiv 0$.
\item[(iv)] If $\Lambda_\xi(\lambda) < \infty$ for some $\lambda > 0$ then $-\infty \leq \expectation{\xi} < \infty$.  When $-\infty \leq \expectation{\xi} < \infty$ we have
\begin{align*}
\Lambda_\xi^*(x) &= \sup_{0 \leq \lambda < \infty} \lbrace \lambda x - \Lambda_\xi(\lambda) \rbrace \text{ for all $x \geq \expectation{\xi}$}
\end{align*}
and moreover $\Lambda_\xi^*$ is non-decreasing on $(\expectation{\xi}, \infty)$.
\item[(v)] If $\Lambda_\xi(\lambda) < \infty$ for some $\lambda < 0$ then $-\infty < \expectation{\xi} \leq \infty$.  When $-\infty < \expectation{\xi} \leq \infty$ we have
\begin{align*}
\Lambda_\xi^*(x) &= \sup_{-\infty < \lambda \leq 0} \lbrace \lambda x - \Lambda_\xi(\lambda) \rbrace \text{ for all $x \leq \expectation{\xi}$}
\end{align*}
and moreover $\Lambda_\xi^*$ is non-increasing on $(-\infty, \expectation{\xi})$.
\item[(vi)] When $-\infty < \expectation{\xi} < \infty$ then $\Lambda_\xi^*(\expectation{\xi}) = 0$ and in all cases $\inf_{-\infty < x < \infty} \Lambda_\xi^*(x) = 0$.
\item[(vii)]$\Lambda_\xi$ is differentiable on the interior of $\domain{\Lambda_\xi} = \lbrace \lambda \mid \Lambda_\xi(\lambda) < \infty \rbrace$ and
\begin{align*}
\Lambda_\xi^\prime(\lambda) &= \frac{\expectation{\xi e^{\lambda \xi}}} {\expectation{e^{\lambda \xi}}}
\end{align*}
moreover $\Lambda_\xi^\prime(\lambda) = y$ implies $\Lambda_\xi^*(y) = \lambda y - \Lambda_\xi(\lambda)$.
\end{itemize}
\end{lem}
\begin{proof}
To see (i) we note that $0 < \expectation{e^{\lambda \xi}} \leq \infty$ and therefore $-\infty < \Lambda_\xi(\lambda) \leq \infty$.  Clearly, $\Lambda_\xi(0) = \log \expectation{1} = 0$ and given $\lambda, \eta \in \reals$, $0 < \theta < 1$ by
H\"{o}lder's inequality with $p = \theta^{-1}$ and $q=(1-\theta)^{-1}$ we get
\begin{align*}
\Lambda_\xi(\theta \lambda + (1-\theta) \eta) &= \log \expectation{e^{(\theta \lambda + (1-\theta) \eta) \xi}} = \log \expectation{(e^{\lambda \xi})^\theta (e^{\eta \xi})^{1-\theta}} \\
&\leq \log \expectation{e^{\lambda \xi}}^\theta \expectation{e^{\eta \xi}}^{1-\theta} = \log \theta\expectation{e^{\lambda \xi}} + (1-\theta) \log \expectation{e^{\eta \xi}} \\
&= \theta \Lambda_\xi(\lambda) + (1-\theta) \Lambda_\xi(\eta)
\end{align*}
and therefore $\Lambda_\xi$ is convex.  Trivially, 
\begin{align*}
\Lambda_\xi(\lambda) &= \log \expectation{e^{\lambda \xi}} = \log \expectation{e^{(-\lambda)(- \xi)}} = \Lambda_{-\xi}(-\lambda) 
\end{align*}

To see (ii) we first note that $\Lambda_\xi^*$ is convex letting $x,y \in \reals$ and $0 < \theta < 1$
\begin{align*}
\Lambda_\xi^*(\theta x + (1-\theta)y) &= \sup_{-\infty < \lambda < \infty} \lbrace \lambda (\theta x + (1-\theta)y) - \Lambda_\xi(\lambda) \rbrace \\
&= \sup_{-\infty < \lambda < \infty} \lbrace \theta (\lambda x - \Lambda_\xi(\lambda) ) + (1-\theta)( \lambda y - \Lambda_\xi(\lambda)) \rbrace \\
&\leq \theta \sup_{-\infty < \lambda < \infty} \lbrace \lambda x - \Lambda_\xi(\lambda) \rbrace + (1-\theta) \sup_{-\infty < \lambda < \infty} \lbrace   \lambda y - \Lambda_\xi(\lambda) \rbrace \\
&=\theta \Lambda_\xi^*(x) + (1-\theta) \Lambda_\xi^*(y)
\end{align*}
As we have shown $\Lambda_\xi(0) = 0$ therefore for all $-\infty < x < \infty$, $\Lambda_\xi^*(x) \geq 0 \cdot x - \Lambda_\xi(0) = 0$.  To see the lower semicontinuity of $\Lambda_\xi^*$ suppose that $\lim_{n \to \infty} x_n = x$ and compute for every $-\infty < \lambda < \infty$,
\begin{align*}
\liminf_{n \to \infty} \Lambda_\xi^*(x_n) 
&= \liminf_{n \to \infty} \sup_{-\infty < \eta < \infty} \lbrace \eta x_n - \Lambda_\xi(\eta) \rbrace 
\geq \liminf_{n \to \infty} \lambda x_n - \Lambda_\xi(\lambda) \\
&= \lambda x - \Lambda_\xi(\lambda) 
\end{align*}
Now take the supremum over all $\lambda$ to see that 
\begin{align*}
\liminf_{n \to \infty} \Lambda_\xi^*(x_n)  &\geq \sup_{-\infty < \lambda < \infty} \lbrace \lambda x - \Lambda_\xi(\lambda)  \rbrace = \Lambda_\xi^*(x)
\end{align*}
Lastly we see that
\begin{align*}
\Lambda_{-\xi}^*(-x)  &= \sup_{-\infty < \lambda < \infty} \lbrace -\lambda x - \Lambda_{-\xi}(\lambda) \rbrace = \sup_{-\infty < \lambda < \infty} \lbrace -\lambda x - \Lambda_{\xi}(-\lambda) \rbrace \\
&= \sup_{-\infty < \lambda < \infty} \lbrace \lambda x - \Lambda_{\xi}(\lambda) \rbrace = \Lambda_{\xi}^*(x)  
\end{align*}

To see (iii) suppose that $\Lambda_\xi(\lambda) = \infty$ for all $\lambda \neq 0$ then for all $-\infty < x < \infty$, $\lbrace \lambda x - \Lambda_\xi(\lambda) \mid -\infty < \lambda < \infty \rbrace = \lbrace -\infty, 0 \rbrace$ and it follows that $\Lambda_\xi^*(x) = 0$.  

To see (iv), suppose that $\Lambda_\xi (\lambda) < \infty$ and $\lambda > 0$.  Letting $\xi_+ = \xi \maxop 0$ then by Lemma \ref{TailsAndExpectations} and Markov's Inequality \ref{MarkovInequality}
\begin{align*}
\expectation{\xi_+} &= \int_0^\infty \probability{\xi_+ > \eta} \, d\eta = \int_0^\infty \probability{\xi > \eta} \, d\eta \\
&=\int_0^\infty \probability{e^{\lambda\xi} > e^{\lambda\eta} }\, d\eta \leq \expectation{e^{\lambda \xi}} \int_0^\infty e^{-\lambda\eta} \, d\eta = \lambda^{-1} \expectation{e^{\lambda \xi}} < \infty
\end{align*}
which shows that $-\infty \leq \expectation{\xi} < \infty$.  If we assume that $\expectation{e^{\lambda \xi}} < \infty$ then
by Jensen's Inequality (Theorem \ref{Jensen}) we get 
\begin{align}\label{CumulantGeneratingFunctionReals:LowerBound}
\Lambda_\xi(\lambda) &= \log \expectation{e^{\lambda \xi}} \geq \expectation{\log  e^{\lambda \xi}}  = \lambda \expectation{\xi}
\end{align}
The inequality \eqref{CumulantGeneratingFunctionReals:LowerBound} trivially holds when $\expectation{e^{\lambda \xi}} = \infty$ and therefore holds for all $-\infty < \lambda < \infty$.  
We now jump ahead and verify the first part of (vi); if we assume $-\infty < \expectation{\xi} < \infty$ then \eqref{CumulantGeneratingFunctionReals:LowerBound}  implies
\begin{align*}
\Lambda_\xi^*(\expectation{\xi}) &= \sup_{-\infty < \lambda < \infty} \lbrace \lambda \expectation{\xi} - \Lambda_\xi(\lambda) \rbrace \leq 0
\end{align*}
from which we conclude $\Lambda_\xi^*(\expectation{\xi})=0$.

We return to (iv) and assume $-\infty \leq \expectation{\xi} < \infty$.  If  $-\infty = \expectation{\xi}$ then for every $\lambda < 0$ we have 
\begin{align*}
\infty &= \expectation{\lambda \xi} \leq \expectation{e^{\lambda \xi} -1} = \Lambda(\lambda) -1
\end{align*}
and therefore we get $\sup_{-\infty < \lambda < 0} \lbrace \lambda x - \Lambda(\lambda) \rbrace = -\infty$.  If $-\infty < \expectation{\xi} < \infty$
then for $x \geq \expectation{\xi}$ and $\lambda < 0$
\begin{align*}
\lambda x - \Lambda_\xi (\lambda) &\leq \lambda \expectation{\xi} - \Lambda_\xi(\lambda) \leq \Lambda_\xi^*( \expectation{\xi} ) = 0
\end{align*}
so that $\sup_{-\infty < \lambda < 0} \lbrace \lambda x - \Lambda_\xi (\lambda)  \rbrace \leq 0$.
In both cases, by the fact  that $0 \cdot x - \Lambda_\xi(0) = 0$ for every $-\infty < x < \infty$ we have $\sup_{0 \leq \lambda < \infty} \lbrace \lambda x - \Lambda_\xi(\lambda) \rbrace \geq 0$ 
and therefore we get
\begin{align*}
\Lambda_\xi^*(x) &= 0 \maxop \Lambda_\xi^*(x) = 0 \maxop \sup_{-\infty < \lambda < 0} \lbrace \lambda x - \Lambda_\xi(\lambda) \rbrace \maxop  \sup_{0 \leq \lambda < \infty} \lbrace \lambda x - \Lambda_\xi(\lambda) \rbrace \\
&= \sup_{0 \leq \lambda < \infty} \lbrace \lambda x - \Lambda_\xi(\lambda) \rbrace
\end{align*}
If $\expectation{\xi} < x \leq y < \infty$ then 
\begin{align*}
\Lambda_\xi^*(x) &= \sup_{0 \leq \lambda < \infty} \lbrace \lambda x - \Lambda_\xi(\lambda) \rbrace \leq \sup_{0 \leq \lambda < \infty} \lbrace \lambda y  - \Lambda_\xi(\lambda) \rbrace = \Lambda_\xi^*(y)
\end{align*}

To see (v) note that if $\Lambda_\xi(\lambda) < \infty$ for $\lambda < 0$ then $\Lambda_{-\xi}(-\lambda) < \infty$ for $-\lambda > 0$ hence applying (iv) to $-\xi$ we get
for $x \leq \expectation{\xi}$ 
\begin{align*}
\Lambda_\xi^*(x) &= \Lambda_{-\xi}^*(-x) = \sup_{0 \leq \lambda < \infty} \lbrace \lambda (-x) - \Lambda_{-\xi}(\lambda) \rbrace 
= \sup_{0 \leq \lambda < \infty} \lbrace -\lambda x - \Lambda_{\xi}(-\lambda) \rbrace \\
&= \sup_{-\infty < \lambda \leq 0 } \lbrace \lambda x- \Lambda_{\xi}(\lambda) \rbrace
\end{align*}
and $\Lambda_{-\xi}^*(x)=\Lambda_{\xi}^*(-x)$ is non-decreasing on $(-\expectation{\xi}, \infty)$ which is equivalent to $\Lambda_{\xi}^*(x)$ is non-increasing on $(-\infty, \expectation{\xi})$.

To see (vi) we have already shown that $-\infty < \expectation{\xi} < \infty$ implies $\Lambda^*(\expectation{\xi}) = 0$; to see that $\inf_{-\infty < x < \infty} \Lambda^*(x) = 0$ in general we consider a few cases.  Firstly if $\Lambda(\lambda) = \infty$ for all $\lambda \neq 0$ then we know that $\Lambda^* \equiv = 0$ so the result holds in this case.  It remains to consider the cases $\Lambda(\lambda) < \infty$ and $\expectation{\xi} = \pm \infty$.   Thus assume that $\expectation{\xi} = -\infty$ and that $\Lambda(\lambda) < \infty$ for 
some $\lambda \neq 0$.  Note first that we may assume that $\Lambda(\lambda) < \infty$ for $\lambda > 0$ since by (v) we know that $\Lambda(\lambda) < \infty$ for $\lambda < 0$ implies $\expectation{\xi} > -\infty$.  For every $\lambda > 0$ we compute using the Markov Inequality
\begin{align*}
\log \probability{\xi \geq x} &= \log \probability{e^{\lambda \xi} \geq e^{\lambda x}} \leq \log \expectation{e^{\lambda \xi}} - \lambda x = \Lambda (\lambda) - \lambda x
\end{align*}
Take the infimum over all $\lambda > 0$ hence
\begin{align*}
\log \probability{\xi \geq x} &\geq \inf \lbrace \Lambda (\lambda) - \lambda x \mid 0 < \lambda < \infty \rbrace \geq \inf \lbrace \Lambda (\lambda) - \lambda x \mid 0 \leq \lambda  < \infty \rbrace\\
&= - \sup \lbrace \lambda x - \Lambda (\lambda) \mid 0 \leq \lambda < \infty \rbrace = -\Lambda^*(x)
\end{align*}
and therefore by the fact that $\Lambda^* \geq 0$ and continuity of measure (Lemma \ref{ContinuityOfMeasure}) we get
\begin{align*}
0 &\leq \lim_{x \to -\infty} \Lambda^*(x) \leq -\lim_{x \to -\infty} \log \probability{\xi \geq x} = 0
\end{align*}

To prove the first part of (vii) amounts to justifying exchanging the order of integration and differentiation.  Let $\lambda \in \interior(\domain{\Lambda_\xi})$.  Pick a $\delta >0$ such that $[\lambda - \delta, \lambda + \delta] \subset \interior(\domain{\Lambda_\xi})$.  By elementary calculus we have pointwise $\lim_{h \to 0} \frac{e^{(\lambda + h) \xi} - e^{\lambda \xi}}{h} = \xi e^{\lambda \xi}$.  Writing 
\begin{align*}
\abs{\frac{e^{(\lambda + h) \xi} - e^{\lambda \xi}}{h}} &=e^{\lambda \xi}\abs{\frac{e^{h \xi} - 1}{h}}
\end{align*}
and by differentiation and the estimate $1+x \leq e^x$
\begin{align*}
\frac{d}{dh} \frac{e^{h \xi} - 1}{h} &= \frac{h \xi e^{h \xi} - e^{h \xi} + 1}{h^2} \geq  \frac{(h \xi  -1)(h \xi + 1)+ 1}{h^2} = \xi^2
\end{align*}
and therefore $\frac{e^{h \xi} - 1}{h}$ is non-decreasing and therefore 
\begin{align*}
\sup_{-\delta \leq h \leq \delta} \abs{\frac{e^{h \xi} - 1}{h}} &= \abs{\frac{e^{-\delta \xi} - 1}{\delta}}  \maxop \abs{\frac{e^{\delta \xi} - 1}{\delta}} = \abs{\frac{e^{\delta \abs{\xi}} - 1}{\delta}}
\end{align*}
We also have
\begin{align*}
\expectation{e^{\lambda \xi}\abs{\frac{e^{\delta \abs{\xi}} - 1}{\delta}}} &=\expectation{e^{\lambda \xi}\left( \frac{e^{\delta \abs{\xi}} - 1}{\delta} \right)} =\frac{1}{\delta}\expectation{e^{\lambda \xi + \delta \abs{\xi}} - e^{\lambda \xi}} \\
&= \frac{1}{\delta} \left(\expectation{e^{(\lambda + \delta)\xi}; \xi \geq 0}  + \expectation{e^{(\lambda - \delta)\xi}; \xi < 0}- \expectation{e^{\lambda \xi}} \right ) \\
&\leq \frac{1}{\delta} \left (\expectation{e^{(\lambda + \delta)\xi}}  + \expectation{e^{(\lambda - \delta)\xi}} - \expectation{e^{\lambda \xi}} \right ) < \infty \\
\end{align*}
Now by Dominated Convergence applies and we conclude that $\frac{d}{d\lambda} \expectation{e^{\lambda \xi}} = \expectation{\xi e^{\lambda \xi}}$;  $\Lambda_\xi^\prime(\lambda) = \frac{\expectation{\xi e^{\lambda \xi}}}{\expectation{e^{\lambda \xi}}}$ follows from the Chain Rule.

Suppose that $\Lambda_\xi^\prime(\lambda) = y$.  The function $g(\eta) = \eta y - \Lambda_\xi(\eta)$ is concave and satisfies $g^\prime(\lambda) = 0$; it follows that $g(\lambda)$ is a global maximum (TODO: Where do we show this???) and therefore $\lambda y - \Lambda_\xi(\lambda) = \sup_{-\infty < \eta < \infty} \lbrace \lambda \eta - \Lambda_\xi(\eta)\rbrace = \Lambda_\xi^*(y)$.
\end{proof}

TODO: The proof of (i) here provides some motivation for the form of the rate function (see discussion in Stroock).
\begin{thm}[Cram\'{e}r's Theorem]\label{CramersTheoremOneDimension} Let $\xi, \xi_1, \xi_2, \dotsc$ be i.i.d. random variables then
\begin{itemize}
\item[(i)] For every closed set $F \subset \reals$ we have 
\begin{align*}
\limsup_{n \to \infty} \frac{1}{n} \log \expectation{\frac{1}{n} \sum_{j=1}^n \xi_j \in F} &\leq -\inf_{x \in F} \Lambda_\xi^*(x)
\end{align*}
\item[(i)] For every open set $G \subset \reals$ we have 
\begin{align*}
\liminf_{n \to \infty} \frac{1}{n} \log \expectation{\frac{1}{n} \sum_{j=1}^n \xi_j \in G} &\geq -\inf_{x \in G} \Lambda_\xi^*(x)
\end{align*}
\end{itemize}
\end{thm}
\begin{proof}
We start with (i).  Note that if either $F$ is empty or $\inf_{x \in F} \Lambda_\xi^*(x) = 0$ then (i) holds trivially so we may assume $F$ is non-empty and that $\inf_{x\in F} \Lambda_\xi^*(x) > 0$.  From Lemma \ref{CumulantGeneratingFunctionReals} it follows that 
\begin{itemize}
\item $\Lambda_\xi^*$ is not identically zero 
\item $\domain{\Lambda_\xi} \neq \emptyset$
\item Either $\expectation{\xi_+} <\infty$ and $\Lambda_\xi^*(x) = \sup_{\lambda \leq 0} \lbrace \lambda x - \Lambda(\lambda) \rbrace$ or $\expectation{\xi_-} <\infty$ and 
$\Lambda_\xi^*(x) = \sup_{\lambda \geq 0} \lbrace \lambda x - \Lambda(\lambda) \rbrace$
\end{itemize}
Let $-\infty < x < \infty$ and $\lambda \geq 0$ be given and suppose that $\expectation{\xi} < \infty$ then by Chernoff bounding and the independence of $\xi_n$ we get
\begin{align*}
\probability{\frac{1}{n} \sum_{j=1}^n \xi_j \geq x} &= \probability{e^{ \sum_{j=1}^n \xi_j} \geq e^{n\lambda x}} \leq e^{-n\lambda x} \expectation{e^{ \lambda \sum_{j=1}^n \xi_j}} \\
&=e^{-n\lambda x}  \prod_{j=1}^n\expectation{e^{ \lambda \xi_j}} = \expectation{e^{-n(\lambda x - \lambda \xi)}} 
\end{align*}
Taking the infimum of the right hand side over all $\lambda \geq 0$ we get
\begin{align*}
\probability{\frac{1}{n} \sum_{j=1}^n \xi_j \geq x} &\leq e^{-n \Lambda_\xi^*(x)}
\end{align*}
If assume that $\expectation{\xi} > -\infty$ and $\lambda \leq 0$ the the same argument yields $\probability{\frac{1}{n} \sum_{j=1}^n \xi_j \leq x} \leq \expectation{e^{-n(\lambda x - \lambda \xi)}}$ and
\begin{align*}
\probability{\frac{1}{n} \sum_{j=1}^n \xi_j \leq x} &\leq e^{-n \Lambda_\xi^*(x)}
\end{align*}

Now assume that $-\infty < \expectation{\xi} < \infty$.  From Lemma \ref{CumulantGeneratingFunctionReals} we know that $\Lambda_\xi^*(\expectation{\xi}) = 0$ and therefore $\expectation{\xi} \notin F$.  Let $(x_-,x_+)$ be the union of open intervals $(a,b) \subset F^c$ that contain $\expectation{\xi}$.  Since $F$ is nonempty either $x_-$ or $x_+$ is finite.  If $x_\pm$ is finite it also follows that $x_\pm \in F$ for otherwise since $F^c$ is open we could find a bigger open interval containing $\expectation{\xi}$.  In either case we have
$\Lambda_\xi^*(x_\pm) \geq \inf_{x \in F} \Lambda_\xi^*(x)$ and therefore by a union bound
\begin{align*}
\probability{\frac{1}{n} \sum_{j=1}^n \xi_j \in F} &\leq \probability{\frac{1}{n} \sum_{j=1}^n \xi_j \geq x_+} + \probability{\frac{1}{n} \sum_{j=1}^n \xi_j \leq x_-}  \\
&\leq e^{-n \Lambda_\xi^*(x_+)} + e^{-n \Lambda_\xi^*(x_-)} \leq 2 e^{-n \inf_{x \in F} \Lambda_\xi^*(x)}
\end{align*}
and 
\begin{align*}
\limsup_{n \to \infty} \frac{1}{n} \log \probability{\frac{1}{n} \sum_{j=1}^n \xi_j \in F} &\leq \limsup_{n \to \infty} \frac{1}{n} (\log 2 - n \inf_{x \in F} \Lambda_\xi^*(x)) = 
- \inf_{x \in F} \Lambda_\xi^*(x)
\end{align*}

If $\expectation{\xi} = -\infty$ then Lemma \ref{CumulantGeneratingFunctionReals} says that $\Lambda_\xi^*$ is nondecreasing and therefore $\lim_{x \to -\infty} \Lambda_\xi^*(x) = 0$.  Let $x_+ = \inf \lbrace x \mid x \in F \rbrace$ and observe that if $x_+ = -\infty$ it follows that $\inf_{x \in F} \Lambda_\xi^*(x) = 0$ which is contradiction.  Thus since $F$ is non-empty $-\infty < x_+ < \infty$ and since $F$ is closed $x_+ \in F$ and $\Lambda_\xi^*(x_+) \geq \inf_{x \in F} \Lambda_\xi^*(x)$.  
\begin{align*}
\probability{\frac{1}{n} \sum_{j=1}^n \xi_j \in F} &\leq \probability{\frac{1}{n} \sum_{j=1}^n \xi_j \geq x_+} \leq e^{-n \Lambda_\xi^*(x_+)} \leq e^{-n \inf_{x \in F} \Lambda_\xi^*(x)}
\end{align*}
and the upper bound follows.  If $\expectation{\xi} = \infty$ argue similarly.

TODO: Tie in the mechanism below with the technique of importance sampling discussed in Steele's development of Girsanov Theory.
\begin{clm}For every $\delta > 0$ we have
\begin{align*}
\liminf_{n \to \infty} \frac{1}{n} \log \probability{-\delta < \frac{1}{n} \sum_{j=1}^n \xi_j < \delta} &\geq \inf_{-\infty < \lambda < \infty} \Lambda_\xi(\lambda) = -\Lambda_\xi^*(0)
\end{align*}
\end{clm}
First we suppose that $\probability{\xi > 0}>0$, $\probability{\xi < 0}>0$ and there exists an $N$ such that $\probability{-N \leq \xi \leq N} =1$.  By continuity of measure there exists $\epsilon > 0$ such that $\probability{\xi > \epsilon}>0$ and $\probability{\xi < -\epsilon}>0$ and therefore
\begin{align*}
\lim_{\lambda \to \infty} \Lambda_\xi(\lambda) &= \lim_{\lambda \to \infty} \expectation{e^{\lambda \xi}} \geq \lim_{\lambda \to \infty} \expectation{e^{\lambda \xi}; \xi > \epsilon} \geq \probability{\xi > \epsilon} \lim_{\lambda \to \infty} e^{\lambda \epsilon} = \infty
\end{align*}
and
\begin{align*}
\lim_{\lambda \to -\infty} \Lambda_\xi(\lambda) &= \lim_{\lambda \to -\infty} \expectation{e^{\lambda \xi}} \geq \lim_{\lambda \to -\infty} \expectation{e^{\lambda \xi}; \xi < -\epsilon} \geq \probability{\xi <  -\epsilon} \lim_{\lambda \to -\infty} e^{-\lambda \epsilon} = \infty
\end{align*}
Since $\probability{\abs{\xi} \leq N}=1$ we have for all $-\infty < \lambda < \infty$
\begin{align*}
\Lambda(\lambda) &= \expectation{e^{\lambda \xi}} = \expectation{e^{\lambda \xi}; \abs{\xi} \leq N} \leq e^{\abs{\lambda} N} < \infty
\end{align*}
Thus $\domain{\Lambda} = \reals$ and applying Lemma \ref{CumulantGeneratingFunctionReals} we see that $\Lambda$ is continuous and differentiable on all of $\reals$ and 
$\Lambda^\prime(\lambda) = \expectation{\xi e^{\lambda \xi}}/\expectation{e^{\lambda \xi}}$.  By continuity of $\Lambda$ and the fact that $\lim_{\lambda \to \pm \infty} \Lambda(\lambda) = \infty$ we know there exists $\lambda_0$ such that $\Lambda(\lambda_0) = \inf_{-\infty < \lambda < \infty} \Lambda(\lambda)$ and the fact that $\Lambda$ is differentiable at $\lambda_0$ implies that $\Lambda^\prime(\lambda_0) = 0$.

Let $\mu = \mathcal{L}(\xi)$ and define $\tilde{\mu} = e^{\lambda_0 x - \Lambda_\xi(\lambda_0)} \cdot \mu$ so that
\begin{align*}
\tilde{\mu}(A) = \int_A e^{\lambda_0 x - \Lambda_\xi(\lambda_0)} \, \mu(dx)
\end{align*}
Since $\int e^{\lambda_0 x} \, \mu(dx) = \expectation{e^{\lambda_0 \xi}} = \Lambda_\xi(\lambda_0)$ it follows that $\tilde{\mu}$ is a probability measure.  Let $\eta, \eta_1, \eta_2, \dotsc$ be i.i.d. with $\mathcal{L}(\eta) = \tilde{\mu}$ and observe that
\begin{align*}
\expectation{\eta} &= \int x  e^{\lambda_0 x  - \Lambda_\xi(\lambda_0)} \, \mu(dx) = e^{-\Lambda_\xi(\lambda_0)} \expectation{\xi e^{\lambda_0 \xi}} = \Lambda^\prime(\lambda_0) = 0
\end{align*}
and $\eta$ is integrable since 
\begin{align*}
\expectation{\abs{\eta}} \leq  e^{-\Lambda_\xi(\lambda_0)} \expectation{\abs{\xi} e^{\lambda_0 \xi}} \leq e^{-\Lambda_\xi(\lambda_0)}  N e^{\abs{\lambda_0} N} < \infty
\end{align*}
Thus we can apply the Weak Law of Large Numbers Theorem \ref{WLLN} to $\eta$ to conclude
\begin{align*}
\lim_{n \to \infty} \probability { -\epsilon < \frac{1}{n} \sum_{j=1}^n \eta_j < \epsilon} = 1 \text{ for every $\epsilon > 0$}
\end{align*}
It remains to see what this implies about the normalized sums $\frac{1}{n} \sum_{j=1}^n \xi_j$.  We compute using the i.i.d. property of the $\xi_n$ and the fact that $-n \epsilon < y < n \epsilon$ implies $\lambda_0 y \leq \abs{\lambda_0} n \epsilon$ to see
\begin{align*}
\probability{-\epsilon < \frac{1}{n} \sum_{j=1}^n \xi_j < \epsilon} &= \int_{\abs{\sum_{j=1}^n x_j} < n \epsilon} \mu(dx_1) \dotsb \mu(dx_n) \\
&\geq \int_{\abs{\sum_{j=1}^n x_j} < n \epsilon} e^{-\abs{\lambda_0} n \epsilon + \lambda_0 \sum_{j=1}^n x_j} \mu(dx_1) \dotsb \mu(dx_n) \\
&=  e^{-\abs{\lambda_0} n \epsilon + n \Lambda_\xi(\lambda_0)} \int_{\abs{\sum_{j=1}^n x_j} < n \epsilon} e^{\sum_{j=1}^n (\lambda_0  x_j - \Lambda_\xi(\lambda_0))} \, \mu(dx_1) \dotsb \mu(dx_n) \\
&=  e^{-\abs{\lambda_0} n \epsilon + n \Lambda_\xi(\lambda_0)} \int_{\abs{\sum_{j=1}^n x_j} < n \epsilon} \tilde{\mu}(dx_1) \dotsb \tilde{\mu}(dx_n) \\
&=e^{-n(\abs{\lambda_0} \epsilon + \Lambda_\xi(\lambda_0))} \probability{-\epsilon < \frac{1}{n} \sum_{j=1}^n \eta_j < \epsilon}
\end{align*}
Therefore for every $0 < \epsilon < \delta$ we have
\begin{align*}
\liminf_{n \to \infty} \frac{1}{n} \log \probability { -\delta < \frac{1}{n} \sum_{j=1}^n \xi_j < \delta} 
&\geq \liminf_{n \to \infty} \frac{1}{n} \log \probability { -\epsilon < \frac{1}{n} \sum_{j=1}^n \xi_j < \epsilon} \\
&\geq \liminf_{n \to \infty} \frac{1}{n} \log e^{-n(\abs{\lambda_0} \epsilon + \Lambda_\xi(\lambda_0))} \probability{-\epsilon < \frac{1}{n} \sum_{j=1}^n \eta_j < \epsilon} \\
&=-\abs{\lambda_0} \epsilon + \Lambda_\xi(\lambda_0) + \liminf_{n \to \infty} \probability{-\epsilon < \frac{1}{n} \sum_{j=1}^n \eta_j < \epsilon} \\
&= -\abs{\lambda_0} \epsilon + \Lambda_\xi(\lambda_0) 
\end{align*}
Taking the limit as $\epsilon \to 0$ we see that $\liminf_{n \to \infty} \frac{1}{n} \log \probability { -\delta < \frac{1}{n} \sum_{j=1}^n \xi_j < \delta} = \Lambda_\xi(\lambda_0)$ and for
this special case the claim is proven.

TODO: Show the claim implies the lower bound and complete the proof of the claim.
\end{proof}

Before attacking the generalization of Cram\'{e}r's Theorem to $\reals^d$ we need to spend some time to arm ourselves with a few technical tools.

Unboundedness of rate functions can be a technical impediment in certain proof scenarios.  The following proposition shows that for proving upper bounds we can reduce to the bounded case by giving up positivity; this is often a good tradeoff.
\begin{prop}$\lim_{\delta \to 0} \inf_{x \in \Gamma} (I(x) - \delta) \minop \frac{1}{\delta} = \inf_{x \in \Gamma} I(x)$.
\end{prop}
\begin{proof}
Suppose $\inf_{x \in \Gamma} I(x) = \infty$ (i.e. $I \equiv \infty$) then if follows that $\inf_{x \in \Gamma} (I(x) - \delta) \minop \frac{1}{\delta} = \frac{1}{\delta}$ and clearly the result holds.  Similarly if $\inf_{x \in \Gamma} I(x) = -\infty$ then we may find $x_n \in \Gamma$ such that $I(x_n) < -n$ for $n \in \naturals$ and therefore $\inf_{x \in \Gamma} (I(x) - \delta) \minop \frac{1}{\delta} = -\infty$ for all $\delta > 0$.  

So we may assume $-\infty < \inf_{x \in \Gamma} I(x) < \infty$ which implies $\inf_{x \in \Gamma} (I(x) - \delta) \minop \frac{1}{\delta}$ is finite for all $\delta>0$.  From the fact that $ (I(x) - \delta) \minop \frac{1}{\delta} < I(x)$ for all $\delta > 0$ it follows that $\inf_{x \in \Gamma} (I(x) - \delta) \minop \frac{1}{\delta} \leq \inf_{x \in \Gamma} I(x)$ for all $\delta > 0$ and thus $\lim_{\delta \to 0} \inf_{x \in \Gamma} (I(x) - \delta) \minop \frac{1}{\delta} \leq \inf_{x \in \Gamma} I(x)$.  Also $(I(x) - \delta) \minop \frac{1}{\delta}$ is a non-decreasing function of $\delta>0$ for each $x \in \Gamma$ thus $C=\lim_{\delta \to 0} \inf_{x \in \Gamma} (I(x) - \delta) \minop \frac{1}{\delta}$ is defined, bounded above by $\inf_{x \in \Gamma} I(x)$ hence finite.  For every $0 < \epsilon < 1$ we may pick $0 < \delta < \epsilon/2 \min /frac{1}{C+1}$ such that $C - \epsilon/2 < \inf_{x \in \Gamma} (I(x) - \delta) \minop \frac{1}{\delta} \leq C$.  Now pick $x \in \Gamma$ such that $C- \epsilon/2 < (I(x) - \delta) \minop \frac{1}{\delta} < C + \epsilon/2$ which by our choice of $\delta$ is equivalent to $C- \epsilon/2 < I(x) - \delta < C + \epsilon/2$.  Thus $I(x) < C + \epsilon$ hence $\inf_{x \in \Gamma} < C + \epsilon$.  Now let $\epsilon \to 0$.
\end{proof}

The most primitive tool for bounding probabilities is the union bound.  When proving large deviation results we often need the following logarithmic union bound.
\begin{prop}\label{LogarithmicUnionBound}Let $(\Omega, \mathcal{A})$ be a measurable space, $\mu_\epsilon$ be a family of probability measures and let $A_1, A_2, \dotsc, A_n$ be measurable sets then 
\begin{align*}
\limsup_{\epsilon \to 0} \epsilon \log \mu_\epsilon( \cup_{j=1}^n A_j) &\leq \max_{1 \leq j\leq n} \limsup_{\epsilon \to 0 } \epsilon \log \mu_\epsilon (A_j)
\end{align*}
\end{prop}
\begin{proof}
By a union bound and the fact that $\log$ is increasing we have $\limsup_{\epsilon \to 0} \epsilon \log \mu_\epsilon( \cup_{j=1}^n A_j)  \leq \limsup_{\epsilon \to 0} \epsilon \log \sum_{j=1}^n\mu_\epsilon( A_j)$; we just need to bound the right hand side.
Note that for every $1 \leq k \leq n$ since $\log$ is increasing
\begin{align*}
\log \mu_\epsilon(A_k) &\leq \log \sum_{j=1}^n \mu_\epsilon(A_j) \leq \log n \max_{1 \leq j \leq n} \mu_\epsilon(A_j) \\
&=\log n + \max_{1 \leq j \leq n} \log \mu_\epsilon(A_j) 
\end{align*}
Multiplying by $\epsilon$, taking the limit and using the fact that $\limsup (f \maxop g) = \limsup f \maxop \limsup g$ we get 
\begin{align*}
\max_{1 \leq j \leq n}  \limsup_{\epsilon \to 0} \epsilon \log \mu_\epsilon(A_j) &= \limsup_{\epsilon \to 0} \epsilon \max_{1 \leq j \leq n}  \log \mu_\epsilon(A_j) 
= \limsup_{\epsilon \to 0} \epsilon \log \sum_{j=1}^n \mu_\epsilon(A_j)
\end{align*}
\end{proof}


TODO: Example of weak LDP for which there is no LDP 

\begin{defn}Let $(\Omega, \mathcal{A})$ be a topological measurable space such that every compact set is measurable and $\mu_\epsilon$ be a family of probability measures. We say that $\mu_\epsilon$ is \emph{exponentially tight} if for every $0 < \alpha < \infty$ there exists a compact set $K_\alpha$ such that
\begin{align*}
\limsup_{\epsilon \to 0} \epsilon \log \mu_\epsilon(K_\alpha^c) < - \alpha
\end{align*}
\end{defn}