\chapter{Gaussian Processes}

In so far as boundedness and sample continuity make the basic strategy clear.  View the isonormal process $L$ as the ``universal'' Gaussian process such that all Gaussian processes are constructed by restricting $L$ to subsets $C \subset H$.  Now to study an arbitrary Gaussian process, study the subset $C \subset H$ (in particular study its ``size'' in an appropriate sense).

\section{Gaussian Random Variables}

\subsection{Gaussians and Convexity}

\begin{lem}\label{InteriorsOfFiniteDimensionalSimplex}Let $v_0, \dotsc, v_d \in \reals^d$ be such that $v_1 - v_0, \dotsc, v_d - v_0$ are linearly independent then
$\convexhull{v_0, \dotsc, v_d}$ has non empty interior.
\end{lem}
\begin{proof}
Consider the point $w = \frac{1}{d+1} \sum_{j=0}^d v_j \in \convexhull{v_0, \dotsc, v_d}$.  

Define the affine map $A : \reals^d \to \reals^d$ given by $A e_j = v_0 + v_j$ for $j=1, \dotsc, d$.  
TODO: Finish
\end{proof}

\begin{prop}\label{InteriorsOfFiniteDimensionalConvexSets}A convex set in $A \subset \reals^d$ is either contained in a hyperplane or has non-empty interior.
\end{prop}
\begin{proof}
Pick a point $v \in A$ and let $V$ be the linear span of $\lbrace w - v \mid w \in A \rbrace$.  Suppose that $V \neq \reals^d$ then there exists a non-zero $z \in \reals^d$ such that $z \perp V$.  Then if $w \in A$ we know that $\langle z, w - v \rangle = 0$ hence $w \in v + \lbrace z \rbrace^{\perp}$.  

Therefore we may assume that $V = \reals^d$ and therefore there exist $w_1, \dotsc, w_d$ such that $w_1 -v, \dotsc, w_d - v$ are linearly independent; apply Lemma \ref{InteriorsOfFiniteDimensionalSimplex}.
\end{proof}

\begin{prop}\label{LebesgueMeasurabilityOfConvexSets}Any convex subset  $A \subset \reals^d$ is Lebesgue measurable and moreover $\lambda^d(\boundary{A}) = 0$.
\end{prop}
\begin{proof}
TODO:
\end{proof}

\section{Gaussian Processes}

Recall the definition of Gaussian processes Definition \ref{defn:GaussianProcess} and the fact that the distribution of a Gaussian process $X$ is uniquely 
determined by the first and second moments $\expectation{X_t}$ and $\expectation{X_s X_t}$ (Lemma \ref{GaussianProcessMoments}).  It is easy to use 
the Daniell-Kolmogorov extension theorem to see that Gaussian processes always exist. 

\begin{prop}\label{ExistenceGaussianProcess}Let $T$ be a set, $\mu : T \to \reals$ and $C : T \times T \to \reals$ be functions such that $C$ is symmetric and for every finite subset $I \subset T$ the restriction $C_I : I \times I \to \reals$ is positive semi-definite, then there exists a Gaussian process $X$ such that $\expectation{X_t} = \mu(t)$ and $\expectation{X_s X_t} = C(s,t)$.
\end{prop}
\begin{proof}
For each finite subset $I = \lbrace t_1, \cdots, t_d \rbrace \subset T$ we let $\mu_I = (\mu(t_1), \cdots, \mu(t_d))$ and $Q_I$ be $d \times d$ matrix $Q_I(i,j) = C(t_i,t_j)$.  By assumption $Q_I$ is positive semidefinite and therefore there is an $N(\mu_I, Q_I)$ random vector on $\reals^I$.  By construction and Example \ref{LinearTransformationGaussian} applied to the projection $\pi : \reals^I \to \reals^J$ it follows that this defines a projective family of probability measures.  Now since $\reals$ is Borel space we may apply Theorem \ref{DaniellKolmogorovExtension}.
\end{proof}

\begin{defn}\label{defn:JointlyGaussianProcesses}Let $X^1, \dotsc, X^n$ be stochastic processes indexed by sets  $T_1, \dotsc, T_n$ respectively and define the disjoint union $T=\lbrace (j,t) \mid 1 \leq j \leq n \text { and } t \in T_j \rbrace$. The $X^1, \dotsc, X^n$ are \emph{jointly Gaussian} if the process  $X$ indexed by $T$ and defined by $X_{(j,t)} = X^j_t$ is Gaussian.
\end{defn}

\begin{prop}\label{IndependentGaussianProcessesAreJointlyGaussian}Let $X^1, \dotsc, X^n$ be independent Gaussian processes indexed by sets  $T_1, \dotsc, T_n$, then $X^1, \dotsc, X^n$ are jointly Gaussian.
\end{prop}
\begin{proof}
Suppose we are given $m_1, \dotsc, m_n \in \naturals$, elements $t^j_1, \dotsc, t^j_{m_j} \in T_j$ and constants $a^j_1, \dotsc, a^j_{m_j} \in \reals$ for $1 \leq j \leq n$.  For each $1 \leq j \leq n$, since $X^j$ is a Gaussian process we know that $a^j_1 X^j_{t_1} + \dotsb + a^j_1 X^j_{t_{m_j}}$ is Gaussian random variable.  By independence of the processes $X^j$ we know that the Gaussian random variables $a^j_1 X^j_{t_1} + \dotsb + a^j_1 X^j_{t_{m_j}}$ are independent.  Therefore the sum 
is a Gaussian random variable by Example \ref{IndependentGaussiansAreJointlyGaussian}.
\end{proof}

The following extends Proposition \ref{GaussianIndependence}.
\begin{prop}\label{GaussianProcessIndependence}Let $X^1, \dotsc, X^n$ be jointly Gaussian processes indexed by $T_1, \dotsc, T_n$ then $X^1, \dotsc, X^n$ are independent if and only if $\scovariance{X^i_s}{X^j_t} = 0$ for all $1 \leq i < j  \leq n$, $s\in T_i$ and $t \in T_j$.
\end{prop}
\begin{proof}
If $X^1, \dotsc, X^n$ are independent then $\scovariance{X^i_s}{X^j_t} = \expectation{X^i_s - \expectation{X^i_s}}\expectation{X^j_t - \expectation{X^j_t}}=0$ by Lemma \ref{IndependenceExpectations}.

On the other hand, by Lemma \ref{IndependenceFinitary} it suffices to show that given finite sets $T^1 \subset T_1, \dotsc, T^n \subset T_n$, the Gaussian vectors $X^1_{T^1}, \dotsc, X^n_{T^n}$ are independent  this follows from Proposition \ref{IndependentGaussianProcessesAreJointlyGaussian} and Proposition \ref{GaussianIndependence} (actually we need the k-ary version of the latter).

TODO: Finish/more detail.
\end{proof}

\begin{defn}Let $\mu$ be a Borel probability measure on a Banach space $V$ then we say that $P$ is \emph{Gaussian} if $\pushforward{\lambda}{\mu}$ is a Gaussian measure on $\reals$ for every continuous linear functional $\lambda : V \to \reals$.
\end{defn}

\begin{defn}Let $H$ be a Hilbert space then an \emph{isonormal process} is a real valued process $X$ indexed by $H$ such that $\expectation{X_v} = 0$ for all $v \in H$ and $\scovariance{X_v}{X_w} = \langle v, w \rangle$ for all $v,w \in H$.
\end{defn}

In the finite dimensional case an isonormal process is trivially constructed.
\begin{prop}\label{IsonormalFiniteDimensional}Let $H$ be a finite dimensional Hilbert space, let $e_1, \dotsc, e_d$ be an orthonormal basis for $H$ and $\xi_1, \dotsc, \xi_d$ be i.i.d. $N(0,1)$ Gaussian random variables. Then 
\begin{align*}
L_v(\omega) &= \langle v, \sum_{j=1}^d \xi_j(\omega) e_j \rangle = \sum_{j=1}^d \langle v, e_j \rangle \xi_j
\end{align*} 
is an isonormal process on $H$.
\end{prop}
\begin{proof}
Clearly $\expectation{L_v} = \sum_{j=1}^d \langle v, e_j \rangle \expectation{\xi_j} = 0$.  To calculate the second moments,
\begin{align*}
\scovariance{L_v}{L_w}  &= \expectation{L_v L_w}  = \sum_{i=1}^d \sum_{j=1}^d \langle v, e_i \rangle \langle w, e_j \rangle \expectation{\xi_i \xi_j} = \sum_{i=1}^d \langle v, e_i \rangle \langle w, e_i \rangle = \langle v, w \rangle
\end{align*}
\end{proof}
The above construction of the isonormal process shows that one can think of it as a random projection : for a $v \in H$, take a random $w \in H$ from a Gaussian distribution and the take the length of the projection of $v$ onto $w$.  

In the infinite dimensional case one needs to appeal to a less elementary construction (e.g. using the Daniell-Kolmogorov Theorem).   Nonetheless it is interesting 
to consider the natural analogue to the finite dimensional construction in the separable infinite dimensional case.  If we let $e_1, e_2, \dotsc$ be an infinite orthonormal set in $H$ and $\xi_1, \xi_2, \dotsc$ be i.i.d. standard normal random variables, then one can consider the two expressions $\langle v, \sum_{j=1}^\infty \xi_j e_j \rangle$ and $\sum_{j=1}^\infty \langle v, e_j \rangle \xi_j$ (which are equivalent in the finite dimensional case).  As it turns out, the first expression is meaningless since $\sum_{j=1}^\infty \xi_j^2=\infty$ a.s. and therefore $\sum_{j=1}^\infty \xi_j e_j$ doesn't define an element of $H$.  On the other hand it turns out that $\sum_{j=1}^\infty \langle v, e_j \rangle \xi_j$ is meaningful.  In fact in the second expression is a valid definition of an isonormal process in the separable infinite dimensional Hilbert space case.  One side effect of this is that one can't make literal sense of the random projection interpretation of the isonormal process; however the interpretation is ``almost'' true and the intuition it provides can be helpful.

\begin{prop}\label{ExistenceIsonormalProcess}Let $H$ be a Hilbert space then an isonormal process on $H$ exists.
\end{prop}
\begin{proof}
This follows from Theorem \ref{ExistenceGaussianProcess} once we observe that for every $v_1, \dotsc, v_d$ the matrix $\langle v_i, v_j \rangle$ is positive semidefinite; this in turn follows by noting that for every $a \in \reals^d$ be have 
\begin{align*}
\sum_{i,j = 1}^d a_i a_j \langle v_i, v_j \rangle &= \langle \sum_{i=1}^d a_i v_i, \sum_{i=1}^d  v_i \rangle \geq 0
\end{align*}

Here is an alternative construction (analogous to the finite dimensional case) that works for separable $H$ (this is from Kallenberg).  
Let $\xi_1, \xi_2, \dotsc$ be a sequence of independent $N(0,1)$ random variables and let $e_1, e_2, \dotsc$ be an orthonormal basis of $H$.  For a general element $v \in H$ we define 
$X_v = \sum_{n=1}^\infty \langle v, e_n \rangle \xi_n$.  Note that 
\begin{align*}
\sum_{n=1}^\infty \variance{\langle v, e_n \rangle \xi_n} 
&= \sum_{n=1}^\infty  \langle v, e_n \rangle^2 = \norm{v}^2 < \infty
\end{align*}
and therefore by the Kolmogorov One-Series criterion Lemma \ref{VarianceCriterionSeries} and the independence of $\xi_n$ we see that $\sum_{n=1}^\infty \langle v, e_n \rangle \xi_n$ converges a.s.  In addition we have 
\begin{align*}
\lim_{n \to \infty} \expectation{\left( \sum_{j=n}^\infty \langle v, e_j \rangle \xi_j \right)^2} 
&=\lim_{n \to \infty} \sum_{j=n}^\infty \langle v, e_j \rangle^2 = 0
\end{align*}
and therefore the series $\sum_{n=1}^\infty \langle v, e_n \rangle \xi_n$ converges in $L^2$ as well.

To validate that $X$ defined in this way is isonormal we first calculate using Cauchy-Schwartz
\begin{align*}
\abs{\expectation{X_v} }
&\leq \lim_{n \to \infty} \left[\abs{\expectation{\sum_{j=1}^n \langle v, e_j \rangle \xi_j}} + \abs{\expectation{\sum_{j=n+1}^\infty \langle v, e_j \rangle \xi_j}} \right] \\
&\leq \lim_{n \to \infty} \expectation{\abs{\sum_{j=n+1}^\infty \langle v, e_j \rangle \xi_j}} \\
&\leq \lim_{n \to \infty} \expectation{\left( \sum_{j=n+1}^\infty \langle v, e_j \rangle \xi_j \right )^2}^{1/2} = 0 \\
\end{align*}

TODO: Finish
Also  by Parseval's Identity (TODO: Where is this)

\begin{align*}
\expectation{X_v X_w} 
&= \sum_{j=1}^\infty \langle v, e_j \rangle \langle w, e_j \rangle = \langle v, w \rangle
\end{align*}
\end{proof}

\begin{prop}\label{LinearityIsonormalProcess} Let $X$ be an isonormal process on a Hilbert space $H$, $v, w \in H$ and $a \in \reals$ then $X_{av + w} = a X_v + X_w$ a.s. 
\end{prop}
\begin{proof}
We calculate using simple algebra, the isonormal property to compute second moments and the bilinearity of the inner product,
\begin{align*}
&\expectation{(X_{av + w} - a X_v - X_w)^2} \\ 
&= \lbrace av + w, av + w \rbrace + a^2 \langle v,v \rangle + \langle w, w \rangle - 2 \langle av + w, av \rangle - 2 \langle av + w, w \rangle - 2 \langle av, w \rangle \\
&= 0
\end{align*}
\end{proof}

Every Gaussian process law can be constructed using an auxilliary isonormal process.
\begin{prop}\label{IsonormalRepresentationOfGaussian} Let $X$ be a centered Gaussian process on a probability space $(\Omega, \mathcal{A}, P)$, $L$ be an isonormal process on $L^2(\Omega, \mathcal{A}, P)$ and $Y_t = L_{X_t}$ then $X \eqdist Y$.
\end{prop}
\begin{proof}
It is worth making sure one understands the construction; $X_t$ is a centered Gaussian random variable on the probability space $(\Omega, \mathcal{A}, P)$ and therefore $X_t \in L^2(\Omega, \mathcal{A}, P)$.  The isonormal process $L$ is defined on $(\Omega^\prime, \mathcal{A}^\prime, P^\prime)$ and is indexed by $L^2(\Omega, \mathcal{A}, P)$ and thus $L_{X_t}$ is a centered Gaussian random variable on the probability space $(\Omega^\prime, \mathcal{A}^\prime, P^\prime)$.  In any case it is clear that for any $t_1,\dotsc,t_d \in T$ we have $\mathcal{L}(Y_{t_1}, \dotsc, Y_{t_d}) = \mathcal{L}(L_{X_{t_1}}, \dotsc, L_{X_{t_d}})$ is a Gaussian law by definition of the isonormal process.  The fact that $Y$ is centered follows immediately from the corresponding property of $L$.  As for covariances by the covariance structure of $L$ and the definition of the inner product in $L^2$,
\begin{align*}
\sexpectation{Y_s Y_t}{P^\prime} &= \sexpectation{L_{X_s}, L_{X_t}}{P^\prime} = \langle X_s, X_t \rangle = \int X_s X_t \, dP = \sexpectation{X_s X_t}{P}
\end{align*}
Now apply Lemma \ref{GaussianProcessMoments} to see that $X \eqdist Y$.
\end{proof}

A different but equivalent way of looking Proposition \ref{IsonormalRepresentationOfGaussian} is to consider the function 
\begin{align*}
d_X(s,t) &= \left( \expectation{(X_s - X_t)^2} \right)^{1/2} = \norm{X_s - X_t}_2
\end{align*}
It follows easily from properties of the $L^2$ norm (actually the $L^2$ seminorm on $\mathcal{L}^2$) that $d_X$ is a pseudometric on $T$.  Moreover by construction, the mapping $t \mapsto X_t$ from $(T,d_X)$ to $L^2(\Omega, \mathcal{A}, P)$ is isometric.  A subtle point is that viewing $X_t$ as an element in $L^2(\Omega, \mathcal{A}, P)$ is viewing $X_t$ as an equivalence class of random variables rather than a random variable; what really want is to use the seminormed space $\mathcal{L}^2(\Omega, \mathcal{A}, P)$.

TODO: Show that we can construct Brownian motion quickly using an
isonormal process and Kolomogorov-Centsov.

One of the uses of Proposition \ref{IsonormalRepresentationOfGaussian} is to use boundedness and sample path continuity properties of $L$ to infer the corresponding properties of a $X$.  The following fact is critical in such an approach.

\begin{prop}\label{VersionsForIsonormal}Let $L$ be an isonormal process restricted to a separable subset $C$ of a Hilbert space $H$.  Then 
\begin{itemize}
\item[(i)] If there exists an isonormal process $M$ with $M \eqdist L$ and $M$ restricted to $C$ has bounded sample paths then there exists a version $N$ of $L$ such that $N$ restricted to $C$ has bounded sample paths.
\item[(ii)] If there exists an isonormal process $M$ with $M \eqdist L$ and $M$ restricted to $C$ has uniformly continuous sample paths then there exists a version $N$ of $L$ such that $N$ restricted to $C$ has uniformly continuous sample paths.
\item[(iii)] If there exists an isonormal process $M,M^\prime$ with $M \eqdist L$, $M^\prime \eqdist L$, $M$ restricted to $C$ has bounded sample paths and $M^\prime$ restricted to $C$ has uniformly continuous sample paths then there exists a version $N$ of $L$ such that $N$ restricted to $C$ has bounded uniformly continuous sample paths.
\end{itemize}
\end{prop}
\begin{proof}
Let $A$ be a countable dense subset of $C$.  For each $v \in C$ and $n \in \naturals$ pick $v_n \in A$ such that $\norm{v - v_n} < 1/n$.  Let $N_v = \limsup_{n \to \infty} L_{v_n}$ if the right hand side is less than infinity and let it be $0$ otherwise.

\begin{clm}For each $v \in C$, $L_{v_n} \toas L_v$.  In particular, $N$ is a version of $L$.
\end{clm}
Then for each $\epsilon > 0$, by linearity of isonormal processes (Proposition \ref{LinearityIsonormalProcess}), a Markov bound and the covariance structure of an isonormal process
\begin{align*}
\probability{\abs{L_{v_n} - L_v} > \epsilon} &= \probability{\abs{L_{v_n - v}} > \epsilon} \leq \frac{\expectation{L^2_{v_n - v}}}{\epsilon^2} \\
&=\frac{\norm{v_n - v}^2}{\epsilon^2} \leq \frac{1}{n^2 \epsilon^2}
\end{align*}
and therefore $\sum_{n = 1}^\infty \probability{\abs{L_{v_n} - L_v} > \epsilon }< \infty$ and the Borel-Cantelli Theorem \ref{BorelCantelli} implies $\probability{\abs{L_{v_n} - L_v} > \epsilon \text{i.o.}} = 0$.  By Lemma \ref{ConvergenceAlmostSureByInfinitelyOften} we have $L_{v_n} \toas L_v$.  By definition of $N_v$, it follows that $N$ is a version of $L$.

Suppose that $M \eqdist L$ and $M$ has bounded sample paths on $C$.  It follows that $M$ has bounded sample paths on $A$ and since $A$ is countable the set of sample paths in $\reals^H$ that are bounded on $A$ is measurable.  It follows that almost surely $L$ has bounded sample paths on $A$ and by definition of $N$ we see that almost surely $N$ has bounded sample paths on all of $C$.  By changing $N$ on a set of probability zero we may assume that $N$ has bounded sample paths everywhere.

Suppose that $M \eqdist L$ and $M$ has uniformly continuous sample paths on $C$.  We argue similarly by observing that the set of sample paths that are uniformly continuous on $A$ is a measurable set in $\reals^H$.  Therefore we conclude that almost surely $L$ has uniformly continuous sample paths on $A$.  $L$ is the unique uniformly continuous extension of $L$ from $A$ to all of $C$.
\end{proof}

\section{Fernique's Theorem}

\begin{thm}Let $\mu$ be a Gaussian law on a separable Banach space $V$, for each $\lambda \in \dual{V}$ define $\sigma^2(\lambda) = \int \lambda^2(v) \, \mu(dv)$ then $\tau^2 = \sup \lbrace \sigma^2(\lambda) \mid \norm{\lambda} \leq 1 \rbrace < \infty$ and 
\begin{align*}
\int \exp\left( \alpha \norm{v}^2 \right) \, \mu(dv) &< \infty \text{ for each $\alpha < 1/(2\tau^2)$}
\end{align*}
\end{thm}
\begin{proof}
TODO:
\end{proof}

\begin{defn}A vector space $V$ with a $\sigma$-algebra $\mathcal{V}$ such that addition is $\mathcal{V} \otimes \mathcal{V}/\mathcal{V}$-measurable and scalar multiplication is 
$\mathcal{B}(\reals) \otimes \mathcal{V} / \mathcal{V}$-measurable is said to be a \emph{measurable vector space}.
\end{defn}

\begin{defn}A probability measure $\mu$ on a measurable vector space $(V, \mathcal{V})$ is said to be a \emph{centered Gaussian law} if given any two independent random elements $\xi$ and $\eta$ in $V$ with distribution $\mu$ and any $0 < \theta < 2\pi$ the random elements $\xi \cos \theta + \eta \sin \theta$ and $- \xi \sin \theta + \eta \cos \theta$ are independent and have distribution $\mu$.
\end{defn}

TODO: Does this definition generalize the previous definition of a Gaussian measure on a Banach space?

\begin{lem}A centered Gaussian law $\mu$ on $(\reals, \mathcal{B}(\reals))$ is $N(0,\sigma^2)$ for some $\sigma^2 \geq 0$.
\end{lem}
\begin{proof}
\begin{clm} The measure $\mu \otimes \mu$ is invariant under rotations.  
\end{clm}
Let $\xi$ and $\eta$ be independent with law $\mu$; then the law of $(\xi, \eta)$ is $\mu \otimes \mu$ (Lemma \ref{IndependenceProductMeasures}).  Let $R_\theta$ be rotation by the angle $\theta$
then for any $A \in \mathcal{B}(\reals \times \reals)$,
\begin{align*}
\mu \otimes \mu (R_\theta A) &= \probability{(\xi, \eta) \in R_\theta A} = \probability{R_{2 \pi - \theta} (\xi, \eta) \in A} = \probability{ (\xi, \eta) \in A} = \mu \otimes \mu(A)
\end{align*}

Now for any $A \in \mathcal{B}(\reals)$ apply rotation through the angle $\pi$ to see that 
\begin{align*}
\mu(A) &= (\mu \otimes \mu)(\reals \times A) = (\mu \otimes \mu)(\reals \times -A) = \mu(-A)
\end{align*}
and it follows that $\mu$ is symmetric and therefore we know the characteristic function $\hat{\mu}(t)$ is real valued.  Furthermore given an 
arbitrary $(t,u) \in \reals^2$ there exists $\theta$ such that $R_\theta \begin{bmatrix}  t \\ u \end{bmatrix} = \begin{bmatrix} \sqrt{t^2 + u^2} \\ 0 \end{bmatrix}$ and therefore
applying Theorem \ref{IndependenceProductCharacteristicFunctions}, Lemma \ref{CharacteristicFunctionOfAffineTransform} and Theorem \ref{CharacteristicFunctionBoundedAndContinuous}
\begin{align*}
\hat{\mu}(t) \hat{\mu}(u) &= \widehat{\mu \otimes \mu} (t,u) = \widehat{\pushforward{R_{2\pi - \theta}}{\mu \otimes \mu}} (t,u) \\
&=\widehat{\mu \otimes \mu} \left (R_\theta \begin{bmatrix} t \\ u \end{bmatrix} \right ) = \widehat{\mu \otimes \mu} (\sqrt{t^2 + u^2}, 0) = \mu(\sqrt{t^2 + u^2})
\end{align*}

\begin{clm}$\hat{\mu}(t) > 0$ for all $t$
\end{clm}
Since $\hat{\mu}(t) = \hat{\mu}(-t)$ it suffices to assume $t \geq 0$ and in this case we may write 
\begin{align*}
\hat{\mu}(t) = \hat{\mu}(\sqrt{(t/\sqrt{2})^2 + (t/\sqrt{2})^2}) = \left(\hat{\mu}(t/\sqrt{2})\right)^2 \geq 0
\end{align*}

If $\hat{\mu}(t) = 0$ for some $t \geq 0$ then let $t_0 = \inf \lbrace t > 0 \mid \hat{\mu}(t) = 0 \rbrace$.  By continuity of $\hat{\mu}$ it follows that
$\hat{\mu}(t_0) = 0$.  For all $u \geq t_0$ we have 
\begin{align*}
\hat{\mu}(u) &= \hat{\mu}(\sqrt{t_0^2 + (\sqrt{u^2 - t_0^2})^2}) = \hat{\mu}(t_0) \hat{\mu}(\sqrt{u^2 - t_0^2}) = 0 
\end{align*}
If $t_0 > 0$ then for $0 \leq u < t_0$ we know that 
\begin{align*}
\hat{\mu}(u) \hat{\mu}(\sqrt{t_0^2 - u^2}) &= \hat{\mu}(\sqrt{u^2 + (\sqrt{t_0^2 - u^2})^2}) = \hat{\mu}(t_0) = 0
\end{align*}
which shows that either $\hat{\mu}(u)=0$ or $\hat{\mu}(\sqrt{t_0^2 - u^2})=0$ which contradicts the minimality of $t_0$.  Therefore we know that $t_0=0$ which in turn
contradicts the fact that $\hat{\mu}(0) = 1$ and therefore we know that $\hat{\mu} (t) > 0$ for all $t$.

By the previous claim we may define $h(t) =  \log \hat{\mu}(\sqrt{t})$ for $t \geq 0$ and note that $h$ is continuous, $h(0) = 0$ and for $t,u \geq 0$,
\begin{align*}
h(t+u) &= \log \hat{\mu}(\sqrt{\sqrt{t}^2 + \sqrt{u}^2}) = \log \hat{\mu}(\sqrt{t}) + \log \hat{\mu}(\sqrt{u}) = h(t) + h(u)
\end{align*}
By a simple induction, for every $t \geq 0$ and $n \in \naturals$ we have $h(nt) = nh(t)$.  For every rational $p/q \geq 0$ and $t \geq 0$ we have
\begin{align*}
h(\frac{p}{q} t) &= p h(\frac{t}{q}) = \frac{p}{q} q h(\frac{t}{q})  = \frac{p}{q} h(t)
\end{align*}
Lastly for every $c \geq 0$ and $t \geq 0$ choose positive rationals $q_n$ such that $c = \lim_{n\to \infty} q_n$ and we have by continuity of $h$ at $c t$, 
\begin{align*}
h(ct) = \lim_{n \to \infty} h(q_n t) = h(t) \lim_{n \to \infty} q_n = c h(t)
\end{align*}
In particular, for every $t \geq 0$ we have $h(t) = h(1) t$.  From this it shows us that for $t \geq 0$ we have $\hat{\mu}(t) = e^{ct^2}$ and because $\hat{\mu}(t)=\hat{\mu}(-t)$ it follows that
$\hat{\mu}(t) = e^{ct^2}$ for all $t$.  Since $0 < \hat{\mu}(t) \leq 1$ it follows that $c < 0$ and we may write $\hat{\mu}(t) = e^{-\sigma^2 t^2}$. By Theorem \ref{GlivenkoLevyContinuity} and Example \ref{FourierTransformGaussian} we see that $\mu$ is $N(0, \sigma^2)$.  
\end{proof}

\begin{thm}\label{CenteredGaussian01}Let $(V, \mathcal{V})$ be a measurable vector space, $\mu$ a centered Gaussian law  and let $W \subset V$ be a subspace then either $\mu(W)=0$ or $\mu(W) =1 $.
\end{thm}
\begin{proof}
Let $\xi$ and $\eta$ be independent random elements in $V$ with distribution $\mu$.  For each $0 \leq \theta \leq \pi/2$ let 
\begin{align*}
A_\theta &= \lbrace \xi \cos \theta + \eta \sin \theta \in W \text{ and } -\xi \sin \theta + \eta \cos \theta \notin W \rbrace
\end{align*}
Now let $0 \leq \phi < \theta \leq \pi/2$ and note that $\cos \theta \sin \phi - \sin \theta \cos \phi = \sin (\phi - \theta) \neq 0$ therefore the matrix
\begin{align*}
B_{\theta, \phi} &= \begin{bmatrix}
\cos \theta & \sin \theta \\
\cos \phi & \sin \phi
\end{bmatrix}
\end{align*}
is invertible.  If $\omega \in A_\theta \cap A_\phi$ with $\theta \neq \phi$ then there exist random elements $v, w \in W$ such that
\begin{align*}
\begin{bmatrix}
\xi (\omega) \\
\eta(\omega)
\end{bmatrix}
&= B_{\theta, \phi}^{-1} 
\begin{bmatrix}
v (\omega) \\
w (\omega)
\end{bmatrix}
\in W
\end{align*}
But this contradicts the conditions $-\xi(\omega) \sin \theta + \eta(\omega) \cos \theta \notin W$ and $-\xi(\omega) \sin \phi + \eta(\omega) \cos \phi \notin W$ thus it follows
that $A_\theta \cap A_\phi = \emptyset$ for $\theta \neq \phi$.  Since $\mu$ is centered Gaussian it follows that $\probability{A_\theta}$ is independent of $0 \leq \theta \leq \pi/2$.  Since there are an infinite number of equiprobable disjoint sets $A_\theta$ it follows that $\probability{A_\theta} = 0$ for all $0 \leq \theta \leq \pi/2$.
Choosing $\theta = 0$ we can also compute using the independence of $\xi$ and $\eta$,
\begin{align*}
0 &= \probability {A_0} = \probability{\xi \in W; \eta \notin W} = \probability{\xi \in W} \probability{\eta \notin W} = \mu(W) \mu(V \setminus W)
\end{align*}
thus $\mu(W) = 0$ or  $\mu(W) = 1$.
\end{proof}

\begin{defn}Let $(V, \mathcal{V})$ be a measurable vector space and let $\norm{\cdot} : V \to [0,\infty]$ be measurable.  We say $\norm{\cdot}$ is a \emph{pseudo-seminorm} if $W = \lbrace v \mid \norm{v} < \infty \rbrace$ is a vector subspace of $V$ and if $\norm{\cdot}$ is a seminorm when restricted to $W$.  
\end{defn}

\begin{lem}\label{lem:FerniqueLemma}Let $(V, \mathcal{V})$ be a measurable vector space, $\mu$ a centered Gaussian law and  pseudo-seminorm $\norm{\cdot}$ such that $\mu(\norm{\cdot} < \infty) >0$.  The for some 
$\epsilon > 0$ we have
\begin{align*}
\int \exp\left( \alpha \norm{x}^2 \right) \, \mu(dx) &< \infty \text{ for all $0 < \alpha < \epsilon$}
\end{align*}
\end{lem}
\begin{proof}
By Lemma \ref{CenteredGaussian01} we conclude $\mu(\norm{\cdot} < \infty) =1$.  
Let $W = \lbrace \norm{\cdot} < \infty \rbrace \subset V$.  By the triangle inequality for $v,w \in W$ we have $\norm{v+w} = \norm{v-w + 2w} \leq \norm{v-w} + 2 \norm{w}$ and symmetrically with the roles of $v$ and $w$ reversed so 
\begin{align}\label{lem:FerniqueLemma:NormInequality}
\norm{v+w} - \norm{v-w} \leq 2(\norm{v} \minop \norm{w})
\end{align}
Let $\xi$ and $\eta$ be independent random elements in $V$ with distribution $\mu$.  
\begin{clm}For all $s,t$ we have 
\begin{align*}
\mu(\norm{\cdot} \leq s)\mu(\norm{\cdot} > t)
&\leq\mu \left( \norm{\cdot} > (t-s)/\sqrt{2}\right)^2
\end{align*}
\end{clm}
Apply the definition of centered Gaussian law with $\theta = 7\pi/4$, \eqref{lem:FerniqueLemma:NormInequality}, $\mu(W)=1$ and independence of $\xi$ and $\eta$ to conclude 
\begin{align*}
&(\mu \otimes \mu)(\lbrace \norm{\cdot} \leq s \times \lbrace \norm{\cdot} > t) 
=\probability{\norm{\xi} \leq s; \norm{\eta} > t} \\
&=\probability{\norm{(\xi-\eta)/\sqrt{2}} \leq s; \norm{(\xi+\eta)/\sqrt{2}} > t} \\
&=\probability{\norm{(\xi-\eta)/\sqrt{2}} \leq s; \norm{(\xi+\eta)/\sqrt{2}} > t; \xi \in W; \eta \in W} \\
&=\probability{\norm{(\xi-\eta)/\sqrt{2}} \leq s; \norm{(\xi+\eta)/\sqrt{2}} > t; \sqrt{2} \norm{\xi} > t-s; \sqrt{2} \norm{\eta} > t-s; \xi \in W; \eta \in W} \\
&\leq \probability{\sqrt{2} \norm{\xi} > t-s; \sqrt{2} \norm{\eta} > t-s} \\
&=\probability{\sqrt{2} \norm{\xi} > t-s} \probability{ \sqrt{2} \norm{\eta} > t-s} \\
&=\mu \left( \norm{\cdot} > (t-s)/\sqrt{2}\right)^2
\end{align*}

Since $\lim_{s \to \infty} \mu(\norm{\cdot} \leq s) = \mu(\norm{\cdot} < \infty ) = 1$ we may pick an $0 < s < \infty$ such that $\mu(\norm{\cdot} \leq s) > 1/2$.  Define
\begin{align*}
q &=\mu(\norm{\cdot} \leq s)
\end{align*}
For $n \in \integers_+$ define
\begin{align*}
t_n &= \left(2^{1/2} +1 \right) \left( 2^{(n+1)/2} - 1 \right) s
\end{align*}
noting that $t_0 = s$ and $t_{n+1} = s + 2^{1/2} t_n$.  Define 
\begin{align*}
x_n &= \mu(\norm{\cdot} > t_n)/q
\end{align*}
and observe that by the previous claim we have for all $n \in \integers_+$,
\begin{align*}
\mu(\norm{\cdot} > t_{n+1}) &\leq \mu \left( \norm{\cdot} > (t_{n+1}-s)/\sqrt{2}\right)^2/\mu(\norm{\cdot} \leq s)
=\mu(\norm{\cdot} > t_{n}/q
\end{align*}
thus $x_{n+1} \leq x_n^2 \leq x_0^{2^{n+1}} = \left((1-q)/q\right)^{2^{n+1}}$; equivalently
\begin{align}\label{lem:FerniqueLemma:NormTailBound}
\mu(\norm{\cdot} > t_{n+1}) &\leq q \left((1-q)/q\right)^{2^{n+1}}
\end{align}


Now use \eqref{lem:FerniqueLemma:NormTailBound} to bound the integral
\begin{align*}
&\int \exp \left( \alpha \norm{x} \right) \, \mu(dx) 
=\int_{\norm{x} \leq s} \exp \left( \alpha \norm{x} \right) \, \mu(dx) + \sum_{n=0}^\infty \int_{t_n < \norm{x} \leq t_{n+1}} \exp \left( \alpha \norm{x} \right) \, \mu(dx) \\
&\leq q e^{\alpha s} + \sum_{n=0}^\infty \mu(t_n < \norm{\cdot} \leq t_{n+1}) e^{\alpha t_{n+1}} \\
&\leq q e^{\alpha s} + \sum_{n=0}^\infty \mu(\norm{\cdot} > t_n) e^{\alpha t_{n+1}} \\
&\leq q e^{\alpha s} + \sum_{n=0}^\infty q  \left((1-q)/q\right)^{2^{n}} \exp\left(\alpha \left(2^{1/2} +1 \right)^2 \left( 2^{(n+2)/2} - 1 \right)^2 s^2 \right) \\
&= q e^{\alpha s} + q  \sum_{n=0}^\infty \exp\left(2^n \left( \log \left((1-q)/q\right) + \alpha \left(2^{1/2} +1 \right)^2 \left(2 - 2^{-n/2} \right)^2 s^2 \right) \right) \\
&\leq q e^{\alpha s} + q  \sum_{n=0}^\infty \exp\left(2^n \left( \log \left((1-q)/q\right) + 4 \alpha \left(2^{1/2} +1 \right)^2 s^2 \right) \right) \\
\end{align*}
The series converges when 
\begin{align*}
\log \left((1-q)/q\right) + 4 \alpha \left(2^{1/2} +1 \right)^2 s^2 < 0
\end{align*}
which is to say for all $0 < \alpha < \frac{\log((q-1)/q)}{4 \left(2^{1/2} +1 \right)^2 s^2}$.
\end{proof}

The Fernique inequality will be a corollary of the following
\begin{thm}Let $(V, \mathcal{V})$ be a measurable vector space with a centered Gaussian law $\mu$, $y_n$ a sequence of linear forms $lambda_n : V \to \reals$ and $\norm{v} = \sup_{n} \abs{\lambda_n(v)}$.  Then $\norm{\cdot}$ is a pseudo-seminorm and if $\mu(\norm{\cdot} < \infty) > 0$ then $\tau := \sqrt{\sup_n \int \lambda_n^2(v) \, \mu(dv)} < \infty$ and 
\begin{align*}
\int \exp(\alpha \norm{v}) \, \mu(dv) < \infty \text{ if and only if $\alpha < 1/(2\tau^2)$}
\end{align*}
\end{thm}
\begin{proof}
TODO:
\end{proof}

Here is a very clean proof of the uniqueness of a solution to the Cauchy functional equation with non-negativity constraints;
\begin{prop}The only solutions to the functional equation $f(x+y) = f(x) + f(y)$ for functions $f : [0,\infty) \to [0, \infty)$ are of the form  $f(x) = cx$.
\end{prop}
\begin{proof}
Note that for $0 \leq x \leq y$ we have 
\begin{align*}
f(y) &= f(x + y - x) = f(x) + f(y-x) \geq f(x)
\end{align*}
hence $f$ is monotonic.  

Now by a simple induction we know that for every $x \geq 0$ we have $f(nx) = nf(x)$ for all $n \in\naturals$.  Now for arbitrary $x \geq 0$ and $n \in \naturals$ combine these facts to see
$f(\floor{nx}) \leq f(nx) \leq f(\ceil{nx})$ and therefore $\floor{nx} f(1) \leq n f(x) \leq \ceil{nx} f(1)$.  Now divide by $n$ and take the limit as $n \to \infty$ to see
\begin{align*}
f(1) x &= \lim_{n \to \infty} f(1)\frac{\floor{nx}}{n} \leq f(x) \leq f(1) \lim_{n \to \infty} \frac{\ceil{nx}}{n} = f(1) x
\end{align*}
hence $f(x) = f(1) x$.
\end{proof}

Here is the proof of the uniqueness of solutions to  the functional equation $f(x+y) = f(x) f(y)$ under a continuity assumption (it reduces to the proof of uniqueness of the Cauchy functional equation):
\begin{proof}
First observe that $f(x) > 0$ for all $x \in \reals$.  Suppose $f(x) = 0$ for some $x$; then for arbitrary $y$ we have $f(y) = f(x) f(y - x) = 0$.  Now from 
$f(x) = f(x/2)^2$ we conclude that $f(x) > 0$ for all $x$.  From $f(x) = f(x+0) = f(x) f(0)$ we see that $f(0) = 1$.
By the positivity of $f$ we can now define $g(x) = \log (f(x))$ which satisfies the Cauchy functional equation $g(x+y) = g(x) + g(y)$.  
By a simple induction we know that $g(nx) = ng(x)$ for all $n \in \integers_+$.  From $0 = g(0) = g(x - x) = g(x) + g(-x)$ we see that $g(-x) = -g(x)$ and it follows that $g(nx) = n g(x)$ for all $n \in \integers$.  For an arbitrary rational number $p/q$ we get 
\begin{align*}
g(\frac{p}{q} x) = p g(\frac{x}{q}) = \frac{p}{q} q g(\frac{x}{q}) = \frac{p}{q} g(x)
\end{align*}
Lastly suppose that $g(x)$ is continuous (right or left continuity suffices) at $x_0$ then for arbitrary $c \in \reals$ we can take a sequence of non-zero rationals $q_n$ such that $q_n \to c$ and it follows that $g(cx_0) = \lim_{n \to \infty} q_n g(\frac{c}{q_n} x_0) = c g(x_0)$.    If $x_0 \neq 0$ then $g(cx) = g(\frac{cx}{x_0} x_0) = \frac{cx}{x_0} g(x_0) = cg(x)$ for all $x, c$.  If $x_0 = 0$ then $g(c x) = \lim_{n \to \infty} q_n g(x) + g((c-q_n) x) = c g(x)$ for all $x,c \in \reals$.  It follows that $g(x) = g(1) x$ for all $x$.
\end{proof}

TODO: It actually suffices to assume that $f$ is measurable; how to see that?
This relies on the following fact that we'll prove later.
\begin{clm}If $A \subset \reals$ has positive Lebesgue measure then $A - A = \lbrace x - y \mid x,y \in A \rbrace$ is a open neighborhood of $0$.
\end{clm}

By the above proof it suffices to show that a measurable $f$ satisfying $f(x+y) = f(x) + f(y)$ is continuous at $0$. Let $U$ be an open neighborhood of $0$.  Choose an open set $V \subset U$ such that $V - V \subset U$.  Let $q_1, q_2, \dotsc$ be an enumeration of $\rationals$ and by openness of $V$ we have $\reals = \cup_{n=1}^\infty q_n + V$ and therefore by Lemma \ref{SetOperationsUnderPullback} 
\begin{align*}
\cup_{n=1}^\infty f^{-1}(q_n + V) &= f^{-1} \left(\cup_{n=1}^\infty q_n + V \right ) = f^{-1}(\reals) = \reals
\end{align*}
Each $q_n + V$ is measurable and since  $\reals =\cup_{n=1}^\infty f^{-1}(q_n + V)$ there exists and $n$ such that $W =  f^{-1}(q_n + V)$ has positive measure.  From the claim it follows that $W - W$ is a open neighborhood of $0$.  Since $f$ is additive it follows that $W-W \subset V - V \subset f^{-1}(U)$ (write $x,y \in W$ as $x=q_n+v$ and $y=q_n+w$ then $x-y=v-w \in V - V$).  

\section{Slepian and Sudakov}

\begin{lem}\label{ChevetLemma}Let $\eta$ be an $N(0,1)$ random variable, $(\xi_1, \dotsc, \xi_n)$ be a centered Gaussian random vector, $M>0$ and $0 < \epsilon \leq 1$ such that $\variance{\xi_i} \leq M^2$ for $i=1, \dotsc, n$ and $\expectation{(\xi_i - \xi_j)^2} > \epsilon^2$ for $1 \leq i < j \leq n$ then 
\begin{align*}
\probability{\eta > 1} \probability{\xi_1 \leq 1; \dotsb ; \xi_n \leq 1}
&\leq \frac{1}{2^{n+1}} + \frac{1}{\sqrt{2\pi}} \int_0^\infty \probability{\eta \leq \frac{\sqrt{2}(M^2+1)t}{\epsilon}}^n e^{-t^2/2} \, dt
\end{align*}
\end{lem}
\begin{proof}
TODO:
\end{proof}

\begin{defn}Let $X$ be a centered Gaussian random process indexed by a set $T$ then for every $\epsilon>0$ define
\begin{align*}
d(\epsilon, T) &= 
\sup_{n \in \naturals} \lbrace \text{there exist $(t_1, \dotsc, t_n) \in T$ such that $\min_{1 \leq i < j \leq n}\expectation{(X_{t_i} -X_{t_j})^2} > \epsilon^2$} \rbrace
\end{align*}
\end{defn}

TODO: Look at the following proof and make the existence of the countable subset more explicit.
\begin{thm}[Sudakov-Chevet]\label{SudakovChevet}Let $X$ be a centered Gaussian process indexed by a set $T$ such that 
\begin{align*}
\limsup_{\epsilon \to 0} \epsilon^2 \log D(\epsilon, T) = +\infty
\end{align*}
then $\sup_{t \in T} \abs{X_t} = +\infty$ almost surely.  In fact there exists a countable subset $B \subset T$ such that $\sup_{t \in B} \abs{X_t} = +\infty$ almost surely.
\end{thm}
\begin{proof}
First suppose that $\sup_{t \in T} \variance{X_t} = +\infty$.  Note that for every $M > 0$ we have the simple bound 
\begin{align*}
\probability{\abs{X_t} \leq M} &= \frac{1}{\sqrt{2\pi \variance{X_t}}} \int_{-M}^M e^{-t^2/2 \variance{X_t}} \, dt \leq \frac{2M}{\sqrt{2\pi \variance{X_t}}}
\end{align*} 
Pick $t_1, t_2, \dotsc \in T$ such that $\variance{X_{t_n}} > n^4$ then it follows that $\sum_{j=1}^\infty \probability{\abs{X_{t_j}} \leq M} < \infty$ and so by the Borel-Cantelli Theorem \ref{BorelCantelli} $\probability{\sup_j \abs{X_{t_j}} \leq M} = 0$.  By continuity of measure we get $\probability{\sup_j \abs{X_{t_j}} = +\infty} = 1$.

We may now assume that  there exists $M > 0$ such that $\sup_{t \in T} \variance{X_t} < \infty$.  Choose a sequence $\epsilon_k$ such that $\lim_{k \to \infty} \epsilon_k = 0$ and $\epsilon_k^2 D(\epsilon_k, T) \geq k$.  Let $n_k = D(\epsilon_k, T)$ and pick $t_1, \dotsc, t_{n_k}$ such that $\expectation{(X_{t_i} -X_{t_j})^2} > \epsilon_k^2$ for all $1 \leq i < j \leq n_k$.  For every $B \geq 1$ it follows that $\variance{X_{t_j}/B} \leq M^2/B^2 \leq M^2$ for all $j=1, \dotsc, n_k$ and $\expectation{(X_{t_i}/B -X_{t_j}/B)^2} > \epsilon_k^2/B^2$ for all $1 \leq i < j \leq n_k$, thus we may apply Lemma \ref{ChevetLemma} and the exponential inequality Lemma \ref{BasicExponentialInequalities} to get (recall $\eta$ represents a standard normal random variable)
\begin{align*}
&\probability{\eta > 1} \probability{X_{t_1} \leq B; \dotsb ; X_{t_{n_k}} \leq B} \\
&\leq \frac{1}{2^{n_k+1}} + \frac{1}{\sqrt{2\pi}} \int_0^\infty \probability{\eta \leq \frac{\sqrt{2}(M^2+1)Bt}{\epsilon_k}}^{n_k} e^{-t^2/2} \, dt
&= \frac{1}{2^{n_k+1}} + \frac{1}{\sqrt{2\pi}} \int_0^\infty \left(1 - \probability{\eta > \frac{\sqrt{2}(M^2+1)Bt}{\epsilon_k}} \right)^{n_k} e^{-t^2/2} \, dt
&\leq \frac{1}{2^{n_k+1}} + \frac{1}{\sqrt{2\pi}} \int_0^\infty e^{-n_k \probability{\eta > \frac{\sqrt{2}(M^2+1)Bt}{\epsilon_k}}}  e^{-t^2/2} \, dt
\end{align*}

Since $e^{-n_k \probability{\eta > \frac{\sqrt{2}(M^2+1)Bt}{\epsilon_k}}} \leq 1$ and $1$ is integrable with respect to the measure $e^{-t^2/2} \, dt$, by Dominated Convergence it suffices to prove the following.
\begin{clm}$\lim_{k \to \infty} n_k \probability{\eta > \frac{\sqrt{2}(M^2+1)Bt}{\epsilon_k}} = +\infty$.
\end{clm}
From Lemma \ref{GaussianTailsElementary} we know that 
\begin{align*}
\probability{\eta > \frac{\sqrt{2}(M^2+1)Bt}{\epsilon_k}}
&\geq \frac{\epsilon_k \sqrt{2}(M^2+1)Bt}{\sqrt{2\pi} (\epsilon_k^2 + 2(M^2+1)^2B^2t^2)} e^{-\frac{(M^2+1)^2B^2t^2}{\epsilon^2_k}}
\end{align*}
and therefore
\begin{align*}
&n_k \probability{\eta > \frac{\sqrt{2}(M^2+1)Bt}{\epsilon_k}} \\
&\geq D(\epsilon_k, T) \frac{\epsilon_k \sqrt{2}(M^2+1)Bt}{\sqrt{2\pi} (\epsilon_k^2 + 2(M^2+1)^2B^2t^2)} e^{-\frac{(M^2+1)^2B^2t^2}{\epsilon^2_k}} \\
&= \frac{\sqrt{2}(M^2+1)Bt}{\sqrt{2\pi}} e^{\left( \epsilon_k^2 \log D(\epsilon_k, T) +  \epsilon_k^2 \log \epsilon_k -  \epsilon_k^2 \log (\epsilon_k^2 + 2(M^2+1)^2B^2t^2) -(M^2+1)^2B^2t^2\right)/\epsilon^2_k} \\
\end{align*}
and the claim follows by noting that $\lim_{k \to \infty} \epsilon_k^2 \log D(\epsilon_k, T) = +\infty$, $\lim_{k \to \infty} \epsilon_k^2 \log \epsilon_k =0$, 
$\lim_{k \to \infty} \epsilon_k^2 \log (\epsilon_k^2 + 2(M^2+1)^2B^2t^2) =0$ and $\lim_{k \to \infty} e_k^{-2} = +\infty$.
\end{proof}

\section{Boundedness and Sample Continuity}

TODO: Motivate the consideration of boundedness and sample continuity properties of Gaussian processes as these wind up being important in uniform central limit theorems/empirical process contexts.  Do they also come up as important properties in Malliavin calculus?

\subsection{Boundedness and Sample Continuity for Isonormal Processes}

In this section we examine questions of sample path boundedness and continuity for isonormal processes.  As it turns out, for an infinite dimensional Hilbert space $H$ sample paths of an isonormal process $L$ are not bounded (TODO: in fact almost surely unbounded?).  Therefore the study of boundedness of the sample paths of an isonormal process is about identifying subsets $C \subset H$ such that the sample paths of $L$ restricted to $C$ are in fact bounded.  

In discussing boundedness of sample paths we have to discuss suprema over an uncountably infinite set.  Absent continuity assumptions on sample paths (such as the cadlag property we could arrange for in discussing martingales and Feller processes) there are some measurability issues to deal with.  We need the following definitions (we'll discuss similar ideas in much more depth later on as part of the Hoffmann-Jorgensen theory of weak convergence).

\begin{defn}Let $\mathcal{J}$ be a subset of $\mathcal{L}^0(\Omega, \mathcal{A}, P, [-\infty, \infty])$ the we say that $f \in \mathcal{L}^0$ is an \emph{essential supremum of $\mathcal{J}$} if 
for every $j \in \mathcal{J}$ we have $f \geq j$ a.s. and moreover for any $g \in \mathcal{L}^0$ such that $g \leq j$ a.s. for every $j \in \mathcal{J}$ we also have $g \geq f$ a.s.  If an essential supremum exists we denote it $\esssup \mathcal{J}$ or $\esssup_{j \in \mathcal{J}} j$
\end{defn}

\begin{prop}\label{AlmostSureUniquenessEssentialSupremum}If $\mathcal{I} \subset \mathcal{J}$ the then the essential supremum of $\mathcal{J}$ is greater than or equal to the essential supremum of $\mathcal{I}$ almost surely.  If $f$ and $g$ are both essential suprema of $\mathcal{J}$ then $f = g$ a.s.
\end{prop}
\begin{proof}
Let $g$ be the essential supremum of $\mathcal{J}$.  Clearly for all $j \in \mathcal{I} \subset \mathcal{J}$ we know $g \geq j$ a.s.  so by definition the essential supremum of $\mathcal{I}$ is lesss than or equal to $f$ a.s.  By the first part of the proposition, we have both $f \geq g$ a.s. and $g \geq f$ a.s. hence $f=g$ a.s.
\end{proof}


\begin{prop}\label{prop:ExistenceEssentialSupremumIsonormal}Let $H$ be a separable Hilbert space, $L$ be an isonormal process on $H$, $C \subset H$ and $B \subset C$ a  countable dense subset.  Then $\sup_{v \in B} L_v$ is an essential supremum of $\lbrace L_v \mid v \in C \rbrace$ and $\sup_{v \in B} \abs{L_v}$ is an essential supremum of $\lbrace \abs{L_v} \mid v \in C \rbrace$.
\end{prop}
\begin{proof}
We consider the case of $\sup_{v \in B} L_v$ as the proof for $\sup_{v \in B} \abs{L_v}$ is the same.  First by countablility of $B$ and Lemma \ref{LimitsOfMeasurable}, $\sup_{v \in B} L_v$ is measurable. By density of $B$ in $C$, for $v \in C$ we may pick a sequence $v_n \in B$ with $\norm{v - v_n}^2 \leq 1/n^2$.  Let $\epsilon > 0$ be
given then a Markov bound and the isonormal property we have
\begin{align*}
\sum_{n =1}^\infty \probability{\abs{L_{v} - L_{v_n}} \geq \epsilon} &\leq \abs{1}{\epsilon^2} \sum_{n =1}^\infty \expectation{(L_v - L_{v_n})^2} \leq \abs{1}{\epsilon^2} \sum_{n =1}^\infty 1/n^2 < \infty
\end{align*}
hence by the Borel-Cantelli Theorem \ref{BorelCantelli} we see that $\probability{\abs{L_{v} - L_{v_n}} \geq \epsilon \text{ i.o.}} = 0$ and thus by Lemma \ref{ConvergenceAlmostSureByInfinitelyOften} we have $L_{v_n} \toas L_v$.  It follows that for $L_v \leq \sup_{v \in B} L_v$ a.s.

Suppose that $g$ is a measurable random variable with $g \geq L_v$ a.s. for all $v \in C$.  Then by subadditivity $\probability {\cup_{v \in B} \lbrace g < L_v \rbrace}=0$ and almost surely $g \geq L_v$ a.s. for all $v \in B$; hence $g \geq \sup_{v \in B} L_v$ a.s.  
\end{proof}

Based on Proposition \ref{prop:ExistenceEssentialSupremumIsonormal} the following definition makes sense.
\begin{defn}Let $H$ be a separable Hilbert space, $L$ be an isonormal process on $H$ and $C \subset H$ then we define 
\begin{align*}
L(C)^* &= \esssup_{v \in C} L_v, &&\abs{L(C)}^* &= \esssup_{v \in C} \abs{L_v}
\end{align*}
We say that $C$ is a \emph{GB-set} if $\abs{L(C)}^* < \infty$ a.s.  for some isonormal process $L$ on $H$.
\end{defn}

Note that the essential suprema $L(C)^*$ and $\abs{L(C)}^*$ are invariant under modification of $L$.

If $C$ is a GB-set then $\abs{L(C)}^* < \infty$ a.s.  for every isonormal process $L$.  That is because the distribution of $L(C)^*$ and $\abs{L(C)}^*$ do not depend on the choice of $L$ and therefore are properties of the set $C \subset H$ itself.
\begin{cor}Let $H$ be a separable Hilbert space, $C \subset H$ and let $L$ and $L^\prime$  each be an isonormal process on $H$ then $L(C)^* \eqdist L^\prime(C)^*$ and $\abs{L(C)}^* \eqdist \abs{L^\prime(C)}^*$.
\end{cor}
\begin{proof}
Since each of $L$ and $L^\prime$ the finite dimensional distributions of $L$ and $L^\prime$ are the same.  Pick a countable dense subset $v_1, v_2, \dotsc \in C$ and $x \in \reals$ then by continuity of measure (Lemma \ref{ContinuityOfMeasure}) and $L \eqfdd L^\prime$
\begin{align*}
\probability{L(C)^* > x} &= \probability{\sup_{1 \leq j < \infty} L_{v_j} > x} = \probability{\cup_{n=1}^\infty \max_{1 \leq j \leq n} L_{v_j} > x} \\
&=  \lim_{n \to \infty} \probability{\max_{1 \leq j \leq n} L_{v_j} > x} =  \lim_{n \to \infty} \probability{\max_{1 \leq j \leq n} L^{\prime}_{v_j} > x} 
= \probability{\cup_{n=1}^\infty \max_{1 \leq j \leq n} L^\prime_{v_j} > x} 
= \probability{\sup_{1 \leq j < \infty} L^\prime_{v_j} > x} 
=\probability{L^\prime(C)^* > x} 
\end{align*}
Therefore by Lemma \ref{DistributionFunctionCharacterizeProbability} we have $L(C)^* \eqdist L^\prime(C)^*$.  The proof for $\abs{L(C)}^*$ is the same.
\end{proof}

\begin{lem}\label{EssentialSupremumUnderContraction}Let $B : H \to H$ be a linear operator with $\norm{B} \leq 1$ and $C \subset H$ be non-empty then for any $\lambda \geq 0$ we have
\begin{align*}
\probability{\abs{L(BC)}^* \leq \lambda} &\geq \probability{\abs{L(C)}^* \leq \lambda}
\end{align*}
\end{lem}
\begin{proof}
TODO:
\end{proof}

With regards to sample continuity we have the following definition.
\begin{defn}Let $H$ be a separable Hilbert space and $C \subset H$ we say that $C$ is a \emph{GC-set} if $C$ is totally bounded and there exists an isonormal process $L$ on $H$ such that all sample functions $L_v$ are uniformly continuous on $C$.
\end{defn}

The Sudakov-Chevet Theorem implies the following fact.
\begin{prop}\label{GCImpliesGB}Let $H$ be a separable Hilbert space then a GB-set is totally bounded.  A GC-set is also a GB-set.
\end{prop}
\begin{proof}
Note that Sudkov-Chevet Theorem can be rephrased as a necessary condition for a set to be a GB-set.   Let $L$ be an isonormal process on $H$ then it centered and Gaussian hence if $C \subset H$ and $\limsup_{\epsilon \to 0} \epsilon^2 D(\epsilon, C) = +\infty$ then by Theorem \ref{SudakovChevet} there exists a countable $B \subset C$ with $\sup_{v \in B} \abs{L_v} = +\infty$ a.s.  By Proposition \ref{prop:ExistenceEssentialSupremumIsonormal} we know that $\sup_{v \in B} \abs{L_v}  = \abs{L(C)}^*$ a.s. therefore we know that $C$ is not a GB-set.  

Therefore suppose that $C$ is a GB-set, we know that $\limsup_{\epsilon \to 0} \epsilon^2 D(\epsilon, C) < \infty$ so there exists $M < \infty$ and $\delta > 0$ such that $D(\epsilon, C) \leq e^{M/\epsilon^2} < \infty$ for all $0 < \epsilon < \delta$.  Let $v_1, \dotsc, v_n$ be a maximial set with the property that $\norm{v_i - v_j} > \epsilon$ for all $1 \leq i < j \leq n$; we know that that $n \leq D(\epsilon,C) < \infty$.  Thus $v_1, \dotsc, v_n$ is an $\epsilon$-net for $C$ and we have $C$ is totally bounded.

Let $C$ be a GC-set and let $L$ be an isonormal process on $H$ such that the sample paths of $L$ are uniformly continuous on $C$.  Since $C$ is totally bounded and a uniformly continuous function on a totally bounded set is bounded (there exists $\epsilon>0$ such that $\norm{v-w} \leq \epsilon$ implies $\abs{L_v - L_w} \leq 1$, pick an $\epsilon$-net $v_1, \dotsc, v_n$ and for any $v \in C$ let $1 \leq m \leq n$ be the smallest integer such that $\norm{v-v_m} \leq \epsilon$ then by the triangle inequality $\abs{L_v} \leq \abs{L_{v_1}} + \abs{L_{v_m} - L_v} + \sum_{j=1}^{m-1} \abs{L_{v_{j+1}} - L_{v_j}} \leq \abs{L_{v_1}} + n$)  it follows that every sample path of $L$ is bounded.  Hence the sample paths of $L$ are bounded on every countable subset of $C$ hence $\abs{L(C)}^* < \infty$ everywhere (not just almost surely). 
\end{proof}


We now establish some conditions that are equivalent to a set being a GC-set.  In order to define them we need some definitions.

First need a definition that characterizes the restrictions of linear functions.
\begin{defn}Let $V$ be a vector space and $C \subset V$, then a function $f : C \to \reals$ is \emph{prelinear} if and only if for every $n \in \naturals$, 
$a_1, \dotsc, a_n \in \reals$ and $v_1, \dotsc, v_n \in C$ such that $a_1 v_1 + \dotsb + a_n v_n = 0$ we have $fa_1 (v_1) + \dotsb + a_n f(v_n) = 0$.
\end{defn}

To see that this is the correct definition we prove the following.
\begin{prop}\label{PrelinearFunctionsAreRestrictionsOfLinear}Let $V$ be a vector space, $C \subset V$ be a subset and $f : C \to \reals$ be a prelinear function.  For every $n \in \naturals$, $a_1, \dotsc, a_n \in \reals$ and $v_1, \dotsc, v_n \in C$ let 
\begin{align*}
g(a_1 v_1 + \dotsb + a_n v_n) = a_1 f(v_1) + \dotsb + a_n f(v_n)
\end{align*}
then $g$ is a well defined linear function from the linear span of $C$ to $\reals$.  Such an extension exists if and only if $f$ is prelinear.
\end{prop}
\begin{proof}
To see that $g$ is well defined, let $a_1, \dotsc, a_n, b_1, \dotsc, b_m \in \reals$ and $v_1, \dotsc, v_n, w_1, \dotsc, w_m \in C$ such that $a_1 v_1 + \dotsb + a_n v_n = b_1 w_1 + \dotsb + b_m w_m$.  Then since $f$ is prelinear we know that $a_1 f(v_1) + \dotsb + a_n f(v_n) - b_1 f(w_1) - \dotsb - b_m f(w_m) = 0$ and therefore $g(a_1 v_1 + \dotsb + a_n v_n) = g(b_1 w_1 + \dotsb + b_m w_m)$.  

To see that $g$ extends $f$ simply let $v \in C$ represent itself so $f(v) = g(v)$.  To see that $g$ is linear, let $v, w$ be in the linear span of $C$ then by using zero coefficients as necessary we may assume there exist $v_1, \dotsc, v_n \in C$, $a_1, \dotsc, a_n, b_1, \dotsc, b_n \in \reals$ such that $v = a_1 v_1 + \dotsb + a_n v_n$ and $w = b_1 v_1 + \dotsb + b_n v_n$.  For any $c, d \in \reals$ we have
\begin{align*}
g(cv + dw) &= g((ca_1 + d b_1) v_1 + \dotsb + (ca_n + d b_n) v_n) = (ca_1 + d b_1) f(v_1) + \dotsb + (ca_n + d b_n) f(v_n) \\
&= c( a_1 f(v_1) + \dotsb a_n f(v_n)) + d( b_1 f(v_1) + \dotsb + b_nf(v_n)) = c g(v) + d g(w)
\end{align*}

Lastly suppose that $f$ has a linear extension $g$ and pick $a_1 v_1 + \dotsb + a_n v_n = 0$ with $a_1, \dotsc, a_n \in \reals$ and $v_1, \dotsc, v_n \in C$.   By linearity of $g$,
\begin{align*}
0 &= g(a_1 v_1 + \dotsb + a_n v_n) = a_1 g(v_1) + \dotsb + a_n g(v_n) = a_1 f(v_1) + \dotsb + a_n f(v_n) 
\end{align*}
\end{proof}


\begin{defn}Let $H$ be a Hilbert space then an orthogonal projection $\pi : H \to H$ with a finite dimensional range is called a \emph{finite dimensional projection}.  If $\pi_n$ is a sequence of projections with $\range{\pi_1} \subset \range{\pi_2} \subset \dotsb$ and $\cup_{n=1}^\infty \range{\pi_n}$ dense in $H$ then we say that $\pi_n \uparrow \IdentityMatrix$.  If $\pi : H \to H$ is any orthogonal projection then we let $\pi^\perp$ be the orthogonal projection onto the orthogonal complement of $\range{\pi_n}$.
\end{defn}

\begin{prop}\label{FDPAdaptedBasis}Suppose that $H$ be a Hilbert space and $\pi_n$ is a sequence of finite dimensional projections with $\pi_n \uparrow \IdentityMatrix$ then there exists an  orthonormal basis $e_1, e_2, \dotsc$ of $H$ and a subsequence $N_n$ such that $e_1,\dotsc, e_{N_n}$ is an orthonormal basis of $\range{\pi_n}$ for every $n \in \naturals$.
\end{prop}
\begin{proof}
We construct the basis by induction.  Begin by picking an orthonormal basis of $\range{\pi_1}$.  Assume that we have an orthonormal basis of $\range{\pi_n}$.
We claim that $\range{\pi_{n+1} - \pi_n}$ is the orthogonal complement of $\range{\pi_n}$ in $\range{\pi_{n+1}}$.  To see this first note that $\pi_n = \pi_{n+1} \circ \pi_n = \pi_n \circ \pi_{n+1}$  adjoin an orthonormal basis for that 

To see that $\range{\pi_{n+1} - \pi_n} \subset \range{\pi_{n+1}}$ note that $\pi_{n+1} v - \pi_n v = \pi_{n+1} v - \pi_{n+1} \pi_{n} v = \pi_{n+1}(v - \pi_n v)$.

TODO: Finish
\end{proof}

TODO: Do we need this result for pseudometric spaces?
\begin{prop}\label{SpaceOfUniformlyContinuousFunctionsOnTotallyBoundedSet}Let $C$ be a totally bounded subset of a metric space $(S,d)$ and let $\hat{C}$ be the completion of $C$.  Then $\hat{C}$ is compact and the space $U^d(C)$ of uniformly continuous functions on $C$ with the supremum norm is a separable Banach space isometric to $C(\hat{C}; \reals)$.
\end{prop}
\begin{proof}
TODO: I think I have already proved this in Lemma \ref{SeparabilityOfBoundedUniformlyContinuous}.

We first claim that the completion of $C$ remains totally bounded.  Let $(\hat{C}, \hat{d})$ be the completion of $C$ and let $\psi : C \to \hat{C}$ be an isometry with $\psi(C)$ dense.  For $\epsilon > 0$ pick an $\epsilon$-net $x_1, \dotsc, x_n \in C$ (i.e. for every $y \in C$ there exists $1 \leq j \leq n$ with $d(x_j,y) < \epsilon$).  Then $\psi(x_1), \dotsc, \psi(x_n)$ is a $2\epsilon$-net in $\hat{C}$: for any $z \in \hat{C}$ we can pick $y \in C$ with $\hat{d}(y,z) < \epsilon$ and then $1 \leq j \leq n$ with $d(x_j,y) < \epsilon$ so that 
\begin{align*}
\hat{d}(\psi(x_j), z) \leq \hat{d}(\psi(x_j), \psi(y)) + \hat{d}(\psi(y), z) = d(x_j, y) + \hat{d}(\psi(y), z) < 2 \epsilon
\end{align*}
Since $\hat{C}$ complete and totally bounded it is compact (Theorem \ref{CompactnessInMetricSpaces}).  

Let $f : C \to \reals$ be a uniformly continuous function.  Since $\psi : C \to \hat{C}$ is an isometry then $\psi^{-1} : \psi(C) \to C$ exists and is continuous.  Then since $\psi(C)$ is dense in $\hat{C}$, by Proposition \ref{ExtensionOfUniformlyContinuousMapCompleteRange} $f \circ \psi^{-1}$ has a unique extension to a continuous function $\overline{f} : \hat{C} \to \reals$.  To see that $\sup_{x \in C} \abs{f(x)} = \sup_{x \in \hat{C}} \abs{\overline{f}(x)}$, let $\epsilon > 0$ be given and pick $y \in \hat{C}$ such that $\sup_{x \in \hat{C}} \abs{\overline{f}(x)} < \abs{\overline{f}(y)} + \epsilon/2$.  Since $\overline{f}$ is continuous and $\psi(C)$ is dense in $\hat{C}$ we may pick $z \in C$ such that $\abs{f(z) - \overline{f}(y)}=\abs{\overline{f}(\psi(z)) - \overline{f}(y)}<\epsilon/2$ hence 
\begin{align*}
\sup_{x \in \hat{C}} \abs{\overline{f}(x)} &< \abs{\overline{f}(\psi(y))} + \epsilon/2 \leq \abs{f(z) - \overline{f}(y)} + \abs{f(z)} + \epsilon/2 \\
&< \abs{f(z)} + \epsilon \leq \sup_{x \in C} \abs{f(x)} + \epsilon 
\end{align*}
Since $\epsilon > 0$ was arbitrary we conclude that $\sup_{x \in \hat{C}} \abs{\overline{f}(x)} \leq  \sup_{x \in C} \abs{f(x)}$.  The oppositie inequality is immediate.

If we are given an arbitrary $g : \hat{C} \to \reals$ then we can define $g \circ \psi : C \to \reals$.  To see that $g \circ \psi$ is uniformly continuous let $\epsilon > 0$ be given.  Since $\hat{C}$ is compact it is uniformly continuous (Theorem \ref{UniformContinuityOnCompactSets}) hence there exists $\delta>0$ such that $\hat{d}(z,w) < \delta$ implies $\abs{g(z) - g(w)} < \epsilon$.  Let $x,y \in C$ such that $d(x,y) < \delta$ then we have $\hat{d}(\psi(x), \psi(y)) < \delta$ hen $\abs{ (g \circ \psi)(x) - (g \circ \psi) (y)} < \epsilon$.

Separability of $U^d(C)$ follows from the fact that it is isometric to $C(\hat{C})$ and the separability of the latter space (Lemma \ref{SeparabilityOfBoundedUniformlyContinuous}).
\end{proof}

\begin{prop}\label{PrelinearUniformlyContinuousFunctions}Let $H$ be a separable Hilbert space and $C \subset H$.  For every $v \in H$ the function $f_v(c) = \langle v, c \rangle$ is a uniformly continuous prelinear function on $C$.  The set of prelinear functions in $U(C)$ is closed in the uniform norm.
\end{prop}
\begin{proof}
Let $v \in H$.  Prelinearity of $f_v$ follows from the bilinearity of the inner product: suppose $v_1, \dotsc, v_n \in C$, $a_1, \dotsc, a_n$ with $a_1v_1 + \dotsb + a_nv_n = 0$ then 
\begin{align*}
a_1 f_v(v_1) + \dotsb + a_n f_v(v_n) &= \langle v, a_1 v_1 + \dotsb + a_n v_n \rangle = 0
\end{align*}
Uniform continuity of $f_v$ follows from the Cauchy Schwartz inequality: suppose $\epsilon > 0$ is given the for all $\norm{c-d} < \epsilon/\norm{v}$ we have
\begin{align*}
\abs{f_v(c) - f_v(d)} &= \abs{\langle v, c - d \rangle} \leq \norm{v} \norm{c-d} < \epsilon
\end{align*}

Now let $f,f_1, f_2, \dotsc \in U(C)$ with $f_{n} \to f$ uniformly and $f_1, f_2, \dotsc$ prelinear.  Suppose $v_1, \dotsc, v_n \in C$, $a_1, \dotsc, a_n$ with $a_1v_1 + \dotsb + a_nv_n = 0$ then  since $f_n \to f$ pointwise we have
\begin{align*}
a_1 f(v_1) + \dotsb + a_n f(v_n) &= \lim_{j \to \infty} a_1 f_j(v_1) + \dotsb + a_n f_j(v_n) = 0
\end{align*}
\end{proof}

\begin{defn}Let $H$ be a separable Hilbert space and $C \subset H$ and $V \subset \reals^C$ be a set of functions on $C$.  Then the isonormal process $L$ can be \emph{realized} on $V$ if there exists a probability measure $\mu$ on $V$ with respect to the product sigma algebra $\mathcal{B}(\reals)^C$ such that the canonical process $X_c(f) = f(c)$ on $V$ is centered Gaussian with covariance $\expectation{X_c X_d} = \langle c , d \rangle$.
\end{defn}
For example if $C$ is a GC-set then $L$ is realized on $U(C)$.

TODO: Show that the product $\sigma$-algebra on $\reals^C$ induces the Borel $\sigma$-algebra on $U(C)$ as well as the prelinear functions in $U(C)$ and the closure of $H$ in $U(C)$ (compare Lemma \ref{BorelGeneratedByProjections}).

\begin{thm}[Levy's Three Series Condition]\label{LevyThreeSeriesCondition}Let $\xi_1, \xi_2, \dotsc$ be independent random variables, then $\sum_{n=1}^\infty \xi_n$ converges a.s. if and only if $\sum_{n=1}^\infty \xi_n$ converges in distribution and if and only if following three conditions hold
\begin{itemize}
\item[(i)] $\sum_{n=1}^\infty \probability{ \abs{\xi_n} > 1} < \infty$
\item[(ii)] $\sum_{n=1}^\infty \expectation{ \xi_n ; \abs{\xi_n} \leq 1}$ converges
\item[(iii)] $\sum_{n=1}^\infty \variance{ \xi_n ; \abs{\xi_n} \leq 1} < \infty$
\end{itemize}
\end{thm}
\begin{proof}
TODO
\end{proof}

\begin{thm}\label{LevySymmetricMaximalInequality}Let $X^1, X^2, \dotsc$ be independent symmetric stochastic processes with countable index set $T$ and $\norm{X^n}_Y < \infty$ a.s. for all $n \in \naturals$ (TODO: Do we really need this?).  For every $n \in \naturals$ define $S_n = X^1 + \dotsc + X^n$. Then for every $\lambda > 0$ and $n \in \naturals$ we have
\begin{align*}
\probability{ \max_{1 \leq j \leq n} \norm{S_j}_Y > \lambda} &\leq 2 \probability{\norm{S_n}_Y > \lambda} 
\end{align*}
\end{thm}
\begin{proof}
TODO
\end{proof}

\begin{lem}\label{lem:OrthogonalProjectionsAndIndependence}Let $H$ be a separable Hilbert space, $Y \subset H$ a countable subset and $\pi_1, \dotsc, \pi_n$ be finite dimensional projections with $\range{\pi_i} \perp \range{\pi_j}$ for all $1 \leq i < j \leq n$ then $L \circ \pi_1, \dotsc, L \circ \pi_n$ are independent on $Y$.
\end{lem}
\begin{proof}
TODO: See argument in Lemma \ref{lem:FDPUniformConvergence} below.

TODO: Is the restriction to $Y$ countable necessary?
\end{proof}

\begin{lem}\label{lem:FDPUniformConvergence}Let $H$ be a separable Hilbert space, $C \subset H$ be totally bounded, $Y \subset C$ a countable dense subset and $\pi_1, \pi_2, \dotsc$ be finite dimensional projections with $\pi_n \uparrow \IdentityMatrix$ and $\abs{L(\pi_n^\perp C)}^* \toprob 0$, then $\norm{L \circ \pi_n - L}_Y \toas 0$.
\end{lem}
\begin{proof}
Suppose we have finite dimensional projections $\pi_1, \pi_2, \dotsc$ with $\pi_n \uparrow \IdentityMatrix$.  For $v \in C$ and $n \in \naturals$ define
\begin{align*}
X^n_v &= L_{\pi_n v} - L_{\pi_{n-1} v}
\end{align*}

\begin{clm}For every $n \in \naturals$, $X^1, \dotsc, X^n$ are independent Gaussian processes on $Y$
\end{clm}
Since $L$ is a centered Gaussian process, for every $n \in \naturals$ and $v \in C$, $( L_{\pi_{n-1} v},  L_{\pi_{n} v})$ is a centered Gaussian random vector  and therefore $X^n_v$ is a centered Gaussian random variable.  More generally the same argument that $X^1, \dotsc, X^n$ are jointly Gaussian random processes.  Moreover for $m < n$ and $v,w \in C$,
\begin{align*}
\expectation{X^n_v X^m_w} &= \expectation{(L_{\pi_n v} - L_{\pi_{n-1} v}) (L_{\pi_m w} - L_{\pi_{m-1} w})} \\
&=\expectation{L_{\pi_n v}L_{\pi_m w}} - \expectation{L_{\pi_{n-1} v}L_{\pi_m w}} - \expectation{L_{\pi_n v}L_{\pi_{m-1} w}} + \expectation{L_{\pi_{n-1} v}L_{\pi_{m-1} w}} \\
&=\langle \pi_n v, \pi_m w \rangle - \langle \pi_{n-1} v, \pi_m w \rangle - \langle \pi_n v, \pi_{m-1} w \rangle + \langle \pi_{n-1} v, \pi_{m-1} w \rangle \\
&=\langle v, \pi_n (\pi_m w) \rangle - \langle v, \pi_{n-1}(\pi_m w) \rangle - \langle v, \pi_n(\pi_{m-1} w) \rangle + \langle v, \pi_{n-1}(\pi_{m-1}) w \rangle \\
&=\langle v, \pi_m w \rangle - \langle v, \pi_m w \rangle - \langle v, \pi_{m-1} w \rangle + \langle v, \pi_{m-1} w \rangle \\
&=0
\end{align*}
and it follows from Proposition \ref{GaussianProcessIndependence} that $X^1, X^2, \dotsc, X^n$ are independent processes.  TODO: Is it not true that $X^1, \dotsc, X^n$ are independent Gaussian processes on all of $C$?

TODO: Show that $\norm{X^j}_Y < \infty$ a.s.
Since Gaussians are symmetric we may apply Levy's Inequality Theorem \ref{LevySymmetricMaximalInequality} to conclude (noting that $X^1 + \dotsb + X^n = L \circ \pi_n$):
\begin{align*}
\probability{\max_{1 \leq j \leq n} \norm{L \circ \pi_j}_Y > \lambda} &\leq 2 \probability{\norm{L \circ \pi_n}_Y > \lambda}
\end{align*}
Equally, for every $k \in \naturals$ we apply Levy's Inequality Theorem \ref{LevySymmetricMaximalInequality} to the sequence $X^{k+1}, X^{k+2}, \dotsc$ (noting that
$X^{k+1} + \dotsb + X^n = L \circ \pi_n - L \circ \pi_k$) to see that for all $n > k$,
\begin{align*}
\probability{\max_{k \leq j \leq n} \norm{L \circ \pi_j - L \circ \pi_k}_Y > \lambda} &\leq 2 \probability{\norm{L \circ \pi_n - L \circ \pi_k}_Y > \lambda}
\end{align*}
TODO: Finish
\end{proof}

TODO: Are the conditions on $\abs{L(\pi_n^\perp C)}^*$ below independent of the choice of orthonormal process $L$?  That is a conclusion of the Theorem but do we use that independence anywhere in the proof?

\begin{lem}\label{lem:DenseSubsetClosedSymmetricConvexHull}Let $V$ be a normed vector space and let $C \subset V$ be a subset.  If $Y$ is a dense subset of $C$ then the set of rational convex combinations of $Y \cup -Y$ is a dense subset of $\closedsymmconvexhull{C}$.
\end{lem}
\begin{proof}
We first address approximation of convex combinations by rational convex combinations.
\begin{clm}Let $v_1, \dotsc, v_n \in V$, $a_1, \dotsc, a_n \in [0,1]$ with $a_1 + \dotsb + a_n = 1$ and $\epsilon > 0$ then there exist $b_1, \dotsc, b_n \in [0,1] \cap \rationals$ such that $\norm{(a_1-b_1) v_1 + \dotsb + (a_n - b_n) v_n} < \epsilon$.
\end{clm}
 The case in which some $a_j=1$ we just define $b_j=a_j$ for all $1 \leq j \leq n$ and f $a_j=0$ then we can just let $b_j=0$ so we assume that $0 < a_j < 1$ for all $1 \leq j \leq n$ (and consequently $n > 1$). For $1 \leq j \leq n -1$ by density of rationals we may pick $b_j \in (0,1) \cap \rationals$ such that 
\begin{align*}
\abs{a_j-b_j} &< \frac{\epsilon}{n\norm{v_j}} \minop \frac{\epsilon}{n(n-1)\norm{v_n}} \minop \frac{a_n}{n-1}
\end{align*}
and define $b_n = 1 - b_1 - \dotsb - b_{n-1}$.  Note that 
\begin{align*}
0 &< b_1 + \dotsb + b_{n-1} \leq a_1 + \dotsb + a_{n-1} + \abs{a_1 - b_1} + \dotsb + \abs{a_{n-1} - b_{n-1} } < 1 - a_n + a_n = 1
\end{align*}
so $b_n \in (0,1) \cap \rationals$.  Also,
\begin{align*}
\norm{a_n - b_n} &=\norm{b_1 - a_1 + \dotsb + b_{n-1} - a_{n-1}} \leq (n-1) \min_{1 \leq j \leq n-1} \norm{a_j - b_j} < \frac{\epsilon}{n\norm{v_n}}
\end{align*}
By the triangle inequality,
\begin{align*}
\norm{a_1 v_1 + \dotsb a_n v_n - b_1 v_1 - \dotsb - b_n v_n} 
&\leq \abs{a_1-b_1} \norm{v_1} + \dotsb + \abs{a_n - b_n}\norm{v_n} 
< \epsilon
\end{align*}

Let $v \in \closedsymmconvexhull{C}$ and $\epsilon > 0$ be given.
Let $w = a_1 w_1 + \dotsb + a_n w_n - a_{n+1} w_{n+1} - \dotsb - a_{n+m} w_{n+m}$ with $a_1, \dotsc, a_{n+m} \in [0,1]$ and $w_1, \dotsc, w_{n+m} \in C$ be a convex combination with $\norm{v - w} < \epsilon/3$.  By density of $Y$ in $C$ for 
$1 \leq j \leq n+m$ pick $u_j \in Y$ with $\norm{w_j - u_j} < \frac{\epsilon}{3(n+m) a_j}$ if $a_j > 0$ or let $w_j$ be an arbitrary element of $Y$ otherwise.  Then
\begin{align*}
&\norm{w - a_1 u_1 - \dotsb - a_n u_n + a_{n+1} u_{n+1} + \dotsb + a_{n+m} u_{n+m}} \\
&\leq a_1 \norm{w_1 - u_1} + \dotsb + a_{n+m} \norm{w_{n+m} - u_{n+m}} < \epsilon/3
\end{align*}
Lastly by the claim pick $b_1, \dotsc, b_{n+m} \in [0,1] \cap \rationals$ with $b_1 + \dotsb + b_{n+m}=1$ such that 
\begin{align*}
\norm{(a_1 - b_1) u_1 + \dotsb + (a_n - b_n) u_n - (a_{n+1} -b_{n+1}) u_{n+1} - \dotsb - (a_{n+m} - b_{n+m}) u_{n+m}} < \epsilon/3
\end{align*}
It follows that $\norm{v - b_1 u_1 - \dotsb - b_n u_n + b_{n+1} u_{n+1} + \dotsb + b_{n+m} u_{n+m}} < \epsilon$.
\end{proof}

\begin{lem}\label{lem:IsonormalFiniteDimensionalBoundedPositiveProbability}Let $H$ be a finite dimensional Hilbert space and $C \subset H$ be bounded then $\probability{\abs{L(C)}^* < \lambda} > 0$ for all $\lambda > 0$.
\end{lem}
\begin{proof}
We construct $L$ by taking an orthonormal basis $e_1, \dotsc, e_d$ and i.i.d. standard normals $\xi_1, \dotsc, \xi_d$ so that $L_v = \langle v, \sum_{j=1}^d \xi_j e_j \rangle$ (Proposition \ref{IsonormalFiniteDimensional}).  Let $M > 0$ be such that $\norm{v} \leq M$ for all $v \in C$, then by Cauchy-Schwartz, we have
\begin{align*}
\abs{L_v} &= \abs{\langle v, \sum_{j=1}^d \xi_j e_j \rangle} \leq \norm{v} (\xi_1^2 + \dotsc + \xi_d^2)^{1/2} \leq M (\xi_1^2 + \dotsc + \xi_d^2)^{1/2}
\end{align*}
Therefore $\probability{\abs{L(C)}^* < \lambda} > \probability{(\xi_1^2 + \dotsc + \xi_d^2)^{1/2} < \lambda/M} > 0$ (TODO: prove the last inequality)
\end{proof}

\begin{thm}\label{GCSetEquivalentConditions}Let $H$ be a separable Hilbert space and $C \subset H$ be totally bounded then the following are equivalent:
\begin{itemize}
\item[(i)] $C$ is a GC-set
\item[(ii)] $\closedsymmconvexhull{C}$ is a GC-set
\item[(iii)] For any $\epsilon > 0$, $\probability{\abs{L(C)}^* < \epsilon} > 0$
\item[(iv)] There exist finite dimensional projections with $\pi_n \uparrow \IdentityMatrix$ such that $\liminf_{n \to \infty} \abs{L(\pi_n^\perp C)}^* = 0$ a.s.
\item[(v)] There exist finite dimensional projections with $\pi_n \uparrow \IdentityMatrix$ such that $\abs{L(\pi_n^\perp C)}^* \toprob 0$ 
\item[(vi)] There exist finite dimensional projections with $\pi_n \uparrow \IdentityMatrix$ such that $\abs{L(\pi_n^\perp C)}^* \toas 0$ 
\item[(vii)] For every sequence of finite dimensional projections with $\pi_n \uparrow \IdentityMatrix$ we have $\abs{L(\pi_n^\perp C)}^* \toprob 0$ 
\item[(viii)] For every sequence of finite dimensional projections with $\pi_n \uparrow \IdentityMatrix$ we have $\abs{L(\pi_n^\perp C)}^* \toas 0$ 
\item[(ix)] $L$ can be realized on the closure of $H$ in $U(C)$ 
\item[(x)] $L$ can be realized on the set of prelinear functions in $U(C)$.
\item[(xi)] $L$ can be realized on $U(C)$.
\end{itemize}
\end{thm}
\begin{proof}
By Lemma \ref{ConvergenceAlmostSureImpliesInProbability} (viii) implies (vii) which trivially implies (v).  

We now show that (v) implies (vi).  The key is the following construction.   By Proposition \ref{FDPAdaptedBasis} let $e_1, e_2, \dotsc$ be an orthonormal basis for $H$ such that there exists a subsequence $N_n$ so that $e_1, \dotsc, e_{N_n}$ is an orthonormal basis for $\range{\pi_n}$ for all $n \in \naturals$.
Define 
\begin{align*}
M^n_v &= \sum_{j=1}^{N_n} \langle v, e_j \rangle L_{e_j} = \langle v, \sum_{j=1}^{N_n} L_{e_j} e_j \rangle
\end{align*} 
and $M_v = \sum_{j=1}^\infty \langle v, e_j \rangle L_{e_j}$. 

\begin{clm}\label{clm:PrelinearModification} $M$ is a modification of $L$ on $C$ and $\norm{M^n - M}_C \toas 0$.  Furthermore $M$ is in the uniform closure of $H$ in $U(C)$.
\end{clm}
Let $v \in C$. Observe that since $L_{e_1}, L_{e_2}, \dotsc$ is an orthonormal set in $L^2(\Omega)$ we have $\norm{M_v}^2 = \sum_{j=1}^\infty \langle v, e_j \rangle = \norm{v}^2 < \infty$ and it follows that $M_v$ is a well defined element of $L^2(\Omega)$ and the partial sums converge in $L^2$: $\sum_{j=1}^n \langle v, e_j \rangle  L_{e_j} \tolp{2} M_v$.  Passing to the subsequence of partial sums we know that $M^n_v \tolp{2} M_v$ as well.  By Lemma \ref{ConvergenceInMeanImpliesInProbability} we know that $\sum_{j=1}^n \langle v, e_j \rangle  L_{e_j} \toprob M_v$.  Since $L_{e_1}, L_{e_2}, \dotsc$ are independent, Theorem \ref{LevyThreeSeriesCondition} applies and we conclude that $\sum_{j=1}^n \langle v, e_j \rangle  L_{e_j} \toas M_v$.  Again, passing to a subsequence we see that $M^n_v \toas M_v$.

By linearity of the isonormal process (Proposition \ref{LinearityIsonormalProcess}) we have for every $v \in C$ that 
\begin{align*}
M^n_v &= \sum_{j=1}^{N_n} \langle v, e_j \rangle L_{e_j} =  L_{\sum_{j=1}^{N_n}  \langle v, e_j \rangle e_j} = L_{\pi_n v} \text{ a.s.}
\end{align*}
Let $Y$ be a countable dense subset of $C$, it then follows $M^n = L \circ \pi_n$ on $Y$ for all $n \in \naturals$ a.s. and therefore since $\norm{L \circ \pi_n - L}_Y \toas 0$ (Lemma \ref{lem:FDPUniformConvergence}) we know that almost surely $M^n$ converges uniformly on $Y$ (specifically $\norm{M^n - L}_Y \toas 0$).  
Let $v \in Y$, then almost surely $\lim_{n \to \infty} M^n_v = M_v$ and $\lim_{n \to \infty} M^n_v = L_v$ hence $M_v = L_v$ a.s. and $M$ is a modification of $L$ on $Y$.  

The expression 
\begin{align*}
M^n_v &= \langle v, \sum_{j=1}^{N_n} L_{e_j} e_j \rangle
\end{align*}
and Proposition \ref{PrelinearUniformlyContinuousFunctions} implies $M^n$ is prelinear and uniformly continuous on $Y$.  Since almost surely $M^n$ converges to $M$ uniformly on $Y$ it follows from Proposition \ref{PrelinearUniformlyContinuousFunctions} that $M$ is uniformly continuous on $Y$ a.s.  Since $Y$ is dense in $C$, by Proposition \ref{ExtensionOfUniformlyContinuousMapCompleteRange}, $M$ extends to an almost surely uniformly continuous process on $C$ (TODO: is it in fact the case that $\lim_{w \to v} M_w = M_v$ or are we redefining $M_v$ on $C \setminus Y$ to be $\lim_{n \to \infty} M_{v_n}$ for some $v_n \to v$ with $v_n \in Y$?)

To see that $M$ is also a modification of $L$ on all of $C$, let $v \in C$ and pick a sequence $v_n \in Y$ such that $\lim_{n \to \infty} v_n = v$ so that $M_v = \lim_{n \to \infty} M_{v_n}$.   Almost surely $M_{v_n} = L_{v_n}$ for all $n \in \naturals$.  Since $L$ is isonormal we have $\expectation{(L_{v_n} - L_v)^2} = \norm{v_n - v}^2 \to 0$ and therefore by Markov bound $L_{v_n} \toprob L_v$.  By Lemma \ref{ConvergenceInProbabilityAlmostSureSubsequence} we know that there is a subsequence $n_k$ such that $\lim_{k \to \infty} L_{v_{n_k}} = L_v$.  Therefore 
\begin{align*}
L_v &= \lim_{k \to \infty} L_{v_{n_k}} = \lim_{k \to \infty} M_{v_{n_k}} = M_v \text{ a.s.}
\end{align*}
Since $M$ is a modification of $L$ on $C$ we know that $\abs{L(\pi_n^\perp(C))}^* = \abs{M(\pi_n^\perp(C))}^*$. It follows that $\norm{M^n - M}_Y \toas 0$ and $\norm{M^n - M}_C \toas 0$ since $\norm{M^n - M}_Y = \norm{M^n - M}_C$ (since $M$ is the unique uniformly continuous extension of $M$ restricted to $Y$).

By definition for $v \in Y$, we have
$M^n_v - M_v = M_{\pi_n(v)} - M_v = M_{\pi_n^\perp(v)}$ and therefore since $Y$ is a countable dense subset of $C$ we have 
\begin{align*}
\abs{L(\pi_n^\perp(C))}^* &= \abs{M(\pi_n^\perp(C))}^* = \sup_{v \in \pi_n^\perp(Y)} \abs{M_v} = \sup_{v \in Y} \abs{M_{\pi_n^\perp(v)} } \\
&= \sup_{v \in Y} \abs{M_{\pi_n(v)} - M_v} \toas 0
\end{align*}
and therefore (vi) is proven.

We now show that (v) implies (vii) and (viii).  Suppose $\pi_n$ is a sequence of finite dimensional projections as in (v) and let 
$\gamma_n$ be any other sequence of finite dimensional projections with $\gamma_n \uparrow \IdentityMatrix$.  Let $\epsilon > 0$ be given, pick $k \in naturals$ such that $\probability{\abs{L(\pi_k^\perp(C)) > \epsilon}} \leq \epsilon$.  Let $e_1, e_2, \dotsc$ be an orthonormal
basis of $H$ as in Proposition \ref{FDPAdaptedBasis} so that $e_1, \dotsc, e_{N_k}$ is an orthonormal basis of $\range{\pi_k}$.  For fixed $j \in \naturals$, $\lim_{m \to \infty} \gamma_m^\perp(e_j) = 0$ (since $\gamma_m \uparrow \IdentityMatrix$).  TODO: For fixed $k \in \naturals$, it follows that $\abs{L(\gamma_m^\perp(\pi_k(C)))}* \toprob 0$.  By Lemma \ref{EssentialSupremumUnderContraction}
\begin{align*}
\probability{\abs{L(\gamma_m^\perp(\pi_k^\perp(C)))}^* > \epsilon} \leq \probability{\abs{L(\pi_k^\perp(C))}^* > \epsilon} \leq \epsilon
\end{align*}
so $\abs{L(\gamma_m^\perp(\pi_k^\perp(C)))}^* \toprob 0$.
Since $\abs{L(\gamma_m^\perp(C))}^* \leq \abs{L(\gamma_m^\perp(\pi_k(C)))}^* + \abs{L(\gamma_m^\perp(\pi_k^\perp(C)))}^*$ a.s. (TODO) we see that
(vii) holds.  Since we know that (v) implies (vi) it follows that (vii) implies (viii).  Thus (v) - (viii) are all equivalent.

As a side effect of the above construction in Claim \ref{clm:PrelinearModification} we see that (v) implies there exists a version of $L$ restricted to $C$ with sample paths in the uniform closure of $H$ in $U(C)$.  Thus (v) implies (ix) which in turn implies (x) and (xi).

(xi) implies (i) is immediate as (xi) is just a restatement of the definition of a GC-set.

To see that (v) implies (ii) first let $\pi : H \to H$ be a finite dimensional projection.  Then we claim that $\abs{L(\pi^\perp\closedsymmconvexhull{C})}^* = \abs{L(\pi^\perp C)}^*$ a.s.  Let $Y$ be a countable dense subset of $C$.  By Lemma \ref{lem:DenseSubsetClosedSymmetricConvexHull} the set of rational convex combinations of $Y \cup -Y$ is a countable dense subset of $\closedsymmconvexhull{C}$.  By Proposition \ref{prop:ExistenceEssentialSupremumIsonormal} we know that $\abs{L(\pi^\perp\closedsymmconvexhull{C})}^*$ is supremum of $\abs{L_v}$ over all projections of rational convex combinations of $Y \cup -Y$.  So pick such a convex combination $a_1 v_1 + \dotsb a_n v_n - a_{n+1} v_{n+1} - \dotsb - a_{n+m} v_{n+m}$.  Then 
\begin{align*}
\abs{L_{\pi^\perp(a_1 v_1 + \dotsb a_n v_n - a_{n+1} v_{n+1} - \dotsb - a_{n+m} v_{n+m})}} &= \abs{a_1 L_{\pi^\perp v_1} + \dotsb a_n L_{\pi^\perp v_n} - a_{n+1} L_{\pi^\perp v_{n+1}} - \dotsb - a_{n+m} L_{\pi^\perp v_{n+m}}} \\
&\leq a_1 \abs{L_{\perp v_1}} + \dotsb a_n \abs{L_{\pi^\perp v_n}} + a_{n+1} \abs{L_{\pi^\perp v_{n+1}}} - \dotsb + a_{n+m} \abs{L_{\pi^\perp v_{n+m}}} 
\leq (a_1 + \dotsb + a_{n+m}) \sup_{v \in Y} \abs{L_{\pi^\perp v}} \\
&=\abs{L(\pi^\perp C)}^*
\end{align*}
Taking the supremum over all such convex combinations we see that $\abs{L(\pi^\perp\closedsymmconvexhull{C})}^* \leq \abs{L(\pi^\perp C)}^*$ a.s.  By Proposition \ref{AlmostSureUniquenessEssentialSupremum}, the opposite inequality follows from the fact that $\pi^\perp C \subset \pi^\perp\closedsymmconvexhull{C}$ so equality holds.  Thus if (v) holds for $C$ it also holds for $\closedsymmconvexhull{C}$ which as we have argued implies that $\closedsymmconvexhull{C}$ is a GC-set.

TODO: (v) implies (iii).  Assume (v) holds, let $\epsilon > 0$  and pick a finite dimensional projection $\pi : H \to H$ such that $\probability{\abs{L(\pi^\perp C)}* > \epsilon/2} < 1/2$.  On the other hand, $\pi C$ is a bounded set in a finite dimensional space so by Lemma \ref{lem:IsonormalFiniteDimensionalBoundedPositiveProbability} we know that $\probability{\abs{L(\pi C)}* < \epsilon/2} > 0$.  Thus since $L \circ \pi$ and $L \circ \pi^\perp$ are independent (Lemma \ref{lem:OrthogonalProjectionsAndIndependence}) we get
\begin{align*}
\probability{\abs{L(C)}* < \epsilon} &= \probability{\abs{L(\pi C \cup \pi^\perp C)}* < \epsilon} = \probability{\abs{L(\pi C)}^* + \abs{L(\pi^\perp C)}* < \epsilon} \\
&\geq \probability{\abs{L(\pi C)}^* \leq \epsilon/2 ; \abs{L(\pi^\perp C)}* < \epsilon/2} = \probability{\abs{L(\pi C)}^* \leq \epsilon/2} \probability{\abs{L(\pi^\perp C)}* < \epsilon/2} > 1/2  \probability{\abs{L(\pi^\perp C)}* < \epsilon/2} > 0
\end{align*}

TODO: (iii) implies (iv).
 
TODO: (iv) implies (i).
 
TODO: (i) implies (v).
 
TODO: Finish
\end{proof}


\subsection{Sample Continuity for Compact Index Sets}

In this section we explore conditions for sample continuity of Gaussian processes with compact index sets.

Note that Dudley uses the term version differently than most other authors (at least those that I have read).  Most authors treat version and modification as synonyms (i.e. for all $t \in T$ we have $\probability{X_t = Y_t}=1$).  Dudley says that $X$ and $Y$ are versions if $X \eqdist Y$.

In this section we explore conditions under which a Gaussian process has a realization with continuous sample paths.  The first important fact is that the question can be reduced to the study of centered Gaussian processes.
\begin{thm}\label{GaussianVersionContinuityCentering}Let $X$ be a Gaussian process indexed by a metric space $(T,d)$ then there exists $Y$ with continuous sample paths such that $X \eqdist Y$ if and only if 
\begin{itemize}
\item[(i)] the function $t \mapsto \expectation{X_t}$ is continuous
\item[(ii)] there is a process $Z_t$ with continuous sample paths such that $Z \eqdist X - \expectation{X}$ 
\end{itemize}
If $X$ is version continuous then the map $X : T \to L^2(P)$ (given by $X (t)(\omega) = X_t(\omega)$) is continuous.
\end{thm}
\begin{proof}
If (i) and (ii) hold then it is clear that $Y_t = Z_t + \expectation{X_t}$ has continuous sample paths.  Furthermore if $t_1, \dotsc, t_d \in T$ and $A \in \mathcal{B}(\reals^d)$ then 
\begin{align*}
\probability{(Y_{t_1}, \dotsc, Y_{t_d}) \in A} &= \probability{(Z_{t_1}, \dotsc, Z_{t_d}) \in A - (\expectation{X_{t_1}}, \dotsc, \expectation{X_{t_d}})}  \\
&= \probability{(X_{t_1} - \expectation{X_{t_1}}, \dotsc, X_{t_d} - \expectation{X_{t_d}}) \in A - (\expectation{X_{t_1}}, \dotsc, \expectation{X_{t_1}})}  \\
&=\probability{(X_{t_1}, \dotsc, X_{t_d}) \in A} 
\end{align*}
and therefore $X \eqdist Y$ by Lemma \ref{ProcessLawsAndFDDs}.

On the other hand, suppose there exists $Y$ with continuous sample paths and $X \eqdist Y$.  For any $t \in T$ and $t_n \to t$ we have $Y_{t_n} \to Y_t$ everywhere.  Since the random vector $(Y_{t_n}, Y_t)$ is Gaussian it follows that $Y_{t_n} - Y_t$ is Gaussian.  By Lemma \ref{LimitOfGaussianRandomVectors} it follows that $\expectation{X_{t_n} - X_t}  = \expectation{Y_{t_n} - Y_t}  \to 0$.  Now subtract $\expectation{Y_t}$ from $Y_t$.
TODO: Finish.  The argument using Lemma \ref{LimitOfGaussianRandomVectors} seems like overkill; isn't the fact that $Y_{t_n} - Y_t \toas 0$ for a sequence of Gaussian r.v.s implies $\expectation{Y_{t_n} - Y_t} \to 0$ easier?  Let $\xi_n$ be a sequence of Gaussian r.v.'s with $\xi_n \todist 0$.  Then we have $e^{iu\mu_n - \frac{1}{2}u^2 \sigma_n^2} \to 1$.  Taking $u=\sqrt{2}$, the modulus and logarithm we get $sigma_n^2 \to 0$ (so $\sigma \to 0$).  Hence $e^{iu\mu_n} \to 1$ then take $u=-i$ and the logarithm to get $\mu_n \to 0$ (do we need the boundedness of $\mu_n$ and $\sigma_n$ as below?)

Here is the outline of an argument that doesn't rely on characteristic functions (it's not trivial either!).  Let $\xi_n \todist \xi$ with $\xi_n$ Gaussian $N(\mu_n, \sigma_n^2)$.  Since $\xi_n$ then they are a tight sequence (Lemma \ref{WeakConvergenceImpliesTight}).  Next use the tightness and Gaussian properties to show that $\mu_n$ and $\sigma_n^2$ are bounded (e.g. pick $0 < \epsilon < 1/2$ and then $M>0$ such that $\probability{\abs{\xi_n} \geq M} < \epsilon$ for all $n \in \naturals$, it follows that $\abs{\mu_n} < M$ for otherwise $\probability{\abs{\xi_n} \geq M} \geq 1/2$;  similarly once we bound the means $\mu_n$ to the range $[-M,M]$ we know that for fixed $\sigma$ the value of $\probability{N(\mu, \sigma^2) \geq M}$ is minimized for $\mu=-M$ and is equal to $\probability{N(0, \sigma^2) \geq 2M}$ the latter probability goes to $1/2$ as $\sigma \to \infty$ so we see that $\sigma_n$ must be bounded above as well; even better note that $\probability{\abs{N(\mu, \sigma^2) } \leq M} \leq \frac{2M}{\sigma \sqrt{2\pi}}$ so 
$\probability{\abs{N(\mu, \sigma^2) } > M} \geq 1 - \frac{2M}{\sigma \sqrt{2\pi}}$ which also implies $\sigma_n$ is bounded).

Assume that $\liminf_{n \to \infty} \mu_n \neq \limsup_{n \to \infty} \mu_n$ and get a contradiction to the distributional convergence.  Do the same with $\sigma_n^2$.  These are easy using characteristic functions as above (so $e^{-\lim_{n \to \infty} \sigma_n^2} = \abs{\varphi_\xi(\sqrt{2})}$ and $e^{\lim_{n \to \infty} \mu_n} = \varphi_\xi(-i)  \abs{\varphi_\xi(1)}$).
\end{proof}

\begin{thm}\label{PullbackContinuousVersions}Suppose $(S,d)$ is a separable metric space, $(T,e)$ is a compact metric space, $(K,\rho)$ is a metric space and $Y$ is a stochastic process indexed by 
$K$ with values in $S$.  If $h : T \to K$ is continuous and surjective then there exists $Z$ with continuous sample paths and $Y \eqdist Z$ if and only if there exists $W$ with continuous sample paths and $Y \circ h \eqdist W$.
\end{thm}
\begin{proof}
Suppose that $Z$ exists then $Z \circ h$ has continuous sample paths and for every $t_1, \dotsc, t_n \in T$ we have 
\begin{align*}
\probability{((Z \circ h)_{t_1}, \dotsc, (Z \circ h)_{t_n}) \in A}  
&= \probability{(Z_{h(t_1)}, \dotsc, Z_{h(t_n)}) \in A} 
= \probability{(Y_{h(t_1)}, \dotsc, Y_{h(t_n)}) \in A} \\
&= \probability{((Y \circ h)_{t_1}, \dotsc, (Y \circ h)_{t_n}) \in A} 
\end{align*}
so $(Y \circ h) \eqdist (Z \circ h)$ by Lemma \ref{ProcessLawsAndFDDs}.  

Now suppose that $W$ exists.  As $(T,e)$ is a compact metric space, it is separable (by Theorem \ref{CompactnessInMetricSpaces}, $T$ is totally bounded so for each $n \in \naturals$ there exists a finite set $F_n$ with $\cup_{x \in F_n} B(x, 1/n) = T$; observe that $\cup_n F_n$ is dense in $T$).   So let $A$ be a countable dense subset of $T$, let $B = h(A)$ and observe that $B$ is a countable dense subset of $K$ (for $y \in K$, by surjectivity of $h$ pick $x \in T$ with $h(x) = y$ and $a_n \in A$ such that $a_n \to x$, by continuity of $h$ we have $h(a_n) \to y$).

Note that $\zeta(s,t) = \rho(h(s), h(t))$ is a pseudometric on $T$ (this doesn't require continuity of $h$).  Let $UC$ be the event that a function $f : A \to \reals$ is uniformly continuous on $A$ with respect to the topology induced by $\zeta$.  More precisely (TODO: note that Dudley has an extra condition that appears to say $e(s,t) < 1 + \diam_e(T)$ which sounds vacuous).  
\begin{align*}
UC &= \bigcap_{n=1}^\infty \bigcup_{m=1}^\infty \bigcap_{\substack{s,t \in A \\ \zeta(s,t) < 1/m }} \left \lbrace f \in \reals^A   \mid d(f(s),f(t)) < 1/n \right \rbrace
\end{align*}
Since $S$ is separable we know continuity of $d$ implies measurability of $d$ hence each set $\left \lbrace f \in \reals^A \mid  d(f(s), f(t)) < 1/n \right \rbrace$ is measurable, thus by countability of $A$ we know that $UC$ is measurable.

\begin{clm} If $\probability{W \in UC} = 1$ then $Z$ exists.
\end{clm}
Since $UC$ is measurable and $W \eqdist Y \circ h$, we know that $\probability{Y \circ h \in UC} = 1$, so almost surely $Y \circ h$ has uniformly continuous sample paths on $A$ for the pseudometric $\zeta$.  It follows that almost surely $Y$ has uniformly continuous sample paths on $B$.  Since $B$ is dense in $K$ and $\reals$ are complete by Proposition \ref{ExtensionOfUniformlyContinuousMapCompleteRange} we may define $Z$ to be the unique continuous extension of $Y$ from $B$ to $K$ (i.e. for $x \in K$ let $Z_x = \lim_{n \to \infty} Y_{b_n}$ for any $b_n \to x$ with $b_n \in B$).  TODO: Show that $Z$ has the correct law.

It remains to show that $\probability{UC} = 1$.
TODO:
\end{proof}

\begin{defn}Let $H$ be a Hilbert space and let $C \subset H$.  We say that $C$ is a \emph{GC-set} if and only if $C$ is totally bounded in $H$ and the isonormal process $L$ restricted to $C$ has a version with uniformly continuous sample paths.
\end{defn}

\begin{cor}Let $X$ be a centered Gaussian process on the probability space $(\Omega, \mathcal{A}, P)$ indexed by a compact metric space $(T,d)$ then $X$ has a continuous version if and only if $t \mapsto X_t$ is continuous from $T$ to $L^2(P)$ and its range is a GC-set.
\end{cor}
\begin{proof}
Define $h : T \to L^2(P)$ to be the function $h(t) = X_t$, let $K = h(T) \subset L^2(P)$.  If we assume that $h$ is continuous and $K$ is a GC-set then we may apply Theorem \ref{PullbackContinuousVersions} to conclude that $L_{X_t}$ has a continuous version.


If we assume that $X$ has a continuous version then by Theorem \ref{GaussianVersionContinuityCentering} we know that $h$ is continuous.  It follows that $h(T)$ is compact hence totally bounded (Lemma \ref{CompactnessInMetricSpaces}).  By Theorem \ref{PullbackContinuousVersions} we conclude that $L$ restricted to $K$ has a continuous version.  Since $K$ is compact that continuous version is a uniformly continuous version so $K$ is  GC-set.
\end{proof}
