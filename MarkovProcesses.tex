\chapter{Markov Processes}

TODO: Thinking about Markov processes as dynamical/deterministic
systems with (transduced) noise.

\section{Markov Processes}
The basic intuition of what a Markov process comprises is that it is a
stochastic process $X$ on a time scale $T$ such that for every time $t \in
T$ the future behavior of $X_u$ for $u \geq t$ only depends on the
past through the current value of $X_t$.  Alas, in practice the types
of problems that we concern ourselves with Markov process leads us to
a definition of a significantly more complicated object.  Rather than
pummel the reader with the definition we take the approach of starting
from simple intuition and building in the complexity by stages.  Some
readers may prefer to first jump to the end of this section to peer at
the final defintion so that it can be kept in mind during the journey.

\begin{defn}Let $X$ be a process in $(S, \mathcal{S})$ with time scale
  $T$ which is adapted to a filtration $\mathcal{F}_t$.  We say that
  $X$ has the \emph{Markov property} if 
$\cindependent{\mathcal{F}_s}{X_t}{X_s}$ for all $s \leq t \in T$.
\end{defn}
Given any process that satisfies the Markov property it is not hard to
show using properties of conditional independence that it
automatically satisfies a seemingly stronger condition
\begin{lem}[Extended Markov Property]\label{ExtendedMarkovProperty}
Let $X$ be a process that satsifies the Markov property
  then $\cindependent{\mathcal{F}_t}{\sigma(\bigvee_{u \geq t}
    X_u)}{X_t}$ for all $t \in T$.
\end{lem}
\begin{proof}
Let $t_0 \leq t_1 \leq \cdots $ with $t_i \in T$.  By the
Markov property we know for each $0 \leq n$ that
$\cindependent{\mathcal{F}_{t_n}}{X_{t_{n+1}}}{X_{t_n}}$.  Because $X$ is
adapted to $\mathcal{F}$, we know that $X_{t_m}$ is
$\mathcal{F}_{t_n}$-measurable for $m \leq n$ and therefore
$\cindependent{\sigma(X_{t_0}, \dots
  ,X_{t_{n-1}},\mathcal{F}_{t_n})}{X_{t_{n+1}}}{X_{t_n}}$.
By Lemma \ref{ConditionalIndependenceChainRule} we conclude that $\cindependent{\mathcal{F}_{t_n}}{X_{t_{n+1}}}{X_{t_0}, \dots
  ,X_{t_n}}$ for all $n \geq 0$; because $\mathcal{F}_{t_0} \subset
\mathcal{F}_{t_n}$ we get $\cindependent{\mathcal{F}_{t_0}}{X_{t_{n+1}}}{X_{t_0}, \dots
  ,X_{t_n}}$ for all $n \geq 0$.  Another application of Lemma
\ref{ConditionalIndependenceChainRule} shows that
$\cindependent{\mathcal{F}_{t_0}}{\sigma(X_{t_1}, X_{t_2},
  \dots)}{X_{t_0}}$.

Since the union of the $\sigma$-algebras $\sigma(X_{t_1}, X_{t_2},
  \dots )$ for all $t_0 \leq t_1 \leq \cdots $ is
  clearly a $\pi$-system that generates $\sigma(\bigvee_{u \geq t_0}X_u)$, the result follows by montone classes (specifically Lemma \ref{ConditionalIndependencePiSystem}).
\end{proof}

TODO: Introduce the example of Markov Chains here as it is quite a bit
simpler and helps the
understanding of the abstract case quite a bit.

We know make a regularity assumption that for each pair $s,t \in T$
with $s \leq t$, we
have a probability kernel $\mu_{s,t} : S \times \mathcal{S} \to
\reals$ such that for every $A \in \mathcal{S}$
\begin{align*}
\mu_{s,t}(X_s, A) &= \cprobability{X_s}{X_t \in A} =
\cprobability{\mathcal{F}_s}{X_t \in A} \text{ a.s.}
\end{align*}
(e.g. if $S$ is a Borel space then this is true by Theorem
\ref{ExistenceConditionalDistribution}).
We let $\nu_t$ denote the distribution of $X_t$.
These conditional distributions characterize the distribution of the
process $X$ itself.  In particular we have the following nice formula
for finite dimensional distributions of the process.
\begin{lem}\label{MarkovDistributions}Let $X$ be a stochastic process
  on a time scale $T \subset \reals_+$ that has the Markov property,
  one dimensional distributions $\nu_t$ and transition kernels
  $\mu_{s,t}$.  Then for all $t_0 \leq \cdots \leq t_n$ and $A \in
  \mathcal{S}^{\otimes n}$ we have
\begin{align*}
\probability{(X_{t_1}, \dots, X_{t_n}) \in A} 
&= \nu_{t_1} \otimes
\mu_{t_1, t_2} \otimes \cdots \otimes \mu_{t_{n-1},t_n}(A) \\
\cprobability{\mathcal{F}_{t_0}}{(X_{t_1}, \dots, X_{t_n}) \in
  A}(\omega) 
&= \mu_{t_0, t_1} \otimes \cdots \otimes \mu_{t_{n-1},t_n}(X_{t_0}(\omega),A) \\
\end{align*}
\end{lem}
\begin{proof}
We begin by proving the first equality via induction.  The case $n=0$
is true by definition.  The induction step is 
really just a specific case of Theorem 
\ref{Disintegration} applied to the Markov transition kernels.  Let $A
\in \otimes_{i=0}^n \mathcal{S}$ then 
\begin{align*}
&\probability{(X_{t_0}, \cdots, X_{t_n}) \in A} \\
&=\expectation{\characteristic{A}(X_{t_0}, \cdots, X_{t_n})} \\
&=\expectation{\int \characteristic{A}(X_{t_0}, \cdots, X_{t_{n-1}},s)
  \, \mu_{t_{n-1}, t_n} (X_{n-1}, ds)} \\
&=\int \left [ \int \characteristic{A}(u_0, \cdots,u_{n-1}, s)   \,
  \mu_{t_{n-1}, t_n} (X_{n-1}, ds) \right ] \nu_{t_0} \otimes \cdots \otimes
  \mu_{t_{n-2}, t_{n-1}}(du_0, \dotsc, du_{n-1})\\
&=\nu_{t_0} \otimes \cdots \otimes  \mu_{t_{n-1}, t_{n}}(A)
\end{align*}

The second equality is derived from the first.  Suppose we have $A \in
\mathcal{S}$ and $B \in \mathcal{S}^{\otimes n}$.  Then we can compute
\begin{align*}
&\expectation{\characteristic{A}(X_{t_0}) \characteristic{B}(X_{t_1},
  \dotsc, X_{t_n})}  \\
&= \nu_{t_0} \otimes \mu_{t_0, t_1} \otimes \cdots \otimes  \mu_{t_{n-1},
  t_{n}}(A \times B) \\
&= \int \left [ \int \characteristic{B}(u_1, \dotsc, u_n) \mu_{t_0,
    t_1} \otimes \cdots \otimes \mu_{t_{n-1}, t_n} (u_0, du_1, \dotsc, du_n) \right ]
\characteristic{A}(u_0) \nu_{t_0}(u_0) \\
&=\expectation{\mu_{t_0, t_1} \otimes \cdots \otimes \mu_{t_{n-1},
    t_n}(X_0, B) \characteristic{A}(X_0)}
\end{align*}
Now the $\sigma(X_{t_0})$-measurability of $\mu_{t_0, t_1} \otimes \cdots \otimes \mu_{t_{n-1},
    t_n}(X_0, B)$ tells us that 
\begin{align*}
\cprobability{X_{t_0}}{(X_{t_1},\dotsc, X_{t_n}) \in B} 
&= \mu_{t_0, t_1} \otimes \cdots \otimes \mu_{t_{n-1},    t_n}(X_0, B)
\end{align*}

The last thing is to show that
$\cprobability{X_{t_0}}{(X_{t_1},\dotsc, X_{t_n}) \in B} =
\cprobability{\mathcal{F}_{t_0}}{(X_{t_1},\dotsc, X_{t_n}) \in B}$
a.s.  This follows from Lemma \ref{ExtendedMarkovProperty} since by
the tower property of conditional expectations and that result 
for any $A \in \mathcal{S}^{\otimes n}$ and $B \in \mathcal{F}_{t_0}$
\begin{align*}
\probability{(X_{t_1}, \dotsc, X_{t_n}) \in A ; B} 
&=\expectation{\cprobability{X_{t_0}}{(X_{t_1}, \dotsc, X_{t_n}) \in A ;  B}} \\
&=\expectation{\cprobability{X_{t_0}}{(X_{t_1}, \dotsc, X_{t_n}) \in A}
\cprobability{X_{t_0}}{B}} \\
&=\expectation{\cprobability{X_{t_0}}{(X_{t_1}, \dotsc, X_{t_n}) \in A}
\characteristic{B}} \\
\end{align*}
so the $\mathcal{F}_{t_0}$-measurability of
$\cprobability{X_{t_0}}{(X_{t_1}, \dotsc, X_{t_n}) \in A}$ gives the
result by the defining property of conditional expectations.
\end{proof}
A special case of the relations above should be called out as it
motivates a property that will assume as part of the definition of a
Markov process.  But first we need a definition.
\begin{defn}Let $\mu$ and $\nu$ be probability kernels from $S$ to
  $S$.  Then we define the probability kernel $\mu \nu$ from $S$ to
  $S$ by
\begin{align*}
\mu \nu (s, A) &= (\mu \otimes \nu)(s, A \times S)
\end{align*}
for all $s \in S$ and $A \in \mathcal{S}$.
\end{defn}
\begin{examp}
Let $S$ be a finite set and view $\mu$ and $\nu$ as $S \times S$
matrices.  Then $\mu \nu$ is just matrix multiplication:
\begin{align*}
\mu \nu (s, \lbrace t \rbrace) 
&= \iint \characteristic{\lbrace t  \rbrace \times S}(u,v) \nu(u, dv) \mu(s, du) 
=\int \nu(u, \lbrace t \rbrace) \mu(s,du) 
=\sum_{u \in S} \nu_{u, t} \mu_{s, u}
\end{align*}
\end{examp}
\begin{cor}[Chapman-Kolmogorov
  Relations]\label{ChapmanKolmogorovWeak}Let $X$ be a stochastic
  process on a time scale $T \subset \reals$ with values in Borel
  space $(S, \mathcal{S})$ and suppose that $X$ has the Markov
  property.  Then for every $s, t, u \in T$ with $s \leq t \leq u$ we
  have
\begin{align*}
\mu_{s,t} \mu_{t,u} &= \mu_{s,u} \text{ a.s. $\nu_s$}
\end{align*}
\end{cor}
\begin{proof}
Since we have assume $S$ is a Borel space we know from Theorem \ref{ExistenceConditionalDistribution} that regular
versions $\mu_{s,t}$ exist.  By definition of $\mu_{s,t} \mu_{t,u}$, Lemma
\ref{MarkovDistributions} and the uniqueness clause of Theorem \ref{ExistenceConditionalDistribution}
\begin{align*}
\mu_{s,t} \mu_{t,u}(x, A) &= 
(\mu_{s,t} \otimes \mu_{t,u})(x, S \times A) \\
&= \cprobability{\mathcal{F}_s}{(X_t, X_u) \in S \times A} \\
&=\cprobability{\mathcal{F}_s}{ X_u \in A} \\
&=\cprobability{X_s}{ X_u \in A} \\
&= \mu_{s,u}(x, A) \text{  a.s. $\nu_s$}
\end{align*}
\end{proof}

The ability to derive the almost sure version of the
Chapman-Kolmogorov relations is really just motivational for our
purposes.  In fact we will want to assume they hold identically in
what follows.  Absent a workable set of conditions from which we can
derive this fact, we build it into our definitions.  Collecting all of
the conditional independence and regularity properties we've
identified we finally make the formal definition of a Markov process.
\begin{defn}
A \emph{Markov process} is a stochastic process $X_t$ on a time scale $T
\subset \reals_+$ and a state space $(S, \mathcal{S})$ such that 
\begin{itemize}
\item[(i)]$\cindependent{\mathcal{F}_s}{X_t}{X_s}$ for all $s \leq t$
\item[(ii)]there exists a regular version $\mu_{s,t} :
  S \times \mathcal{S} \to [0,1]$ of $\cprobability{\mathcal{F}_s}{X_t
    \in \cdot}$ for each $s \leq t$.
\item[(iii)]$\mu_{s,t} \mu_{t,u} = \mu_{s,u}$ everywhere on $S$ for
  each $s \leq t \leq u$.
\end{itemize}
\end{defn}

TODO: Note that in the discrete (or countable?) state space case we
can in fact assume that Chapman-Kolmogorov are satisfied identically.

In lieu of general techinique for proving that a process is Markov
from general principles, we give a result that shows that we can
construct them from a set of transition kernels that obey the
Chapman-Kolmogorov relations.

TODO: There are other ways of proving a process is Markov : the
semigroup approach, the stochastic differential equation approach and
the martingale problem approach.  These are things we'll get to but
not quite yet!

\begin{thm}\label{ExistenceMarkovProcess}Suppose we are given
\begin{itemize}
\item[(i)] a  time scale starting at $0$, $T \subset \reals_+$ 
\item[(ii)]a Borel space $(S, \mathcal{S})$ 
\item[(iii)]a probability distribution $\nu$ on $(S, \mathcal{S})$
\item[(iv)]probability kernels $\mu_{s,t} : S \times
  \mathcal{S} \to [0,1]$ for each $s \leq t \in T$ such that 
\begin{align*}
\mu_{s,t} \mu_{t,u} &= \mu_{s,u} \text{ for all $s\leq
  t\leq u \in T$}
\end{align*}
then there exists a Markov process $X_t$ with initial distribution
$\nu$ and transition kernels $\mu_{s,t}$.
\end{itemize} 
\end{thm}
\begin{proof}
This is an application of the Daniell-Kolmogorov Theorem.  We first
define the finite dimensional distributions and show that they form a
projective family.  For every $n \in \naturals$ and $0 \leq t_1 \leq
\dotsb \leq t_n$ we define
\begin{align*}
\nu_{t_1, \dotsc, t_n} &= \nu \mu_{0,t_1} \otimes \mu_{t_1, t_2}
\otimes \dotsb \otimes \mu_{t_{n-1}, t_n}
\end{align*}
Let $B \in \mathcal{S}^{\otimes n-1}$ and let $1 \leq k \leq n$.
Define
\begin{align*}
B_k &= \lbrace (x_1, \dotsc, x_n) \in S^n \mid (x_1, \dotsc, x_{k-1},
x_{k+1}, \dotsc, x_n) \in B \rbrace
\end{align*}
and calculate
\begin{align*}
&\nu_{t_1, \dotsc, t_n}(B_k) = ( \nu \mu_{0,t_1} \otimes \mu_{t_1, t_2}
\otimes \dotsb \otimes \mu_{t_{n-1}, t_n})(B_k) \\
&=\int \left [ \int \left[ \dotsb \left [ \int \characteristic{B_k}(s_1,
    \dotsc, s_n) \, \mu_{t_{n-1},t_n}(s_{n-1}, ds_n) \right] \dotsb
\right] \, \mu_{t_1, t_2}(s_1, ds_2) \right ] \, \nu \mu_{0,
t_1}(ds_1) \\
&=\int \left [ \int \left[ \dotsb \left [ \int \characteristic{B}(s_1,
    \dotsc, s_{k-1}, s_{k+1}, \dotsc, s_n) \, \mu_{t_{n-1},t_n}(s_{n-1}, ds_n) \right] \dotsb \right] \, \mu_{t_1, t_2}(s_1, ds_2) \right ] \, \nu \mu_{0, t_1}(ds_1)
\end{align*}

The point here is that for every fixed $s_1, \dotsc, s_{k-1}$, the inner integral
\begin{align*}
\int \left[ \dotsb \left [ \int \characteristic{B}(s_1,
    \dotsc, s_{k-1}, s_{k+1}, \dotsc, s_n) \, \mu_{t_{n-1},t_n}(s_{n-1}, ds_n) \right] \dotsb \right] \, \mu_{t_k, t_{k+1}}(s_k, ds_{k+1}) 
\end{align*}
is a function of $s_{k+1}$ only.  From the Chapman-Kolmogorov relation
$\mu_{t_{k-1}, t_k} \mu_{t_{k}, t_{k+1}}  = \mu_{t_{k-1}, t_{k+1}}$ we
know that for any function of $f : S \to \reals$ we have 
\begin{align*}
\int \left [
  \int
f(z) \, \mu_{t_{k}, t_{k+1}} (y, dz) \right ] \, \mu_{t_{k-1}, t_k}
(x, dy) &= \int
f(z) \, \mu_{t_{k-1}, t_{k+1}} (x, dz) 
\end{align*}
which when applied to the inner integral above yields
\begin{align*}
&\int \left[ \dotsb \left [ \int \characteristic{B}(s_1,
    \dotsc, s_{k-1}, s_{k+1}, \dotsc, s_n) \,
    \mu_{t_{n-1},t_n}(s_{n-1}, ds_n) \right] \dotsb \right] \, \mu_{t_{k-1},
t_{k}}(s_{k-1}, ds_{k}) \\
&=\int \left[ \dotsb \left [ \int \characteristic{B}(s_1,
    \dotsc, s_{k-1}, s_{k+1}, \dotsc, s_n) \,
    \mu_{t_{n-1},t_n}(s_{n-1}, ds_n) \right] \dotsb \right] \,
\mu_{t_{k-1}, t_{k+1}}(s_{k-1}, ds_{k+1})
\end{align*}
for every fixed $s_1, \dotsc, s_{k-1}$.  Now we can use this to
conclude that 
\begin{align*}
\nu_{t_1, \dotsc, t_n}(B_k) &= ( \nu \mu_{0,t_1} \otimes \mu_{t_1, t_2}
\otimes \dotsb \otimes \mu_{t_{n-1}, t_n})(B_k) \\
&= ( \nu \mu_{0,t_1} \otimes \mu_{t_1, t_2} \otimes \dotsb \otimes
\mu_{t_{k-1}, t_{k+1}} \otimes \dotsb \otimes \mu_{t_{n-1}, t_n})(B)
\\
&=\nu_{t_1, \dotsc, t_{k-1}, t_{k+1}, \dotsc, t_n}(B)
 \end{align*}
and we have show that the $\nu_{t_1, \dotsc, t_n}$ are a projective
family.
Now we can apply the Daniell-Kolmogorov Theorem
\ref{DaniellKolmogorovExtension} to conclude that there is an $S$
valued process $X$ on $T$ such that 
\begin{align*}
\mathcal{L}(X_{t_1}, \dotsc, X_{t_n}) = \nu_{t_1, \dotsc, t_n} &= \nu \mu_{0,t_1} \otimes \mu_{t_1, t_2}
\otimes \dotsb \otimes \mu_{t_{n-1}, t_n}
\end{align*}
for all $n \in \naturals$ and $0 \leq t_1 \leq \dotsb \leq t_n$.  The
case $n=1$ and $t_1 = 0$ shows us that $\mathcal{L}(X_0) = \nu
\mu_{0,0} = \nu$.

For every $t \in T$ define $\mathcal{F}_t = \sigma(X_s ; s \leq t)$ to
be filtration induced by $X$.  We must show that $X_t$ is a Markov process with transition
kernels $\mu_{s,t}$ (the fact that the initial distribution is $\nu$
was already noted).  Let $s \leq t$ be given and suppose that we have
$s_1 \leq \dotsb \leq s_n = s$.  Pick $B \in \mathcal{S}^{\otimes n}$
and $C \in \mathcal{S}$ and calculate using the FDDs of $X_t$ and the
expectation rule (Lemma \ref{ExpectationRule})
\begin{align*}
&\probability{(X_{s_1}, \dotsc, X_{s_n}) \in B ; X_t \in C}  = \probability{(X_{s_1}, \dotsc, X_{s_n}, X_t) \in B \times C} =
\nu_{s_1, \dotsc, s_n, t}(B \times C) \\
&=\int \left [ \int \left[ \dotsb \left [ \int \characteristic{B}(u_1,
    \dotsc, u_n) \characteristic{C}(u_{n+1})\, \mu_{s,t}(u_{n}, du_{n+1}) \right] \dotsb
\right] \, \mu_{s_1, s_2}(u_1, du_2) \right ] \, \nu \mu_{0,
s_1}(du_1) \\
&=\int \left [ \int \left[ \dotsb \left [ \int \characteristic{B}(u_1,
    \dotsc, u_n) \mu_{s,t}(u_{n}, C)\, \mu_{s_{n-1},s_n}(u_{n-1}, du_{n}) \right] \dotsb
\right] \, \mu_{s_1, s_2}(u_1, du_2) \right ] \, \nu \mu_{0,
s_1}(du_1) \\
&=\expectation{\mu_{s,t}(X_s, C) ; (X_{s_1}, \dotsc, X_{s_n}) \in B}
\end{align*}
Sets of the form $(X_{s_1}, \dotsc, X_{s_n}) \in B$ for $s_1 \leq
\dotsb \leq s_n=s$ are a $\pi$-system generating $\mathcal{F}_s$ and
therefore by a monotone class argument (specifically Lemma \ref{ConditionalExpectationExtension}) we may
conclude that $\cexpectationlong{\mathcal{F}_s}{X_t \in \cdot} =
\mu_{s,t}(X_s, \cdot)$ a.s.
\end{proof}

\begin{defn}Suppose that a family of transition kernels $\mu_{s,t}$ is
  given.  For a distribution $\nu$ on $(S, \mathcal{S})$, let
  $\sprobabilityop{\nu}$ denote the distribution on $S^T$ of the
  Markov process with initial distribution $\nu$.  If $\nu=\delta_x$
  for some $x \in S$ then it is customary to write
  $\sprobabilityop{x}$ instead of $\sprobabilityop{\delta_x}$.
\end{defn}
\begin{lem}\label{MarkovMixtures}The family $\sprobabilityop{x}$ is a kernel from $S$ to
  $S^T$.  Futhermore, given an initial distribution $\nu$
\begin{align*}
\sprobability{A}{\nu} = \int \sprobability{A}{x} \, d\nu(x)
\end{align*}
\end{lem}
\begin{proof}
First assume that $A = (\pi_{t_1}, \dots, \pi_{t_n})^{-1}(B)$ for some
$B \in \mathcal{S}^{\otimes n}$.  We can use Lemma \ref{MarkovDistributions} to compute for
any $\nu$,
\begin{align*}
\sprobability{A}{\nu} &= \probability{(\pi_0, \pi_{t_1}, \dots,
  \pi_{t_n})^{-1}(S \times B)} \\
&=\nu \otimes \mu_{0, t_1} \otimes \cdots \otimes \mu_{t_{n-1}, t_n}(S \times
B) \\
&= \int \mu_{0, t_1} \otimes \cdots \otimes \mu_{t_{n-1}, t_n}(x,B) \, d\nu(x)
\end{align*}
In particular, for $\nu = \delta_x$ we get
\begin{align*}
\sprobability{A}{x} &=\mu_{0, t_1} \otimes \cdots \otimes \mu_{t_{n-1},
  t_n}(x,B)
\end{align*}
which shows both that $\sprobability{A}{x}$ is a measurable function
of $x$ for fixed $A$ (Lemma
\ref{KernelTensorProductMeasurability}) and that
$\sprobability{A}{\nu} = \int \sprobability{A}{x} \, d\nu(x)$.

To extend to general measurable sets, we note that the set of $A$ of
the form given above is a $\pi$-system therefore we can apply Lemma
\ref{KernelMeasurability} to conclude $\sprobabilityop{x}$ is a
kernel.  Similarly we may conclude that $\sprobability{A}{\nu} = \int
\sprobability{A}{x} \, d\nu(x)$ for arbitrary measurable $A$ by the fact that probability measures
are uniquely determined by their values on a generating $\pi$-system
(Lemma \ref{UniquenessOfMeasure}).
\end{proof}

TODO:  This may not be the correct definition of a Markov process to
settle on.   We may want to select the picture of a Markov process as
being a single stochastic process with a family of probability
measures $\probabilityop_x$ for $x \in S$ such that under $\probabilityop_x$ the
stochastic process is Markov (as above) starting at $x$.  This
definition assumes that we have a kernel property (so Lemma
\ref{MarkovMixtures} proves such a kernel property
holds in the ``canonical'' case).  The work we have done to this point
shows that a set of transition kernels gives rise to a Markov process
with on the canonical space $S^T$.  The interpretation as a family of measures
without assuming the probability space is $S^T$ is apparently useful (e.g. when we want to
assume randomization variables exist for some construction).  I still find the variety of
interpretations of what a Markov process is to be very confusing.
Perhaps we should define this latter concept as a Markov family and
keep the current notion as a Markov process (I think Karatzas and
Shreve do this).  In the Karatzas and Shreve definition we wind up
with an interesting new concept which is that the kernel
$\probabilityop_x$ in a Markov family is only assumed to be \emph{universally measurable}
which is a looser condition than Borel measurability (the universal
$\sigma$-algebra being the intersection of the completions of the
Borel $\sigma$-algebra under all probability measures; hence being a
superset of the Borel $\sigma$-algebra).  This loosening
seems to come up as important in the context of stochastic control.  I
am not at all clear on how important it is in the context of Markov
processes as we are likely to develop it; it seems from Karatzas and
Shreve that this loosening comes up in Markov process theory when
trying to find a right-continuous complete filtration with respect to which a
Markov process (in particular Brownian motion) gives us a Markov
family.  So, we have shown that a by Kolmogorov existence we can
construct a Markov family given a set of transition kernels however
the filtration is not right continuous or complete and this
construction results in a Borel measurable kernel $\probabilityop_x$.
However if one tries to modify the construction to get a
right-continuous complete filtration (usual conditions) then one has
to give up Borel measurability in the kernel and make due with
universal measurability.

\begin{defn}
A \emph{Markov family} is a stochastic process $X_t$ with a
probability space $(\Omega, \mathcal{A})$, a time scale $T
\subset \reals_+$ and a state space $(S, \mathcal{S})$ and a family of
probability measures $\probabilityop_x$ on $\Omega$ for $x \in S$ such that 
\begin{itemize}
\item[(i)]$\probabilityop_x$ is a (universally measurable?) kernel
  from $S$ to $\Omega$.
\item[(ii)]$\sprobability{X_0 = x}{x} = 1$ for all $x \in S$.
\item[(iii)]$\cindependent{\mathcal{F}_s}{X_t}{X_s}$ under
  $\probabilityop_x$ for all $s \leq
  t$ and $x \in S$ (i.e. for all $x \in S$, $A \in \mathcal{S}$ and $s \leq t$ we have
  $\csexpectationlong{\mathcal{F}_s}{X_t \in A}{x} =
  \csexpectationlong{X_s}{X_t \in A}{x}$ $\probabilityop_x$-a.s.)
\item[(iv)]there exists a regular version $\mu^x_{s,t} :
  S \times \mathcal{S} \to [0,1]$ of $\csprobability{\mathcal{F}_s}{X_t
    \in \cdot}{x}$ for each $s \leq t$ and $x \in S$ (is there any coherence
  requirement with respect to $x \in S$ here???).
\end{itemize}
\end{defn}

Question:  Given a Markov family as above then given an arbitrary
initial distribution $\nu$ on $S$ we can define $\probabilityop_\nu$
by $\sprobability{A}{\nu} = \int \sprobability{A}{x} \, d\nu(x)$.  Is
$X$ a Markov process with initial distribution $\nu$ under
$\probabilityop_\nu$?

\section{Homogeneous Markov Processes}

We have described a relatively general version of Markov processes
compared to what it needed in many applications and the goal of this
section is to define the assumptions that lead to useful
simplifications and to understand how to look at these simplifying
assumptions from a couple of points of view.

\begin{defn}Suppose $(S, \mathcal{S})$ is a measurable Abelian group
  and $\mu : S \times \mathcal{S} \to [0,1]$ is a kernel.  We say
  $\mu$ is \emph{homogeneous} if for every $s \in S$ and
  $A\in \mathcal{S}$ we have $\mu(0, A) = \mu(s, A+s)$.
\end{defn}

A useful observation for computing conditional expectations is that
integrals are invariant under certain changes of variables.
\begin{lem}\label{HomogeneousKernelExpectationRule}Let $(S, \mathcal{S})$ be a measurable Abelian group with a
  homogeneous kernel $\mu : S \times \mathcal{S} \to [0,1]$, then for
  each $y,z \in S$ and integrable $f : S \to \reals$,
\begin{align*}
\int f(x + y) \, \mu(z, dx) &= \int f(x) \, \mu(y+z, dx)
\end{align*}
\end{lem}
\begin{proof}
For $y \in S$, let $t_y : S \to S$ be translation by $y$: $t_y(x) = x
+ y$.  Thinking of the kernel as a measurable measure
valued map (which we denote $\mu(z)$) we compute the pushforward of $\mu(z)$ under $t_y$
using homogeneity
\begin{align*}
\pushforward {t_y}{\mu(z)}(A) &= \mu(z, t_y^{-1}(A)) = \mu(z, A
- y) = \mu(z+y, A)
\end{align*}
thus showing $\pushforward {t_y}{\mu(z)} = \mu(y+z)$.
Now we can apply the Expectation Rule (Lemma \ref{ExpectationRule}) to
see that 
\begin{align*}
\int f(x + y) \, \mu(z, dx) &= \int f(x) \, d\left [ \pushforward {t_y}{\mu(z)} \right] = \int f(x) \, \mu(y+z, dx)
\end{align*}
\end{proof}

A Markov process with homogeneous kernels is said to be
\emph{space-homogeneous}; intuitively the probability of starting out
in a set $A$ at time $s$ and winding up in set $B$ at time $t$ only
depends on the relative positions of $A$ and $B$ (under translations).
\begin{defn}Suppose $(S, \mathcal{S})$ is a measurable Abelian group
  and let $X_t$ be a Markov process with transition kernels
  $\mu_{s,t}$.  Then $X_t$ is \emph{space-homogeneous} if and only if
  $\mu_{s,t}$ is homogeneous for every $s \leq t$.
\end{defn}

\begin{lem}\label{SpaceHomogeneousMarkovDistributions}Let $\mu_{s,t}$
  be a family of space homogeneous transition kernels on a
  measurable Abelian group, then for every $A \in \mathcal{S}^T$ and $x \in S$,
  $\sprobability{A}{x} = \sprobability{A-x}{0}$.
\end{lem}
\begin{proof}
TODO: This proof only seems to require space homogeneity of the
kernels $\mu_{0,t}$; is this a mistake (or does Chapman Kolmogorov
imply the rest of the kernels are space homogeneous as well...)

We begin by establishing the result for sets of the form $\lbrace (X_{t_1},
\dotsc, X_{t_n}) \in A \rbrace$ for $A \in \mathcal{S}^{\otimes n}$ and $t_1 \leq
\cdots \leq t_n$.  The key point is that we know from the proof of
Lemma \ref{MarkovMixtures} that $\sprobability{(X_{t_1},
\dotsc, X_{t_n}) \in A }{x} = \mu_{0, t_1} \otimes \dotsc \otimes
\mu_{t_{n-1}, t_n}(x, A)$, so in particular the case $n=1$ follows
directly from the assumption that each $\mu_{0, t}$ is homogeneous.
To see the result for $n > 1$ we calculate using Lemma \ref{HomogeneousKernelExpectationRule}
\begin{align*}
&\sprobability{(X_{t_1},\dotsc, X_{t_n}) \in A }{x} \\
&= \mu_{0, t_1} \otimes \dotsc \otimes \mu_{t_{n-1}, t_n}(x, A) \\
&= \int \int \characteristic{A}(x_1, x_2, \dotsc, x_n) \mu_{t_1, t_2} \otimes \cdots \otimes \mu_{t_{n-1},
  t_n}(x_1, dx_2, \dotsc, dx_n) \, \mu_{0,t_1}(x, dy) \\
&= \int \int \characteristic{A}(x_1+x, x_2, \dotsc, x_n) \mu_{t_1, t_2} \otimes \cdots \otimes \mu_{t_{n-1},
  t_n}(x_1, dx_2, \dotsc, dx_n) \, \mu_{0,t_1}(0, dy) \\
&= \int \int \characteristic{A-x}(x_1, x_2, \dotsc, x_n) \mu_{t_1, t_2} \otimes \cdots \otimes \mu_{t_{n-1},
  t_n}(x_1, dx_2, \dotsc, dx_n) \, \mu_{0,t_1}(0, dy) \\
&= \mu_{0, t_1} \otimes \dotsc \otimes \mu_{t_{n-1}, t_n}(0, A-x) \\
&=\sprobability{(X_{t_1},\dotsc, X_{t_n}) \in A - x}{0} \\
\end{align*}

Now we complete the result by a monotone class argument.  We know that
sets of the form $\lbrace (X_{t_1}, \dotsc, X_{t_n}) \in A \rbrace$
are a generating $\pi$-system so by the $\pi$-$\lambda$ Theorem
(Theorem \ref{MonotoneClassTheorem}) it suffices to show that $\mathcal{C}
= \lbrace A \mid \sprobability{A}{x} = \sprobability{A-x}{0} \rbrace$ is a
$\lambda$-system. If $A,B \in \mathcal{C}$ with $A \subset B$ then 
\begin{align*}
\sprobability{B\setminus A}{x} &= \sprobability{B}{x} -
\sprobability{A}{x} = \sprobability{B-x}{0} - \sprobability{A-x}{0} =
\sprobability{B\setminus A-x}{0} 
\end{align*}
where we have used the elementary fact that $B\setminus A - x =
(B-x)\setminus(A-x)$ (let $y \in B$ and $y \notin A$ then clearly $y-x
\in B-x$ and $y-x \notin A-x$).  Similarly if $A_n \in \mathcal{C}$
for $n \in \naturals$ with $A_1 \subset A_2 \subset \cdots$ then it is
also true that $A_1 -x  \subset A_2-x \subset \cdots$ and continuity
of measure (Lemma \ref{ContinuityOfMeasure}) shows
\begin{align*}
\sprobability{\cup_n A_n}{x} &=
\lim_{n \to \infty}\sprobability{A_n}{x} = 
\lim_{n \to \infty}\sprobability{A_n-x}{0} = 
\sprobability{\cup_n A_n-x}{0} 
\end{align*}
\end{proof}

There is another way of thinking about the space-homogeneous Markov
processes.  We know that for any $s \leq t$, given the value of $X_s$ the probability
distribution of $X_t$ is independent of the history of $X$ up to
$s$.  Space homogeneity tells us that moreover that the probability
distribution $X_t$ only depends on the \emph{increment} $X_t - X_s$.
Putting these two observations together we should expect that $X_t -
X_s$ is independent (not just conditionally independent) of the
history of $X$ up to $s$.  In fact this provides an equivalent
characterisation of space homogeneous Markov processes as we prove in
the following result.

\begin{defn}Let $(S, \mathcal{S})$ be a measurable Abelian group with a
  time scale $T \subset \reals_+$, a filtration $\mathcal{F}_t$ and an
 $S$-valued $\mathcal{F}$-adapted process $X_t$. We say that $X_t$ has
 $\mathcal{F}$-independent increments if and only if $X_t - X_s$ is
 independent of $\mathcal{F}_s$ for all $s \leq t$.
\end{defn}

\begin{lem}\label{IndependentIncrements}Let $(S, \mathcal{S})$ be a measurable Abelian group with a
  time scale $T \subset \reals_+$, a filtration $\mathcal{F}_t$ and an
 $S$-valued $\mathcal{F}$-adapted process $X_t$.  The $X_t$ has
$\mathcal{F}$-independent increments if and only if $X_t$ is a space-homogeneous
 Markov process.  In this case the transition kernels of $X_t$ are
 given by
\begin{align*}
\mu_{s, t}(x, A) &= \probability{X_t - X_s \in A - x} \text{ for $x
  \in S$, $A \in \mathcal{S}$ and $s \leq t \in T$}
\end{align*}
TODO: The proof actually requires regular versions of
$\cprobability{\mathcal{F}_s}{X_t}$; do we need to assume that
$G$ is Borel or something?  Also we've defined a Markov process as
satisfying the Chapman Kolmogorov relations identically; can that be derived?
\end{lem}
\begin{proof}
Suppose that $X_t$ is a space homogeneous Markov Process with
transition kernels $\mu_{s,t}$.  Then for every $s\leq t$ and $A \in \mathcal{S}$,
\begin{align*}
\cprobability{\mathcal{F}_s}{X_t - X_s \in A} &= 
\int \characteristic{A}(x - X_s) \, \mu_{s,t}(X_s, dx) & & \text{by
  Theorem \ref{Disintegration}} \\
&=\int \characteristic{A}(x) \, \mu_{s,t}(0, dx) & & \text{by Lemma
  \ref{HomogeneousKernelExpectationRule}} \\
&= \mu_{s,t}(0,A)
\end{align*}
which shows that $\cprobability{\mathcal{F}_s}{X_t - X_s \in A}$ is
almost surely constant hence $\cindependent{X_t -
  X_s}{\mathcal{F}_s}{}$.  Moreover by the tower rule we also know
that $\probability{X_t - X_s \in A}=\cprobability{\mathcal{F}_s}{X_t -
  X_s \in A}=\mu_{s,t}(0,A)$ and therefore by another application of
space homogeneity, $\mu_{s, t}(x, A) = \mu_{s,t}(0, A-x) = \probability{X_t - X_s \in A}$.

Suppose that $X_t$ has independent increments.  The key point is that
this property determines the conditional distributions 
\begin{align*}
\mu_{s,t}(x, A) &= \probability{X_t - X_s \in A - x}
\end{align*}
and moreover this form is a regular version.  First note that $\probability{X_t - X_s \in A - x}$ is a probability
kernel since for fixed $A$ it is measurable in $x$ by Lemma
\ref{MeasurableSections} and for fixed $x$ it is just the distribution
of the measurable random element $X_t - X_s -x$.  

Showing that $\probability{X_t - X_s \in A - x}$ is a version of
$\cprobability{\mathcal{F}_s}{X_t \in A}$ is not hard but requires a
bit of care because the random element $X_s$ plays two different roles
in the calculation and it is worth making this fact explicit.
We start by defining 
$\tilde{\mu}_{s,t}(x, A) = \probability{X_t - X_s \in A}$ and observing
that because $\cindependent{X_t - X_s}{\mathcal{F}_s}{}$,
$\tilde{\mu}_{s,t}$ is a kernel for $\cprobability{\mathcal{F}_s}{X_t
  - X_s \in \cdot}$.  With this fact and the $\mathcal{F}$-adaptedness
of $X$, we can apply Theorem
\ref{Disintegration} (using the function $f(x,y) =
\characteristic{A-y}(x)$ evaluated at $(X_t - X_s, X_s)$) to conclude
\begin{align*}
\cprobability{\mathcal{F}_s}{X_t \in A} &= 
\cprobability{\mathcal{F}_s}{X_t - X_s \in A - X_s} \\
&=\int \characteristic{A - X_s}(x) \, \tilde{\mu}_{s,t}(dx) \\
&=\tilde{\mu}_{s,t}(A - X_s) \\
&=\mu_{s,t}(X_s, A)
\end{align*}
Now note that $\mu_{s,t}(X_s, A)$ is $X_s$-measurable hence we have
$\cprobability{\mathcal{F}_s}{X_t \in A} = \cprobability{X_s}{X_t
  \in A} $ for all $A \in \mathcal{S}$ thus the Markov property
holds by Lemma \ref{ConditionalIndependenceDoob}.  
Using the explicit form of the kernel we calculate
\begin{align*}
\mu_{s,t}(x, A) &=  \probability{X_t - X_s \in A-x} = \mu_{s,t}(0, A-x) 
\end{align*}
demonstrating space homogeneity.
\end{proof}

Here is what the proof that space homogeneous Markov implies
independent increments looks like in elementary probability theory
(discrete time countable state space).
\begin{proof}
Space homogeneity means that $\cprobability{X_{n-1} = y}{X_n = x} =
\cprobability{X_{n-1} = 0}{X_n = x-y}$.  This implies that for any
$y\in S$ we have
$\probability{X_n - X_{n-1} = z} = \cprobability{X_{n-1} = y}{X_n =
  z+y}$:
\begin{align*}
\probability{X_n - X_{n-1} = z} &= \sum_x \probability{X_n - X_{n-1} =
  z ; X_{n-1}=x} \\
& =\sum_x \cprobability {X_{n-1}=x}{X_n - X_{n-1} =  z} \probability
{X_{n-1}=x} \\
& =\sum_x \cprobability {X_{n-1}=x}{X_n =  z+x} \probability
{X_{n-1}=x} \\
& =\cprobability {X_{n-1}=y}{X_n =  z+y} \sum_x \probability
{X_{n-1}=x} \\
&= \cprobability {X_{n-1}=y}{X_n =  z+y} 
\end{align*}
Now we use this fact along with the Markov property to see
\begin{align*}
&\probability{X_n - X_{n-1} = z; X_1=x_1 ; \cdots ; X_{n-1}=x_{n-1}} \\
&=\probability{X_n = z+x_{n-1}; X_1=x_1 ; \cdots ; X_{n-1}=x_{n-1}} \\
&=\cprobability{X_1=x_1 ; \cdots ; X_{n-1}=x_{n-1}}{X_n = z+x_{n-1}} 
\probability{X_1=x_1 ; \cdots ; X_{n-1}=x_{n-1}}\\
&=\cprobability{X_{n-1}=x_{n-1}}{X_n = z+x_{n-1}} 
\probability{X_1=x_1 ; \cdots ; X_{n-1}=x_{n-1}}\\
&=\probability{X_n = z} 
\probability{X_1=x_1 ; \cdots ; X_{n-1}=x_{n-1}}\\
\end{align*}
\end{proof}

TODO: Motivate time homogeneity by thinking about discrete time and
the fact that you can generate everything from the unit time
transitions.  Time homogeneity is the property that all of these
transition kernels are the same and therefore the Markov process is
determined by a single kernel (and the initial distribution).

\begin{defn}A Markov process on $\integers_+$ or $\reals_+$ is said to
  be \emph{time homogeneous} if and only for all $s,t,u \in T$ and $B
  \in \mathcal{S}^T$ we have
  $\cexpectationlong{\mathcal{F}_s}{X_t \in B} = \cexpectationlong{\mathcal{F}_{s+u}}{X_{t+u} \in B}$.
\end{defn}

TODO: Show that a time homogenous Markov process is a dynamical system
with noise.

\section{Strong Markov Property}

In dealing with Markov processes we make a lot of use of constructions
that involve the following
\begin{defn}If $T$ is equal to $\integers_+$ or $\reals_+$, for each
  $t \in T$ we define
  the \emph{shift operator}  $\theta_t : S^T \to S^T$ by $\theta_t f
  (s) = f(s +t)$.
\end{defn}

It is clear that for a fixed $t \in T$ the shift operator $\theta_t$
is measurable but we often need a stronger property the requires some
more assumptions.
\begin{lem}\label{MeasurabilityOfShiftOperator}For any fixed $t \in T$ the shift operator $\theta_t : S^T
  \to S^T$ is measurable. If $U$ is equal to $S^\infty$, $C(T; S)$ or
  $D(T; S)$, then $\theta_t X$ defines a measurable function $\theta : U \cap S^T \times
  T \to U \cap S^T$.  
\end{lem}
\begin{proof}
First let $t \in T$ be fixed pick $s \in T$ and $A \in \mathcal{S}$.
Then $\theta_t^{-1} \lbrace f(s) \in A \rbrace = \lbrace f(s+t) \in A
\rbrace \in \mathcal{S}^T$.  Therefore since sets of the form $\lbrace f(s) \in A \rbrace$ generate
$\mathcal{S}^T$, we see that $\theta_t$ is measurable by Lemma
\ref{MeasurableByGeneratingSet}.

Now let $U$ be as above.  It is clear that the shift operator
preserves the necessary continuity and limit properties and thus is
well defined as a function $\theta : U \cap S^T \times T \to U \cap S^T$.  To
see measurability of $\theta$, first note that the evaluation map $\pi : U \cap S^T \times
T \to S$ given by $\pi(f,t) = f(t)$ is measurable (e.g. this follows
by considering the process defined by the identity $U
\cap S^T \to U \cap S^T$ and using Lemma
\ref{ContinuityAndProgressiveMeasurability} to see that it is jointly
measurable).
Now let $s \in T$ and $A
\in \mathcal{S}$ as before and calculate
\begin{align*}
\lbrace (f, t) \mid \theta_t f \in \pi_s^{-1} A \rbrace
&=
\lbrace (f, t) \mid \theta_t f(s) \in  A \rbrace 
=\lbrace (f, t) \mid \theta_s f(t) \in  A \rbrace \\
&= (\theta_s, id)^{-1} \lbrace (f, t) \mid f(t) \in  A \rbrace
= (\theta_s, id)^{-1} \pi^{-1} A
\end{align*}
which is measurable by the joint measurability of $\pi$ noted above
and the measurability of $\theta_s$ for fixed $s \in T$.
\end{proof}

When considering Markov processes on the canonical space there is a
very useful construction of time shifting optional times.  Intuitively
the construction is that given two optional times $\sigma$ and $\tau$
one constructs the random time which is ``the first time $\tau$
happens after $\sigma$ happens''.  The following Lemma makes the
construction precise and shows that under some assumption on the path
space that the construction gives us a weak optional time.
\begin{lem}\label{TimeShiftOptionalTimes}Let $S$ be a metric space and
  let $\sigma$ and $\tau$ be weakly optional times on any of the canonical
  spaces $S^\infty$, $C([0,\infty); S)$ or $D([0,\infty); S)$ provided
  with the canonical filtration $\mathcal{F}$.   Then 
\begin{align*}
\gamma &= \begin{cases}
\sigma + \tau  \circ \theta_\sigma & \text{when $\sigma<\infty$} \\
\infty & \text{when $\sigma=\infty$}
\end{cases}
\end{align*}
is also weakly $\mathcal{F}$-optional.
\end{lem}
\begin{proof}
Let $X$ be the canonical process (i.e. $X_t$ is the evaluation
function $\pi_t$).  

First we claim that $\gamma$ is measurable.  This follows by noting
that $\theta_\sigma$ is measurable by writing it as $\theta \circ (id,
\sigma)$ and using by Lemma \ref{MeasurabilityOfShiftOperator}.
Therefore $\gamma$ is measurable by the measurability of
$\theta_\sigma$, $\sigma$
and $\tau$ and application of Lemma \ref{CompositionOfMeasurable} and
Lemma \ref{ArithmeticCombinationsOfMeasurableFunctions}.

Next we claim that if we pull back $\mathcal{F}_t$ by $\theta_\sigma$
then result should only depend on values  $X_s$ for $\sigma \leq s
\leq \sigma + t$ hence should be $\mathcal{F}^+_{\sigma +
  t}$-measurable.  We have to be a bit careful with this claim, because $\sigma$
can be infinite in which case $\theta_\sigma$ isn't defined.  To make
the claim precise and to prove it pick $n \geq 0$ and note that by either discreteness or by continuity of sample
paths together with Lemma
\ref{ContinuityAndProgressiveMeasurability} we know that $X$ is $\mathcal{F}$-progressively measurable. 
By $\mathcal{F}^+$-optionality of $\sigma \wedge n$ and Lemma
\ref{StoppedProgressivelyMeasurableProcess} we know that
$X_{\sigma \wedge n +s} = X_s \circ \theta_{\sigma \wedge n}$ is
$\mathcal{F}^+_{\sigma \wedge n+s}$-measurable for all $s \geq 0$.
Now fix $t \geq 0$ then for $0 \leq s \leq t$, pick a measurable set $B
\in \mathcal{S}$
and let $A = \lbrace X_s \in B \rbrace$; we note that
 $\theta_{\sigma \wedge n}^{-1} A = (X_s \circ \theta_{\sigma \wedge n})^{-1}(B) =
X_{\sigma \wedge n+s}^{-1}(B) \in \mathcal{F}^+_{\sigma \wedge n+s} \subset
\mathcal{F}^+_{\sigma \wedge n+t} $.  
Since $\lbrace A \mid \theta_{\sigma \wedge n}^{-1} A \in
\mathcal{F}^+_{\sigma \wedge n+t} \rbrace$ is a $\sigma$-algebra (Lemma
\ref{SigmaAlgebraPullback}) and sets of the form $\lbrace X_s \in B
\rbrace$ for $0 \leq s \leq t$ generate $\mathcal{F}_t$, we know that
$\theta_{\sigma \wedge n}^{-1} \mathcal{F}_t \subset
\mathcal{F}^+_{\sigma \wedge n +t}$ for all $t \geq 0$ and $n \geq 0$.

Now fix $0 \leq t < \infty$, let $n = \floor{t}+1$
and note that
\begin{align*}
\lbrace \gamma < t \rbrace &= \cup_{\substack{0 < r < t\\r \in
    \rationals}} \lbrace \sigma < r ; \tau \circ \theta_\sigma < t -
r\rbrace \\
&=\cup_{\substack{0 < r < t\\r \in
    \rationals}} \lbrace \sigma  \wedge n < r ; \tau \circ
\theta_{\sigma \wedge n} < t - r\rbrace
\end{align*}
Since $\tau$ is weakly $\mathcal{F}$-optional we know that $\lbrace
\tau < t-r\rbrace \in \mathcal{F}_{t-r}$ hence $\theta_{\sigma \wedge n}^{-1}
\lbrace \tau < t-r \rbrace \in \mathcal{F}^+_{\sigma \wedge n + t -r}$ and
therefore using Lemma \ref{WeaklyOptionalCharacterization} applied to
the stopped $\sigma$-algebra $\mathcal{F}^+_{\sigma  \wedge n + t -r}$ we get
\begin{align*}
\lbrace \sigma \wedge n < r ; \tau \circ \theta_{\sigma \wedge n}< t - r\rbrace &=
\lbrace \sigma \wedge n +t -r < t \rbrace \cap \theta_{\sigma \wedge n}^{-1} \lbrace \tau
< t -r\rbrace \in \mathcal{F}_t
\end{align*}
and therefore $\gamma$ is weakly $\mathcal{F}$-optional.
\end{proof}

Note: Kallenberg's proof of the above Lemma is a little bit different and
from what I can tell has a small error.  He first
proves the result for $\sigma$ bounded, and then claims that
$\gamma_n = \sigma \wedge n + \tau \circ \theta_{\sigma \wedge n} \uparrow \gamma$ enabling us to apply the result for
the bounded case to
$\gamma_n$ and to conclude that $\gamma = \sup_n \gamma_n$ is weakly
$\mathcal{F}$-optional via Lemma
\ref{InfSupStoppedFiltration}.  The problem is that $\gamma_n$ as
defined is not increasing.  To see a counter example let $S = \lbrace
H,T \rbrace$ and consider the result for $S^\infty$ (here time is $\integers_+$).  Define 
\begin{align*}
\tau &= \min \lbrace n \mid n \text{ is even and } X_n = H \rbrace
\end{align*}
It is easy to see that $\tau$ is a stopping time as 
\begin{align*}
\lbrace \tau = n \rbrace  &= 
\begin{cases}
\lbrace X_0 =  H \rbrace & \text{for $n = 0$} \\
\lbrace X_n = H \rbrace \cap \lbrace X_{n-2} = T \rbrace \cap \dotsb
\cap \lbrace X_0 = T \rbrace & \text{if $n$ is even and $n > 0$} \\
\emptyset & \text{if $n$ is odd}
\end{cases} 
\end{align*}
Now let $\sigma$ be
a suitably large deterministic time (say $\sigma = 2$) so that for $n \leq
2$ we have $\gamma_n = n + \tau \circ \theta_n$.  Consider
$\omega = (T,H,H,H,\dotsc) \in S^\infty$.  Note that $\tau(\omega)
= 2$ thus $\gamma_0(\omega) = 2$ but $\tau(\theta_1(\omega)) = 0$ and
therefore $\gamma_1(\omega) = 1 < \gamma_0(\omega)$. 

It is worth noting that even when we are not considering the canonical
case many optional times of interest (in particular hitting times) are
pull backs of optional times on the path space (i.e. are of the form
$\tau \circ X$ where $\tau$ is an optional time defined on $S^T$).  If
we are given a pair of these optional times then we can
apply the time shift construction of the optional times on the path
space and pull back (i.e. forming $\sigma \circ X + \tau \circ
\theta_{\sigma \circ X} \circ X$).  

\begin{lem}Let $X$ be a time homogeneous Markov process on
  $\integers_+$ or $\reals_+$ and let $\tau$ be an optional
  time with at most countably many values.  Then for every measurable
  $A \subset S^T$,
\begin{align*}
\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in A}(\omega) &=
\sprobability{A}{X_\tau(\omega)} \text{ for almost all $\omega$ such that $\tau(\omega) < \infty$}
\end{align*}
\end{lem}
\begin{proof}
Before starting on the proof we first need to make some remarks about
the well-definedness of the quantities in the result.  Specifically we
have not defined $\theta_\tau X$ nor $\sprobability{A}{X_\tau}$ when $\tau = \infty$ but neither have
we assumed that $\tau$ is almost surely finite.  The first point is that we
can extend $\sprobability{A}{X_\tau}$ can be defined to be an
arbitrary value on $\lbrace \tau = \infty \rbrace$ without affecting
the values of $\sprobability{A}{X_\tau}$ on $\lbrace \tau < \infty
\rbrace$ hence the assertion of the result.  By locality of
conditional expectation (Lemma \ref{ConditionalExpectationIsLocal})
and the $\mathcal{F}_\tau$-measurability of
$\tau$ (Lemma
\ref{StoppedFiltration}) we can define $\theta_\tau X$ arbitrarily on $\lbrace \tau =
\infty \rbrace$ without affecting the values of
$\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in A}$ on $\lbrace \tau < \infty
\rbrace$ hence the assertion of the result.  Therefore the result
makes sense assuming that such extensions have made and is independent
of the extensions chosen.

We first prove the result for deterministic times and extend to
countably valued optional times.  Note that the content of result is
vacuous for an infinite deterministic time, so pick a finite
deterministic time $t$, $t_1
\leq \cdots \leq t_n$, $B \in \mathcal{S}^{\otimes n}$, $A =
(\pi_{t_1}, \dotsc, \pi_{t_n})^{-1}(B)$ and
calculate using Lemma \ref{MarkovDistributions}, time homogeneity and
the proof of Lemma \ref{MarkovMixtures}
\begin{align*}
\cprobability{\mathcal{F}_t}{\theta_t X \in A} &=
\cprobability{\mathcal{F}_t}{((\theta_t X)_{t_1}, \dotsc,
  (\theta_t X)_{t_n}) \in B} \\
&= \cprobability{\mathcal{F}_t}{(X_{t
    + t_1}, \dotsc, X_{t+ t_n}) \in B} \\
&= \mu_{t, t + t_1} \otimes \cdots \otimes \mu_{t+t_{n-1}, t+t_n}(X_t,
B) \\
&=\mu_{0, t_1} \otimes \cdots \otimes \mu_{t_{n-1}, t_n}(X_t, B) \\
&= \sprobability {A}{X_t}
\end{align*}
Now we know that sets of the form 
$(\pi_{t_1}, \dotsc, \pi_{t_n})^{-1}(B)$ are a
generating $\pi$-system for the $\sigma$-algebra $\mathcal{S}^T$,
the full result for deterministic times $t$ follows from a monotone
class argument.  Specifically, we simply show that the set of $A$ such that
$\cprobability{\mathcal{F}_t}{\theta_t X \in A} =
\sprobability{A}{X_t}$ a.s. is a $\lambda$-system.  The case for $B
\setminus A$ follows from linearity of conditional expectation and
finite additivity of measure and the case $A_1 \subset A_2 \subset
\cdots$ follows from monotone convergence for conditional expectations
and continuity of measure.

Now we extend to the case of countably valued optional times.  Let $A
\in \mathcal{S}^T$ and $B \in \mathcal{F}_\tau$ and calculate using
Monotone Convergence and the result for deterministic times
\begin{align*}
\expectation{\characteristic{A}(\theta_\tau X) ; B} &= \sum_t
\expectation{\characteristic{A}(\theta_t X) ; \lbrace \tau = t \rbrace
  \cap B} \\
&= \sum_t \expectation{\sprobability{A}{X_t} ; \lbrace \tau = t \rbrace
  \cap B} \\
&= \expectation{\sprobability{A}{X_\tau} ; B} 
\end{align*}
so the result follows by the definition of conditional expectation. 

An alternative argument that extends the case of deterministic times
to countable optional times uses the localization of the stopped
filtration Lemma \ref{LocalizationOfStoppedFiltration} and the local
property of conditional expectations Lemma
\ref{ConditionalExpectationIsLocal}.  Let $t$ be a value in the range
of $\tau$, combining these two results and
using the result for deterministic times we
know that on the set $\lbrace \tau = t \rbrace$ we have
\begin{align*}
\cprobability{\mathcal{F}_{\tau}}{\theta_\tau X \in A} &=
\cprobability{\mathcal{F}_t}{\theta_t X \in A} = \sprobability{A}{X_t}
 = \sprobability{A}{X_\tau} \text{ a.s.}
\end{align*}
Let the set where the above inequality fails be called $N_t$.  Since
we have assumed the set of values of $\tau$ is countable, the union of
the $N_t$ is also a null set and the result holds off of this null
set.

TODO: What about the $\mathcal{F}_\tau$-measurability of
$\sprobability{A}{X_\tau}$?  Note that this is a consequence of result
since we haven't assumed $X$ is progressive(see Lemma
\ref{StrongIndependentIncrements} below where we make this implication explicit).  Double check that we
don't assume it in the proof above.
\end{proof}

In the case of a space homogeneous Markov process the strong Markov
property can be expressed more concisely as an extension of the
independent increments characterization of Lemma
\ref{IndependentIncrements} to optional times.  In many scenarios it
is more convenient to use these properties.  Note that the Lemma does
not require the countable range assumption.
\begin{lem}\label{StrongIndependentIncrements}Let $S$ be a measurable
  Abelian group with a filtration $\mathcal{F}$, $X$ be a time
  homogeneous and space homogeneous $S$-
  valued Markov process and $\tau$ be an almost surely finite optional time.  
Then 
\begin{align*}
\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in A} &= \sprobability{A}{X_\tau}
\end{align*}
if and only if $X_\tau$ is $\mathcal{F}_\tau$-measurable, $\cindependent{\theta_\tau X -
  X_\tau}{\mathcal{F}_\tau}{}$ and $X - X_0 \eqdist \theta_\tau X - X_\tau$
\end{lem}
\begin{proof}
Assume that $X$ satisfies $\cprobability{\mathcal{F}_\tau}{\theta_\tau
  X \in A} = \sprobability{A}{X_\tau}$ for all $A \in \mathcal{S}^T$.  To see that $X_\tau$ is
$\mathcal{F}_\tau$-measurable observe that if we let $\pi_0 : S^T \to
S$ be
evaluation at time $0$, then for any $B \in \mathcal{S}$ and $x \in S$,
\begin{align*}
\sprobability{\pi_0^{-1}B}{x} &= \begin{cases}
1 & \text{if $x \in B$} \\
0 & \text{if $x \notin B$}
\end{cases}
\end{align*}
therefore we have
\begin{align*}
\characteristic{X_\tau \in B} &=
\sprobability{\pi_0^{-1}B}{X_\tau} =
\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in \pi_0^{-1} B}
\end{align*}
which shows that $\lbrace X_\tau \in B \rbrace \in \mathcal{F}_\tau$.

Having established $\mathcal{F}_\tau$-measurability of $X_\tau$ we
know that $P_{X_\tau}$ is a not just a \emph{regular} version for
$\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in \cdot}$ and we can
apply Theorem \ref{Disintegration} and space homogeneity of
$\probabilityop_x$ (Lemma \ref{SpaceHomogeneousMarkovDistributions})
to calculate for $A \in \mathcal{S}^T$ (using $f: S^T \times S \to
\reals_+$ given by $f(x,y) = \characteristic{A+y}(x)$ in the disintegration) 
\begin{align*}
\cprobability{\mathcal{F}_\tau}{\theta_\tau X - X_\tau \in A} &= 
\int \characteristic{A + X_\tau} (x) \,
\probabilityop_{X_\tau}(dx) 
=\sprobability{A + X_\tau}{X_\tau}
=\sprobability{A}{0} \text{ a.s.}
\end{align*}
which is almost surely constant and therefore independence is proven.
This also shows that the distribution of $\theta_\tau X - X_\tau$ is
equal to $\probabilityop_0$ and letting $\tau = 0$ shows $\theta_\tau
X - X_\tau \eqdist X - X_0$.

To prove the converse, note that $X - X_0$ is has
initial distribution $\delta_0$ hence using our independence and
equidistribution assumptions and the definition of the measure
$\probabilityop_0$ we get for any $A \in \mathcal{S}^T$,
\begin{align*}
\cprobability{\mathcal{F}_\tau}{\theta_\tau X - X_\tau \in A} &=
\probability{\theta_\tau X - X_\tau \in A} 
=\probability{X - X_0 \in A} 
=\sprobability{A}{0}
\end{align*}
which provides us with a regular version for
$\cprobability{\mathcal{F}_\tau}{\theta_\tau X - X_\tau \in \cdot}$.
Now by the $\mathcal{F}_\tau$-measurability of $X_\tau$ we can apply
Theorem \ref{Disintegration} and Lemma
\ref{SpaceHomogeneousMarkovDistributions} to get
\begin{align*}
\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in A}
&=\cprobability{\mathcal{F}_\tau}{\theta_\tau X - X_\tau \in A-X_\tau}\\
&= \int \characteristic{A-X_\tau}(x) \, \probabilityop_0(dx) \\
&=\sprobability{0}{A - X_\tau} \\
&=\sprobability{X_\tau}{A }
\end{align*}
and we are done.
\end{proof}

\section{Markov Chains}

\begin{defn}Let $P$ be a finite Markov chain on $S$, we say a function
  $h : S \to \reals$ is \emph{harmonic} if for all $x \in S$, $\sum_{y
    \in S} P(x,y) h(y) = h(x)$.
\end{defn}

\begin{lem}\label{MarkovChainHarmonicFunctionConstant}Let $P$ be an irreducible finite Markov chain on $S$ and let $h :
  S \to \reals$ be harmonic, then $h$ is constant.
\end{lem}
\begin{proof}
Let $M$ be the maximum value of $h$ and let $x \in S$ be such that
$h(x) = M$.  Suppose there exists $y \in S$ such that $P(x,y) > 0$ and
$h(y) < M$.  It would then follow that
\begin{align*}
M &= h(x) = \sum_{y \in S} h(x) P(x,y) < M \sum_{y \in S} P(x,y) = M
\end{align*}
which is a contradiction.  Thus we know that $h(y) = M$ for all $y \in
S$ such that $P(x,y) > 0$.  Now we do an induction.  Suppose $h(y) =
M$ for all $y \in S$ such that $P^{n-1}(x, y) > 0$ and suppose $z \in
S$ is such that $P^n(x,z) > 0$.  It follows from the expression of
matrix multiplication $P^n(x,z) = \sum_{y \in S} P^{n-1}(x,y) P(y,z)$
that there exists a $y \in S$ such that $P^{n-1}(x,y) > 0$ and $P(y,z)
> 0$.  So by the induction hypothesis we know that $h(y) = M$ and by
replaying the case of $n=1$ with $y$ we get that $h(z) = M$.  

By irreducibility we know that for every $y \in S$, there exists $n
\geq 0$ such that $P^n(x,y) > 0$ and thus we have
$h(y) = M$  for every $y \in S$.
\end{proof}

\begin{lem}Let $P$ be an irreducible finite Markov chain, if the
  stationary distribution exists, then is unique.
\end{lem}
\begin{proof}
Let $I$ denote the $\card(S) \times \card(S)$ identity matrix.
By Lemma \ref{MarkovChainHarmonicFunctionConstant} we know that the
matrix $P -I$ has a one dimensional null space given by the constant
functions.  Thus column rank of $P - I$ is $\card(S) - 1$ and the same
is true for the row rank; thus there is a unique solution of $\pi (P
-I) = 0$ that satisfies $\sum_{x \in S} \pi(x) = 1$.  Note that this
does not guarantee the existence of a stationary distribution as that
requires that the entries of $\pi$ be non-negative.
\end{proof}

\begin{lem}If $\pi(x) P(x,y) = \pi(y) P(y,x)$ for all $x,y \in S$ then
  $\pi \cdot P = \pi$.
\end{lem}
\begin{proof}
This is a simple computation for each $y \in S$,
\begin{align*}
(\pi \cdot P)(y) &=\sum_{x \in S} \pi(x) P (x,y)  = \sum_{x \in S} \pi(y) P(y,x) =
\pi(y) \sum_{x \in S} P(y,x) = \pi(y)
\end{align*}
\end{proof}

The detail balance equation says ``the probability of starting at $x$
and making a transition to $y$ is equal to the probability of starting
at $y$ and making a transition to $x$''.  To be more concise we may
say that with starting distribution $\pi$, the probability of a trajectory $x \to y$ is the same as the
probability of a trajectory $y \to x$.  This is a type of symmetry
that is sometime described as the equivalence running the chain
forward and running the chain backward.  By induction it is not hard
to see that this symmetry extends to reversing trajectories of
arbitrary finite length.  We shall prove something more general by
showing how to ``reverse'' a Markov chain that doesn't necessarily
satisfy the detail balance equations.

\begin{defn}The \emph{time reversal} of an irreducible Markov chain
  with transition matrix $P$ and stationary distribution $\pi$ is
  given by 
\begin{align*}
\hat{P}(x,y) &= \frac{\pi(y) P(y,x)}{\pi(x)}
\end{align*}
\end{defn}

\begin{lem}The time reversal is a stochastic matrix and $\pi$ is
  stationary for $\hat{P}$.  Moreover, for every $x_0, \dotsc, x_n \in
  S$, we have
\begin{align*}
\sprobability{X_0 = x_0; \dotsb ; X_n = x_n}{\pi} &=
\sprobability{\hat{X}_0 = x_n ; \dotsb ; \hat{X}_n = x_0}{\pi}
\end{align*}
\end{lem}
\begin{proof}
By stationarity of $\pi$ with respect to $P$ for all $x \in S$,
\begin{align*}
\sum_{y \in S} \hat{P}(x,y) &=\sum_{y \in S} \frac{\pi(y)
  P(y,x)}{pi(x)} \frac{1}{\pi(x)} \sum_{y \in S} \pi(y)  P(y,x) = 1
\end{align*}
To see $\pi$ is stationary for $\hat{P}$, compute for all $y \in S$,
\begin{align*}
(\pi \cdot \hat{P})(y) &= \sum_{x \in S} \pi(x) \hat{P}(x,y) = \sum_{x
  \in S} \pi(y) P(y,x) =\pi(y)
\end{align*}
The last fact follows from an induction argument where the case $n=1$
is the definition of the time reversal matrix $\hat{P}$.  If we assume
that the result holds for $n-1$ then
\begin{align*}
\sprobability{X_0 = x_0; \dotsb ; X_n = x_n}{\pi}  &= \pi(x_0) P(x_0,
x_1) \dotsb P(x_{n-1}, x_n) \\
&= \hat{P}(x_1, x_0) \pi(x_1) P(x_1, x_2) \dotsb P(x_{n-1},x_n) \\
&= \hat{P}(x_1, x_0) \pi(x_n) \hat{P}(x_n, x_{n-1}) \dotsb \hat{P}
(x_2,x_1) \\
&= \sprobability{\hat{X}_0 = x_n ; \dotsb ; \hat{X}_n = x_0}{\pi}
\end{align*}
\end{proof}

\section{Pure Jump-Type Markov Processes}
In this section we discuss a simple subclass of time homogeneous
Markov Processes on $\reals_+$.
\begin{defn}A time homogenous Markov process on $\reals_+$ with values
  in a metric (topological?) space $(S, \mathcal{B}(S))$ is said to be \emph{pure
    jump-type} if almost surely its sample paths are piecewise
  constant with isolated jump discontinuities.
\end{defn}

The first goal is to get a more constructive description of the class
of pure jump-type Markov processes.  The key idea in achieving that goal
is to study the random time to the jumps of the process; in fact these
random times are optional with respect to the right continuous
filtration generated by the process.

\begin{defn}Let $X$ be a pure jump-type Markov process then the
  \emph{first jump time} is the random time
\begin{align*}
\tau_1 &=\inf \lbrace t \geq 0 \mid X_t \neq X_0 \rbrace
\end{align*}
 the \emph{$n^{th}$ jump time} is defined to be 
\begin{align*}
\tau_n &= \tau_{n-1} + \tau_1 \circ \theta_{\tau_{n-1}} = \inf \lbrace
t \geq \tau_{n-1} \mid X_t \neq X_{\tau_{n-1}} \rbrace \text{ for $n > 1$}
\end{align*}
and the \emph{$0^{th}$ jump time} is $\tau_0 = 0$.
\end{defn}

\begin{lem}Let $X$ be a pure jump-type Markov process then
$\tau_n$ is a weakly $\mathcal{F}$-optional time for all $n \geq 0$.
\end{lem}
\begin{proof}
The case $\tau_0$ is trivial as it is a deterministic time.
For each $n \in \naturals$, define $\sigma_n = \min \lbrace k/2^n \mid
X_{k/2^n} \neq X_0 \rbrace$.  Note that because of the right
continuity of sample paths of $X$ we have $\sigma_n \downarrow
\tau_1$.  Moreover we have
\begin{align*}
\lbrace \sigma_n \leq t \rbrace &= \cup_{k=0}^{\floor{2^n t}}
\lbrace X_{k/2^n} \neq X_0 \rbrace 
\in \mathcal{F}_{\floor{2^n t}/2^n} \subset \mathcal{F}_t
\end{align*}
and therefore $\sigma_n$ is $\mathcal{F}$-optional.  Therefore by
Lemma \ref{InfSupStoppedFiltration} we see that $\tau_1 = \lim_{n
  \to\infty} \sigma_n = \inf_n \sigma_n$ is weakly
$\mathcal{F}$-optional.

The fact that $\tau_{n}$ is weakly optional follows by induction using
Lemma
\ref{TimeShiftOptionalTimes} applied to the expression $\tau_n =
\tau_{n-1} + \tau_1 \circ \theta_{\tau_{n-1}}$.
\end{proof}

\begin{lem}\label{PureJumpFirstJumpTime}Let $X$ be a pure jump-type Markov process and let $x \in
  S$ be nonabsorbing, then under $\probabilityop_x$ the optional time
  $\tau_1$ is exponentially distributed and independent of $\theta_{\tau_1}X$.
\end{lem}
\begin{proof}
To see that $\tau_1$ is exponentially distributed note that
\begin{align*}
\sprobability{\tau_1 > t+s}{x} &= \sprobability{\tau_1 > s; \tau_1
  \circ \theta_s > t}{x} = \sprobability{\tau_1 > s}{x} \sprobability{\tau_1 > t}{x} 
\end{align*}
If we let $\sprobability{\tau_1 > 1}{x} = e^{-c}$ for some $c \in [0,
\infty]$, then from this functional equation we immediately see that for every $n \in
\naturals$, $\sprobability{\tau_1 > n}{x} = e^{-cn}$ and then for all
positive rationals $p/q \in \rationals_+$ we have
$\sprobability{\tau_1 > p/q}{x} = e^{-cp/q}$.  Now since
$\sprobability{\tau_1 > t}{x}$ is non-decreasing we can conclude that
$\sprobability{\tau_1 > t}{x} = e^{-ct}$ for all $0 \leq t < \infty$.
By our assumption that $x$ is nonabsorbing we know that $\tau_1 > 0$
a.s. and therefore $c < \infty$; thus we have shown that $\tau_1$ is
exponentially distributed.

Recall from Lemma
\ref{HittingTimesContinuous} that when restricted to
$D([0,\infty);S)$, one can think of $\tau_1$
as being a composition of the process $X$ with a measurable function on
$S^{[0,\infty)}$ which by abuse of notation we also call $\tau_1$
(of course in the canonical case there is no abuse of notation as
these two interpretations of $\tau_1$ are exactly the same).  
Moreover, note that the random variable $\theta_{\tau_1 }X$
can be written as a composition of $X$ and the measurable function
$\phi : D([0,\infty) ; S) \to \reals$ defined by $\phi(f,t) =
\characteristic{B}(\theta_{\tau_1(f)}f)$. Measurability of $\phi$
follows from writing it as the composition 
\begin{align*}
D([0,\infty) ; S) \overset{(id, \tau_1)}  \to D([0,\infty) ; S) \times
[0, \infty)
\overset{\theta} \to D([0,\infty) ; S) \overset{\characteristic{B}}
\to \reals
\end{align*}
(recall that $\theta$ as above is measurable by Lemma \ref{MeasurabilityOfShiftOperator}).

Let $\tau^t_1 = \inf \lbrace s \geq
t \mid X_s \neq x \rbrace$ and note that $\tau^t_1(X) =
\tau_1(\theta_t X) + t$.   From this we get
\begin{align*}
\left( \theta_{\tau^t_1}X \right)_s &= X(\tau^t_1(X) + s) =
X(\tau_1(\theta_tX) + t + s) = \left(\theta_{\tau_1(\theta_t X)} \circ \theta_tX
\right)_s
\end{align*} 
Now we can compute (in rather excruciating detail I might add)
\begin{align*}
\probability{\tau_1 > t ; \theta_{\tau_1}X \in B}
&=
\probability{\tau_1 > t ; \theta_{\tau^t_1}X \in B}\\
&=
\probability{\tau_1 > t ;
  \cexpectationlong{\mathcal{F}_t}{\theta_{\tau_1(\theta_t X)}\circ \theta_t X \in B}}\\
&=
\probability{\tau_1 > t ;
  \cexpectationlong{\mathcal{F}_t}{\phi(\theta_t X)}}\\
&=
\probability{\tau_1 > t ;
  \sexpectation{\phi}{X_t}}\\
&= \probability{\tau_1 > t } \sexpectation{\phi}{x}\\
&= \probability{\tau_1 > t } \probability{\phi(X)}\\
&= \probability{\tau_1 > t } \probability{\theta_{\tau_1} X \in B}\\
\end{align*}

\end{proof}

Based on the previous result we see that the distribution of the first
jump of a pure
jump type Markov process boils down to two independent distributions:
the first being an exponential distribution that describes when a jump
happens and the second being a general distribution that describes
where the jump goes to.  This observation can be used to give us a
nice description of the entire process.  Before providing the
construction we settle on some terminology.


\begin{defn}Given a pure jump Markov process $X$ with a first jump
  time $\tau_1$ we define the \emph{rate function} to be 
\begin{align*}
c(x) &=\begin{cases}
  1/\sexpectation{\tau_1}{x} & \text{if $x$ is non-absorbing}\\
0 & \text{if $x$ is absorbing}
\end{cases}
\end{align*}
 the \emph{jump transition kernel} to be
  $\mu(x,B) = \sprobability{\theta_{\tau_1}X \in B}{x}$ and the \emph{rate
    kernel} to be $\alpha(x,B) = c(x) \mu(x,B)$.  
\end{defn}
Note that in the above definition we are thinking of the Markov
process as the family of measures $\probabilityop_x$ on
$S^{[0,\infty)}$ and interpreting $\tau_1$ as a
function from $S^{[0,\infty)}$ to $\reals_+$.

Before proceeeding to our structure theory for pure jump type Markov
processes we establish the basic measurability properties of the
functions just defined.
\begin{lem}The rate function $c(x)$ is a measurable function on $S$
  and the jump transition kernel and rate kernel are both kernels from
  $S$ to $S^{[0,\infty)}$.  The rate kernel is a measurable function
  of the jump transition kernel.
\end{lem}
\begin{proof}
We know that $\probabilityop_x$ is a kernel by Lemma
\ref{MarkovMixtures} and therefore $\sexpectation{\tau_1}{x}$ is a
measurable function of $x$ by Lemma
\ref{KernelTensorProductMeasurability}.  Lastly we see that 
\begin{align*}
\lbrace x \text{ is non-absorbing} \rbrace &= \sprobability{\tau_1 <
  \infty}{x}
\end{align*}
is measurable because $\probabilityop_x$ is a kernel; thus $c(x)$ is measurable.

The fact that $\mu(x,B)$ is a measurable function of $x$ for fixed $B$
follows from the fact $\probabilityop_x$ is a kernel.  The fact that
for fixed $\mu(x,B)$ is a probability measure for fixed $x$ follows
from measurability of the mapping taking $X$ to $\theta_{\tau_1}X$ and
Lemma \ref{PushforwardMeasure}.

To see that $\mu(x,B)$ is a measurable function of $\alpha(x,B)$ just
observe that 
\begin{align*}
\mu(x,B) &= \begin{cases}
\alpha(x,B)/\alpha(x, S^{[0,\infty)}) & \text{if $\alpha(x, \cdot)
  \neq 0$} \\
\delta_x(B) & \text{if $\alpha(x, \cdot) = 0$}
\end{cases}
\end{align*}
\end{proof}

Extending these ideas further we see that every pure jump-type Markov
process decomposes into a discrete time Markov chain that describes
the jumps that occur and a sequence of independent exponential random
variables that describe the time between jumps.   This make intuitive sense given the
last lemma and the Strong Markov property: our process begins by
waiting for an exponentially distributed time then makes an
independent jump to a new state; by the Strong Markov property the
process starts afresh in the new state waits for another independent
exponentially distributed time and makes another independent jump and
so on.  The subtlety arises because the heuristic argument just given
ignores the fact that our process may jump into an absorbing state.
Handling that problem and making things precise is the job of the
next theorem.

\begin{thm}Let $X$ be a pure jump Markov process with rate kernel
  $\alpha=c\mu$, then there is a Markov process $Y$ on $\integers_+$ with $Y$ has transition kernel $\mu$
  and a
  sequence of i.i.d. exponential random variables $\gamma_0, \gamma_1,
  \dotsc$ of rate $1$ that are independent of $Y$ such that
\begin{align*}
\tau_n &= \begin{cases}
\sum_{k=0}^{n-1} \frac{\gamma_k}{c(Y_k)} &\text{when $c(Y_k)
  \neq 0$ for all $k=0, \dotsc, n-1$} \\
\infty & \text{when $c(Y_k) = 0$ for some $k=0, \dotsc, n-1$}
\end{cases}
\end{align*}
and
\begin{align*}
X_t &= Y_n  \text{ a.s. for $\tau_n \leq t < \tau_{n+1}$}
\end{align*} 
when $\tau_n < \infty$.  If $\tau_n = \infty$ for some $n$ then let
$N = \max \lbrace n \mid \tau_n < \infty \rbrace$, then we have $Y_n =
Y_{N-1} = X_{\tau_{N}}$ for all $n > N$.
\end{thm}
\begin{proof}
To simply notation, in the case in which $\tau_n = \infty$ for some
$n$, let $X_\infty = X_{\tau_N}$ where $N$ is defined in the statement
of the Theorem (it is the position of $X$ after its last jump). With
that definition in hand we know that the result of the Theorem
requires that we define $Y_n = X_{\tau_n}$.  The work is in
constructing the $\gamma_n$ and validating the Markov property.

Our first real task is to understand the relationship between the condition
$\lbrace \tau_n < \infty \rbrace$ and the condition $\lbrace c(Y_{n-1})
\neq 0 \rbrace$ in order to make proper sense of the expression for
$\tau_n$.  One of the critical facts in these calculation is the fact
that 
\begin{align}\label{AbsorbingDichotomy}
\sprobability{\tau_1 < \infty} {x} &= \begin{cases}
0 & \text{when $c(x) = 0$ } \\
1 & \text{when $c(x) \neq 0$}
\end{cases}
\end{align}
which is a consequence of Lemma \ref{PureJumpFirstJumpTime} and the
definition of $c(x)$.

Claim: $\tau_n < \infty$ almost surely when $c(Y_{n-1}) \neq 0$ and
$\tau_{n-1} < \infty$
(i.e. $\probability{\tau_n < \infty;c(Y_{n-1}) \neq 0; \tau_{n-1} < \infty} =
\probability{c(Y_{n-1}) \neq 0; \tau_{n-1} < \infty}$).

First note that for any $x \in S$, by definition $c(x) \neq 0$ implies
that $\sexpectation{\tau_1}{x} < \infty$ which certainly implies that
$\sprobability{\tau_1 < \infty}{x} =1$.  Now for all $n \geq 1$ we can
calculate using the tower property and pullout property of conditional
expectations and the Strong Markov property
\begin{align*}
&\probability{\tau_n < \infty;c(Y_{n-1}) \neq 0; \tau_{n-1} < \infty} \\
&=\expectation{\cprobability{\mathcal{F}_{\tau_{n-1}}}{\tau_n <
    \infty;c(Y_{n-1}) \neq 0; \tau_{n-1} < \infty}} \\
&=\expectation{\cprobability{\mathcal{F}_{\tau_{n-1}}}{\tau_1(\theta_{\tau_{n-1}(X)}X) <
    \infty};c(Y_{n-1}) \neq 0; \tau_{n-1} < \infty} \\
&=\expectation{\sprobability{\tau_1 <
    \infty}{Y_{n-1}};c(Y_{n-1}) \neq 0; \tau_{n-1} < \infty} \\
&=\probability{c(Y_{n-1}) \neq 0; \tau_{n-1} < \infty} \\
\end{align*}
and the claim is proved.

Claim: $\lbrace c(Y_{n-1}) = 0 \rbrace$ = $\lbrace \tau_n = \infty
\rbrace$ a.s.

What does this mean?  I think $\probability{\lbrace c(Y_{n-1}) = 0
  \rbrace \triangle \lbrace \tau_n = \infty\rbrace} = 0$.
Calculate
\begin{align*}
&\probability{c(Y_{n-1}) = 0; \tau_n < \infty} \\
&=
\probability{c(Y_{n-1}) = 0; \tau_{n-1} < \infty;
  \tau_1(\theta_{\tau_{n-1}(X)}(X)) < \infty } \\
&= \probability{c(Y_{n-1}) = 0; \tau_{n-1} < \infty;
  \cprobability{\mathcal{F}_{\tau_{n-1}}}{\tau_1(\theta_{\tau_{n-1}(X)}(X)) < \infty }} \\
&= \probability{c(Y_{n-1}) = 0; \tau_{n-1} < \infty;
  \sprobability{\tau_1 < \infty }{Y_{n-1}}} = 0 \text{ by \eqref{AbsorbingDichotomy}}\\
\end{align*}
\end{proof}

\section{Feller Processes}

We now specialize to the case of time homogeneous Markov processes and
develop an approach that allow one to bring powerful tools of
functional analysis to bear on the theory of Markov processes and
ultimately elucidates a deep connection between Markov processes and
partial differential equations.  

The first step is to change the point of view on transition kernels
slightly.  In the case of a time homogeneous Markov process, the
family of transition kernels is a single parameter family of kernels
$\mu_t$.  Note that in the discrete time case it is clear that the
entire family of kernels is generated by the single time unit kernel
$\mu = \mu_1$ via kernel multiplication $\mu_n = \mu^{n}$ (in
the case of Markov chains this is just matrix multiplication).  The
first question that we will pursue is whether there is an analogy in
the continuous time case.  The Chapman Kolmogorov relation gives us a
hint on how to proceed.  In the time homogeneous case the Chapman
Kolmogorov relation says that $\mu_s \mu_t = \mu_{s+t}$ which is the
\emph{semigroup property} and suggest that we may be able to write
$\mu_s$ as $exp(s A)$ for some appropriately defined $A$.  With
some additional assumptions this may be done, but first we want to
recast the transition kernels in a different light in which these
questions may be more naturally resolved.  Let $f$ be a measurable
function on $S$ that is either non-negative or bounded.  For any
probability kernel $\mu : S \to \mathcal{P}(S)$, by
Lemma \ref{KernelTensorProductMeasurability} we know that $\int f(t)
\, \mu(s,dt)$ is a itself a measurable function of $s$ that is non-negative
or bounded when $f$ is.  Thus if we are given the transition kernels
of a time homogeneous Markov process we may define an operator $T_tf
(s) = \int f(u) \, \mu_t(s, du)$ on an appropriate space of measurable
functions to itself.  The first thing to observe is that the
Chapman-Kolmogorov relations are equivalent to the semigroup property
for these operators.

