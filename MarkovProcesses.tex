\chapter{Markov Processes}

TODO: Thinking about Markov processes as dynamical/deterministic
systems with (transduced) noise.

\section{Markov Processes}
The basic intuition of what a Markov process comprises is that it is a
stochastic process $X$ on a time scale $T$ such that for every time $t \in
T$ the future behavior of $X_u$ for $u \geq t$ only depends on the
past through the current value of $X_t$.  Alas, in practice the types
of problems that we concern ourselves with Markov process leads us to
a definition of a significantly more complicated object.  Rather than
pummel the reader with the definition we take the approach of starting
from simple intuition and building in the complexity by stages.  Some
readers may prefer to first jump to the end of this section to peer at
the final defintion so that it can be kept in mind during the journey.

\subsection{The Markov Property}
\begin{defn}Let $X$ be a process in $(S, \mathcal{S})$ with time scale
  $T$ which is adapted to a filtration $\mathcal{F}_t$.  We say that
  $X$ has the \emph{Markov property} if 
$\cindependent{\mathcal{F}_s}{X_t}{X_s}$ for all $s \leq t \in T$.
\end{defn}
Given any process that satisfies the Markov property it is not hard to
show using properties of conditional independence that it
automatically satisfies a seemingly stronger condition
\begin{lem}[Extended Markov Property]\label{ExtendedMarkovProperty}
Let $X$ be a process that satsifies the Markov property
  then $\cindependent{\mathcal{F}_t}{\sigma(\bigvee_{u \geq t}
    X_u)}{X_t}$ for all $t \in T$.
\end{lem}
\begin{proof}
Let $t_0 \leq t_1 \leq \cdots $ with $t_i \in T$.  By the
Markov property we know for each $0 \leq n$ that
$\cindependent{\mathcal{F}_{t_n}}{X_{t_{n+1}}}{X_{t_n}}$.  Because $X$ is
adapted to $\mathcal{F}$, we know that $X_{t_m}$ is
$\mathcal{F}_{t_n}$-measurable for $m \leq n$ and therefore
$\cindependent{\sigma(X_{t_0}, \dots
  ,X_{t_{n-1}},\mathcal{F}_{t_n})}{X_{t_{n+1}}}{X_{t_n}}$.
By Lemma \ref{ConditionalIndependenceChainRule} we conclude that $\cindependent{\mathcal{F}_{t_n}}{X_{t_{n+1}}}{X_{t_0}, \dots
  ,X_{t_n}}$ for all $n \geq 0$; because $\mathcal{F}_{t_0} \subset
\mathcal{F}_{t_n}$ we get $\cindependent{\mathcal{F}_{t_0}}{X_{t_{n+1}}}{X_{t_0}, \dots
  ,X_{t_n}}$ for all $n \geq 0$.  Another application of Lemma
\ref{ConditionalIndependenceChainRule} shows that
$\cindependent{\mathcal{F}_{t_0}}{\sigma(X_{t_1}, X_{t_2},
  \dots)}{X_{t_0}}$.

Since the union of the $\sigma$-algebras $\sigma(X_{t_1}, X_{t_2},
  \dots )$ for all $t_0 \leq t_1 \leq \cdots $ is
  clearly a $\pi$-system that generates $\sigma(\bigvee_{u \geq t_0}X_u)$, the result follows by montone classes (specifically Lemma \ref{ConditionalIndependencePiSystem}).
\end{proof}

TODO: Merge previous lemma into this propostion.
By Lemma \ref{ConditionalIndependenceDoob} the conditional independence in the Markov property can be captured by statements about conditional probabilities; it useful to have alternative equivalent characterizations of the Markov property in terms of conditional expectations of more general functions.
\begin{prop}\label{MarkovPropertyViaConditionalExpectations}Let $X$ be a process in $(S, \mathcal{S})$ with time scale
  $T$ which is adapted to a filtration $\mathcal{F}_t$.  Then the
  following are equivalent
\begin{itemize}
\item[(i)] $X$ has the Markov property
\item[(ii)] for all $t \in T$ and non-negative bounded $\sigma(\bigvee_{u \geq t}  X_u)$-measurable random variables $\xi$ we have
\begin{align*}
\cexpectationlong{\mathcal{F}_t}{\xi} &= \cexpectationlong{X_t}{\xi} \text{ a.s.}
\end{align*}
\item[(iii)] for all $s \leq t \in T$ and non-negative or bounded Borel measurable $f : S \to \reals$ we have
\begin{align*}
\cexpectationlong{\mathcal{F}_s}{f(X_t)} &= \cexpectationlong{X_s}{f(X_t)} \text{ a.s.}
\end{align*}
\end{itemize}
\end{prop}
\begin{proof}
To see that (i) implies (ii) suppose that $X$ has the Markov property and let $A \in \sigma(\bigvee_{u \geq t}  X_u)$.  By Lemma \ref{ExtendedMarkovProperty}
and Lemma \ref{ConditionalIndependenceDoob} we have
\begin{align*}
\cprobability{X_t}{A} &=
  \cprobability{\mathcal{F}_t, X_t}{A} =   \cprobability{\mathcal{F}_t}{A} \text{ a.s.}
\end{align*}
thus (ii) holds for indicator functions.  By linearity of conditional expectation (ii) holds for $\sigma(\bigvee_{u \geq t}  X_u)$-measurable simple functions.  By approximation by simple functions and Monotone Convergence for conditional expectations (ii) holds for non-negative $\sigma(\bigvee_{u \geq t}  X_u)$-measurable functions.  By linearity of conditional expectations we get (ii) for bounded $\sigma(\bigvee_{u \geq t}  X_u)$-measurable functions.

(ii) implies (iii) is immediate as any $f(X_t)$ if $\sigma(\bigvee_{u \geq t})$-measurable.

To see that (iii) implies (i) let $A \in \mathcal{S}$ and $s \leq t \in T$, then (iii) implies that 
\begin{align*}
\cprobability{X_s}{X_t \in A} 
&=  \cprobability{\mathcal{F}_s}{X_t \in A} 
= \cprobability{\mathcal{F}_s, X_s}{X_t \in A} \text{ a.s.}
\end{align*}
and Lemma \ref{ConditionalIndependenceDoob} implies that $\cindependent{\mathcal{F}_s}{X_t}{X_s}$.
\end{proof}

\subsection{Markov Transition Kernels}
 
TODO: Introduce the example of Markov Chains here as it is quite a bit
simpler and helps the
understanding of the abstract case quite a bit.

We know make a regularity assumption that for each pair $s,t \in T$
with $s \leq t$, we
have a probability kernel $\mu_{s,t} : S \times \mathcal{S} \to
\reals$ such that for every $A \in \mathcal{S}$
\begin{align*}
\mu_{s,t}(X_s, A) &= \cprobability{X_s}{X_t \in A} =
\cprobability{\mathcal{F}_s}{X_t \in A} \text{ a.s.}
\end{align*}
(e.g. if $S$ is a Borel space then this is true by Theorem
\ref{ExistenceConditionalDistribution}).
We let $\nu_t$ denote the distribution of $X_t$.
These conditional distributions characterize the distribution of the
process $X$ itself.  In particular we have the following nice formula
for finite dimensional distributions of the process.
\begin{lem}\label{MarkovDistributions}Let $X$ be a stochastic process
  on a time scale $T \subset \reals_+$ that has the Markov property,
  one dimensional distributions $\nu_t$ and transition kernels
  $\mu_{s,t}$.  Then for all $t_0 \leq \cdots \leq t_n$ and $A \in
  \mathcal{S}^{\otimes n}$ we have
\begin{align*}
\probability{(X_{t_1}, \dots, X_{t_n}) \in A} 
&= \nu_{t_1} \otimes
\mu_{t_1, t_2} \otimes \cdots \otimes \mu_{t_{n-1},t_n}(A) \\
\cprobability{\mathcal{F}_{t_0}}{(X_{t_1}, \dots, X_{t_n}) \in
  A}(\omega) 
&= \mu_{t_0, t_1} \otimes \cdots \otimes \mu_{t_{n-1},t_n}(X_{t_0}(\omega),A) \\
\end{align*}
\end{lem}
\begin{proof}
We begin by proving the first equality via induction.  The case $n=0$
is true by definition.  The induction step is 
really just a specific case of distintegration (Theorem 
\ref{Disintegration}) applied to the Markov transition kernels.  Let $A
\in \otimes_{i=0}^n \mathcal{S}$ then 
\begin{align*}
&\probability{(X_{t_0}, \cdots, X_{t_n}) \in A} \\
&=\expectation{\characteristic{A}(X_{t_0}, \cdots, X_{t_n})} \\
&=\expectation{\int \characteristic{A}(X_{t_0}, \cdots, X_{t_{n-1}},s)
  \, \mu_{t_{n-1}, t_n} (X_{n-1}, ds)} \\
&=\int \left [ \int \characteristic{A}(u_0, \cdots,u_{n-1}, s)   \,
  \mu_{t_{n-1}, t_n} (X_{n-1}, ds) \right ] \nu_{t_0} \otimes \cdots \otimes
  \mu_{t_{n-2}, t_{n-1}}(du_0, \dotsc, du_{n-1})\\
&=\nu_{t_0} \otimes \cdots \otimes  \mu_{t_{n-1}, t_{n}}(A)
\end{align*}

The second equality is derived from the first.  Suppose we have $A \in
\mathcal{S}$ and $B \in \mathcal{S}^{\otimes n}$.  Then we can compute
\begin{align*}
&\expectation{\characteristic{A}(X_{t_0}) \characteristic{B}(X_{t_1},
  \dotsc, X_{t_n})}  \\
&= \nu_{t_0} \otimes \mu_{t_0, t_1} \otimes \cdots \otimes  \mu_{t_{n-1},
  t_{n}}(A \times B) \\
&= \int \left [ \int \characteristic{B}(u_1, \dotsc, u_n) \mu_{t_0,
    t_1} \otimes \cdots \otimes \mu_{t_{n-1}, t_n} (u_0, du_1, \dotsc, du_n) \right ]
\characteristic{A}(u_0) \nu_{t_0}(u_0) \\
&=\expectation{\mu_{t_0, t_1} \otimes \cdots \otimes \mu_{t_{n-1},
    t_n}(X_0, B) \characteristic{A}(X_0)}
\end{align*}
Now the $\sigma(X_{t_0})$-measurability of $\mu_{t_0, t_1} \otimes \cdots \otimes \mu_{t_{n-1},
    t_n}(X_0, B)$ tells us that 
\begin{align*}
\cprobability{X_{t_0}}{(X_{t_1},\dotsc, X_{t_n}) \in B} 
&= \mu_{t_0, t_1} \otimes \cdots \otimes \mu_{t_{n-1},    t_n}(X_0, B)
\end{align*}

The last thing is to show that
$\cprobability{X_{t_0}}{(X_{t_1},\dotsc, X_{t_n}) \in B} =
\cprobability{\mathcal{F}_{t_0}}{(X_{t_1},\dotsc, X_{t_n}) \in B}$
a.s.  This follows from Lemma \ref{ExtendedMarkovProperty} since by
the tower property of conditional expectations and that result 
for any $A \in \mathcal{S}^{\otimes n}$ and $B \in \mathcal{F}_{t_0}$
\begin{align*}
\probability{(X_{t_1}, \dotsc, X_{t_n}) \in A ; B} 
&=\expectation{\cprobability{X_{t_0}}{(X_{t_1}, \dotsc, X_{t_n}) \in A ;  B}} \\
&=\expectation{\cprobability{X_{t_0}}{(X_{t_1}, \dotsc, X_{t_n}) \in A}
\cprobability{X_{t_0}}{B}} \\
&=\expectation{\cprobability{X_{t_0}}{(X_{t_1}, \dotsc, X_{t_n}) \in A}
\characteristic{B}} \\
\end{align*}
so the $\mathcal{F}_{t_0}$-measurability of
$\cprobability{X_{t_0}}{(X_{t_1}, \dotsc, X_{t_n}) \in A}$ gives the
result by the defining property of conditional expectations.
\end{proof}
A special case of the relations above should be called out as it
motivates a property that will assume as part of the definition of a
Markov process.  But first we need a definition.
\begin{defn}Let $\mu$ and $\nu$ be probability kernels from $S$ to
  $S$.  Then we define the probability kernel $\mu \nu$ from $S$ to
  $S$ by
\begin{align*}
\mu \nu (s, A) 
&= (\mu \otimes \nu)(s,S \times A)
= \iint \characteristic{S \times A}(t,u) \, \nu(t, du) \mu(s, dt) 
= \int \nu(t, A) \mu(s, dt)
\end{align*}
for all $s \in S$ and $A \in \mathcal{S}$.
\end{defn}
\begin{examp}
Let $S$ be a finite set and view $\mu$ and $\nu$ as $S \times S$
matrices as in Example \ref{ProbabilityKernelFiniteSampleSpace}.  Then $\mu \nu$ is just matrix multiplication:
\begin{align*}
\mu \nu (s, \lbrace t \rbrace) 
&= \int \nu(u, \lbrace t \rbrace) \mu(s,du) 
=\sum_{u \in S} \nu_{u, t} \mu_{s, u} = (\mu \nu)_{s,t}
\end{align*}
\end{examp}
\begin{cor}[Chapman-Kolmogorov
  Relations]\label{ChapmanKolmogorovWeak}Let $X$ be a stochastic
  process on a time scale $T \subset \reals$ with values in Borel
  space $(S, \mathcal{S})$ and suppose that $X$ has the Markov
  property.  Then for every $s, t, u \in T$ with $s \leq t \leq u$ we
  have
\begin{align*}
\mu_{s,t} \mu_{t,u} &= \mu_{s,u} \text{ a.s. $\nu_s$}
\end{align*}
\end{cor}
\begin{proof}
Since we have assume $S$ is a Borel space we know from Theorem \ref{ExistenceConditionalDistribution} that regular
versions $\mu_{s,t}$ exist.  By definition of $\mu_{s,t} \mu_{t,u}$, Lemma
\ref{MarkovDistributions} and the uniqueness clause of Theorem \ref{ExistenceConditionalDistribution}
\begin{align*}
\mu_{s,t} \mu_{t,u}(X_s, A) &= 
(\mu_{s,t} \otimes \mu_{t,u})(X_s, S \times A) \\
&= \cprobability{\mathcal{F}_s}{(X_t, X_u) \in S \times A} \\
&=\cprobability{\mathcal{F}_s}{ X_u \in A} \\
&=\cprobability{X_s}{ X_u \in A} \\
&= \mu_{s,u}(X_s, A) \text{  a.s.}
\end{align*}
Therefore for each $A \in \mathcal{S}$,
\begin{align*}
\nu_s(\mu_{s,t} \mu_{t,u} (\cdot, A) \neq \mu_{s,u}(\cdot, A)) &= \probability{\mu_{s,t} \mu_{t,u}(X_s, A) \neq \mu_{s,u}(X_s, A)} = 0
\end{align*}
Since $S$ is Borel we can choose a common null set for all $A \in \mathcal{S}$ (when $S=[0,1]$ just pick the union of null sets for $A$ an
interval with rational endpoints, show that this null set works for all intervals by continuity of measure and then use monotone classes; for general $S$ just
use the Borel isomorphism to reduce to the above case).
\end{proof}

\subsection{Existence of Markov Processes}

TODO: Example of process with the Markov property but for which the Chapman-Kolmogorov relations do not hold identically.

The ability to derive the almost sure version of the
Chapman-Kolmogorov relations is really just motivational for our
purposes.  In fact we will want to assume they hold identically in
what follows.  Absent a workable set of conditions from which we can
derive this fact, we build it into our definitions.  Collecting all of
the conditional independence and regularity properties we've
identified we finally make the formal definition of a Markov process.
\begin{defn}A \emph{Markov transition kernel} on time scale $T$ and 
state space $(S, \mathcal{S})$ is a probability  kernel $\mu_{s,t} : S \times \mathcal{S} \to [0,1]$ 
for each $s \leq t \in T$ such that 
\begin{align*}
\mu_{s,t} \mu_{t,u} &= \mu_{s,u}  \text{ everywhere on $S$ for  each $s \leq t \leq u$}
\end{align*}
\end{defn}

\begin{defn}
Let stochastic process $X_t$ on a time scale $T
\subset \reals_+$ and  state space $(S, \mathcal{S})$ such that $X_t$ is adapted to a filtration $\mathcal{F}_t$.  We say that
$X_t$ is a \emph{Markov process} if there exists a
Markov transition kernel $\mu_{s,t}$ such that for all $s \leq t \in T$
\begin{align*}
\cprobability{\mathcal{F}_s}{X_t \in \cdot} &= \mu_{s,t}(X_s, \cdot) \text{ a.s.}
\end{align*}
\end{defn}

Note that we have not specified in the definition that a Markov process posesses the Markov property; showing that
it does is not hard however.
\begin{prop}A Markov process has the Markov property.  Moreover for any non-negative or bounded function $f : S \to \reals$ we have
\begin{align*}
\cexpectationlong{\mathcal{F}_s}{f(X_t)} &= \cexpectationlong{X_s}{f(X_t)} = \int f(u) \, \mu_{s,t}(X_s, du)
\end{align*}
\end{prop}
\begin{proof}
Since we know $\cprobability{\mathcal{F}_s}{X_t \in A} = \mu_{s,t}(X_s, A)$ it follows that $\cprobability{\mathcal{F}_s}{X_t \in A}$ is $X_s$-measurable and therefore 
$\cprobability{\mathcal{F}_s}{X_t \in A} = \cprobability{X_s}{X_t \in A}$ a.s.  The Markov property follows by Proposition \ref{MarkovPropertyViaConditionalExpectations}.

Since $\mu_{s,t}(X_s, \cdot)$ is a regular version for $\cprobability{\mathcal{F}_s}{X_t \in \cdot}$ we apply Theorem \ref{Disintegration} to see
\begin{align*}
\cexpectationlong{\mathcal{F}_s}{f(X_t)} &= \int f(u) \, \mu_{s,t}(X_s, du)
\end{align*}
\end{proof}


TODO: Note that in the discrete (or countable?) state space case we
can in fact assume that Chapman-Kolmogorov are satisfied identically.

In lieu of general techinique for proving that a process is Markov
from general principles, we give a result that shows that we can
construct them from a set of transition kernels that obey the
Chapman-Kolmogorov relations.

TODO: There are other ways of proving a process is Markov : the
semigroup approach, the stochastic differential equation approach and
the martingale problem approach.  These are things we'll get to but
not quite yet!

\begin{thm}\label{ExistenceMarkovProcess}Suppose we are given
\begin{itemize}
\item[(i)] a  time scale starting at $0$, $T \subset \reals_+$ 
\item[(ii)]a Borel space $(S, \mathcal{S})$ 
\item[(iii)]a probability distribution $\nu$ on $(S, \mathcal{S})$
\item[(iv)]probability kernels $\mu_{s,t} : S \times
  \mathcal{S} \to [0,1]$ for each $s \leq t \in T$ such that 
\begin{align*}
\mu_{s,t} \mu_{t,u} &= \mu_{s,u} \text{ for all $s\leq
  t\leq u \in T$}
\end{align*}
\end{itemize} 
then there exists a Markov process $X_t$ with initial distribution
$\nu$ and transition kernels $\mu_{s,t}$.
\end{thm}
\begin{proof}
This is an application of the Daniell-Kolmogorov Theorem.  We first
define the finite dimensional distributions and show that they form a
projective family.  For every $n \in \naturals$ and $0 \leq t_1 \leq
\dotsb \leq t_n$ we define
\begin{align*}
\nu_{t_1, \dotsc, t_n}(A) 
&= \nu \mu_{0,t_1} \otimes \mu_{t_1, t_2} \otimes \dotsb \otimes \mu_{t_{n-1}, t_n}(A) \\
&=\nu \otimes \mu_{0,t_1} \otimes \mu_{t_1, t_2} \otimes \dotsb \otimes \mu_{t_{n-1}, t_n}(S \times A) 
\end{align*}
Let $A \in \mathcal{S}^{\otimes n-1}$ and let $1 \leq k \leq n$.
Define
\begin{align*}
A_k &= \lbrace (x_1, \dotsc, x_n) \in S^n \mid (x_1, \dotsc, x_{k-1},
x_{k+1}, \dotsc, x_n) \in A \rbrace
\end{align*}
and calculate
\begin{align*}
&\nu_{t_1, \dotsc, t_n}(A_k) = ( \nu \mu_{0,t_1} \otimes \mu_{t_1, t_2}
\otimes \dotsb \otimes \mu_{t_{n-1}, t_n})(A_k) \\
&=\int \left [ \int \left[ \dotsb \left [ \int \characteristic{A_k}(s_1,
    \dotsc, s_n) \, \mu_{t_{n-1},t_n}(s_{n-1}, ds_n) \right] \dotsb
\right] \, \mu_{t_1, t_2}(s_1, ds_2) \right ] \, \nu \mu_{0,
t_1}(ds_1) \\
&=\int \left [ \int \left[ \dotsb \left [ \int \characteristic{A}(s_1,
    \dotsc, s_{k-1}, s_{k+1}, \dotsc, s_n) \, \mu_{t_{n-1},t_n}(s_{n-1}, ds_n) \right] \dotsb \right] \, \mu_{t_1, t_2}(s_1, ds_2) \right ] \, \nu \mu_{0, t_1}(ds_1)
\end{align*}

The point here is that the integral
\begin{align*}
\int \left[ \dotsb \left [ \int \characteristic{A}(s_1,
    \dotsc, s_{k-1}, s_{k+1}, \dotsc, s_n) \, \mu_{t_{n-1},t_n}(s_{n-1}, ds_n) \right] \dotsb \right] \, \mu_{t_k, t_{k+1}}(s_{k+1}, ds_{k+2}) 
\end{align*}
is a function of $s_1, \dotsc, s_{k-1}, s_{k+1}$ only (i.e. it has no dependence on $s_k$).  From the Chapman-Kolmogorov relation
$\mu_{t_{k-1}, t_k} \mu_{t_{k}, t_{k+1}}  = \mu_{t_{k-1}, t_{k+1}}$ we
know that for any function of $f : S \to \reals$ we have 
\begin{align*}
\int \left [
  \int
f(s_{k+1}) \, \mu_{t_{k}, t_{k+1}} (s_k, ds_{k+1}) \right ] \, \mu_{t_{k-1}, t_k}
(s_{k-1}, ds_{k}) &= \int
f(s_{k+1}) \, \mu_{t_{k-1}, t_{k+1}} (s_{k-1}, ds_{k+1}) 
\end{align*}
which when applied to the integral above with $s_1, \dotsc, s_{k-2}$ fixed yields
\begin{align*}
&\int \left[ \dotsb \left [ \int \characteristic{A}(s_1,
    \dotsc, s_{k-1}, s_{k+1}, \dotsc, s_n) \,
    \mu_{t_{n-1},t_n}(s_{n-1}, ds_n) \right] \dotsb \right] \, \mu_{t_{k-1},
t_{k}}(s_{k-1}, ds_{k}) \\
&=\int \left[ \dotsb \left [ \int \characteristic{A}(s_1,
    \dotsc, s_{k-1}, s_{k+1}, \dotsc, s_n) \,
    \mu_{t_{n-1},t_n}(s_{n-1}, ds_n) \right] \dotsb \right] \,
\mu_{t_{k-1}, t_{k+1}}(s_{k-1}, ds_{k+1})
\end{align*}
Now we can use this to conclude that 
\begin{align*}
\nu_{t_1, \dotsc, t_n}(A_k) &= ( \nu \mu_{0,t_1} \otimes \mu_{t_1, t_2}
\otimes \dotsb \otimes \mu_{t_{n-1}, t_n})(A_k) \\
&= ( \nu \mu_{0,t_1} \otimes \mu_{t_1, t_2} \otimes \dotsb \otimes
\mu_{t_{k-1}, t_{k+1}} \otimes \dotsb \otimes \mu_{t_{n-1}, t_n})(A)
\\
&=\nu_{t_1, \dotsc, t_{k-1}, t_{k+1}, \dotsc, t_n}(A)
 \end{align*}
and we have show that the $\nu_{t_1, \dotsc, t_n}$ are a projective
family.
Now we can apply the Daniell-Kolmogorov Theorem
\ref{DaniellKolmogorovExtension} to conclude that there is an $S$
valued process $X$ on $T$ such that 
\begin{align*}
\mathcal{L}(X_{t_1}, \dotsc, X_{t_n}) = \nu_{t_1, \dotsc, t_n} &= \nu \mu_{0,t_1} \otimes \mu_{t_1, t_2}
\otimes \dotsb \otimes \mu_{t_{n-1}, t_n}
\end{align*}
for all $n \in \naturals$ and $0 \leq t_1 \leq \dotsb \leq t_n$.  The
case $n=1$ and $t_1 = 0$ shows us that $\mathcal{L}(X_0) = \nu
\mu_{0,0} = \nu$.

For every $t \in T$ define $\mathcal{F}_t = \sigma(X_s ; s \leq t)$ to
be filtration induced by $X$.  We must show that $X_t$ is a Markov process with transition
kernels $\mu_{s,t}$ (the fact that the initial distribution is $\nu$
was already noted).  Let $s \leq t$ be given and suppose that we have
$s_1 \leq \dotsb \leq s_n = s$.  Pick $A \in \mathcal{S}^{\otimes n}$
and $B \in \mathcal{S}$ and calculate using the FDDs of $X_t$ and the
expectation rule (Lemma \ref{ExpectationRule})
\begin{align*}
&\probability{ X_t \in B ; (X_{s_1}, \dotsc, X_{s_n}) \in A}  = \probability{(X_{s_1}, \dotsc, X_{s_n}, X_t) \in A \times B} =
\nu_{s_1, \dotsc, s_n, t}(A \times B) \\
&=\int \left [ \int \left[ \dotsb \left [ \int \characteristic{A}(u_1,
    \dotsc, u_n) \characteristic{B}(u_{n+1})\, \mu_{s,t}(u_{n}, du_{n+1}) \right] \dotsb
\right] \, \mu_{s_1, s_2}(u_1, du_2) \right ] \, \nu \mu_{0,
s_1}(du_1) \\
&=\int \left [ \int \left[ \dotsb \left [ \int \characteristic{A}(u_1,
    \dotsc, u_n) \mu_{s,t}(u_{n}, B)\, \mu_{s_{n-1},s_n}(u_{n-1}, du_{n}) \right] \dotsb
\right] \, \mu_{s_1, s_2}(u_1, du_2) \right ] \, \nu \mu_{0,
s_1}(du_1) \\
&=\expectation{ \mu_{s,t}(X_s, B) ; (X_{s_1}, \dotsc, X_{s_n}) \in A }
\end{align*}
Sets of the form $(X_{s_1}, \dotsc, X_{s_n}) \in A$ for $s_1 \leq
\dotsb \leq s_n=s$ are a $\pi$-system generating $\mathcal{F}_s$ and
therefore by a monotone class argument (specifically Lemma \ref{ConditionalExpectationExtension}) we may
conclude that $\cexpectationlong{\mathcal{F}_s}{X_t \in \cdot} =
\mu_{s,t}(X_s, \cdot)$ a.s.
\end{proof}

The previous theorem constructs a Markov process with an arbitrary initial distribution.  As it turns out in many cases
it is useful to consider the collection of Markov processes indexed by the initial distribution.  Such a collection has a
nice structure that results from the Markov property.  The uncover the structure the first thing to do is to move all of the 
constructed Markov processes into the canonical picture so that we have a family of probability measures on $S^T$.

\begin{defn}Suppose that a family of transition kernels $\mu_{s,t}$ is
  given.  For a distribution $\nu$ on $(S, \mathcal{S})$, let
  $\sprobabilityop{\nu}$ denote the distribution on $S^T$ of the
  Markov process with initial distribution $\nu$.  If $\nu=\delta_x$
  for some $x \in S$ then it is customary to write
  $\sprobabilityop{x}$ instead of $\sprobabilityop{\delta_x}$.
\end{defn}
\begin{lem}\label{MarkovMixtures}The family $\sprobabilityop{x}$ is a kernel from $S$ to
  $S^T$.  Futhermore, given an initial distribution $\nu$
\begin{align*}
\sprobability{A}{\nu} = \int \sprobability{A}{x} \, d\nu(x)
\end{align*}
\end{lem}
\begin{proof}
First assume that $A = (\pi_{t_1}, \dots, \pi_{t_n})^{-1}(B)$ for some
$B \in \mathcal{S}^{\otimes n}$ and $\pi_t : S^T \to S$ is the evaluation map $\pi_t f = f(t)$.  
We can use Lemma \ref{MarkovDistributions} and the expectation rule Lemma \ref{ExpectationRule}
to compute for any $\nu$,
\begin{align*}
\sprobability{A}{\nu} &= \sprobability{(\pi_{t_1}, \dotsc,  \pi_{t_n}) \in B}{\nu} \\
&=\sexpectation{\cprobability{\mathcal{F}_0}{(\pi_{t_1}, \dotsc,  \pi_{t_n}) \in B}}{\nu} \\
&=\sexpectation{\mu_{0, t_1} \otimes \cdots \otimes \mu_{t_{n-1}, t_n}(X_0, B)}{\nu} \\
&= \int \mu_{0, t_1} \otimes \cdots \otimes \mu_{t_{n-1}, t_n}(x,B) \, \nu(dx)
\end{align*}
In particular, for $\nu = \delta_x$ we get
\begin{align*}
\sprobability{A}{x} &=\mu_{0, t_1} \otimes \cdots \otimes \mu_{t_{n-1},  t_n}(x,B)
\end{align*}
which shows both that $\sprobability{A}{x}$ is a measurable function
of $x$ for fixed $A$ (Lemma
\ref{KernelTensorProductMeasurability}) and that
$\sprobability{A}{\nu} = \int \sprobability{A}{x} \, d\nu(x)$.

To extend to general measurable sets, we note that the set of $A$ of
the form given above is a $\pi$-system therefore we can apply Lemma
\ref{KernelMeasurability} to conclude $\sprobabilityop{x}$ is a
kernel.  Similarly we may conclude that $\sprobability{A}{\nu} = \int
\sprobability{A}{x} \, d\nu(x)$ for arbitrary measurable $A$ by the fact that probability measures
are uniquely determined by their values on a generating $\pi$-system
(Lemma \ref{UniquenessOfMeasure}).
\end{proof}

TODO:  This may not be the correct definition of a Markov process to
settle on.   We may want to select the picture of a Markov process as
being a single stochastic process with a family of probability
measures $\probabilityop_x$ for $x \in S$ such that under $\probabilityop_x$ the
stochastic process is Markov (as above) starting at $x$.  This
definition assumes that we have a kernel property (so Lemma
\ref{MarkovMixtures} proves such a kernel property
holds in the ``canonical'' case).  The work we have done to this point
shows that a set of transition kernels gives rise to a Markov process
with on the canonical space $S^T$.  The interpretation as a family of measures
without assuming the probability space is $S^T$ is apparently useful (e.g. when we want to
assume randomization variables exist for some construction).  I still find the variety of
interpretations of what a Markov process is to be very confusing.
Perhaps we should define this latter concept as a Markov family and
keep the current notion as a Markov process (I think Karatzas and
Shreve do this).  In the Karatzas and Shreve definition we wind up
with an interesting new concept which is that the kernel
$\probabilityop_x$ in a Markov family is only assumed to be \emph{universally measurable}
which is a looser condition than Borel measurability (the universal
$\sigma$-algebra being the intersection of the completions of the
Borel $\sigma$-algebra under all probability measures; hence being a
superset of the Borel $\sigma$-algebra).  This loosening
seems to come up as important in the context of stochastic control.  I
am not at all clear on how important it is in the context of Markov
processes as we are likely to develop it; it seems from Karatzas and
Shreve that this loosening comes up in Markov process theory when
trying to find a right-continuous complete filtration with respect to which a
Markov process (in particular Brownian motion) gives us a Markov
family.  So, we have shown that a by Kolmogorov existence we can
construct a Markov family given a set of transition kernels however
the filtration is not right continuous or complete and this
construction results in a Borel measurable kernel $\probabilityop_x$.
However if one tries to modify the construction to get a
right-continuous complete filtration (usual conditions) then one has
to give up Borel measurability in the kernel and make due with
universal measurability.  Perhaps it is worth having a definition of a Markov family 
and a ``relaxed'' or ``complete'' Markov family.  What I don't have any intuition of 
is the circumstances under which we are forced to pass to impose the usual conditions and/or require measurability of $\sprobability{A}{x}$ for arbitrary $A \in \mathcal{A}$.  
The most obvious answer is that we can't consider mixed initial states $\sprobability{A}{\nu}$ to be defined (via $\sprobability{A}{\nu} = \int \sprobability{A}{x}  \, \nu(dx)$) unless we have measurability of $\sprobability{A}{x}$ and therefore we don't even get measures $\probabilityop_{\nu}$ on all of $\mathcal{A}$ until we deal with the measurability issue (but we can always define $\probabilityop_{\nu}$ on $\mathcal{F}^X_\infty$).  Note also that the 
concept of universal measurability comes up in the general theory of processes in which
we have the Debut Theorem that states that every hitting time is universally measurable.  It can also be shown that
analytic sets are universally measurable.  

\begin{defn}\label{MarkovFamilyDefn}
A \emph{time homogeneous Markov family} is a stochastic process $X_t$ with a
probability space $(\Omega, \mathcal{A})$, a time scale $T
\subset \reals_+$, a filtration $\mathcal{F}_t$, a ($\mathcal{F}_t$?) measureable $\theta_t : \Omega \to \Omega$, a state space $(S, \mathcal{S})$ and a family of
probability measures $\probabilityop_x$ on $\Omega$ for $x \in S$ such that 
\begin{itemize}
\item[(i)]$\probabilityop_x$ is a (universally measurable?) kernel
  from $S$ to $\Omega$.  TODO: Is this what we want?  Blumenthal and Getoor say that $\sprobability{X_t \in A}{x}$ is $\mathcal{S}$ measurable for every
$0 \leq t < \infty$ and $A \in \mathcal{S}$; so $\probabilityop_x$ is a kernel to $(\Omega, \mathcal{F}^X_\infty)$ but not necessarily to $(\Omega, \mathcal{A})$ or 
$(\Omega, \mathcal{F}_\infty)$.
\item[(ii)]$\sprobability{X_0 = x}{x} = 1$ for all $x \in S$ (note Blumenthal and Getoor do not assume this).
\item[(iii)]$\cindependent{\mathcal{F}_s}{X_t}{X_s}$ under
  $\probabilityop_x$ for all $s \leq
  t$ and $x \in S$ (i.e. for all $x \in S$, $A \in \mathcal{S}$ and $s \leq t$ we have
  $\csexpectationlong{\mathcal{F}_s}{X_t \in A}{x} =
  \csexpectationlong{X_s}{X_t \in A}{x}$ $\probabilityop_x$-a.s.)  Alternatively do we just say:
\begin{align*}
\cexpectationlong{\mathcal{F}_s}{f (X_t \circ \theta_s)} &= \sexpectation{f(X_t)}{X_s}
\end{align*}
\item[(iv)]there exists a regular version $\mu^x_{s,t} :
  S \times \mathcal{S} \to [0,1]$ of $\csprobability{\mathcal{F}_s}{X_t
    \in \cdot}{x}$ for each $s \leq t$ and $x \in S$ (is there any coherence
  requirement with respect to $x \in S$ here???).
\end{itemize}
\end{defn}

\begin{lem}\label{AugmentingMeasurableSpace}Let $(S, \mathcal{S})$ be a measurable space and take a point $\Delta \notin S$, let $S^{\Delta} = S \cup \lbrace \Delta \rbrace$ and let $\mathcal{S}^\Delta$ be the $\sigma$-algebra on $S^\Delta$ generated by $\mathcal{S}$, then 
\begin{align*}
\mathcal{S}^\Delta &= \mathcal{S} \cup \lbrace A \cup \lbrace \Delta \rbrace \mid A \in \mathcal{S}\rbrace
\end{align*}
\end{lem}
\begin{proof}
Since $S \in \mathcal{S}$ and $\lbrace \Delta \rbrace = S^\Delta \setminus S$ it follows that $\lbrace \Delta \rbrace$ is $\mathcal{S}^\Delta$-measurable and therefore the right hand side is included in $\mathcal{S}^\Delta$.  It suffices to show that the right hand side is a $\sigma$-algebra.  Clearly $\emptyset \in \mathcal{S} \subset \mathcal{S}^\Delta$ and for any $A \in \mathcal{S}$ we have both $S^\Delta \setminus A = (S\setminus A) \cup \lbrace \Delta \rbrace$ and $S^\Delta \setminus (A \cup \lbrace \Delta \rbrace) = S \setminus A$ and therefore the right hand side is closed under set complement.  Given countable index sets $I$ and $J$ and sets $A_i \in \mathcal{S}$ and $B_j \in \mathcal{S}$ we have
\begin{align*}
\cup_{i \in I} A_i \cup \cup_{j \in J} (B_j \cup \lbrace \Delta \rbrace) &=
\begin{cases}
(\cup_{i \in I} A_i \cup \cup_{j \in J} B_j) \cup \lbrace \Delta \rbrace & \text{if $J \neq \emptyset$} \\
\cup_{i \in I} A_i & \text{if $J = \emptyset$} \\
\end{cases}
\end{align*}
which shows that the right hand side is closed under countable unions. 
\end{proof}
Adjoining a point to a measurable space as in the previous lemma will be referred to as \emph{augmenting} the space with the point $\Delta$.

\begin{defn}\label{HomogeneousMarkovFamilyBlumenthalGetoor}
A \emph{Markov family} is a
probability space $(\Omega, \mathcal{A})$ with a distinguished point $\omega_\Delta$, a time scale $T
\subset [0,\infty]$ with $\infty \in T$, a filtration $\mathcal{F}_t$, a measurable $\theta_t : \Omega \to \Omega$ such that $\theta_\infty \equiv \omega_\Delta$, a state space $(S, \mathcal{S})$ augmented with a point $\Delta$, an $\mathcal{F}$-adapted stochastic process $X_t$ with time scale $T$ and state space $S^\Delta$ and a family of
probability measures $\probabilityop_x$ on $(\Omega, \mathcal{A})$ for $x \in S^\Delta$ such that 
\begin{itemize}
\item[(i)] $X_t(\omega_\Delta) \equiv \Delta$ for all $t \in T$
\item[(ii)] $X_t(\omega) = \Delta$ for some $t \in T$ and $\omega \in \Omega$ then $X_u(\omega) = \Delta$ for all $u \geq t$.
\item[(iii)] $X_\infty(\omega) = \Delta$ for all $\omega \in \Omega$.
\item[(iv)] $\sprobability{X_0 = \Delta}{\Delta} = 1$
\item[(v)] For all $t \in T \setminus \lbrace \infty \rbrace$, $A \in \mathcal{S}$, $\sprobability{X_t \in A}{x}$ is $\mathcal{S}$-measurable (i.e. $\pushforward{X_t}{\probabilityop_x}$ is a kernel $S \times \mathcal{S} \to [0,1]$ for all $t \in T \setminus \lbrace \infty \rbrace$).
\item[(vi)] For all $s,t \in T$, $A \in \mathcal{S}^\Delta$ and $x \in S^\Delta$
\begin{align*}
\csprobability{\mathcal{F}_s}{X_{t} \circ \theta_s \in A}{x}&= \sprobability{X_t \in A}{X_s}
\end{align*}
\end{itemize}
If in addition we have $X_t \circ \theta_s = X_{t+s}$ for all $t,s \in T$ then we say that the Markov family is \emph{time-homogeneous}.
\end{defn}

It is worth calling out some subtle points of the defintion.  One of the more significant subtleties is the fact that we do not require that $\probabilityop_x : S^\Delta \times \mathcal{A} \to [0,1]$ is a kernel; while for fixed $x \in S^\Delta$ we know that $\probabilityop_x$ is a probability measure we do not assume that for fixed $A \in \mathcal{A}$ we have $\sprobability{A}{x}$ is $\mathcal{S}^\Delta$-measurable.  This issue will be discussed in some detail later on.  The other most notable issue is that our guiding intuition is that under the probability measure $\probabilityop_x$ $X_t$ is a Markov process starting at $x \in S$, yet we haven't stated that clearly as part of the definition.  At least the fact that $X_t$ is a Markov process under $\probabilityop_x$ $X_t$ can be proven.

\begin{prop} Let $(\Omega, \mathcal{A}, \mathcal{F}_t, X_t, \theta_t, \probabilityop_x)$ be a Markov family then
\begin{itemize}
\item[(i)]For all $t \in T$, $A \in \mathcal{S}^\Delta$, $\sprobability{X_t \in A}{x}$ is $\mathcal{S}^\Delta$-measurable (i.e. $\pushforward{X_t}{\probabilityop_x}$ is a kernel $S^\Delta \times \mathcal{S}^\Delta \to [0,1]$ for all $t \in T$).
\item[(ii)] If $X_t$ is time homogeneous then $\theta_t$ is $\mathcal{F}^X_\infty/\mathcal{F}^X_\infty$-measurable.
\item[(ii)] If $X_t$ is time homogeneous then $\mu_t(x,A) = \probabilityop_x(X_t \in A)$ defines a Markov transition kernel and the pair $X_t$, $\mu_t$ is a Markov process under $\probabilityop_x$.
\end{itemize}
\end{prop}
\begin{proof}
To see (i) note that for $T = \infty$ we have for all $A \in \mathcal{S}^\Delta$ and $x \in S^\Delta$
\begin{align*}
\sprobability{X_\infty \in A}{x} &= \sprobability{\Delta \in A}{x} = \begin{cases}
1 & \text{if $\Delta \in A$} \\
0 & \text{if $\Delta \notin A$} 
\end{cases}
\end{align*}
so is a constant function of $x$ and therefore $\mathcal{S}^\Delta$-measurable.  For $t \in T$ with $t \neq \infty$ and $A \in \mathcal{S}$ then as a function of $S^\Delta$,
\begin{align*}
\sprobability{X_t \in A}{x} &= \characteristic{S}(x) \sprobability{X_t \in A}{x} + \characteristic{\Delta}(x) \sprobability{X_t \in A}{\Delta} 
\end{align*}
which is $\mathcal{S}^\Delta$-measurable since $S, \lbrace \Delta \rbrace \in \mathcal{S}^\Delta$ and $\sprobability{X_t \in A}{x}$ is $\mathcal{S}$-measurable.

For $t \in T$ with $t \neq \infty$ and $B \in \mathcal{S}^\Delta \setminus \mathcal{S}$ by Lemma \ref{AugmentingMeasurableSpace} we know that $B = A \cup \lbrace \Delta \rbrace$ for some $A \in \mathcal{S}$ and thus
\begin{align*}
\sprobability{X_t \in A \cup \lbrace \Delta \rbrace }{x} &=\sprobability{X_t \in A}{x} + \sprobability{X_t = \Delta }{x} = \sprobability{X_t \in A}{x} + 1 - \sprobability{X_t \in S}{x} 
\end{align*}
is seen to be $\mathcal{S}^\Delta$-measurable.

To see (ii) note that by time homogeneity we have for $s, t \in T$ and $A \in \mathcal{S}^\Delta$, $\theta_s^{-1} X_t^{-1}(A) = X_{t+s}^{-1}(A)$ which shows that $\theta_s$ is $\mathcal{F}^X_t/\mathcal{F}^X_{t+s}$-measurable.  In particular, (ii) follows.

To see (iii) we first show that $\mu_t(x,A) = \sprobability{X_t \in A}{x}$ is a Markov transition kernel.  By (i) we have shown that it is a kernel from $S^\Delta$ to $\mathcal{P}(S^\Delta)$ so it remains to show the Chapman Kolmogorov relations.  Let $t,s \in T$, $x \in S^\Delta$ and $A \in \mathcal{S}^\Delta$ then by the tower property of conditional expectation, property (vi) of the defintion of a Markov family and the Expectation Rule Lemma \ref{ExpectationRule} (applied to $\pushforward{X_s}{\probabilityop_X} = \mu_s(x, \cdot)$)
\begin{align*}
\mu_{s+t}(x, A) &= \sprobability{X_{s+t} \in A}{x} \\
&=\sexpectation{\csprobability{\mathcal{F}_s}{X_{t+s} \in A}{x}}{x} \\
&=\sexpectation{\sprobability{X_{t} \in A}{X_s}}{x} \\
&=\sexpectation{\mu_t(X_s, A)}{x} \\
&=\int \mu_t(u, A) \mu_s(x, du) = \mu_s \mu_t (x, A) \\
\end{align*}
showing the Chapman-Kolmogorov relations.  Now property (vi) of the definition of a Markov family shows $\csprobability{\mathcal{F}_s}{X_{t+s} \in A}{x} = \mu_t(X_s,A)$ and
therefore $X_t$ is a Markov process under $\probabilityop_x$ for every $x \in S^\Delta$.
TODO: A non-homogeneous version of this proof and the proper definition of a non-homogeneous Markov family????
\end{proof}

Note that Bass doesn't require that $\probabilityop_x$ is a kernel $S
\to \mathcal{P}(\Omega)$ rather he only requires that
$\pushforward{X}{\probabilityop_x}$ is a kernel from $S$ to
$\mathcal{P}(S^T)$ (equivalently for each $t \in T$ and $A \in \mathcal{S}$ we have
$\sprobability{X_t \in A}{x}$ is a measurable function of $x$ or again
equivalently $\probabilityop_x$ is a kernel on the natural filtration
$\mathcal{F}^X_\infty$ (this is the same as Blumenthal and Getoor as I mention above).  What I don't know if whether a universally
measurable kernel $S \to  \mathcal{P}(\Omega)$ is necessarily Borel
measurable when restricted to $\mathcal{F}^X_\infty$; heck this isn't necessarily a well posed question since $S$ is not assumed to be
a topological space at this point.  It is worth noting that Blumenthal and Getoor do show that every Markov family extends to a completed one with 
the state space given by the universal completion and they assume this completion is in place for much of the subsequent theory.  That said, they
don't modify the defintion of Markov family in doing so.

TODO: Question:  Given a Markov family as above then given an arbitrary
initial distribution $\nu$ on $S$ we can define $\probabilityop_\nu$
by $\sprobability{A}{\nu} = \int \sprobability{A}{x} \, d\nu(x)$.  Is
$X$ a Markov process with initial distribution $\nu$ under
$\probabilityop_\nu$?  Blumenthal and Getoor do this but alas universal measurability arises here as well (not when defining
$\sprobability{A}{\nu}$ for $A \in \sigma(\bigvee_{t \in T} X_t)$ but only when trying to extend to $\mathcal{F}_\infty$)!

\section{Homogeneous Markov Processes}

We have described a relatively general version of Markov processes
compared to what it needed in many applications and the goal of this
section is to define the assumptions that lead to useful
simplifications and to understand how to look at these simplifying
assumptions from a couple of points of view.

\begin{defn}Suppose $(S, \mathcal{S})$ is a measurable Abelian group
  and $\mu : S \times \mathcal{S} \to [0,1]$ is a kernel.  We say
  $\mu$ is \emph{homogeneous} if for every $s \in S$ and
  $A\in \mathcal{S}$ we have $\mu(0, A) = \mu(s, A+s)$.
\end{defn}

A useful observation for computing conditional expectations is that
integrals are invariant under certain changes of variables.
\begin{lem}\label{HomogeneousKernelExpectationRule}Let $(S, \mathcal{S})$ be a measurable Abelian group with a
  homogeneous kernel $\mu : S \times \mathcal{S} \to [0,1]$, then for
  each $y,z \in S$ and integrable $f : S \to \reals$,
\begin{align*}
\int f(x + y) \, \mu(z, dx) &= \int f(x) \, \mu(y+z, dx)
\end{align*}
\end{lem}
\begin{proof}
For $y \in S$, let $t_y : S \to S$ be translation by $y$: $t_y(x) = x
+ y$.  Thinking of the kernel as a measurable measure
valued map (which we denote $\mu(z)$) we compute the pushforward of $\mu(z)$ under $t_y$
using homogeneity
\begin{align*}
\pushforward {t_y}{\mu(z)}(A) &= \mu(z, t_y^{-1}(A)) = \mu(z, A
- y) = \mu(z+y, A)
\end{align*}
thus showing $\pushforward {t_y}{\mu(z)} = \mu(y+z)$.
Now we can apply the Expectation Rule (Lemma \ref{ExpectationRule}) to
see that 
\begin{align*}
\int f(x + y) \, \mu(z, dx) &= \int f(x) \, d\left [ \pushforward {t_y}{\mu(z)} \right] = \int f(x) \, \mu(y+z, dx)
\end{align*}
\end{proof}

A Markov process with homogeneous kernels is said to be
\emph{space-homogeneous}; intuitively the probability of starting out
in a set $A$ at time $s$ and winding up in set $B$ at time $t$ only
depends on the relative positions of $A$ and $B$ (under translations).
\begin{defn}Suppose $(S, \mathcal{S})$ is a measurable Abelian group
  and let $X_t$ be a Markov process with transition kernels
  $\mu_{s,t}$.  Then $X_t$ is \emph{space-homogeneous} if and only if
  $\mu_{s,t}$ is homogeneous for every $s \leq t$.
\end{defn}

\begin{lem}\label{SpaceHomogeneousMarkovDistributions}Let $\mu_{s,t}$
  be a family of space homogeneous transition kernels on a
  measurable Abelian group, then for every $A \in \mathcal{S}^T$ and $x \in S$,
  $\sprobability{A}{x} = \sprobability{A-x}{0}$.
\end{lem}
\begin{proof}
TODO: This proof only seems to require space homogeneity of the
kernels $\mu_{0,t}$; is this a mistake (or does Chapman Kolmogorov
imply the rest of the kernels are space homogeneous as well...)

We begin by establishing the result for sets of the form $\lbrace (X_{t_1},
\dotsc, X_{t_n}) \in A \rbrace$ for $A \in \mathcal{S}^{\otimes n}$ and $t_1 \leq
\cdots \leq t_n$.  The key point is that we know from the proof of
Lemma \ref{MarkovMixtures} that $\sprobability{(X_{t_1},
\dotsc, X_{t_n}) \in A }{x} = \mu_{0, t_1} \otimes \dotsc \otimes
\mu_{t_{n-1}, t_n}(x, A)$, so in particular the case $n=1$ follows
directly from the assumption that each $\mu_{0, t}$ is homogeneous.
To see the result for $n > 1$ we calculate using Lemma \ref{HomogeneousKernelExpectationRule}
\begin{align*}
&\sprobability{(X_{t_1},\dotsc, X_{t_n}) \in A }{x} \\
&= \mu_{0, t_1} \otimes \dotsc \otimes \mu_{t_{n-1}, t_n}(x, A) \\
&= \int \int \characteristic{A}(x_1, x_2, \dotsc, x_n) \mu_{t_1, t_2} \otimes \cdots \otimes \mu_{t_{n-1},
  t_n}(x_1, dx_2, \dotsc, dx_n) \, \mu_{0,t_1}(x, dy) \\
&= \int \int \characteristic{A}(x_1+x, x_2, \dotsc, x_n) \mu_{t_1, t_2} \otimes \cdots \otimes \mu_{t_{n-1},
  t_n}(x_1, dx_2, \dotsc, dx_n) \, \mu_{0,t_1}(0, dy) \\
&= \int \int \characteristic{A-x}(x_1, x_2, \dotsc, x_n) \mu_{t_1, t_2} \otimes \cdots \otimes \mu_{t_{n-1},
  t_n}(x_1, dx_2, \dotsc, dx_n) \, \mu_{0,t_1}(0, dy) \\
&= \mu_{0, t_1} \otimes \dotsc \otimes \mu_{t_{n-1}, t_n}(0, A-x) \\
&=\sprobability{(X_{t_1},\dotsc, X_{t_n}) \in A - x}{0} \\
\end{align*}

Now we complete the result by a monotone class argument.  We know that
sets of the form $\lbrace (X_{t_1}, \dotsc, X_{t_n}) \in A \rbrace$
are a generating $\pi$-system so by the $\pi$-$\lambda$ Theorem
(Theorem \ref{MonotoneClassTheorem}) it suffices to show that $\mathcal{C}
= \lbrace A \mid \sprobability{A}{x} = \sprobability{A-x}{0} \rbrace$ is a
$\lambda$-system. If $A,B \in \mathcal{C}$ with $A \subset B$ then 
\begin{align*}
\sprobability{B\setminus A}{x} &= \sprobability{B}{x} -
\sprobability{A}{x} = \sprobability{B-x}{0} - \sprobability{A-x}{0} =
\sprobability{B\setminus A-x}{0} 
\end{align*}
where we have used the elementary fact that $B\setminus A - x =
(B-x)\setminus(A-x)$ (let $y \in B$ and $y \notin A$ then clearly $y-x
\in B-x$ and $y-x \notin A-x$).  Similarly if $A_n \in \mathcal{C}$
for $n \in \naturals$ with $A_1 \subset A_2 \subset \cdots$ then it is
also true that $A_1 -x  \subset A_2-x \subset \cdots$ and continuity
of measure (Lemma \ref{ContinuityOfMeasure}) shows
\begin{align*}
\sprobability{\cup_n A_n}{x} &=
\lim_{n \to \infty}\sprobability{A_n}{x} = 
\lim_{n \to \infty}\sprobability{A_n-x}{0} = 
\sprobability{\cup_n A_n-x}{0} 
\end{align*}
\end{proof}

There is another way of thinking about the space-homogeneous Markov
processes.  We know that for any $s \leq t$, given the value of $X_s$ the probability
distribution of $X_t$ is independent of the history of $X$ up to
$s$.  Space homogeneity tells us that moreover that the probability
distribution $X_t$ only depends on the \emph{increment} $X_t - X_s$.
Putting these two observations together we should expect that $X_t -
X_s$ is independent (not just conditionally independent) of the
history of $X$ up to $s$.  In fact this provides an equivalent
characterisation of space homogeneous Markov processes as we prove in
the following result.

\begin{defn}Let $(S, \mathcal{S})$ be a measurable Abelian group with a
  time scale $T \subset \reals_+$, a filtration $\mathcal{F}_t$ and an
 $S$-valued $\mathcal{F}$-adapted process $X_t$. We say that $X_t$ has
 $\mathcal{F}$-independent increments if and only if $X_t - X_s$ is
 independent of $\mathcal{F}_s$ for all $s \leq t$.
\end{defn}

\begin{lem}\label{IndependentIncrements}Let $(S, \mathcal{S})$ be a measurable Abelian group with a
  time scale $T \subset \reals_+$, a filtration $\mathcal{F}_t$ and an
 $S$-valued $\mathcal{F}$-adapted process $X_t$.  The $X_t$ has
$\mathcal{F}$-independent increments if and only if $X_t$ is a space-homogeneous
 Markov process.  In this case the transition kernels of $X_t$ are
 given by
\begin{align*}
\mu_{s, t}(x, A) &= \probability{X_t - X_s \in A - x} \text{ for $x
  \in S$, $A \in \mathcal{S}$ and $s \leq t \in T$}
\end{align*}
TODO: The proof actually requires regular versions of
$\cprobability{\mathcal{F}_s}{X_t}$; do we need to assume that
$G$ is Borel or something?  Also we've defined a Markov process as
satisfying the Chapman Kolmogorov relations identically; can that be derived?
\end{lem}
\begin{proof}
Suppose that $X_t$ is a space homogeneous Markov Process with
transition kernels $\mu_{s,t}$.  Then for every $s\leq t$ and $A \in \mathcal{S}$,
\begin{align*}
\cprobability{\mathcal{F}_s}{X_t - X_s \in A} &= 
\int \characteristic{A}(x - X_s) \, \mu_{s,t}(X_s, dx) & & \text{by
  Theorem \ref{Disintegration}} \\
&=\int \characteristic{A}(x) \, \mu_{s,t}(0, dx) & & \text{by Lemma
  \ref{HomogeneousKernelExpectationRule}} \\
&= \mu_{s,t}(0,A)
\end{align*}
which shows that $\cprobability{\mathcal{F}_s}{X_t - X_s \in A}$ is
almost surely constant hence $\cindependent{X_t -
  X_s}{\mathcal{F}_s}{}$.  Moreover by the tower rule we also know
that $\probability{X_t - X_s \in A}=\cprobability{\mathcal{F}_s}{X_t -
  X_s \in A}=\mu_{s,t}(0,A)$ and therefore by another application of
space homogeneity, $\mu_{s, t}(x, A) = \mu_{s,t}(0, A-x) = \probability{X_t - X_s \in A}$.

Suppose that $X_t$ has independent increments.  The key point is that
this property determines the conditional distributions 
\begin{align*}
\mu_{s,t}(x, A) &= \probability{X_t - X_s \in A - x}
\end{align*}
and moreover this form is a regular version.  First note that $\probability{X_t - X_s \in A - x}$ is a probability
kernel since for fixed $A$ it is measurable in $x$ by Lemma
\ref{MeasurableSections} and for fixed $x$ it is just the distribution
of the measurable random element $X_t - X_s -x$.  

Showing that $\probability{X_t - X_s \in A - x}$ is a version of
$\cprobability{\mathcal{F}_s}{X_t \in A}$ is not hard but requires a
bit of care because the random element $X_s$ plays two different roles
in the calculation and it is worth making this fact explicit.
We start by defining 
$\tilde{\mu}_{s,t}(x, A) = \probability{X_t - X_s \in A}$ and observing
that because $\cindependent{X_t - X_s}{\mathcal{F}_s}{}$,
$\tilde{\mu}_{s,t}$ is a kernel for $\cprobability{\mathcal{F}_s}{X_t
  - X_s \in \cdot}$.  With this fact and the $\mathcal{F}$-adaptedness
of $X$, we can apply Theorem
\ref{Disintegration} (using the function $f(x,y) =
\characteristic{A-y}(x)$ evaluated at $(X_t - X_s, X_s)$) to conclude
\begin{align*}
\cprobability{\mathcal{F}_s}{X_t \in A} &= 
\cprobability{\mathcal{F}_s}{X_t - X_s \in A - X_s} \\
&=\int \characteristic{A - X_s}(x) \, \tilde{\mu}_{s,t}(dx) \\
&=\tilde{\mu}_{s,t}(A - X_s) \\
&=\mu_{s,t}(X_s, A)
\end{align*}
Now note that $\mu_{s,t}(X_s, A)$ is $X_s$-measurable hence we have
$\cprobability{\mathcal{F}_s}{X_t \in A} = \cprobability{X_s}{X_t
  \in A} $ for all $A \in \mathcal{S}$ thus the Markov property
holds by Lemma \ref{ConditionalIndependenceDoob}.  
Using the explicit form of the kernel we calculate
\begin{align*}
\mu_{s,t}(x, A) &=  \probability{X_t - X_s \in A-x} = \mu_{s,t}(0, A-x) 
\end{align*}
demonstrating space homogeneity.
\end{proof}

Here is what the proof that space homogeneous Markov implies
independent increments looks like in elementary probability theory
(discrete time countable state space).
\begin{proof}
Space homogeneity means that $\cprobability{X_{n-1} = y}{X_n = x} =
\cprobability{X_{n-1} = 0}{X_n = x-y}$.  This implies that for any
$y\in S$ we have
$\probability{X_n - X_{n-1} = z} = \cprobability{X_{n-1} = y}{X_n =
  z+y}$:
\begin{align*}
\probability{X_n - X_{n-1} = z} &= \sum_x \probability{X_n - X_{n-1} =
  z ; X_{n-1}=x} \\
& =\sum_x \cprobability {X_{n-1}=x}{X_n - X_{n-1} =  z} \probability
{X_{n-1}=x} \\
& =\sum_x \cprobability {X_{n-1}=x}{X_n =  z+x} \probability
{X_{n-1}=x} \\
& =\cprobability {X_{n-1}=y}{X_n =  z+y} \sum_x \probability
{X_{n-1}=x} \\
&= \cprobability {X_{n-1}=y}{X_n =  z+y} 
\end{align*}
Now we use this fact along with the Markov property to see
\begin{align*}
&\probability{X_n - X_{n-1} = z; X_1=x_1 ; \cdots ; X_{n-1}=x_{n-1}} \\
&=\probability{X_n = z+x_{n-1}; X_1=x_1 ; \cdots ; X_{n-1}=x_{n-1}} \\
&=\cprobability{X_1=x_1 ; \cdots ; X_{n-1}=x_{n-1}}{X_n = z+x_{n-1}} 
\probability{X_1=x_1 ; \cdots ; X_{n-1}=x_{n-1}}\\
&=\cprobability{X_{n-1}=x_{n-1}}{X_n = z+x_{n-1}} 
\probability{X_1=x_1 ; \cdots ; X_{n-1}=x_{n-1}}\\
&=\probability{X_n = z} 
\probability{X_1=x_1 ; \cdots ; X_{n-1}=x_{n-1}}\\
\end{align*}
\end{proof}

TODO: Motivate time homogeneity by thinking about discrete time and
the fact that you can generate everything from the unit time
transitions.  Time homogeneity is the property that all of these
transition kernels are the same and therefore the Markov process is
determined by a single kernel (and the initial distribution).

\begin{defn}A family of transition kernels $\mu_{s,t}$  $\integers_+$ or $\reals_+$ is said to
  be \emph{time homogeneous} if and only if there exist a family of
  kernels $\tilde{\mu}_t$ such that $\mu_{s,t}(x, B) =
  \tilde{\mu}_{t-s}(x,B)$ for all $x \in S$ and $B \in \mathcal{A}$.
  A Markov process $X$ is said to be time homogeneous if it has a
  family of time homogeneous transition kernels.
\end{defn}

\begin{prop}If $X$ is a time homogeneous Markov process then
for all $s,t,u \in T$ and $B
  \in \mathcal{S}^T$ we have
  $\cexpectationlong{\mathcal{F}_s}{X_t \in B} = \cexpectationlong{\mathcal{F}_{s+u}}{X_{t+u} \in B}$.
\end{prop}
\begin{proof}
Immediate from the defintions,
\begin{align*}
\cexpectationlong{\mathcal{F}_s}{X_t \in B} = \mu_{t-s}(X_s, B) = \cexpectationlong{\mathcal{F}_{s+u}}{X_{t+u} \in B}
\end{align*}
\end{proof}

\section{Strong Markov Property}

In dealing with Markov processes we make a lot of use of constructions
that involve the following
\begin{defn}If $T$ is equal to $\integers_+$ or $\reals_+$, for each
  $t \in T$ we define
  the \emph{shift operator}  $\theta_t : S^T \to S^T$ by $\theta_t f
  (s) = f(s +t)$.
\end{defn}

It is clear that for a fixed $t \in T$ the shift operator $\theta_t$
is measurable but we often need a stronger property the requires some
more assumptions.
\begin{lem}\label{MeasurabilityOfShiftOperator}For any fixed $t \in T$ the shift operator $\theta_t : S^T
  \to S^T$ is measurable. If $U$ is equal to $S^\infty$, $C(T; S)$ or
  $D(T; S)$, then $\theta_t X$ defines a measurable function $\theta : U \cap S^T \times
  T \to U \cap S^T$.  
\end{lem}
\begin{proof}
First let $t \in T$ be fixed pick $s \in T$ and $A \in \mathcal{S}$.
Then $\theta_t^{-1} \lbrace f(s) \in A \rbrace = \lbrace f(s+t) \in A
\rbrace \in \mathcal{S}^T$.  Therefore since sets of the form $\lbrace f(s) \in A \rbrace$ generate
$\mathcal{S}^T$, we see that $\theta_t$ is measurable by Lemma
\ref{MeasurableByGeneratingSet}.

Now let $U$ be as above.  It is clear that the shift operator
preserves the necessary continuity and limit properties and thus is
well defined as a function $\theta : U \cap S^T \times T \to U \cap S^T$.  To
see measurability of $\theta$, first note that the evaluation map $\pi : U \cap S^T \times
T \to S$ given by $\pi(f,t) = f(t)$ is measurable (e.g. this follows
by considering the process defined by the identity $U
\cap S^T \to U \cap S^T$ and using Lemma
\ref{ContinuityAndProgressiveMeasurability} to see that it is jointly
measurable).
Now let $s \in T$ and $A
\in \mathcal{S}$ as before and calculate
\begin{align*}
\lbrace (f, t) \mid \theta_t f \in \pi_s^{-1} A \rbrace
&=
\lbrace (f, t) \mid \theta_t f(s) \in  A \rbrace 
=\lbrace (f, t) \mid \theta_s f(t) \in  A \rbrace \\
&= (\theta_s, id)^{-1} \lbrace (f, t) \mid f(t) \in  A \rbrace
= (\theta_s, id)^{-1} \pi^{-1} A
\end{align*}
which is measurable by the joint measurability of $\pi$ noted above
and the measurability of $\theta_s$ for fixed $s \in T$.
\end{proof}

When considering Markov processes on the canonical space there is a
very useful construction of time shifting optional times.  Intuitively
the construction is that given two optional times $\sigma$ and $\tau$
one constructs the random time which is ``the first time $\tau$
happens after $\sigma$ happens''.  The following Lemma makes the
construction precise and shows that under some assumption on the path
space that the construction gives us a weak optional time.
\begin{lem}\label{TimeShiftOptionalTimes}Let $S$ be a metric space and
  let $\sigma$ and $\tau$ be weakly optional times on any of the canonical
  spaces $S^\infty$, $C([0,\infty); S)$ or $D([0,\infty); S)$ provided
  with the canonical filtration $\mathcal{F}$.   Then 
\begin{align*}
\gamma &= \begin{cases}
\sigma + \tau  \circ \theta_\sigma & \text{when $\sigma<\infty$} \\
\infty & \text{when $\sigma=\infty$}
\end{cases}
\end{align*}
is also weakly $\mathcal{F}$-optional.
\end{lem}
\begin{proof}
Let $X$ be the canonical process (i.e. $X_t$ is the evaluation
function $\pi_t$).  

First we claim that $\gamma$ is measurable.  This follows by noting
that $\theta_\sigma$ is measurable by writing it as $\theta \circ (id,
\sigma)$ and using by Lemma \ref{MeasurabilityOfShiftOperator}.
Therefore $\gamma$ is measurable by the measurability of
$\theta_\sigma$, $\sigma$
and $\tau$ and application of Lemma \ref{CompositionOfMeasurable} and
Lemma \ref{ArithmeticCombinationsOfMeasurableFunctions}.

Next we claim that if we pull back $\mathcal{F}_t$ by $\theta_\sigma$
then result should only depend on values  $X_s$ for $\sigma \leq s
\leq \sigma + t$ hence should be $\mathcal{F}^+_{\sigma +
  t}$-measurable.  We have to be a bit careful with this claim, because $\sigma$
can be infinite in which case $\theta_\sigma$ isn't defined.  To make
the claim precise and to prove it pick $n \geq 0$ and note that by either discreteness or by continuity of sample
paths together with Lemma
\ref{ContinuityAndProgressiveMeasurability} we know that $X$ is $\mathcal{F}$-progressively measurable. 
By $\mathcal{F}^+$-optionality of $\sigma \wedge n$ and Lemma
\ref{StoppedProgressivelyMeasurableProcess} we know that
$X_{\sigma \wedge n +s} = X_s \circ \theta_{\sigma \wedge n}$ is
$\mathcal{F}^+_{\sigma \wedge n+s}$-measurable for all $s \geq 0$.
Now fix $t \geq 0$ then for $0 \leq s \leq t$, pick a measurable set $B
\in \mathcal{S}$
and let $A = \lbrace X_s \in B \rbrace$; we note that
 $\theta_{\sigma \wedge n}^{-1} A = (X_s \circ \theta_{\sigma \wedge n})^{-1}(B) =
X_{\sigma \wedge n+s}^{-1}(B) \in \mathcal{F}^+_{\sigma \wedge n+s} \subset
\mathcal{F}^+_{\sigma \wedge n+t} $.  
Since $\lbrace A \mid \theta_{\sigma \wedge n}^{-1} A \in
\mathcal{F}^+_{\sigma \wedge n+t} \rbrace$ is a $\sigma$-algebra (Lemma
\ref{SigmaAlgebraPullback}) and sets of the form $\lbrace X_s \in B
\rbrace$ for $0 \leq s \leq t$ generate $\mathcal{F}_t$, we know that
$\theta_{\sigma \wedge n}^{-1} \mathcal{F}_t \subset
\mathcal{F}^+_{\sigma \wedge n +t}$ for all $t \geq 0$ and $n \geq 0$.

Now fix $0 \leq t < \infty$, let $n = \floor{t}+1$
and note that
\begin{align*}
\lbrace \gamma < t \rbrace &= \cup_{\substack{0 < r < t\\r \in
    \rationals}} \lbrace \sigma < r ; \tau \circ \theta_\sigma < t -
r\rbrace \\
&=\cup_{\substack{0 < r < t\\r \in
    \rationals}} \lbrace \sigma  \wedge n < r ; \tau \circ
\theta_{\sigma \wedge n} < t - r\rbrace
\end{align*}
Since $\tau$ is weakly $\mathcal{F}$-optional we know that $\lbrace
\tau < t-r\rbrace \in \mathcal{F}_{t-r}$ hence $\theta_{\sigma \wedge n}^{-1}
\lbrace \tau < t-r \rbrace \in \mathcal{F}^+_{\sigma \wedge n + t -r}$ and
therefore using Lemma \ref{WeaklyOptionalCharacterization} applied to
the stopped $\sigma$-algebra $\mathcal{F}^+_{\sigma  \wedge n + t -r}$ we get
\begin{align*}
\lbrace \sigma \wedge n < r ; \tau \circ \theta_{\sigma \wedge n}< t - r\rbrace &=
\lbrace \sigma \wedge n +t -r < t \rbrace \cap \theta_{\sigma \wedge n}^{-1} \lbrace \tau
< t -r\rbrace \in \mathcal{F}_t
\end{align*}
and therefore $\gamma$ is weakly $\mathcal{F}$-optional.
\end{proof}

Note: Kallenberg's proof of the above Lemma is a little bit different and
from what I can tell has a small error.  He first
proves the result for $\sigma$ bounded, and then claims that
$\gamma_n = \sigma \wedge n + \tau \circ \theta_{\sigma \wedge n} \uparrow \gamma$ enabling us to apply the result for
the bounded case to
$\gamma_n$ and to conclude that $\gamma = \sup_n \gamma_n$ is weakly
$\mathcal{F}$-optional via Lemma
\ref{InfSupStoppedFiltration}.  The problem is that $\gamma_n$ as
defined is not increasing.  To see a counter example let $S = \lbrace
H,T \rbrace$ and consider the result for $S^\infty$ (here time is $\integers_+$).  Define 
\begin{align*}
\tau &= \min \lbrace n \mid n \text{ is even and } X_n = H \rbrace
\end{align*}
It is easy to see that $\tau$ is a stopping time as 
\begin{align*}
\lbrace \tau = n \rbrace  &= 
\begin{cases}
\lbrace X_0 =  H \rbrace & \text{for $n = 0$} \\
\lbrace X_n = H \rbrace \cap \lbrace X_{n-2} = T \rbrace \cap \dotsb
\cap \lbrace X_0 = T \rbrace & \text{if $n$ is even and $n > 0$} \\
\emptyset & \text{if $n$ is odd}
\end{cases} 
\end{align*}
Now let $\sigma$ be
a suitably large deterministic time (say $\sigma = 2$) so that for $n \leq
2$ we have $\gamma_n = n + \tau \circ \theta_n$.  Consider
$\omega = (T,H,H,H,\dotsc) \in S^\infty$.  Note that $\tau(\omega)
= 2$ thus $\gamma_0(\omega) = 2$ but $\tau(\theta_1(\omega)) = 0$ and
therefore $\gamma_1(\omega) = 1 < \gamma_0(\omega)$. 

It is worth noting that even when we are not considering the canonical
case many optional times of interest (in particular hitting times) are
pull backs of optional times on the path space (i.e. are of the form
$\tau \circ X$ where $\tau$ is an optional time defined on $S^T$).  If
we are given a pair of these optional times then we can
apply the time shift construction of the optional times on the path
space and pull back (i.e. forming $\sigma \circ X + \tau \circ
\theta_{\sigma \circ X} \circ X$).  The notation for the non-canonical
case is a bit ugly so sometimes we will simply use the notation $\sigma + \tau \circ
\theta_{\sigma} $ as a shorthand.

\begin{thm}[Strong Markov Property]\label{StrongMarkovPropertyMarkovProcessCountableValues}Let $X$ be a time homogeneous Markov process on
  $\integers_+$ or $\reals_+$ and let $\tau$ be an optional
  time with at most countably many values.  Then for every measurable
  $A \subset S^T$,
\begin{align*}
\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in A}(\omega) &=
\sprobability{A}{X_\tau(\omega)} \text{ for almost all $\omega$ such that $\tau(\omega) < \infty$}
\end{align*}
\end{thm}
\begin{proof}
Before starting on the proof we first need to make some remarks about
the well-definedness of the quantities in the result.  Specifically we
have not defined $\theta_\tau X$ nor $\sprobability{A}{X_\tau}$ when $\tau = \infty$ but neither have
we assumed that $\tau$ is almost surely finite.  The first point is that we
can extend $\sprobability{A}{X_\tau}$ can be defined to be an
arbitrary value on $\lbrace \tau = \infty \rbrace$ without affecting
the values of $\sprobability{A}{X_\tau}$ on $\lbrace \tau < \infty
\rbrace$ hence the assertion of the result.  By locality of
conditional expectation (Lemma \ref{ConditionalExpectationIsLocal})
and the $\mathcal{F}_\tau$-measurability of
$\tau$ (Lemma
\ref{StoppedFiltration}) we can define $\theta_\tau X$ arbitrarily on $\lbrace \tau =
\infty \rbrace$ without affecting the values of
$\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in A}$ on $\lbrace \tau < \infty
\rbrace$ hence the assertion of the result.  Therefore the result
makes sense assuming that such extensions have made and is independent
of the extensions chosen.

We first prove the result for deterministic times and extend to
countably valued optional times.  Note that the content of result is
vacuous for an infinite deterministic time, so pick a finite
deterministic time $t$, $t_1
\leq \cdots \leq t_n$, $B \in \mathcal{S}^{\otimes n}$, $A =
(\pi_{t_1}, \dotsc, \pi_{t_n})^{-1}(B)$ and
calculate using Lemma \ref{MarkovDistributions}, time homogeneity and
the proof of Lemma \ref{MarkovMixtures}
\begin{align*}
\cprobability{\mathcal{F}_t}{\theta_t X \in A} &=
\cprobability{\mathcal{F}_t}{((\theta_t X)_{t_1}, \dotsc,
  (\theta_t X)_{t_n}) \in B} \\
&= \cprobability{\mathcal{F}_t}{(X_{t  + t_1}, \dotsc, X_{t+ t_n}) \in B} \\
&= \mu_{t, t + t_1} \otimes \cdots \otimes \mu_{t+t_{n-1}, t+t_n}(X_t,B) \\
&=\mu_{0, t_1} \otimes \cdots \otimes \mu_{t_{n-1}, t_n}(X_t, B) \\
&= \sprobability {A}{X_t}
\end{align*}
Now we know that sets of the form 
$(\pi_{t_1}, \dotsc, \pi_{t_n})^{-1}(B)$ are a
generating $\pi$-system for the $\sigma$-algebra $\mathcal{S}^T$,
the full result for deterministic times $t$ follows from a monotone
class argument.  Specifically, we simply show that the set of $A$ such that
$\cprobability{\mathcal{F}_t}{\theta_t X \in A} =
\sprobability{A}{X_t}$ a.s. is a $\lambda$-system.  The case for $B
\setminus A$ follows from linearity of conditional expectation and
finite additivity of measure and the case $A_1 \subset A_2 \subset
\cdots$ follows from monotone convergence for conditional expectations
and continuity of measure.

Now we extend to the case of countably valued optional times.  Let $A
\in \mathcal{S}^T$ and $B \in \mathcal{F}_\tau$ and calculate using
Monotone Convergence and the result for deterministic times
\begin{align*}
\expectation{\characteristic{A}(\theta_\tau X) ; B} &= \sum_t
\expectation{\characteristic{A}(\theta_t X) ; \lbrace \tau = t \rbrace
  \cap B} \\
&= \sum_t \expectation{\sprobability{A}{X_t} ; \lbrace \tau = t \rbrace
  \cap B} \\
&= \expectation{\sprobability{A}{X_\tau} ; B} 
\end{align*}
so the result follows by the definition of conditional expectation. 

An alternative argument that extends the case of deterministic times
to countable optional times uses the localization of the stopped
filtration Lemma \ref{LocalizationOfStoppedFiltration} and the local
property of conditional expectations Lemma
\ref{ConditionalExpectationIsLocal}.  Let $t$ be a value in the range
of $\tau$, combining these two results and
using the result for deterministic times we
know that on the set $\lbrace \tau = t \rbrace$ we have
\begin{align*}
\cprobability{\mathcal{F}_{\tau}}{\theta_\tau X \in A} &=
\cprobability{\mathcal{F}_t}{\theta_t X \in A} = \sprobability{A}{X_t}
 = \sprobability{A}{X_\tau} \text{ a.s.}
\end{align*}
Let the set where the above inequality fails be called $N_t$.  Since
we have assumed the set of values of $\tau$ is countable, the union of
the $N_t$ is also a null set and the result holds off of this null
set.

TODO: What about the $\mathcal{F}_\tau$-measurability of
$\sprobability{A}{X_\tau}$?  Note that this is a consequence of result
since we haven't assumed $X$ is progressive(see Lemma
\ref{StrongIndependentIncrements} below where we make this implication explicit).  Double check that we
don't assume it in the proof above (note that Ethier and Kurtz do make the progressive assumption in their
discussion of the strong markov property).
\end{proof}

The key part of the above proof is the computation of finite dimensional distributions as a bridge to
lift the simple Markov property $\cprobability{\mathcal{F}_s}{X_{t+s} \in A} = \mu_{t}(X_s, A) = \sprobability{X_t \in A}{X_s}$ 
on one dimensional distributions to the full $\sigma$-algebra $\mathcal{S}^T$.  For an arbitrary optional time $\tau$ we have an analogous
argument using finite dimensional distributions to show the strong Markov property for the one dimensional case is sufficient to 
prove the full strong Markov property for that optional time.  This result will be used later in the text when we want to show that 
special classes of Markov processes have the strong Markov property.  Note that in this case we are now dealing with arbitrary 
optional times $\tau$ and therefore we must assume the process $X$ is progressive so that $X_\tau$ is $\mathcal{F}_\tau$-measurable 
(Lemma \ref{StoppedProgressivelyMeasurableProcess}).

\begin{prop}\label{StrongMarkovFromOneDimensionalDistribution}Let $X$ be a progressive time homogeneous Markov process on
  $\reals_+$ and let $\tau$ be a finite optional
  time.  If for every $s,t \geq 0$ and $B \in \mathcal{S}$ we have
\begin{align*}
\cprobability{\mathcal{F}_{\tau+s}}{X_{\tau + t + s} \in B} &= \mu_t(X_{\tau+s}, B) \text{ a.s.}
\end{align*}
then for every measurable
  $A \subset S^T$,
\begin{align*}
\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in A} &= \sprobability{A}{X_\tau} \text{ a.s.}
\end{align*}
\end{prop}
\begin{proof}
The crux of the proof is to show the result for finite dimensional distributions.
\begin{clm}Let $n \in \naturals$ and $f_1, \dotsc, f_n$ be bounded measurable functions from $S$ to $\reals$ and
let $t_1 \leq \dotsc \leq t_n$ then 
\begin{align*}
\cexpectationlong{\mathcal{F}_\tau}{\prod_{i=1}^n f_i(X_{\tau + t_i})} 
&= \int \prod_{i=1}^n f_i(u_i) \, \mu_{t_1} \otimes \dotsb \otimes \mu_{t_n - t_{n-1}}(X_\tau, du_1, \dotsc, du_n)
\end{align*}
\end{clm}
The proof is by induction on $n$.  For $n=1$ we have by hypothesis 
\begin{align*}
\cprobability{\mathcal{F}_\tau}{X_{\tau + t} \in \cdot} &= \mu_t(X_\tau, \cdot)
\end{align*} 
and therefore
by Theorem \ref{Disintegration} we have 
\begin{align*}
\cexpectationlong{\mathcal{F}_\tau}{f(X_{\tau + t})} &= \int f(u) \, \mu_t(X_\tau, du)
\end{align*}
which is the claim for $n=1$.  For the induction step, assume the result for all 
$m \in \naturals$ such that $1 \leq m \leq n$ and let $f_1, \dotsc, f_{n+1}$ and $t_1 \leq \dotsb \leq t_{n+1}$ be given then we apply the tower and pullout properties of conditional expectation, the induction 
hypothesis for cases $n=1$ and $n$ and then the definition of kernel products to see
\begin{align*}
&\cexpectationlong{\mathcal{F}_\tau}{\prod_{i=1}^{n+1} f_i(X_{\tau + t_i})}  \\
&=\cexpectationlong{\mathcal{F}_\tau}{\cexpectationlong{\mathcal{F}_{\tau+t_n}}{\prod_{i=1}^{n+1} f_i(X_{\tau + t_i})} } \\
&=\cexpectationlong{\mathcal{F}_\tau}{\prod_{i=1}^n f_i(X_{\tau + t_i}) \cexpectationlong{\mathcal{F}_{\tau+t_n}}{f_{n+1}(X_{\tau + t_{n+1}})} } \\
&=\cexpectationlong{\mathcal{F}_\tau}{\prod_{i=1}^n f_i(X_{\tau + t_i}) \int f_{n+1}(u_{n+1}) \, \mu_{t_{n+1} - t_n}(X_{\tau + t_n}, du_{n+1})} \\
&=\int \prod_{i=1}^n f_i(u_i) \left[\int f_{n+1}(u_{n+1}) \, \mu_{t_{n+1} - t_n}(u_n, du_{n+1}) \right] \, \mu_{t_1} \otimes \dotsb \otimes \mu_{t_n - t_{n-1}}(X_\tau, du_1, \dotsc, du_n)\\
&=\int \prod_{i=1}^{n+1} f_i(u_i) \mu_{t_1} \otimes \dotsb \otimes \mu_{t_{n+1} - t_{n}}(X_\tau, du_1, \dotsc, du_{n+1})\\
\end{align*}
and the claim is proved.

From the claim and the proof of Lemma \ref{MarkovMixtures} we see that for arbitrary $B_1, \dotsc B_n \in \mathcal{S}$ and $t_1 \leq \dotsb \leq t_n$ if we
define $A = (\pi_{t_1}, \dotsc, \pi_{t_n})^{-1}(B_1 \times \dotsb \times B_n)$ (where $\pi_t : S^T \to S$ is the evaluation map $\pi_t f = f(t)$) we have
\begin{align*}
&\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in A} \\
&=\cprobability{\mathcal{F}_\tau}{(X_{\tau+t_1}, \dotsc, X_{\tau+t_n}) \in B_1 \times \dotsb \times B_n} \\
&=\mu_{t_1} \otimes \dotsb \otimes \mu_{t_n - t_{n-1}}(X_\tau, B_1 \times \dotsb \times B_n) \\
&= \sprobability{A}{X_\tau}
\end{align*}
The set of such $A$ is clearly a $\pi$-system and generates $\mathcal{S}^T$ (the latter being generated by the one dimensional $\pi_t^{-1}(B)$ in fact).  By montone classes as in Theorem \ref{StrongMarkovPropertyMarkovProcessCountableValues} we get the result for all $\mathcal{S}^T$.
\end{proof}

The previous result that shows how to establish the Strong Markov property for finite optional times is in fact sufficient to establish the Strong Markov property for all optional times by virtue of the following argument.
\begin{prop}\label{StrongMarkovFromStrongMarkovFiniteOptional}Let $X$ be a progressive time homogeneous Markov process on
  $\reals_+$ and suppose that for all finite optional
  times $\tau$ and all measurable sets
  $A \subset S^T$ we have
\begin{align*}
\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in A} &= \sprobability{A}{X_\tau} \text{ a.s.}
\end{align*}
then for all optional times $\tau$ and measurable sets
  $A \subset S^T$ we have
\begin{align*}
\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in A} &= \sprobability{A}{X_\tau} \text{ a.s. on $\tau < \infty$}
\end{align*}
\end{prop}
\begin{proof}
The fact that the terms in the conclusion of the result are in fact well defined see the discussion in Theorem \ref{StrongMarkovPropertyMarkovProcessCountableValues}.  Given an arbitrary optional time $\tau$, let $n \in \naturals$ and note that $\mathcal{F}_\tau = \mathcal{F}_{\tau \wedge n}$, $\theta_\tau X = \theta_{\tau \wedge n}$ and $\sprobability{A}{X_\tau}=\sprobability{A}{X_{\tau \wedge n}}$ on $\lbrace \tau \leq n \rbrace = \lbrace \tau = \tau \wedge n \rbrace$ (see Proposition \ref{StoppedAlgebraMinOfOptionalTimes} for the first assertion).  Observe that for $t \geq n$, 
\begin{align*}
\lbrace \tau \leq n \rbrace \cap \lbrace \tau \wedge n \leq t \rbrace &= \lbrace \tau \leq n \rbrace \in \mathcal{F}_n
\end{align*} 
and for $t < n$
\begin{align*}
\lbrace \tau \leq n \rbrace \cap \lbrace \tau \wedge n \leq t \rbrace &= \lbrace \tau \leq t \rbrace \in \mathcal{F}_t \subset \mathcal{F}_n
\end{align*} 
Thus $\lbrace \tau \leq n \rbrace  \in \mathcal{F}_\tau \cap \mathcal{F}_{\tau \wedge n} = \mathcal{F}_{\tau \wedge n}$ and we may apply  localization of conditional expectations Lemma \ref{ConditionalExpectationIsLocal} to see that on $\lbrace \tau \leq n \rbrace$,
\begin{align*}
\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in A} &= \cprobability{\mathcal{F}_{\tau \wedge n}}{\theta_{\tau  \wedge n} X \in A} = \sprobability{A}{X_{\tau \wedge n}}  = \sprobability{A}{X_\tau}  \text{ a.s.}
\end{align*}
Now write $\lbrace \tau < \infty \rbrace = \cup_{n=1}^\infty \lbrace \tau \leq n \rbrace$ and the the union of a countable number of null sets.
\end{proof}

In the case of a space homogeneous Markov process the strong Markov
property can be expressed more concisely as an extension of the
independent increments characterization of Lemma
\ref{IndependentIncrements} to optional times.  In many scenarios it
is more convenient to use these properties.  Note that the Lemma does
not require the countable range assumption.
\begin{lem}\label{StrongIndependentIncrements}Let $S$ be a measurable
  Abelian group with a filtration $\mathcal{F}$, $X$ be a time
  homogeneous and space homogeneous $S$-
  valued Markov process and $\tau$ be an almost surely finite optional time.  
Then 
\begin{align*}
\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in A} &= \sprobability{A}{X_\tau}
\end{align*}
if and only if $X_\tau$ is $\mathcal{F}_\tau$-measurable, $\cindependent{\theta_\tau X -
  X_\tau}{\mathcal{F}_\tau}{}$ and $X - X_0 \eqdist \theta_\tau X - X_\tau$
\end{lem}
\begin{proof}
Assume that $X$ satisfies $\cprobability{\mathcal{F}_\tau}{\theta_\tau
  X \in A} = \sprobability{A}{X_\tau}$ for all $A \in \mathcal{S}^T$.  To see that $X_\tau$ is
$\mathcal{F}_\tau$-measurable observe that if we let $\pi_0 : S^T \to
S$ be
evaluation at time $0$, then for any $B \in \mathcal{S}$ and $x \in S$,
\begin{align*}
\sprobability{\pi_0^{-1}B}{x} &= \begin{cases}
1 & \text{if $x \in B$} \\
0 & \text{if $x \notin B$}
\end{cases}
\end{align*}
therefore we have
\begin{align*}
\characteristic{X_\tau \in B} &=
\sprobability{\pi_0^{-1}B}{X_\tau} =
\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in \pi_0^{-1} B}
\end{align*}
which shows that $\lbrace X_\tau \in B \rbrace \in \mathcal{F}_\tau$.

Having established $\mathcal{F}_\tau$-measurability of $X_\tau$ we
know that $P_{X_\tau}$ is a not just a \emph{regular} version for
$\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in \cdot}$ and we can
apply Theorem \ref{Disintegration} and space homogeneity of
$\probabilityop_x$ (Lemma \ref{SpaceHomogeneousMarkovDistributions})
to calculate for $A \in \mathcal{S}^T$ (using $f: S^T \times S \to
\reals_+$ given by $f(x,y) = \characteristic{A+y}(x)$ in the disintegration) 
\begin{align*}
\cprobability{\mathcal{F}_\tau}{\theta_\tau X - X_\tau \in A} &= 
\int \characteristic{A + X_\tau} (x) \,
\probabilityop_{X_\tau}(dx) 
=\sprobability{A + X_\tau}{X_\tau}
=\sprobability{A}{0} \text{ a.s.}
\end{align*}
which is almost surely constant and therefore independence is proven.
This also shows that the distribution of $\theta_\tau X - X_\tau$ is
equal to $\probabilityop_0$ and letting $\tau = 0$ shows $\theta_\tau
X - X_\tau \eqdist X - X_0$.

To prove the converse, note that $X - X_0$ is has
initial distribution $\delta_0$ hence using our independence and
equidistribution assumptions and the definition of the measure
$\probabilityop_0$ we get for any $A \in \mathcal{S}^T$,
\begin{align*}
\cprobability{\mathcal{F}_\tau}{\theta_\tau X - X_\tau \in A} &=
\probability{\theta_\tau X - X_\tau \in A} 
=\probability{X - X_0 \in A} 
=\sprobability{A}{0}
\end{align*}
which provides us with a regular version for
$\cprobability{\mathcal{F}_\tau}{\theta_\tau X - X_\tau \in \cdot}$.
Now by the $\mathcal{F}_\tau$-measurability of $X_\tau$ we can apply
Theorem \ref{Disintegration} and Lemma
\ref{SpaceHomogeneousMarkovDistributions} to get
\begin{align*}
\cprobability{\mathcal{F}_\tau}{\theta_\tau X \in A}
&=\cprobability{\mathcal{F}_\tau}{\theta_\tau X - X_\tau \in A-X_\tau}\\
&= \int \characteristic{A-X_\tau}(x) \, \probabilityop_0(dx) \\
&=\sprobability{0}{A - X_\tau} \\
&=\sprobability{X_\tau}{A }
\end{align*}
and we are done.
\end{proof}

\begin{defn}Let $X$ be a time homogeneous Markov process with
  transition kernel $\mu_t$ we say an initial distribution $\nu$ is
  \emph{invariant} if $\nu \mu_t = \nu$ for all $t \in T$, i.e. we
  have
\begin{align*}
\int \mu_t(x, A) \, \nu(dx) = \nu(A)
\end{align*}
for all $t \in T$ and $A \in \mathcal{S}$.
\end{defn}

\begin{defn}Let $X$ be a stochastic process with time scale $T$ then
  we say $X$ is \emph{stationary} if $\theta_t X \eqdist X$
  for all $t \in T$.
\end{defn}


\begin{lem}\label{InvarianceImpliesStationary}Let $X$ be a time
  homogeneous Markov process with transition kernel $\mu$ and an invariant initial distribution
  $\nu$, then $X$ is stationary.
\end{lem}
\begin{proof}
Fix $t \in T$, $s_1 < \dotsb < s_n$ and $A \in \mathcal{S}^{\otimes
  n}$, then using Lemma
\ref{MarkovDistributions} and time homogeneity we compute
\begin{align*}
\probability{(X_{t + s_1}, \dotsc, X_{t + s_n}) \in A} &= \nu_{t+s_1}
                                                         \otimes
                                                         \mu_{s_2 -
                                                         s_1} \otimes
                                                         \dotsm
                                                         \otimes
                                                         \mu_{s_n -
                                                         s_{n-1}}(A)
  \\
&=\nu_{s_1}
                                                         \otimes
                                                         \mu_{s_2 -
                                                         s_1} \otimes
                                                         \dotsm
                                                         \otimes
                                                         \mu_{s_n -
                                                         s_{n-1}}(A)
= \probability{(X_{ s_1}, \dotsc, X_{s_n}) \in A}
\end{align*}
Since the finite dimensional distributions characterize the
distribution of $\theta_t X$ (Lemma \ref{ProcessLawsAndFDDs}) it follows that $X$ is stationary.
\end{proof}


\section{Discrete Time Markov Chains}

In this section we discuss the theory of Markov processes on a time
scale $\integers_+$.  This part of Markov process theory has many
applications and we'll be able to construct lots of important examples
that both illustrate and motivate the accompanying theory.  Moreover
much of the theory in discrete time illustrates concerns that are also
present in more general cases but with fewer technical distractions.

One of our first concerns is to think about constructing examples of
Markov processes.  The obvious way to approach this is the way we have
done it up until now: specify a transition kernel and an initial
distribution.  As it turns out, it can be surprisingly difficult to
get a handle on the transition kernel of a concrete process and it is
desirable to have alternative ways of constructing and characterizing
Markov processes.  In the discrete time case we can think of a Markov
process as a deterministic system that is perturbed by noise (or
alternatively a  ``transduced'' noise sequence).  We make this precise
in the following theorem.
\begin{thm}\label{RandomMappingRepresentationExistence}Let $X$ be a
  process on time scale $\integers_+$ with a Borel state space
  $S$, then $X$ is Markov if and only if there exist a measurable
  space $(T, \mathcal{T})$, measurable
  functions $f_1, f_2, \dotsc : S \times T \to S$ and
  i.i.d. random elements $\vartheta_1, \vartheta_2, \dotsc
  \Independent X_0$ such that $X_n = f_n(X_{n-1}, \vartheta_n)$
  a.s. for all $n \in \naturals$.   If $X$ is Markov we may find such
  a representation with $T=[0,1]$ and $\vartheta_n$ i.i.d. $U(0,1)$
  random variables.  We may choose $f_1 = f_2 = \dotsb$
  if and only if $X$ is time homogeneous. 
\end{thm}
\begin{proof}
First assume that $X$ has the hypothesized representation.  Let
$\mathcal{F}$ be the filtration generated by $X$.  Let $\nu$ be the
law of $\vartheta_1, \vartheta_2, \dotsc$.  Pick a
random element $\vartheta$ with law $\nu$ and for $A \in
\mathcal{S}$ define $\mu_n(x, A) = \probability{f_n(x, \vartheta) \in
  A}$.  Note that it follows from a simple induction using
$(\vartheta_1, \vartheta_2, \dotsc) \Independent X_0$ and the
expression $X_n = f_n(X_{n-1}, \vartheta_n)$ that $\vartheta_n$ is
independent of $X_m$ for all $m = 0, \dotsc, n-1$.  Therefore $\cprobability{\vartheta_n \in \cdot}
{\mathcal{F}_{n-1}} = \probability{\vartheta \in \cdot} = \nu$ and in particular has a regular
version. Furthermore since $X_{n-1}$ is $\mathcal{F}_{n-1}$-measurable
we can apply Lemma \ref{DisintegrationIndependentLaws} to compute for any $A \in \mathcal{S}$
\begin{align*}
\cprobability{X_n \in A}{\mathcal{F}_{n-1}} &=
\cprobability{f_n(X_{n-1}, \vartheta_n) \in A}{\mathcal{F}_{n-1}} \\
&=\int \characteristic{f_n (X_{n-1}, s)\in A} \, \nu(ds) =
\probability{f_n (X_{n-1}, \vartheta)\in A} = \mu_n(X_{n-1}, A)
\end{align*}
which shows that $X$ is Markov with transition kernel $\mu_n$ (recall
in discrete time the Chapman Kolmogorov relations hold identically for
free).  Note also that $f_1 = f_2 = \dotsc$ if and only if $\mu_1 =
\mu_2 = \dotsc$ which is to say that $X$ is time homogeneous.

Now let $X$ be Markov.  Since $S$ is Borel we may apply Lemma \ref{RandomizationAndKernels}
to each transition kernel $\mu_n$ and construct a measurable function
$f_n : S \times [0,1] \to S$ such that for a $U(0,1)$ random variable
$\vartheta$ we know that $\probability{f_n(s, \vartheta) \in \cdot} = \mu_n(s,
\cdot)$.  Let $\tilde{X}_0$ be a random element such that $\tilde{X}_0
\eqdist X_0$ (e.g. just take the identity on $(S, \mathcal{S})$
provided with the probability measure $\mathcal{L}(X_0)$).  By
extending the probability space of $\tilde{X}_0$ if necessary we
can assume the existence of i.i.d. $U(0,1)$ random variables
$\tilde{\vartheta}_1, \tilde{\vartheta}_2, \dotsc$.  Recursively
define $\tilde{X}_n = f_n(\tilde{X}_{n-1}, \tilde{\vartheta}_n)$ for
$n \in \naturals$ and apply the first part of this theorem to conclude
that $\tilde{X}$ is a Markov process with transition kernels $\mu_n$
and initial distribution $\mathcal{L}(\tilde{X}_0) =
\mathcal{L}(X_0)$.  We now apply Lemma \ref{MarkovDistributions} to
conclude that the of $X \eqfdd \tilde{X}$ and thus $X
\eqdist \tilde{X}$ by Lemma \ref{ProcessLawsAndFDDs}.  Now since
$[0,1]^\infty$ is a Borel space we may
apply Lemma \ref{Transfer} to conclude there are
random variables $\vartheta_1, \vartheta_2, \dotsc$ such that $(X,
(\vartheta_1, \vartheta_2, \dotsc)) \eqdist (\tilde{X},
(\tilde{\vartheta}_1, \tilde{\vartheta}_2, \dotsc))$.  By considering
marginal distributions we conclude that 
$\vartheta_1, \vartheta_2, \dotsc$ are i.i.d. $U(0,1)$ and that
$(\vartheta_1, \vartheta_2, \dotsc) \Independent X_0$. Also using $(X,
(\vartheta_1, \vartheta_2, \dotsc)) \eqdist (\tilde{X},
(\tilde{\vartheta}_1, \tilde{\vartheta}_2, \dotsc))$, the
measurability of the diagonal $\Delta \subset S \times S$ and the
definition of $\tilde{X}$ we conclude
that for each $n \in \naturals$ 
\begin{align*}
\probability{X_n = f_n(X_{n-1}, \vartheta_n)} &= \probability{(X_n,
  f_n(X_{n-1}, \vartheta_n)) \in \Delta} \\
&= \probability{(\tilde{X}_n,
  f_n(\tilde{X}_{n-1}, \tilde{\vartheta}_n)) \in \Delta} = 1
\end{align*}
and we are done.
\end{proof}

The representation of a Markov process as in the preceeding theorem is
refered to as a \emph{random mapping representation} and we'll soon
put it use in constructing examples of Markov processes.

We proceed to study the special subclass of time homogenous Markov processes with time
scale $\integers_+$.  A further important specialization occurs when
the state space $S$ is countable or finite.  We establish
some terminology while recording the definitions.
\begin{defn}A time homogeneous Markov process $X$ with time scale
  $\integers_+$, transition kernels $\mu_n$ and state space $S$ is called a
  \emph{discrete time Markov process}.  Furthermore,
\begin{itemize}
\item[(i)]If  $S$ countable then $X$ is a
  \emph{discrete time Markov chain} 
\item[(ii)]If  $S$ is finite then $X$ is a \emph{finite discrete time
    Markov chain}
\item[(iii)]For each $y \in S$ we let $\tau^+_y
  = \inf \lbrace n \in \naturals \mid X_n = y \rbrace$ and then
  recursively define the \emph{return times}
\begin{align*}
\tau^0_y &= 0 \\
\tau^{k+1}_y &= \tau^k_y + \tau^+_y \circ \theta_{\tau^k_y} \text{ for
  $k \in \integers_+$}
\end{align*}
\item[(iv)]For each $y \in S$ we define the \emph{occupation time} at $y \in S$ as
\begin{align*}
\kappa_y &= \sup \lbrace k \in \integers_+ \mid \tau^k_y < \infty \rbrace
\end{align*}
\item[(v)]For each $x,y \in S$ we define the \emph{hitting probability} 
\begin{align*} 
r_{xy} &= \sprobability{\tau^+_y < \infty}{x} = \sprobability{\kappa_y >
0}{x} 
\end{align*}
\item[(vi)]For each $x,y \in S$ and we define the \emph{transition probabilities}
\begin{align*}
p_{xy} &= \mu_1(x, \lbrace y \rbrace) &
p^n_{xy} &= \mu_n(x, \lbrace y \rbrace) \text{ for $n \in \naturals$}
\end{align*}
\end{itemize}
\end{defn}
For the case of a discrete time Markov chain, the transition
probabilities $p_{xy}$ characterize the transition kernels and recall
from Example \ref{ProbabilityKernelProductFiniteSampleSpace} it is
convenient to interpret the $p_{xy}$ as being the entries of a
\emph{transition matrix}
we shall call $p$.  Moreover
from Example \ref{ProbabilityKernelProductFiniteSampleSpace} and the
Chapman Kolmogorov relations we have 
\begin{align*}
\mu_n(x, \lbrace y \rbrace) &= \mu_1^n(x, \lbrace y \rbrace) =
p_{xy}^n
\end{align*}
where in the last equality we are taking the $(x,y)^{th}$ entry of the
$n$-fold product of the matrix $p$.  This explains the use of the
notation in the above definition of transition probabilites and also shows that for Markov chains
the notation is consistent with transition matrix point of view.  We
emphasize that $p^n_{xy}$ does not signify $p_{xy}$ raised to the
$n^{th}$ power!

We have two initial goals in our study of Markov chains.  The first is
to develop a little macroscopic structure theory of Markov chains.

\begin{prop}\label{RecurrenceTransienceDiscreteTime}Let $X$ be a discrete time Markov process on state space
  $S$.  Then for $y \in S$ we have
\begin{align*}
\kappa_y &= \sum_{n=1}^\infty \characteristic{X_n = y}
\end{align*}
Moreover for all $x,y \in S$ and $n \in \naturals$,
\begin{align*}
\sprobability{\kappa_y \geq n}{x} &= \sprobability{\tau^n_y <
  \infty}{x} = r_{xy} r_{yy}^{n-1} 
\end{align*}
If $r_{xy} = 0$ then $\kappa_y = 0$  $P_x$-almost surely, if $r_{xy} > 0$ and
$r_{yy} = 1$ then $\sprobability{\kappa_y = \infty}{x} = r_{xy} > 0$, otherwise
$\kappa_y$ is integrable with expectation
\begin{align*}
\sexpectation{\kappa_y}{x} &= \frac{r_{xx}}{1 - r_{yy}} =
\sum_{n=1}^\infty p^n_{xy}
\end{align*}
\end{prop}
\begin{proof}
First to see that $\kappa_y = \sum_{n=1}^\infty \characteristic{X_n =
  y}$, simply note that both represent the number of times that $X$
visits $y$.
  
To see that $\sprobability{\kappa_y \geq n}{x} = \sprobability{\tau^n_y <
  \infty}{x}$ simply note that equality holds at the level of events: $\lbrace \kappa_y \geq n \rbrace$ if
and only if $\lbrace \tau^n_y <  \infty \rbrace$.  Since $\tau^{n+1}_y
< \infty$ if and only if $\tau^n_y < \infty$ and $\theta_{\tau^n_y}
\circ \tau^+_y < \infty$ we can use the Strong Markov property to
calculate
\begin{align*}
\sprobability{\tau^{n+1}_y < \infty}{x} &= \sprobability{\tau^{n}_y
  < \infty ; \theta_{\tau^n_y} \circ \tau^+_y < \infty } {x} \\
&=\sexpectation{\tau^{n}_y
  < \infty ; \cprobability{\mathcal{F}_{\tau^n_y}}{\theta_{\tau^n_y} \circ \tau^+_y < \infty}
} {x} \\
&=\sexpectation{\tau^{n}_y
  < \infty ; \sprobability{\tau^+_y < \infty}{y}} {x} = \sprobability{\tau^{n}_y
  < \infty} {x} \sprobability{\tau^+_y < \infty}{y}
\end{align*}
which we use in an induction argument to get $\sprobability{\tau^{n}_y <
  \infty}{x} =  r_{xy} r_{yy}^{n-1}$.

Now we apply this fact along with Lemma \ref{TailsAndExpectations} to
see that
\begin{align*}
\sexpectation{\kappa_y}{x} &= \sum_{n=1}^\infty \sprobability{\kappa_y
  \geq n}{x} = r_{xy} \sum_{n=1}^\infty r_{yy}^{n-1} = \frac{r_{xy}}{1-r_{yy}}
\end{align*}
The rest of the statements in the proposition are trivial consequences
of what we have proven.
\end{proof}
By virtue of this result we can see that for every $x \in S$ there is
a dichotomy: either we have $r_{xx} = 1$ in which case $\kappa_x =
\infty$ $P_x$-a.s. (almost surely $X$ returns to $x$ infinitely many
times) or $0 \leq r_{xx} < 1$ in which case the number of times that
$X$ returns to $x$ has finite expectation $\frac{r_{xx}}{1-r_{xx}}$.
This attribute of states is worthy of a definition.
\begin{defn}Let $X$ be a discrete time Markov process on state space
  $S$, we say a state $x \in S$ is \emph{recurrent} if and only if $X$
  returns to $x$ infinitely many times $P_x$-a.s.  We say $x \in S$ is
  \emph{transient} if and only if $X$  returns to $x$ only finitely many times $P_x$-a.s.
\end{defn}

The theory of Markov processes tends to be concerned with long term
behavior of the process and therefore recurrent states are more
important than transient states (just wait long enough and you'll
never see a transient state again!)  Being able to detect recurrent
states is therefore a useful thing to be able to do.  A simple and
useful criterion can be found when there is an invariant distribution
for $X$.

\begin{prop}\label{RecurrenceFromInvariantDistribution}Let $X$ be a discrete time Markov process with state space
  $S$ and assume that an invariant distribution $\nu$ exists, then for
  every $x \in S$ if $\nu(x) > 0$ it follows that $x$ is recurrent.
\end{prop}
\begin{proof}
Using the invariance of $\nu$ we get for every $n \in \naturals$
\begin{align*}
0 &< \nu(x) = \int p^n_{xy} \, \nu(dy)
\end{align*}
Therefore using the fact that $r_{yx} \leq 1$ for all $x,y \in S$,
Proposition \ref{RecurrenceTransienceDiscreteTime} and Tonelli's Theorem \ref{Fubini} we get
\begin{align*}
\frac{1}{1 - r_{xx}} &\geq \int \frac{r_{yx}}{1-r_{xx}} \, \nu(dy) =
\int \sum_{n=1}^\infty p^n_{yx} \, \nu(dy) = \sum_{n=1}^\infty\int
p^n_{yx} \, \nu(dy) = \infty
\end{align*}
and thus it follows that $r_{xx} = 1$.
\end{proof}

\begin{defn}Let $p^n_{xy}$ be the transition probabilities of a
  discrete time Markov process on $S$.  The \emph{period} of a state
  $x \in S$ is 
\begin{align*}
d_x &= \gcd \lbrace n \in \naturals \mid p^n_{xx} > 0 \rbrace
\end{align*}
If $d_x = 1$ then we say that the state $x$ is \emph{aperiodic}.
\end{defn}

\begin{prop}\label{PeriodicReturnDiscreteTimeMarkov}Let $p^n_{xy}$ be the transition probabilities of a
  discrete time Markov process on $S$, if $x$ has period $d$
  then there exists an $N > 0$ such that $p^{nd}_{xx} > 0$ for all $n
  \geq N$.
\end{prop}
\begin{proof}
We need the following number theoretic fact:

\begin{lem}Let $A \subset \integers_+$ then there exists an integer $m_A$
such that for all $m \geq m_S$ there exist constants $c_1, \dotsc ,
c_n \in \integers_+$ and $x_1, \dotsc, x_n \in A$ such that $m \gcd A = c_1 x_1 + \dotsb +
c_n x_n$.
\end{lem}
\begin{proof}
To prove the lemma we first recall that the greatest common divisor of
a set is an integer linear combination of elements of the set.  

Claim: For any subset $B \subset \integers_+$ there exist elements
$x_1, \dotsc, x_n \in B$ and constants $c_1, \dotsc, c_n \in \integers$
such that $\gcd B = c_1 x_1 + \dotsb + c_n x_n$.

To see this, let $g^*_B$ be
smallest element in the set 
\begin{align*}
C &= \lbrace c_1 x_1 + \dotsb +
c_n x_n > 0 \mid n \in \naturals, c_1, \dotsc, c_n \in \integers \text{ and } x_1,
\dotsc, x_n \in B \rbrace
\end{align*}
  Note that
$g^*_B$ divides every $x \in B$; for if not then there is an $x$ such that
we can write $x = c
g^*_B + r$ with $c \in \integers_+$ and $0 < r < g^*_S$ thus $r = x
- g_B^* \in C$.  Therefore it follows that $\gcd B$ divides $g^*_B$.
On the other hand, since $g^*_B$ is an integer linear combination of a
finite number of
elements of $B$ it follows that $\gcd B$ divides $g^*_B$ and
therefore $\gcd B = g^*_B$.

Claim: For any set $B \subset
\integers_+$ there is a finite subset $F \subset B$ such that $\gcd F
= \gcd B$.  

To see this consider the sequence $g_n = \gcd B \cap \lbrace 0,
\dots, n \rbrace$.  Clearly, $g_n$ is non-increasing and non-negative
so there exists an $N > 0$ such that $g_n = g_N$ for all $n \geq N$.
It is also clear that $g_N$ divides every element of $B$ since every
element of $B$ is in some $B \cap \lbrace 0,
\dots, n \rbrace$ and it follows by a similar argument that $\gcd S
\leq g_N$. Thus $\gcd S = g_N$.

From the previous claim note that it suffices to prove the lemma for
finite sets $A$.  To prove the lemma for finite sets we proceed by
induction on the cardinality of $A$.

The result is vacuous for singleton sets so let $A = \lbrace a, b \rbrace$ and let $g = \gcd A$.  For
every $m \in \naturals$ we can write $m g = ca + db$ for some $c,d \in
\integers$.  By replacing $c$ and $d$ by $c + kb$ and $d - ka$ for
suitable $k \in \integers$ we may assume that $0 \leq c < b$ as well.
Thus in this case, define $m_A = (ab - a -b)/g +1$ and note that for
any $m \geq m_A$ we have
\begin{align*}
m g &=ca + db  \geq (ab - a - b) + g >  ab - a - b
\end{align*}
with $0 \leq c < b$ which implies
\begin{align*}
(d+1)b  &> ab - a - ca \geq 0
\end{align*}
which in turn implies $d \geq 0$.  Thus the result is proven for a two
point set.

Now we do induction on the cardinality of $A$.  Suppose the result is
proven for all $A$ with cardinality less than or equal to $n$.  Let $A$ be a finite
subset of $\integers_+$ with $A = \lbrace a_1, \dotsc, a_n \rbrace$ and $\gcd A = g_A$.  Let $a \in \integers_+
\setminus A$ and note the facts that $\gcd (A \cup \lbrace a \rbrace ) =
\gcd(\gcd A, a)$ and $\gcd(A \cup \lbrace a \rbrace )$ divides $\gcd
A$.  Define $g =  \gcd( A \cup \lbrace a \rbrace)$ and 
\begin{align*}
m_{A \cup  \lbrace a \rbrace} &= (m_{\lbrace a,  g_A \rbrace} g + m_A g_A)/g
\end{align*}
and pick any $m \geq m_{A \cup \lbrace a \rbrace}$ :  trivially we have
$m g \geq m_{\lbrace a,  g_A \rbrace} g + m_A g_A$.  It follows from the
fact that $g = \gcd(\gcd A, a)$ that $g$ divides $g_A$ and therefore
there is a $\tilde{m} \in \integers_+$ such that $mg - m_A g_A = \tilde{m} g \geq m_{\lbrace a,  g_A
  \rbrace} g$.
By the definition of $m_{\lbrace a,  g_A \rbrace}$ we know that
there are integers $c,d \geq 0$ such that $mg - m_A g_A = c a + d
g_A$.  Therefore
\begin{align*}
mg &= ca + (d + m_A) g_A = ca + \sum_{j=1}^n c_j a_j
\end{align*}
with suitable $c_1, \dotsc c_n \in \integers_+$ and the lemma is
proved.
\end{proof}

Now to prove the proposition, let $x \in S$,  let $A = \lbrace n \in \naturals \mid
p^n_{xx} > 0 \rbrace$ assume that $\gcd A = d$.  Applying the lemma we
see that there is an $N > 0$ such that for all $n \geq N$, $nd = c_1 n_1 + \dotsb + c_k
n_k$ for suitable $k \in \naturals$, $n_1, \dotsc, n_k \in A$ and $c_1, \dotsc, c_k \in
\integers_+$.  On the other hand suppose $n, m \in A$ and note that by
the Chapman Kolmogorov relations we have 
\begin{align*}
p^{n+m}_{xx} &= \mu_{n+m}(x, \lbrace x \rbrace) = \mu_{n}\mu_m(x,
\lbrace x \rbrace) = \int \mu_m(y , \lbrace x \rbrace) \, \mu_n(x, dy)
\\
&\geq \int \mu_m(y, \lbrace x \rbrace) \characteristic{x=y} \,
\mu_n(x, dy) 
= \mu_m(x, \lbrace x \rbrace) \mu_n(x, \lbrace x \rbrace) 
> 0
\end{align*}
which shows that $A$ is closed under addition.  It follows that $nd
\in A$ and the result is proven.
\end{proof}

\begin{defn}Let $X$ be a discrete time Markov process with initial
  distribution $\nu$ and transition kernel $\mu$.  We say that $X$ is
  \emph{reversible} if for every non-negative measurable or integrable $f : S \times
  S \to \reals$ we have
\begin{align*}
\int f(x,y) \, (\nu \otimes \mu)(dx,dy) 
&= \iint f(x,y) \, \mu(x, dy) \, \nu(dx) \\
&= \iint f(y,x) \, \mu(x, dy) \, \nu(dx)
= \int f(y,x) \, (\nu \otimes \mu)(dx,dy) 
\end{align*}
\end{defn}

There are a couple of immediate consequences of reversibility that
follow by looking at the finite dimensional distributions of a
reversible $X$.  The first very useful implication is that
reversibility implies stationarity.

\begin{prop}\label{ReversibleImpliesInvariant}Let $X$ be a reversible discrete time Markov process with initial
  distribution $\nu$ and transition kernel $\mu$, then $\nu$ is
  invariant for $X$.
\end{prop}
\begin{proof}
Let $A \in \mathcal{S}$ then using Lemma \ref{MarkovDistributions} and reversibility
\begin{align*}
\pushforward{X}{\probabilityop_{\nu}}
&= \nu \mu (A) 
= (\nu \otimes \mu)(S \times A) 
=  \iint \characteristic{A}(y) \, \mu(x, dy) \, \nu(dx) \\
&=  \iint \characteristic{A}(x) \, \mu(x, dy) \, \nu(dx) 
= \int \characteristic{A}(x) \, \nu(dx) 
= \nu(A)
\end{align*}
\end{proof}

The next implication explains the origin of the term reversible.
Prosaically one says that a reversible Markov process looks the same
if run backwards.
\begin{prop}Let $X$ be a reversible discrete time Markov process then
  for all $n,k \geq 0$ and $A \in \mathcal{S}^{\otimes n}$ 
\begin{align*}
\probability{(X_k, \dotsc, X_{n+k}) \in A} &= 
\probability{(X_{n+k}, \dotsc, X_k) \in A} 
\end{align*}
\end{prop}
\begin{proof}
Because $\nu$ is invariant, it follows that $X$ is a stationary
process (Lemma \ref{InvarianceImpliesStationary}) and therefore it
suffices to prove the result for $k=0$.
In fact we prove a bit more; we show that 
\begin{align}
\int f(x_0, \dotsc, x_n)
\nu \otimes \mu^{\otimes n}(dx_0, \dotsc, dx_n) &= \int f(x_n, \dotsc,x_0)
\nu \otimes \mu^ {\otimes n}(dx_0,\dotsc, dx_n)
\end{align}\label{ReversibleIntegrals}
for all $n \in \naturals$ and all non-negative measurable functions $f
: S^{n+1} \to [0,\infty)$.  By Lemma
\ref{MarkovDistributions} the current result follows from \eqref{ReversibleIntegrals}.
The proof is by induction on $n$ with the case $n=1$ being part of the
definition of reversibility.

Now supposing the result is true for $n-1$, we use Lemma
\ref{MarkovDistributions}, Tonelli's Theorem
and two applications of the induction hypothesis
\begin{align*}
&\int f(s_0, \dotsc, s_n) \, \mu(s_{n-1},ds_n) \dotsm
  \mu(s_0, ds_1) \nu(ds_0) \\
&=\int \left [ \int f(s_0, \dotsc, s_n) \,
  \mu(s_{n-1},ds_n) \right ]\mu(s_{n-2},ds_{n-1}) \dotsm
  \mu(s_0, ds_1) \nu(ds_0) \\
&=\int f(s_{n-1}, \dotsc, s_0, s_n) \, \mu(s_0,ds_n)
  \mu(s_{n-2}, ds_{n-1})\dotsm
  \mu(s_0, ds_1) \nu(ds_0) \\
&=\int \left [ \int f(s_{n-1}, \dotsc, s_0, s_n) \, 
  \mu(s_{n-2}, ds_{n-1})\dotsm
  \mu(s_0, ds_1) \right ] \, \mu(s_0,ds_n) \nu(ds_0) \\
&=\int f(s_{n-1}, \dotsc, s_n, s_0) \, 
  \mu(s_{n-2}, ds_{n-1})\dotsm
  \mu(s_n, ds_1) \mu(s_0,ds_n) \nu(ds_0) \\
&=\int f(t_{n}, \dotsc, t_1, t_0) \, 
  \mu(t_{n-1}, dt_{n})\dotsm
  \mu(t_0,t_1) \nu(dt_0) \\
\end{align*}
where in the last line we have defined new integration variables $t_0
= s_0$, $t_1 = s_n$ and 
$t_k = s_{k-1}$ for $2 \leq k \leq n$.
\end{proof}

We now make the transition to discussing discrete time Markov chains
(that is to say we restrict ourselves to countable state spaces.

Recurrence is a somewhat contagious property; if you start with a
recurrent state $x$ and can reach a state $y$ from $x$ with positive
probability then it will follow that $y$ is recurrent.  Intuitively
this can be seen by making the following observations:
\begin{itemize}
\item If $x$ is recurrent and I can reach $y$ from $x$ with positive
  probability then I must be able to reach $x$ from $y$ with positive
  probability; otherwise with positive probability $x$ reaches $y$
  (returning to itself only a finite number of times on the way) and
  then never again returns to itself contradicting recurrence.
\item One way for $y$ to return to itself is to first travel to $x$,
  then return to itself some number of times, then to make the return
  trip from $x$ to $y$; since $x$ is recurrent with positive
  probability this may be done in infinitely many ways hence $y$ is
  also recurrent.
\end{itemize}
These facts and a few more are captured less prosaically in the
following lemma.
\begin{lem}\label{RecurrenceClassesMarkovChains}Let $X$ be a discrete
  time Markov chain with state space $S$, let $x \in S$ be recurrent
  and define $S_x = \lbrace y \in S \mid r_{xy} > 0 \rbrace$.  Then
  for all $y \in S_x$, it follows that $y$ is recurrent
and for every $y,z \in S_x$ we have $r_{yz} = 1$.
\end{lem}
\begin{proof}
We first handle the case of showing that $r_{yx} = 1$.  For this, we
use a union bound, the Strong Markov property and the fact that
$X_{\tau^+_y} = y$ on $\lbrace \tau^+_y < \infty \rbrace$ to see
\begin{align*}
0 &= \sprobability{\tau^+_x = \infty} {x} 
\geq \sprobability{\tau^+_y < \infty ; \theta_{\tau^+_y} \circ
  \tau^+_x = \infty} {x} \\
&= \sexpectation{\tau^+_y < \infty ; \cprobability {\mathcal{F}_{\tau^+_y}}{\theta_{\tau^+_y} \circ
  \tau^+_x = \infty} } {x} \\
&=\sprobability{\tau^+_y < \infty ; \sprobability{\tau^+_x = \infty}
  {y}} {x} = \sprobability{\tau^+_y < \infty } {x}
\sprobability{\tau^+_x = \infty}{y} = r_{xy}(1 -r_{yx})
\end{align*}
which implies $r_{yx} =1$ since we assumed $r_{xy} > 0$.

Now we turn to the task of showing that all $y \in S_x$ are recurrent.
We know that $r_{xy} > 0$ and $r_{yx} > 0$ and therefore there exist
$m, n \in \naturals$ such that $p^n_{xy} > 0$ and $p^m_{yx} > 0$.
Thus, by Proposition \ref{RecurrenceTransienceDiscreteTime} and two
applications of the 
Chapman Kolmogorov relations and the recurrence of $x$  we get
\begin{align*}
\sexpectation{\kappa_y}{y} &= \sum_{j=1}^\infty p^j_{yy} \geq
\sum_{j=1}^\infty p^{j+m+n}_{yy} =\sum_{j=1}^\infty \sum_{z \in S} \sum_{w \in S} p^m_{yz}p^j_{zw}
p^n_{wy} \\
&\geq \sum_{j=1}^\infty p^m_{yx}p^j_{xx}
p^n_{xy} =\infty
\end{align*}
which implies that $y$ is recurrent.  Knowing that $y$ is recurrent
and having already shown that $r_{yx} =1 >0$, we know that $x \in S_y$
and we can apply the first
argument in the proof to conclude that $r_{xy} =1$ as well.

Lastly let $y,z \in S_x$.  We use the fact that one way for $X$ to get
from $y$ to $z$ is by passing through $x$ first.  Formally we use a
union bound and the Strong Markov Property to see
\begin{align*}
r_{yz} &= \sprobability{\tau^+_z < \infty}{y} \geq
\sprobability{\tau^+_x < \infty ; \theta_{\tau^+_x} \circ \tau^+_z < \infty}{y}
\\
&=\sexpectation{\tau^+_x < \infty ;
  \cprobability{\mathcal{F}_{\tau^+_x}}{\theta_{\tau^+_x} \circ
    \tau^+_z < \infty}}{y} \\
&= \sprobability{\tau^+_x < \infty }{y}
\sprobability{\tau^+_z < \infty}{x} = r_{yx} r_{xz} = 1
\end{align*}
which shows us that $r_{yz} = 1$.
\end{proof}

\begin{defn}Let $X$ be a discrete time Markov chain with state space
  $S$ then we say that $X$ is \emph{irreducible} if $r_{xy} > 0$ for
  all $x,y \in S$.  If $X$ is not irreducible we say that $X$ is \emph{reducible}.
\end{defn}

There are generalizations of the notion of irreducibility to the
general discrete time Markov process case but they will be dealt with
later; the countable state space case is historically the first to be
handled and provides important motivation while avoid some subtle
points.  The first thing is to record some alternative
characterizations of irreducibility; in the sequel we'll fell free to
use these equivalences without explicit mention.  They are all just
slightly different ways of capturing the notion that a Markov chain is
irreducible if it is possible for the chain to reach any part of state
space regardless of the starting point.

\begin{prop}\label{IrreducibleEquivalencesDiscreteTimeMarkovChain}Let
  $X$ be a discrete time Markov chain with state space $S$ then $X$ is
  irreducible if and only if for every $x,y \in S$ there exists $n \in
  \naturals$ such that $p^n_{xy} > 0$.
\end{prop}
\begin{proof}
Suppose $X$ is irreducible and let $x,y \in S$; it follows that
$\sprobability{\tau^+_y < \infty}{x} > 0$.  Writing
$\sprobability{\tau^+_y < \infty}{x} = \cup_{n=1}^\infty
\sprobability{\tau^+_y = n}{x}$ we conclude there is an $n \in
\naturals$ such that $\sprobability{\tau^+_y = n}{x} > 0$.  Now
observe that by a union bound
\begin{align*}
0 &< \sprobability{\tau^+_y = n}{x} \leq \sprobability{X_n = y}{x} = p^n_{xy}
\end{align*}

On the other hand suppose that $p^n_{xy} > 0$.  Then we know that
\begin{align*}
\lbrace X_n = y \rbrace &\subset \lbrace \tau^+_y \leq n \rbrace
                          \subset \lbrace \tau^+_y < \infty \rbrace
\end{align*}
and therefore $ 0 < p^n_{xy} \leq \sprobability{\tau^+_y < \infty}{x}$.
\end{proof}

\begin{prop}\label{IrreducibleChainProperties}Let $X$ be an irreducible discrete time Markov chain, then 
\begin{itemize}
\item[(i)]Either every $x \in S$ is transient or every $x \in S$ is
  recurrent.  Moreover $r_{xy} = 1$ for every $x,y \in S$.
\item[(ii)]Every $x \in S$ has the same period
\item[(iii)]If $\nu$ is an invariant distribution then $\nu(x) > 0$
  for every $x \in S$.
\end{itemize}
\end{prop}
\begin{proof}
Property (i) is an immediate consequence of Lemma
\ref{RecurrenceClassesMarkovChains} since for irreducible $X$ we have $S = S_x$ for any $x \in S$.

To see (ii), let $x,y \in S$ and pick $m,n \in \naturals$ such that
$p^n_{xy} > 0$ and $p^m_{yx} > 0$.  Now by the Chapman Kolmogorov
relations we see that for all $j \in \integers_+$
\begin{align*}
p^{j+m+n}_{yy} &= \sum_{z \in S} \sum_{w \in S} p^m_{yz} p^j_{zw}
p^n_{wy} \geq 
p^m_{yx} p^j_{xx} p^n_{xy} 
\end{align*}
If we choose $j = 0$ then we get inequality $p^{m+n}_{yy} \geq p^m_{yx} p^n_{xy} > 0$ which
implies that $d_y$ divides $m+n$.  With this fact in hand, we see that
for $j > 0$ for which $p^j_{xx} > 0$ it follows that $p^{j+m+n}_{yy} >
0$ and therefore $d_y$ divides $j$ as well.  By definition of the
period we then get $d_y \leq d_x$.  The argument we just made is
symmetric in $x$ and $y$ so the opposite inequality holds as well and
we conclude that $d_x = d_y$.

To see (iii), suppose that $\nu$ is an invariant distribution and
pick an $x \in S$ such that $\nu(x) > 0$.  If we let $y \in S$ by
irreducibility we find $n > 0$ such that $p^n_{xy} > 0$ and by
invariance of $\nu$ we get
\begin{align*}
\nu(y) &= \sum_{x \in S} p^n_{xy} \nu(x) \geq \nu(x) p^n_{xy} > 0
\end{align*}
\end{proof}

We now move to the theorem that gives us a useful criterion for the
existence of an invariant distribution for a discrete time Markov
chain and also shows that in a strong sense any initial distribution
converges to that invariant distribution.

\begin{thm}\label{ErgodicTheoremDiscreteTimeMarkovChains}Let $X$ be an
  irreducible and aperiodic discrete time Markov chain with countable state
  space $(S, \mathcal{S})$.  Then exactly
  one of the following holds
\begin{itemize}
\item[(i)]There exists a unique invariant distribution $\nu$ for which
  $\nu(x) > 0$ for all $x \in S$ and moreover for every initial
  distribution $\mu$ we have
\begin{align}
\lim_{n \to \infty} \sup_{A \in \mathcal{S}^\infty}
  \abs{\pushforward{\theta_n}{\probabilityop_\mu}\lbrace A \rbrace -
  \sprobability{A}{\nu}} = 0
\end{align}\label{ConvergenceToInvariantDiscreteTimeMarkovChain}
\item[(ii)]An invariant distribution does not exist and 
\begin{align*}
\lim_{n \to \infty} p^n_{xy} = 0 & \text{ for all $x,y \in S$}
\end{align*}
\end{itemize}
\end{thm}
The proof breaks down is a few different lemmas.  The proof technique used here is referred to as a
\emph{coupling} argument; it will reappear with increasing levels of
sophistication later in this book.  The common thread in coupling
arguments is the construction of a joint distribution on a product
space (called a \emph{coupling}) and its use to compare a process under study to one with simpler properties.

The first part of the coupling argument is the construction of the
process on the product space.  In this case a pair of independent
Markov chains suffices but we need a few details of about such
products of Markov chains to execute the coupling argument.

\begin{lem}\label{CouplingIndependentMarkovChainsDiscreteTime}Let $X$ and $Y$ be independent discrete time
  Markov chains with state space $S$ and $T$ and transition matrices $p_{xy}$
  and $q_{xy}$ respectively.  Then $(X,Y)$ is an irreducible discrete
  Markov chain with state space $S \times T$ and transition matrix
  $r_{xz,yw} = p_{xy}q_{zw}$.  If $X$ and $Y$ are both irreducible
    and aperiodic then $(X,Y)$ is as well.  If in addition invariant
    distributions exists for both $X$ and $Y$ then it follows that
    $(X,Y)$ is recurrent.
\end{lem}
\begin{proof}
The fact that $(X,Y)$ is a discrete time Markov chain with transition
matrix $p_{xy}q_{zw}$ is a special case of Exercise
\ref{ProductOfIndependentMarkov}.  If we assume that $X$ is
irreducible and aperiodic then for all $x,y \in S$ we know that there
exists $n \in \naturals$ such that $p_{xy}^n > 0$ by irreducibility
and furthermore by aperiodicity we know that $p_{yy}^m > 0$ for all by
finitely many $m \in \naturals$ (Proposition
\ref{PeriodicReturnDiscreteTimeMarkov}) and therefore $p^{m+n}_{xy}
\geq p^n_{xy}p^m{yy} > 0$ for all by finitely many $m \in \naturals$.
Applying the same argument to $Y$ we see that for each $x,y \in S$ and
$z,w \in T$ we have $r_{xz,yw}^n =
p^n_{xy}q^n_{zw} > 0$ for all  but finitely many $n \in \naturals$.
Thus $(X,Y)$ is irreducible and aperiodic.

If we assume that $\nu$ and $\mu$ are invariant distributions for $X$
and $Y$ respectively then it follows the fact that the transition
kernel of $(X,Y)$ is a product measure that the product measure $\nu
\otimes \mu$ is invariant for $(X,Y)$.  Now apply Proposition
\ref{RecurrenceFromInvariantDistribution} to see that $(X,Y)$ has a
recurrent state $(x,y) \in S \times T$ and Proposition
\ref{IrreducibleChainProperties} to see that $(X,Y)$ is recurrent.
\end{proof}

We now apply the coupling to compare the behavior of a pair Markov
chains with the same transition matrix but different initial
distributions.  

\begin{lem}\label{StrongErgodicityMarkovChainsDiscreteTime}Let $X$ and $Y$ be independent discrete time Markov chains
  both with state space $S$ and transition matrix $p_{xy}$ but with
  initial distributions $\nu$ and $\mu$ respectively.  If $(X,Y)$ is
  irreducible, aperiodic and recurrent then
\begin{align*}
\lim_{n \to \infty} \sup_{A \in \mathcal{S}^\infty}
  \abs{\pushforward{\theta_n}{\probabilityop_\nu}\lbrace A \rbrace -
  \pushforward{\theta_n}{\probabilityop_\mu}\lbrace A \rbrace} = 0
\end{align*}
\end{lem}
\begin{proof}
By Lemma \ref{CouplingIndependentMarkovChainsDiscreteTime} we know
that $(X,Y)$ is a Markov chain with transition matrix $p_{xy}p_{zw}$.
Let $\mathcal{F}$ be the induced filtration of $(X,Y)$ and note that
by the independence of $X$ and $Y$ 
each of $X$ and $Y$ is
Markov with respect to $\mathcal{F}$.
Consider the optional time $\tau = \min \lbrace n \in \naturals \mid
X_n = Y_n \rbrace$ and note that by recurrence of $(X,Y)$ we can apply
Lemma \ref{RecurrenceClassesMarkovChains} see that $\tau$ is almost
surely finite (in fact for every $x \in S$,  $\min \lbrace n \in \naturals \mid
X_n = Y_n = x\rbrace < \infty$ almost surely).  Let $A \in
\mathcal{S}^\infty$, then since $\tau$ is
countably valued and almost surely finite by the Strong Markov
Property applied to $X$ and $Y$, 
\begin{align*}
\cprobability {\mathcal{F}_\tau} {\theta_\tau X \in A}
&=\sprobability{A}{X_\tau}
=\sprobability{A}{Y_\tau}
=\cprobability {\mathcal{F}_\tau} {\theta_\tau Y \in A}
\end{align*}
From this and the $\mathcal{F}_\tau$-measurability of $X^\tau$ and
$\tau$ it follows that $(X^\tau, \tau, \theta_\tau X) \eqdist (X^\tau,
\tau, \theta_\tau Y)$.  Define 
$\psi : S^\infty \times \integers_+ \times S^\infty \to S^\infty$
by 
\begin{align*}
\psi(s,n,t)_m = \begin{cases}
s_m & \text{if $m < n$} \\
t_{m-n} & \text{if $m \geq n$} 
\end{cases}
\end{align*}
and note that for $A \in \mathcal{S}$, 
\begin{align*}
\lbrace \psi_m \in A \rbrace = 
\cup_{n < m} \lbrace s_m \in A \rbrace \times \lbrace n \rbrace \times
  S^\infty 
\cup 
S^\infty \times \lbrace n \rbrace \times
\cup_{n \geq m} \lbrace s_{m-n} \in A \rbrace
\end{align*}
and therefore $\psi$ is measurable.  Define $\tilde{X} = \psi(X^\tau,
\tau, \theta_\tau Y)$ so that
\begin{align*}
\tilde{X}_n &= \begin{cases}
X_n & \text{if $n < \tau$} \\
Y_n & \text{if $n \geq \tau$} \\
\end{cases}
\end{align*}
and also note that $X = \psi(X^\tau, \tau, \theta_\tau X)$.  It
follows from the Expectation Rule that $X \eqdist \tilde{X}$ and
therefore for any $A \in \mathcal{S}^{\infty}$
\begin{align*}
\abs{\probability{\theta_n X \in A} - \probability{\theta_n Y \in A} } 
&=\abs{\probability{\theta_n \tilde{X} \in A} - \probability{\theta_n Y \in A} } \\
&=\abs{\probability{\theta_n \tilde{X} \in A; \tau > n} -
  \probability{\theta_n Y \in A; \tau > n} } 
\leq 2 \probability{\tau > n}
\end{align*}
and therefore since $\tau$ is almost surely finite we have
\begin{align*}
\lim_{n \to \infty} \sup_{A \in \mathcal{S}^{\infty}}
  \abs{\probability{\theta_n X \in A} - \probability{\theta_n Y \in A}
  } \leq 2 \lim_{n \to \infty} \probability{\tau > n} = 0
\end{align*}
\end{proof}

The proof of the existence of an invariant distribution also benefits
from the coupling argument of the previous lemma.
\begin{lem}\label{TransitionMatrixConvergeToZeroNoInvariantDistribution}Let $X$ be an irreducible aperiodic Markov chain with state
  space $S$ and transition matrix $p_{xy}$ such that
  there exists $x_0,y_0 \in S$ for which $\limsup_{n \to \infty}
  p^n_{x_0 y_0} > 0$, then an invariant distribution for $X$ exists.
\end{lem}
\begin{proof}
Take a subsequence $N$ such that $\lim_{n \to \infty} p^n_{x_0 y_0}$
exists and is positive.  By countability of $S$ we can use a diagonal
argument to pass to a further subsequence if necessary and assume that
there are non-negative constants $c_y$ for $y \in S$ with $c_{y_0} > 0$ such that 
$\lim_{n\to \infty} p^n_{x_0 y} = c_y$ along $N$ for all $y \in S$.
Note that by Fatou's Lemma 
\begin{align*}
0 < \sum_{y \in S} c_y &\leq \liminf_{n \to \infty} \sum_{y \in S}
                            p^n_{x_0 y} = 1
\end{align*}

Claim: $\lim_{n \to \infty} p^n_{xy} = c_y$ along $N$ for all $x,y \in
S$.

The proof of the claim uses the coupling argument.  Pick an $x \in S$
and let $Y$ be an
Markov chain  independent of $X$ with transition matrix $p_{xy}$ and
initial distribution $\delta_x$, then
$Y$ is
also irreducible and aperiodic thus it follows from Lemma
\ref{CouplingIndependentMarkovChainsDiscreteTime}
that $(X,Y)$ is an irreducible and aperiodic Markov chain with
transition matrix $r_{xz,yw} = p_{xy} p_{zw}$.  Suppose
that $(X,Y)$ is transient then it follows from Proposition
\ref{RecurrenceTransienceDiscreteTime} that 
\begin{align*}
\sum_{n=1}^\infty (p^n_{x_0y_0})^2 &= \sum_{n=1}^\infty r^n_{x_0x_0,y_0y_0} < \infty
\end{align*}
which would imply $\lim_{n \to \infty} p^n_{x_0y_0} = 0$ which is a
contradiction. Thus we know that $(X,Y)$ is recurrent and we may apply
Lemma \ref{StrongErgodicityMarkovChainsDiscreteTime} to conclude that
$\lim_{n \to \infty} (p^n_{xy} - p^n_{x_0 y}) = 0$ for all $y \in S$
and therefore the claim follows.

Now note from the Chapman Kolomogorov relation that for each $x,y \in
S$ and $n \in \naturals$
\begin{align*}
\sum_{z \in S} p^n_{xz}p_{zy} &= p^{n+1}_{xy} = \sum_{z \in S} p_{xz}p^n_{zy}
\end{align*}
Note that $p_{xz}p^n_{zy} \leq p_{xz}$ and $\sum_{z \in S} p_{xz} = 1$
and so we may use Dominated Convergence when taking limits in the
second sum.  In the first sum we can only use Fatou so we get
\begin{align*}
\sum_{z \in S} c_z p_{zy} &\leq \lim_{n \to \infty} \sum_{z \in S}
                           p^n_{xz}p_{zy} 
= \lim_{n \to \infty} \sum_{z \in S}p_{xz}p^n_{zy} 
= c_y \sum_{z \in S}p_{xz}  = c_y
\end{align*}
where all of the limits are taken along the subsequence $N$.
Now suppose we have a strict inequality for some $y \in S$, then
summing over $y$ and using Tonelli's Theorem 
and the finiteness of $\sum_{z \in S} c_z$ we get
\begin{align*}
\sum_{z \in S} c_z &= \sum_{y \in S}\sum_{z \in S} p^n_{xz}p_{zy} =
                     \sum_{z \in S} \sum_{y \in S} p^n_{xz}p_{zy} < \sum_{z \in S} c_z
\end{align*}
which is a contradiction.  Thus we in fact have $\sum_{z \in S} c_z
p_{zy} = c_y$ for all $y \in S$.  We have observed that $\sum_{z \in S}
c_z > 0$ and therefore we may define $\nu(x) = c_x / \sum_{z \in S}
c_z$ to get an invariant distribution.
\end{proof}

It remains to assemble the pieces into the proof of the theorem.
\begin{proof}
By Lemma \ref{TransitionMatrixConvergeToZeroNoInvariantDistribution}
if $X$ has no invariant distribution then $\limsup_{n \to \infty}
p^n_{xy} = 0 \leq \liminf_{n \to \infty} p^n_{xy}$ for all $x,y \in
S$; thus $\lim_{n \to \infty} p^n_{xy} = 0$ for all $x,y \in S$.  Now
suppose that an invariant distribution $\nu$ exists.  Since $X$ is
irreducible we know that
$\nu(x) > 0$ for all $x \in S$ by Proposition
\ref{IrreducibleChainProperties}.  Furthermore by the existence of
$\nu$ and Lemma \ref{CouplingIndependentMarkovChainsDiscreteTime}, if
we let $Y$ be an independent discrete time chain with transition
matrix $p_{xy}$ and initial distribution $\nu$ we
know that $(X,Y)$ is irreducible, aperiodic and recurrent.  Thus we
may apply Lemma \ref{StrongErgodicityMarkovChainsDiscreteTime} and the
fact that $\pushforward{\theta_n}{\probabilityop_\nu} = \nu$ to
conclude that 
\begin{align*}
\lim_{n \to \infty} \sup_{A \in \mathcal{S}^\infty} \abs{\pushforward{\theta_n}{\probabilityop_\mu}\lbrace A \rbrace -
  \sprobability{A}{\nu}} = 0
\end{align*}

To see uniqueness of $\nu$ suppose that we have a second invariant
distribution $\tilde{\nu}$ and note that by invariance of
$\tilde{\nu}$ and the convergence property \eqref{ConvergenceToInvariantDiscreteTimeMarkovChain}
$\sup_{A \in \mathcal{S}^\infty} \abs{\sprobability{A}{\tilde{\nu}}-
  \sprobability{A}{\nu}} = 0$ which implies $\nu =\tilde{\nu}$.
\end{proof}

\begin{prop}Let $X$ be a discrete time Markov chain with state space
  $S$ and let $x,y \in S$ with $y$ aperiodic then it follows that
\begin{align*}
\lim_{n \to \infty} p^n_{xy} &= \frac{\sprobability{\tau^+_y < \infty}{x}}{\sexpectation{\tau^+_y}{y}}
\end{align*}
\end{prop}
\begin{proof}
Let's first consider the case in which $x=y$.  Suppose that $x$ is
transient.  In that case Proposition
\ref{RecurrenceTransienceDiscreteTime} implies $\sum_{n=1}^\infty
p^n_{xx} = \sexpectation{\kappa_x}{x} < \infty$ and thus
$\lim_{n\to \infty} p^n_{xx} = 0$.  Moreover when $x$ is transient we
know that $\sprobability{\tau^+_x = \infty}{x} = 1-r_{xx} > 0$ and
therefore $\sexpectation{\tau^+_x}{x} = \infty$ and therefore the
result holds in this case.  So we now suppose that $x$ is recurrent.
Let $S_x = \lbrace y \in S \mid r_{xy} > 0 \rbrace$ be the irreducible
component containing $x$.  We may restrict $X$ to $S_x$ and then by
Proposition \ref{RecurrenceClassesMarkovChains} it follows that the
restriction is irreducible and recurrent and by Proposition
\ref{IrreducibleChainProperties} it follows that the restriction is
aperiodic.  Now we may apply Theorem
\ref{StrongErgodicityMarkovChainsDiscreteTime} to conclude that $\lim
p^n_{xx}$ exists.

Note that if we let $\xi_1 = \tau^+_x$ and $\xi_{n+1} = \tau^{n+1}_x -
\tau^{n}_x$ for $n \in \naturals$ then by the Strong Markov property
the $\xi_n$ are an i.i.d. sequence with respect to
$\probabilityop_x$.  Moreover $\sexpectation{\tau^+_x} < \infty$
(TODO: I don't believe I've shown this...)

TODO:  Finish
\end{proof}

\begin{defn}Let $P$ be a finite discrete time Markov chain on $S$, we say a function
  $h : S \to \reals$ is \emph{harmonic} if for all $x \in S$, $\sum_{y
    \in S} P(x,y) h(y) = h(x)$.
\end{defn}

\begin{lem}\label{MarkovChainHarmonicFunctionConstant}Let $P$ be an irreducible finite Markov chain on $S$ and let $h :
  S \to \reals$ be harmonic, then $h$ is constant.
\end{lem}
\begin{proof}
Let $M$ be the maximum value of $h$ and let $x \in S$ be such that
$h(x) = M$.  Suppose there exists $y \in S$ such that $P(x,y) > 0$ and
$h(y) < M$.  It would then follow that
\begin{align*}
M &= h(x) = \sum_{y \in S} h(x) P(x,y) < M \sum_{y \in S} P(x,y) = M
\end{align*}
which is a contradiction.  Thus we know that $h(y) = M$ for all $y \in
S$ such that $P(x,y) > 0$.  Now we do an induction.  Suppose $h(y) =
M$ for all $y \in S$ such that $P^{n-1}(x, y) > 0$ and suppose $z \in
S$ is such that $P^n(x,z) > 0$.  It follows from the expression of
matrix multiplication $P^n(x,z) = \sum_{y \in S} P^{n-1}(x,y) P(y,z)$
that there exists a $y \in S$ such that $P^{n-1}(x,y) > 0$ and $P(y,z)
> 0$.  So by the induction hypothesis we know that $h(y) = M$ and by
replaying the case of $n=1$ with $y$ we get that $h(z) = M$.  

By irreducibility we know that for every $y \in S$, there exists $n
\geq 0$ such that $P^n(x,y) > 0$ and thus we have
$h(y) = M$  for every $y \in S$.
\end{proof}

\begin{lem}Let $P$ be an irreducible finite Markov chain, if the
  invariant distribution exists, then is unique.
\end{lem}
\begin{proof}
Let $I$ denote the $\card{S} \times \card{S}$ identity matrix.
By Lemma \ref{MarkovChainHarmonicFunctionConstant} we know that the
matrix $P -I$ has a one dimensional null space given by the constant
functions.  Thus column rank of $P - I$ is $\card{S} - 1$ and the same
is true for the row rank; thus there is a unique solution of $\pi (P
-I) = 0$ that satisfies $\sum_{x \in S} \pi(x) = 1$.  Note that this
does not guarantee the existence of a invariant distribution as that
requires that the entries of $\pi$ be non-negative.
\end{proof}

\begin{lem}\label{DetailBalanceEquationImpliesInvariance}If $\pi(x) P(x,y) = \pi(y) P(y,x)$ for all $x,y \in S$ then
  $\pi \cdot P = \pi$.
\end{lem}
\begin{proof}
This is a simple computation for each $y \in S$,
\begin{align*}
(\pi \cdot P)(y) &=\sum_{x \in S} \pi(x) P (x,y)  = \sum_{x \in S} \pi(y) P(y,x) =
\pi(y) \sum_{x \in S} P(y,x) = \pi(y)
\end{align*}
\end{proof}

The detail balance equation says ``the probability of starting at $x$
and making a transition to $y$ is equal to the probability of starting
at $y$ and making a transition to $x$''.  To be more concise we may
say that with starting distribution $\pi$, the probability of a trajectory $x \to y$ is the same as the
probability of a trajectory $y \to x$.  This is a type of symmetry
that is sometime described as the equivalence running the chain
forward and running the chain backward.  By induction it is not hard
to see that this symmetry extends to reversing trajectories of
arbitrary finite length.  We shall prove something more general by
showing how to ``reverse'' a Markov chain that doesn't necessarily
satisfy the detail balance equations.

\begin{defn}The \emph{time reversal} of an irreducible Markov chain
  with transition matrix $P$ and invariant distribution $\pi$ is
  given by 
\begin{align*}
\hat{P}(x,y) &= \frac{\pi(y) P(y,x)}{\pi(x)}
\end{align*}
\end{defn}

\begin{lem}The time reversal is a stochastic matrix and $\pi$ is
  invariant for $\hat{P}$.  Moreover, for every $x_0, \dotsc, x_n \in
  S$, we have
\begin{align*}
\sprobability{X_0 = x_0; \dotsb ; X_n = x_n}{\pi} &=
\sprobability{\hat{X}_0 = x_n ; \dotsb ; \hat{X}_n = x_0}{\pi}
\end{align*}
\end{lem}
\begin{proof}
By stationarity of $\pi$ with respect to $P$ for all $x \in S$,
\begin{align*}
\sum_{y \in S} \hat{P}(x,y) &=\sum_{y \in S} \frac{\pi(y)
  P(y,x)}{pi(x)} \frac{1}{\pi(x)} \sum_{y \in S} \pi(y)  P(y,x) = 1
\end{align*}
To see $\pi$ is invariant for $\hat{P}$, compute for all $y \in S$,
\begin{align*}
(\pi \cdot \hat{P})(y) &= \sum_{x \in S} \pi(x) \hat{P}(x,y) = \sum_{x
  \in S} \pi(y) P(y,x) =\pi(y)
\end{align*}
The last fact follows from an induction argument where the case $n=1$
is the definition of the time reversal matrix $\hat{P}$.  If we assume
that the result holds for $n-1$ then
\begin{align*}
\sprobability{X_0 = x_0; \dotsb ; X_n = x_n}{\pi}  &= \pi(x_0) P(x_0,
x_1) \dotsb P(x_{n-1}, x_n) \\
&= \hat{P}(x_1, x_0) \pi(x_1) P(x_1, x_2) \dotsb P(x_{n-1},x_n) \\
&= \hat{P}(x_1, x_0) \pi(x_n) \hat{P}(x_n, x_{n-1}) \dotsb \hat{P}
(x_2,x_1) \\
&= \sprobability{\hat{X}_0 = x_n ; \dotsb ; \hat{X}_n = x_0}{\pi}
\end{align*}
\end{proof}

\section{Poisson Process}
The Poisson process is the standard example of a continuous time
stochastic process that has discontinuous sample paths.  It is a
Markov process and is (almost) a martingale.  

\subsection{Poisson Random Variables}

\begin{defn}A Poisson distribution of rate $r$ is the probability
  measure on $\integers_+$ given by $\probability{n} = e^{-r}
  \frac{r^n}{n!}$. A random variable $\xi$ whose law is a Poisson
  distribution is said to be a \emph{Poisson random variable}.
\end{defn}

\begin{prop}\label{MomentsPoissonDistribution}The Poisson distribution of rate $r$ is a probability measure with mean $r$ and variance $r$.
\end{prop}
\begin{proof}
To see that the Poisson distribution is a probability measure simply note that $\probability {\integers_+} = \sum_{n=0}^\infty  e^{-r} \frac{r^n}{n!} =1$ by the power series for the exponential function.  To calculate the mean 
\begin{align*}
e^{-r} \sum_{n=0}^\infty  n \frac{r^n}{n!} = e^{-r} \left( \sum_{n=0}^\infty  (n+1) \frac{r^n}{n!} - e^r \right) = e^{-r} \frac{d}{dr} re^r - 1 = r
\end{align*}
to calculate the variance we calculate the second moment in a similar way,
\begin{align*}
e^{-r} \sum_{n=0}^\infty  n^2 \frac{r^n}{n!} = e^{-r} \sum_{n=0}^\infty  (n+2) (n+1) \frac{r^n}{n!} - 3 r - 2  
= e^{-r} \frac{d^2}{dr^2} r^2 e^r - 3 r - 2  = 2 + 4 r + r^2 - 3r - 2 = r^2 + r
\end{align*}
from which it follows that variance is $r^2 + r - r^2 = r$.
\end{proof}

\subsection{Exponential Random Variables}
The standard construction of the Poisson process uses sums of a sequence of
i.i.d. exponential random variables so it is therefore useful to
discuss such random variables first.  As explained below exponential
random variables will figure prominently in subsequent theory of
Markov processes as well so it will be a good investment of time to
get familiar with them.

\begin{defn}Given a parameter $\lambda > 0$, the probability measure
  on $\reals_+$ given by $\mu(A) = \lambda \int_A e^{-\lambda x} \,
  dx$ is called the \emph{exponential distribution with rate
    $\lambda$}.  A random variable $\xi$ whose law is an exponential
  distribution is said to be a \emph{exponential random variable}.
\end{defn}

The reader may have learned at some point that incandescent lightbulbs
have peculiar property; the probability that such a light bulb will
fail does not depend on the age of the light bulb.  Expressed using
our notation, if we let $\xi$ be age of a light bulb when it fails we
are saying that for all $t > s$ we have $\cprobability{\xi > s}{\xi >
  t} = \probability{\xi > t -s}$ or equivalently 
\begin{align*}
\probability{\xi > t} &= \probability{\xi > t ; \xi > s} = \cprobability{\xi > s}{\xi >
  t} \probability{\xi > s}= \probability{\xi > t -s}\probability{\xi > s}
\end{align*}

While the stated fact about light bulbs is only
approximately true, it is a concrete illustration of a property
that we call memorylessness.
The reason that exponential random variables figure so prominently in
subsequent theory is that they are precisely the random variables that
have the property of being memoryless.
\begin{prop}\label{ExponentialMemoryless}Let $\gamma$ be an
  exponential random variable then for each $t,s \geq 0$ we have the
  functional equation
\begin{align}
\probability{\gamma > t+s} &= \probability{\gamma > t}\probability{\gamma > s}
\label{ExponentialDistributionFunctionalEquation}\end{align}
Moreover if $\gamma$ is a nonnegative random variable that is not almost surely
equal to $0$ and satisfies \eqref{ExponentialDistributionFunctionalEquation}, it
follows that $\gamma$ is exponential.
\end{prop}
\begin{proof}
The memorylessness property of exponential random variable is a
trivial computation, 
\begin{align*}
\probability{\gamma > t+s} &= e^{-\lambda(t+s)} = e^{-\lambda
  t}e^{-\lambda s} = \probability{\gamma >
  t}\probability{\gamma > s}
\end{align*}

If we let $\probability{\gamma > 1}= e^{-c}$ for some $c \in [0,
\infty]$, then from the functional equation
\eqref{ExponentialDistributionFunctionalEquation}
we immediately see that for every $n \in
\naturals$, $\probability{\gamma > n}=\probability{\gamma >
  1}^n= e^{-cn}$ and then for all
positive rationals $p/q \in \rationals_+$ we have
$\probability{\gamma > p/q} = e^{-cp/q}$.  Now since
$\probability{\gamma > t}$ is right continuous we can conclude that
$\probability{\gamma > t}= e^{-ct}$ for all $0 \leq t < \infty$.
By our assumption that there exists some $t \geq 0$ such that
$\probability{\gamma > t} > 0$
it follows that $c < \infty$ and we have shown that $\gamma$ is
exponentially distributed.
\end{proof}

\begin{prop}\label{SumOfExponentialMemoryless}Let $\gamma_1, \dotsc, \gamma_n$ be a sequence of
  i.i.d. exponential random variables with rate $\lambda$ then for all
  $t > s$ we have
\begin{align*}
\probability{\gamma_1 +\dotsb + \gamma_n > t; \gamma_1 > s} &=
\probability{\gamma_1 +\dotsb + \gamma_n > t-s} \probability{\gamma_1 > s} 
\end{align*}
\end{prop}
\begin{proof}
We proceed by induction.  The initial case is just the memorylessness
of a single exponential random variable.  For $n \geq 2$ we compute using Fubini's theorem (specifically Lemma
\ref{DisintegrationIndependentLaws}) and the non-negativity of
exponential random variables
\begin{align*}
&\probability{\gamma_1 + \dotsb + \gamma_n > t ; \gamma_1 > s} \\
&= \probability{\gamma_1 + \dotsb + \gamma_n > t ; \gamma_1 > s;
  \gamma_n < t-s} + \probability{\gamma_1 + \dotsb + \gamma_n > t ; \gamma_1 > s;
  \gamma_n \geq t-s}  \\
&= \expectation{\probability{\gamma_1 + \dotsb + \gamma_{n-1} > t-u ;
    \gamma_1 > s} \mid_{u = \gamma_n} ;
  \gamma_n < t-s} \\
&+ \probability{\gamma_1 + \dotsb + \gamma_n > t ; \gamma_1 > s;
  \gamma_n \geq t-s}  \\
&= \expectation{\probability{\gamma_1 + \dotsb + \gamma_{n-1} > t-u-s}
  \mid_{u = \gamma_n} ; 
  \gamma_n < t-s} \probability{\gamma_1 > s} \\
&+ \probability{\gamma_n \geq t-s} \probability{\gamma_1 > s} \\
&= \left(\probability{\gamma_1 + \dotsb + \gamma_{n} > t-s;  \gamma_n < t-s}
+ \probability{\gamma_n \geq t-s} \right) \probability{\gamma_1 > s} \\
&= \probability{\gamma_1 + \dotsb + \gamma_{n} > t-s}\probability{\gamma_1 > s} 
\end{align*}
\end{proof}

It is also worth having the density and cumulative distribution of a
sum of i.i.d. exponential random variables handy
\begin{prop}Let $\gamma_1, \dotsc, \gamma_n$ be i.i.d. exponential
  random variables with rate $\lambda$, then the density of $\gamma_1
  + \dotsb + \gamma_n$ is $\lambda^n e^{-\lambda t}
  \frac{t^{n-1}}{(n-1)!}$ and 
\begin{align*}
\probability{\gamma_1 + \dotsb + \gamma_n > t} = e^{-\lambda t}
\sum_{j=1}^{n-1} \frac{\lambda^k t^k}{k!}
\end{align*}
\end{prop}
\begin{proof}
Straightforward induction calculation using the convolution formula.
\end{proof}

We are now in a position to show that Poisson processes exist.  
\begin{thm}\label{HomogeneousPoissonProcessExistence}Let $\gamma_1,
  \gamma_2, \dotsc$ be i.i.d. exponential random variables with rate
  $\lambda > 0$.  For each $n \in \naturals$ define $S_n = \gamma_1 +
  \dotsc + \gamma_n$ and for each $t \in \reals_+$ let 
\begin{align*}
N_t = \max  \lbrace n \in \naturals \mid S_n \leq t \rbrace
\end{align*}
where the maximum of the empty set is taken to be $0$.  
Then $N$ is a homogeneous Poisson process with rate $\lambda$.
\end{thm}
\begin{proof}
Since $\lbrace N_t \geq m \rbrace = \lbrace S_m \leq t \rbrace$ the
measurability of $N_t$ follows from the measurability of the
$\gamma_n$ and thus $N$ is a stochastic process.  

It remains to
show that $N$ has independent increments.  Let $0 \leq s < t < \infty$
and consider the computation of $\cprobability{\mathcal{F}_s}{N_t
  - N_s \in \cdot}$. 
Let $\mathcal{F}$ be the filtration generated by $N$, let
$\mathcal{G}_0 = \lbrace \emptyset, \Omega \rbrace$ and for each $n
\in \naturals$ let $\mathcal{G}_n = \sigma(\gamma_1, \dotsc,
\gamma_n)$.   We wish to do this computation locally on events of the
form $\lbrace N_s = n \rbrace$ for $n \in \integers_+$ by reducing to
a conditional expectation with respect to $\mathcal{G}_n$.  

Rather than appealing to the general Lemma
\ref{ConditionalExpectationIsLocal} we use the following simple version.

Claim: Let $0 \leq s < \infty$, $n \in \integers_+$ and $A \in
\mathcal{F}_s$.  There exists a $B \in \mathcal{G}_n$ such that $A
\cap \lbrace N_s = n \rbrace = B \cap \lbrace N_s = n \rbrace$.

Note first that the set $\mathcal{C}$ of all $A \in \mathcal{F}_s$ for which an
appropriate $B \in \mathcal{G}_n$ exists is a $\sigma$-algebra.  This
is elementary since if $A \cap \lbrace N_s = n \rbrace = B \cap \lbrace
N_s = n \rbrace$ then it follows that $A^c \cap \lbrace N_s = n \rbrace = B^c \cap \lbrace
N_s = n \rbrace$ and morever if $A_m \cap \lbrace N_s = n \rbrace = B_m \cap \lbrace
N_s = n \rbrace$ for all $m \in \naturals$ then 
\begin{align*}
  \left(\cup_{m=1}^\infty A_m \right) \cap \lbrace N_s = n \rbrace
  &=
    \cup_{m=1}^\infty
    \left(A_m
    \cap
    \lbrace
    N_s
    = n
    \rbrace\right)
    =
    \cup_{m=1}^\infty
    \left(B_m
    \cap
    \lbrace
    N_s
    = n
    \rbrace\right) \\
  &=
    \left
    (\cup_{m=1}^\infty
    B_m\right)
    \cap
    \lbrace
    N_s = n \rbrace
\end{align*}
Since $\mathcal{C}$ is a $\sigma$-algebra is suffices to that that
$\lbrace N_u = m \rbrace \in \mathcal{C}$ for all $0 \leq u \leq s$
and $m \in \integers_+$ since such sets generate $\mathcal{F}_s$.  To
see this note that $\lbrace N_u = m \rbrace
\cap \lbrace N_s = n \rbrace = \emptyset$ for $m > n$, $\lbrace N_u = m \rbrace
\cap \lbrace N_s = n \rbrace = \lbrace S_m \leq u < S_{m+1} \rbrace
\cap \lbrace N_s = n \rbrace$ for $m < n$ and $\lbrace N_u = n \rbrace
\cap \lbrace N_s = n \rbrace = \lbrace S_n \leq u \rbrace
\cap \lbrace N_s = n \rbrace$.

We now use the claim to calculate  $\cprobability{\mathcal{F}_s}{N_t
  - N_s \in \cdot}$.  Let $A \in \mathcal{F}_s$ and for each $n \in \integers_+$
we pick $B_n \in \mathcal{G}_n$ such that $A \cap \lbrace N_s = n
\rbrace = B_n \cap \lbrace N_s = n \rbrace$.  We let $k \in
\integers_+$ and use the definition of $N_t$, the independence of the
$\gamma$, Lemma \ref{DisintegrationIndependentLaws} and Proposition \ref{SumOfExponentialMemoryless}
\begin{align*}
  &\probability{N_t - N_s \leq k; A} 
    = \sum_{n=0}^\infty \probability{N_t - N_s \leq k; N_s = n ; A} 
    =\sum_{n=0}^\infty \probability{N_t- N_s \leq k; N_s = n ; B_n}  \\
  &=\sum_{n=0}^\infty \probability{S_{n+k+1} > t; S_{n+1} > s; s \geq S_n ; B_n}  \\
  &=\sum_{n=0}^\infty \expectation{\probability{\gamma_{n+1} + \dotsb
    + \gamma_{n+k+1} > t-u; \gamma_{n+1} > s-u}\mid_{u=S_n}; s \geq S_n ; B_n}  \\
  &=\sum_{n=0}^\infty \expectation{\probability{\gamma_{n+1} + \dotsb
    + \gamma_{n+k+1} > t-s}\probability{\gamma_{n+1} > s-u}\mid_{u=S_n}; s \geq S_n ; B_n}  \\
  &=\probability{\gamma_{1} + \dotsb
    + \gamma_{k+1} > t-s} \sum_{n=0}^\infty \probability{S_{n+1} > s; s \geq S_n ; B_n}  \\
  &=\probability{\gamma_{1} + \dotsb
    + \gamma_{k+1} > t-s} \sum_{n=0}^\infty \probability{N_s=n ; B_n}  \\
  &=\probability{\gamma_{1} + \dotsb
    + \gamma_{k+1} > t-s} \sum_{n=0}^\infty \probability{N_s=n ; A}  \\
  &=\probability{\gamma_{1} + \dotsb
    + \gamma_{k+1} > t-s} \probability{A}  \\
\end{align*}
which shows that 
\begin{align*}
\cprobability{\mathcal{F}_s}{N_t  - N_s \leq k} 
&= \probability{\gamma_{1} + \dotsb  + \gamma_{k+1} > t-s}
= e^{-\lambda(t-s)} \sum_{j=0}^{k} \frac{\lambda^j(t-s)^j}{j!}
\end{align*}
Since the conditional probability is a constant it follows that $N_t -
N_s \Independent \mathcal{F}_s$ and moreover by taking expectations it
follows that $N_t - N_s$ is
Poisson distributed with rate $\lambda(t-s)$.
\end{proof}

A homogeneous Poisson process provides us with another important
example of a continuous time martingale.
\begin{prop}\label{CompensatedHomogeneousPoissonProcessMartingale}Let $N$ be a homogeneous Poisson process with rate
  $\lambda$ then $N_t - \lambda t$ is a cadlag martingale.
\end{prop}
\begin{proof}
It is clear that $N_t - \lambda t$ is a cadlag process, moreover since
$N_t$ is Poisson distributed with rate $\lambda t$ it follows that
$N_t - \lambda t$ is integrable and has mean zero.  The martingale
property follows from the independent increments property
\begin{align*}
\cexpectationlong{\mathcal{F}_s}{N_t} 
&=\cexpectationlong{\mathcal{F}_s}{N_t - N_s}  + N_s = \lambda(t-s) + N_s
\end{align*}
\end{proof}

\section{Pure Jump-Type Markov Processes}
In this section we discuss a simple subclass of time homogeneous
Markov Processes on $\reals_+$.
\begin{defn}A time homogenous Markov process on $\reals_+$ with values
  in a metric (topological?) space $(S, \mathcal{B}(S))$ is said to be \emph{pure
    jump-type} if almost surely its sample paths are piecewise
  constant with isolated jump discontinuities.
\end{defn}

The first goal is to get a more constructive description of the class
of pure jump-type Markov processes.  The key idea in achieving that goal
is to study the random time to the jumps of the process; in fact these
random times are optional with respect to the right continuous
filtration generated by the process.

\begin{defn}Let $X$ be a pure jump-type Markov process then the
  \emph{first jump time} is the random time
\begin{align*}
\tau_1 &=\inf \lbrace t \geq 0 \mid X_t \neq X_0 \rbrace
\end{align*}
 the \emph{$n^{th}$ jump time} is defined to be 
\begin{align*}
\tau_n &= \tau_{n-1} + \tau_1 \circ \theta_{\tau_{n-1}} = \inf \lbrace
t \geq \tau_{n-1} \mid X_t \neq X_{\tau_{n-1}} \rbrace \text{ for $n > 1$}
\end{align*}
and the \emph{$0^{th}$ jump time} is $\tau_0 = 0$.
\end{defn}

\begin{lem}Let $X$ be a pure jump-type Markov process then
$\tau_n$ is a weakly $\mathcal{F}$-optional time for all $n \geq 0$.
\end{lem}
\begin{proof}
The case $\tau_0$ is trivial as it is a deterministic time.
For each $n \in \naturals$, define $\sigma_n = \min \lbrace k/2^n \mid
X_{k/2^n} \neq X_0 \rbrace$.  Note that because of the right
continuity of sample paths of $X$ we have $\sigma_n \downarrow
\tau_1$.  Moreover we have
\begin{align*}
\lbrace \sigma_n \leq t \rbrace &= \cup_{k=0}^{\floor{2^n t}}
\lbrace X_{k/2^n} \neq X_0 \rbrace 
\in \mathcal{F}_{\floor{2^n t}/2^n} \subset \mathcal{F}_t
\end{align*}
and therefore $\sigma_n$ is $\mathcal{F}$-optional.  Therefore by
Lemma \ref{InfSupStoppedFiltration} we see that $\tau_1 = \lim_{n
  \to\infty} \sigma_n = \inf_n \sigma_n$ is weakly
$\mathcal{F}$-optional.

The fact that $\tau_{n}$ is weakly optional follows by induction using
Lemma
\ref{TimeShiftOptionalTimes} applied to the expression $\tau_n =
\tau_{n-1} + \tau_1 \circ \theta_{\tau_{n-1}}$.
\end{proof}

The definition of the optional time $\tau_1$ allows us to define an
important property of elements of $S$.
\begin{defn}A state $x \in S$ is said to be \emph{absorbing} if
  $\probability{X_t \equiv x} = \sprobability{\tau_1 = \infty}{x} =
  1$.  If $x$ is not absorbing we say it is \emph{non-absorbing}.
\end{defn}
By the Markov property we see that if a pure jump-type Markov process
$X$ reaches an absorbing state $x$ it remains there indefinitely
almost surely.  If $X$ is in a non-absorbing state one might ask
whether there is a positive probability that it remains there forever
(i.e. the state is ``partially absorbing'').  In fact in a
non-absorbing state it is almost sure that a jump to a new state will
occur (a kind of 0-1 law).  This fact is a corollary of the following
result that describes the distribution to the next jump from a
non-absorbing state.
\begin{lem}\label{PureJumpFirstJumpTime}Let $X$ be a pure jump-type Markov process and let $x \in
  S$ be nonabsorbing, then under $\probabilityop_x$ the optional time
  $\tau_1$ is exponentially distributed and independent of $\theta_{\tau_1}X$.
\end{lem}
\begin{proof}
To see that $\tau_1$ is exponentially distributed note that
\begin{align*}
\sprobability{\tau_1 > t+s}{x} &= \sprobability{\tau_1 > s; \tau_1
  \circ \theta_s > t}{x} = \sprobability{\tau_1 > s}{x} \sprobability{\tau_1 > t}{x} 
\end{align*}
By our assumption that $x$ is nonabsorbing we know that $\tau_1 > 0$
with positive probability and therefore we can apply Proposition
\ref{ExponentialMemoryless} to conclude that $\tau_1$ is
exponentially distributed.

Recall from Lemma
\ref{HittingTimesContinuous} that when restricted to
$D([0,\infty);S)$, one can think of $\tau_1$
as being a composition of the process $X$ with a measurable function on
$S^{[0,\infty)}$ which we call $\tilde{\tau}_1$
(of course if $X$ is the canonical process $\tau_1=\tilde{\tau}_1$).  
Let $B$ be a measurable set in $S^{[0,\infty)}$ and define the set
\begin{align*}
\tilde{B} &= \lbrace f \in D([0,\infty) \mid \theta_{\tilde{\tau}_1(f)} f \in
B \rbrace
\end{align*}
By writing the indicator of $\tilde{B}$
as the composition 
\begin{align*}
D([0,\infty) ; S) \overset{(id, \tilde{\tau}_1)}  \to D([0,\infty) ; S) \times
[0, \infty)
\overset{\theta} \to D([0,\infty) ; S) \overset{\characteristic{B}}
\to \reals
\end{align*}
we see that $\tilde{B}$ is also measurable
(recall that $\theta$ as above is measurable by Lemma
\ref{MeasurabilityOfShiftOperator}).  It is also noted that we have
the equality $\lbrace X \in \tilde{B} \rbrace = \lbrace
\theta_{\tau_1} X \in B \rbrace$.

Let $\tau^t_1 = \inf \lbrace s \geq
t \mid X_s \neq X_t \rbrace$ and note that $\tau^t_1(X) =
\tilde{\tau}_1(\theta_t X) + t$.   From this we get
\begin{align*}
\left( \theta_{\tau^t_1}X \right)_s &=
X(\tilde{\tau}_1(\theta_tX) + t + s) = \left(\theta_{\tilde{\tau}_1(\theta_t X)} \theta_tX
\right)_s
\end{align*} 
Now we can compute (in rather excruciating detail I might add) using
the fact that $\tau^t_1 = \tau_1$ on the set $\lbrace \tau_1 >
t \rbrace$, the Markov Property of $X$, the definition of $\tilde{B}$ and the fact that $X_t = x$ on
$\lbrace \tau_1 > t \rbrace$ to see
\begin{align*}
\probability{\tau_1 > t ; \theta_{\tau_1}X \in B}
&=
\probability{\tau_1 > t ; \theta_{\tau^t_1}X \in B}\\
&=
\probability{\tau_1 > t ;
  \cexpectationlong{\mathcal{F}_t}{\theta_{\tilde{\tau}_1(\theta_t X)} \theta_t X \in B}}\\
&=
\probability{\tau_1 > t ;
  \cprobability{\mathcal{F}_t}{\theta_t X \in \tilde{B}}}\\
&=
\probability{\tau_1 > t ;
  \sprobability{\tilde{B}}{X_t}}\\
&= \probability{\tau_1 > t } \sprobability{\tilde{B}}{x}\\
&= \probability{\tau_1 > t } \probability{\theta_{\tau_1} X \in B}\\
\end{align*}
\end{proof}

With the distribution of first jump time available we can now see that
a the first jump time is either almost surely finite or almost surely
infinite depending on whether the process starts in a non-absorbing or
absorbing state.
\begin{cor}\label{AbsorbingDichotomy}Let $\probabilityop_x$ be a
  Markov family for pure jump-type Markov process and let $\tau_1$ be
  the first jump time then
\begin{align*}
\sprobability{\tau_1 < \infty} {x} &= \begin{cases}
0 & \text{when $x$ is non-absorbing} \\
1 & \text{when $x$ is absorbing}
\end{cases}
\end{align*}
\end{cor}
\begin{proof}
By Lemma \ref{PureJumpFirstJumpTime} we know that for $x$
non-absorbing $\sexpectation{\tau_1}{x} < \infty$ which implies
$\sprobability{\tau_1 < \infty}{x} < \infty$.  
\end{proof}

It should be noted that in the literature it is very uncommon to make
the subtle distinction between the interpretation of $\tau_1$ as
either a random variable or a function on $D([0, \infty); \reals)$.
On the one hand, authors may deal with the issue by glossing over the
distinction and abusing notation through the use of $\tau_1$ to denote
both functions.  On the other hand authors may try to define the
problem away by restricting attention to the canonical case; this
restriction later biting the reader when results proven in the
canonical case are implicitly extended to the non-canonical case.  At
some point we will start to take the abuse of notation approach
but we want to have some examples in which all of the fine
distinctions are made so that the reader can refer back to them in
times of confusion.

Based on the previous result we see that the distribution of the first
jump of a pure
jump type Markov process boils down to two independent distributions:
the first being an exponential distribution that describes when a jump
happens and the second being a general distribution that describes
where the jump goes to.  This observation can be used to give us a
nice description of the entire process.  Before providing the
construction we settle on some terminology.


\begin{defn}Given a pure jump Markov process $X$ with a first jump
  time $\tau_1$ we define the \emph{rate function} to be 
\begin{align*}
c(x) &=\begin{cases}
  1/\sexpectation{\tau_1}{x} & \text{if $x$ is non-absorbing}\\
0 & \text{if $x$ is absorbing}
\end{cases}
\end{align*}
 the \emph{jump transition kernel} to be
 \begin{align*}
\mu(x,B) &= \begin{cases}
\sprobability{\theta_{\tau_1}X \in B}{x} & \text{ if $x$ is non-absorbing} \\
\delta_x(B) & \text{ if $x$ is absorbing}
\end{cases}
\end{align*} 
and the \emph{rate
    kernel} to be $\alpha(x,B) = c(x) \mu(x,B)$.  
\end{defn}
Note that in the above definition we are thinking of the Markov
process as the family of measures $\probabilityop_x$ on
$S^{[0,\infty)}$ and interpreting $\tau_1$ as a
function from $S^{[0,\infty)}$ to $\reals_+$.

Before proceeeding to our structure theory for pure jump type Markov
processes we establish the basic measurability properties of the
functions just defined.
\begin{lem}\label{MeasurabilityRateKernel}The rate function $c(x)$ is a measurable function on $S$
  and the jump transition kernel and rate kernel are both kernels from
  $S$ to $S^{[0,\infty)}$.  The rate kernel is a measurable function
  of the jump transition kernel.
\end{lem}
\begin{proof}
We know that $\probabilityop_x$ is a kernel by Lemma
\ref{MarkovMixtures} and therefore $\sexpectation{\tau_1}{x}$ is a
measurable function of $x$ by Lemma
\ref{KernelTensorProductMeasurability}.  Lastly we see that 
\begin{align*}
\lbrace x \text{ is non-absorbing} \rbrace &= \lbrace \sprobability{\tau_1 <
  \infty}{x} = 1 \rbrace
\end{align*}
is measurable because $\probabilityop_x$ is a kernel; thus $c(x)$ is measurable.

The fact that $\mu(x,B)$ is a measurable function of $x$ for fixed $B$
follows from the fact $\probabilityop_x$ is a kernel.  The fact that
for fixed $\mu(x,B)$ is a probability measure for fixed $x$ follows
from measurability of the mapping taking $X$ to $\theta_{\tau_1}X$ and
Lemma \ref{PushforwardMeasure}.

To see that $\mu(x,B)$ is a measurable function of $\alpha(x,B)$ just
observe that 
\begin{align*}
\mu(x,B) &= \begin{cases}
\alpha(x,B)/\alpha(x, S) & \text{if $\alpha(x, S)
  \neq 0$} \\
\delta_x(B) & \text{if $\alpha(x, S) = 0$}
\end{cases}
\end{align*}
\end{proof}

Extending these ideas further we will see that every pure jump-type Markov
process decomposes into a discrete time Markov chain that describes
the state transition of the jumps that occur and a sequence of independent exponential random
variables that describe the time between jumps.   This make intuitive sense given the
last lemma and the Strong Markov property: our process begins by
waiting for an exponentially distributed time then makes an
independent jump to a new state; by the Strong Markov property the
process starts afresh in the new state waits for another independent
exponentially distributed time and makes another independent jump and
so on. 
One subtlety arises because the heuristic argument just given
ignores the fact that our process may jump into an absorbing state.
The other subtlety is that the mean time to the next jump depends on
the current state.  If we normalize by the rate function of the
current state then the means are all unity and we might be able
``integrate'' the waiting times into the single source of randomness
that a sequence of i.i.d. exponential random variables would provide.
Handling these problems and making things precise is the job of the
next theorem.

\begin{thm}Let $X$ be a pure jump Markov process with rate kernel
  $\alpha=c\mu$ and jump times $\tau_0, \tau_1, \dotsc$, then there is a Markov process $Y$ on $\integers_+$ with transition kernel $\mu$
  and a
  sequence of i.i.d. exponential random variables $\gamma_0, \gamma_1,
  \dotsc$ of rate $1$ that are independent of $Y$ such that for all $n
  \geq 1$ 
\begin{align*}
\tau_n &= \begin{cases}
\sum_{k=0}^{n-1} \frac{\gamma_k}{c(Y_k)} &\text{when $c(Y_k)
  \neq 0$ for all $k=0, \dotsc, n-1$} \\
\infty & \text{when $c(Y_k) = 0$ for some $k=0, \dotsc, n-1$}
\end{cases}
\end{align*}
and
\begin{align*}
X_t &= Y_n  \text{ a.s. for $\tau_n \leq t < \tau_{n+1}$}
\end{align*} 
when $\tau_n < \infty$.  If $\tau_n = \infty$ for some $n$ then let
$N = \max \lbrace n \mid \tau_n < \infty \rbrace$, then we have $Y_n =
Y_{N-1} = X_{\tau_{N}}$ for all $n > N$.
\end{thm}
\begin{proof}
To simply notation, in the case in which $\tau_n = \infty$ for some
$n$, let $X_\infty = X_{\tau_N}$ where $N$ is defined in the statement
of the Theorem (it is the position of $X$ after its last jump). With
that definition in hand we know that the result of the Theorem
requires that we define $Y_n = X_{\tau_n}$.  The work is in
constructing the $\gamma_n$ and validating the Markov property.

Our first real task is to understand the relationship between the condition
$\lbrace \tau_n < \infty \rbrace$ and the condition $\lbrace c(Y_{n-1})
\neq 0 \rbrace$ in order to make proper sense of the expression for
$\tau_n$.  

Claim: $\tau_n < \infty$ almost surely when $c(Y_{n-1}) \neq 0$ and
$\tau_{n-1} < \infty$
(i.e. $\probability{\tau_n < \infty;c(Y_{n-1}) \neq 0; \tau_{n-1} < \infty} =
\probability{c(Y_{n-1}) \neq 0; \tau_{n-1} < \infty}$).

First note that for any $x \in S$, by definition $c(x) \neq 0$ implies
that $\sexpectation{\tau_1}{x} < \infty$ which certainly implies that
$\sprobability{\tau_1 < \infty}{x} =1$.  Now for all $n \geq 1$ we can
calculate using the tower property and pullout property of conditional
expectations and the Strong Markov property
\begin{align*}
&\probability{\tau_n < \infty;c(Y_{n-1}) \neq 0; \tau_{n-1} < \infty} \\
&=\expectation{\cprobability{\mathcal{F}_{\tau_{n-1}}}{\tau_n <
    \infty;c(Y_{n-1}) \neq 0; \tau_{n-1} < \infty}} \\
&=\expectation{\cprobability{\mathcal{F}_{\tau_{n-1}}}{\tau_1(\theta_{\tau_{n-1}(X)}X) <
    \infty};c(Y_{n-1}) \neq 0; \tau_{n-1} < \infty} \\
&=\expectation{\sprobability{\tau_1 <
    \infty}{Y_{n-1}};c(Y_{n-1}) \neq 0; \tau_{n-1} < \infty} \\
&=\probability{c(Y_{n-1}) \neq 0; \tau_{n-1} < \infty} \\
\end{align*}
and the claim is proved.

Claim: $\lbrace c(Y_{n-1}) = 0 \rbrace$ = $\lbrace \tau_n = \infty
\rbrace$ a.s.

What does this mean?  I think $\probability{\lbrace c(Y_{n-1}) = 0
  \rbrace \triangle \lbrace \tau_n = \infty\rbrace} = 0$.
Calculate
\begin{align*}
&\probability{c(Y_{n-1}) = 0; \tau_n < \infty} \\
&=
\probability{c(Y_{n-1}) = 0; \tau_{n-1} < \infty;
  \tau_1(\theta_{\tau_{n-1}(X)}(X)) < \infty } \\
&= \probability{c(Y_{n-1}) = 0; \tau_{n-1} < \infty;
  \cprobability{\mathcal{F}_{\tau_{n-1}}}{\tau_1(\theta_{\tau_{n-1}(X)}(X)) < \infty }} \\
&= \probability{c(Y_{n-1}) = 0; \tau_{n-1} < \infty;
  \sprobability{\tau_1 < \infty }{Y_{n-1}}} = 0 \text{ by Corollary \ref{AbsorbingDichotomy}}\\
\end{align*}

TODO: Here is the crux of where I get confused.  Kallenberg says the
following:
let $\gamma_1^\prime, \gamma_2^\prime, \dotsc$ be i.i.d. exponentially
distributed of mean 1 and such that $\gamma_n^\prime \Independent X$
which means we must be willing to break out of the canonical case (this is technically not
true of Kallenberg's proof since he states that all randomization variables are assumed to exist
in the canonical process setup).  Define 
\begin{align*}
\gamma_n &= (\tau_n - \tau_{n-1})c(Y_{n-1}) \characteristic{\tau_{n} <
  \infty} + \gamma_n^\prime \characteristic{\tau_{n} =
  \infty} 
\end{align*}
and we claim that if $c(x) > 0$ then we have
\begin{align*}
\sprobability{\gamma_1 > t; Y_1 \in B}{x} &= \sprobability{\tau_1 c(x)
  > t; Y_1 \in B}{x} = e^{-t} \mu(x, B)
\end{align*}
and that if $c(x) = 0$ then 
\begin{align*}
\sprobability{\gamma_1 > t; Y_1 \in B}{x} &= \sprobability{\gamma^\prime_1 
  > t; Y_1 \in B}{x} = e^{-t} \mu(x, B)
\end{align*} 
and this is where I get hung up on a subtlety.  The measure
$\probabilityop_x$ was defined to be on the path space but $\gamma_1$
is not defined on the path space but on an extension.  Probably the
right way to make sense of this is to consider a Markov family as in
Definition \ref{MarkovFamilyDefn} and then consider $\probabilityop_x$
in that context.  Of course, we have not proven that Markov families
exist (though I believe that is implicit in the proof of Markov
processes and the proof of Daniell-Kolmogorov) nor have we proven that
Markov families are preserved under extension (see Blumenthal and Getoor for an exercise that
is sufficient for the case of extending by $[0,1]$).  In any case if we
succeed in doing that then $\probabilityop_x$ is the probability
measure on $\Omega$ under which $X$ is a Markov process with $X_0 = x$
almost surely and computation is a straightforward application of
Lemma \ref{PureJumpFirstJumpTime} and the independence of
$\gamma_n^\prime$ and $X$.  The thing that I am unsatisfied with in this context
is the fact that the statement of the result does not involve or
require Markov
families.  Also, what if $X$ has a non-point mass initial
distribution?  Of course the other issue is that Kallenberg's formula for
$\gamma_n$ is wrong!!!!  He writes
\begin{align*}
\gamma_n &= (\tau_n - \tau_{n-1})c(Y_n) \characteristic{\tau_{n-1} <
  \infty} + \gamma_n^\prime \characteristic{\tau_{n-1} =
  \infty} 
\end{align*}
\end{proof}

It is useful to turn this description of  a pure jump-type Markov
around and use it to construct a pure jump-type Markov process.  

\begin{thm}Let $\alpha = c \mu$ be a kernel on $S$ such that
  $\alpha(x, \lbrace x \rbrace) \equiv 0$, let $Y$ be a Markov chain
  with transition kernel $\mu$ and let $\gamma_1, \gamma_2, \dotsc$ be
  i.i.d. exponential random variables of mean 1 such that $\gamma_1,
  \gamma_2, \dotsc \Independent Y$.  Pick an arbitrary element $s_0
  \in S$ and define $\tau_0 = 0$ and for $n
  \in \naturals$ we define
\begin{align*}
\tau_n &= \sum_{j=1}^n  \frac{\gamma_j}{c(Y_{j-1})} \\
\intertext{and}
X_t &= \begin{cases}
Y_n \text{ for $\tau_n \leq t < \tau_{n+1}$} \\
s_0 \text{ for $t \geq \sup_n \tau_n$} \\
\end{cases}
\end{align*}
If $\lim_{n \to \infty} \tau_n = \infty$ a.s. for every initial distribution
  of $Y$ then $X$ is a pure jump-type Markov process with rate kernel $\alpha$.
\end{thm}
\begin{proof}
We consider $(Y, \gamma)$ as a Markov chain on the state space $S
\times \reals_+$.  (TODO: Show that independence of $Y$ and $\gamma_1,
\gamma_2, \dotsc$ implies this is valid).  Define $\tau_n$ and $X$ as
in the statement of the theorem, let $\mathcal{G}_n$ be the filtration
generated by $(Y, \gamma)$ and let $\mathcal{F}_t$ be the filtration
generated by $X$.

We need leverage our knowledge that $(Y,\gamma)$ is a Markov process
to show that $X$ has the Markov property.  In order to do this we want
to be able use information about conditional expectations with respect
to $\mathcal{G}$ in order to compute conditional expectations with
respect to $\mathcal{F}$; thus we first clarify the relationship
between the two filtrations.  The trick is that in general a given
$X_t$ can be equal to any $Y_n$ and therefore to restrict the set of
possible $Y_n$ we must restrict the number of jumps that occur before
$t$.  In other words we must restrict the possible values of some
$\tau_n$; in this way the random variables $\gamma_1, \gamma_2,
\dotsc$ enter the picture.

Claim: Let $t \geq 0$ and $n \in \naturals$ be fixed, then
$\mathcal{G}_n \vee \lbrace \tau_{n+1} > t \rbrace$ and
$\mathcal{F}_t$ agree on $\lbrace \tau_n \leq t < \tau_{n+1}\rbrace$.
Furthermore $\lbrace \tau_n \leq t < \tau_{n+1}\rbrace$ is
$\mathcal{G}_n \vee \lbrace \tau_{n+1} > t\rbrace\cap \mathcal{F}_t$-measurable.

The $\mathcal{G}_n \vee \lbrace \tau_{n+1} > t \rbrace$ of $\lbrace
\tau_n \leq t < \tau_{n+1}$ is immediate as $\tau_n$ is a function of
$\gamma_1, \dotsc, \gamma_n$ and $Y_0, \dotsc, Y_{n-1}$ and therefore is
$\mathcal{G}_n$ measurable.  To see $\mathcal{F}_t$-measurability
first note that, by construction, $\tau_n$ is the $n^{th}$ jumping
time of $X$ (TODO: What about the fact that we have probability zero
event that $Y_m = Y_{m+1}$?  This seems like a real issue since $X$
cannot detect $\tau_n$ unless the value of $X$ changes there.  What we
do know is that if $X$ sees $n$ jumps then at least $n$ of the timers
$\gamma$ have gone off; maybe this is enough...)
TODO:  Note that I believe Blumenthal and Getoor handle this issue as part of their construction which they redefine the probability space by 
removing the set of probability zero where $Y_m = Y_{m+1}$.  This seems to indicate that either Kallenberg needs the flexibility to much with the
probability space in a similar way.

Note that even here we have $Y$ assumed to be a Markov family and we
are constructing $X$ as a Markov family.

TODO: Finish and properly understand Kallenberg's proof here.

This is how Blumenthal and Getoor do this.  Not clear that they allow for the existence of absorbing states but they certainly do allow for explosion by appending
the cemetary state.

Let $T=S \times [0,\infty)$, $\mathcal{T} = \mathcal{S} \otimes
\borel{[0,\infty)}$, $\Omega = T^\infty$ and $\mathcal{A} =
\mathcal{T}^\infty$.  We write a generic element $\omega \in \Omega$
as $\omega = ((x_0, t_0), (x_1, t_1), \dotsc)$.  Define $Z_n : \Omega
\to T$ as $Z_n(\omega) = (x_n, t_n)$ to be the $n^{th}$ coordinate
projection and let $Y_n : \Omega \to S$ and $\tau_n : \Omega \to
[0,\infty)$ be defined by $Y_n(\omega) = x_n$ and $\tau_n(\omega) =
t_n$ respectively.  By the definition of the product $\sigma$-algebra
we know that each of $Z_n$, $Y_n$ and $\tau_n$ is measurable.  

\begin{clm} For each $A \in \mathcal{T}$ let 
\begin{align*}
\tilde{\mu}((x,t), A) &=
\int \characteristic{A}(y,s) \characteristic{[t,\infty)}(s) \, \mu(x, dy) c(x) e^{-c(x)(s-t)} ds
\end{align*}
then $\tilde{\mu}$ is a probability kernel and moreover $\tilde{\mu}$ is translation invariant in $t$.
\end{clm}
By the monotone class argument of Lemma \ref{KernelMeasurability} we can reduce to considering
set $A = B \times C$ and since 
\begin{align*}
\tilde{\mu}((x,t), B \times C) &= \mu(x, B) \int_t^\infty \characteristic{C}(s) c(x) e^{-c(x)(s-t)} ds
\end{align*}
and $\mu$ is assumed a kernel we must only show $\int_t^\infty \characteristic{C}(s) c(x) e^{-c(x)(s-t)} ds$
is a probability kernel.  The fact that it is a probability measure is elementary calculus and the fact that $c(x) > 0$,
\begin{align*}
\int_t^\infty c(x) e^{-c(x)(s-t)} ds &= e^{c(x)t} \int_t^\infty c(x) e^{-c(x)s} ds = e^{c(x)t} e^{-c(x)t} =1
\end{align*}
The fact that it is a kernel follows from Lemma \ref{KernelTensorProductMeasurability}.  To see that $\tilde{\mu}$ is translation invariant we consider
$B \times C \in \mathcal{S} \times \borel{[0,\infty)}$ and calculate as above and using a change of integration variable 
\begin{align*}
\tilde{\mu}((x,t), B \times C) &= \mu(x, B) \int_t^\infty \characteristic{C}(s) c(x) e^{-c(x)(s-t)} ds \\
&= \mu(x, B) \int_{t+v}^\infty \characteristic{C}(s-v) c(x) e^{-c(x)(s - v -t)} ds \\
&= \mu(x, B) \int_{t+v}^\infty \characteristic{C+v}(s) c(x) e^{-c(x)(s - (t+v))} ds \\
&= \tilde{\mu}((x,t+v), B \times C+(0,v))
\end{align*}
By Lemma \ref{UniquenessOfMeasure} probability measures are uniquely determined by values on a generating $\pi$-system it follows that 
$\tilde{\mu}((x,t), A) = \tilde{\mu}((x,t+v), A+(0,v))$ for all $A \in \mathcal{S} \times \borel{[0,\infty)}$.

Now can apply the Ionescu Tulcea Theorem ??? to see that for every probability measure $\nu$ on $T$ there
exists a probability measure $\probabilityop_\nu$ on $\Omega$ such that $Z_n$ is a discrete time Markov process
with initial distribution, the transition kernel $\tilde{\mu}$ and the natural filtration $\mathcal{G}_n = \sigma(Z_0, Z_1, \dotsc, Z_n)$.

Observe that for all $\alpha > 0$ we have from the tower property of conditional expectation and Theorem \ref{Disintegration} the following
\begin{align*}
\sexpectation{e^{-\alpha (\tau_{n+1} - \tau_n)}}{\nu} &= 
\sexpectation{\csexpectationlong{\mathcal{G}_n}{ e^{-\alpha (\tau_{n+1} - \tau_n)} }{\nu} }{\nu} \\
&=\sexpectation{\int e^{-\alpha (s - \tau_n) } \, \characteristic{[\tau_n,\infty)}(s) \mu(Y_n, dy) c(Y_n) e^{-c(Y_n)(s - \tau_n)} ds} {\nu} \\
&=\sexpectation{e^{(\alpha + c(Y_n)) \tau_n} c(Y_n) \int_{\tau_n}^\infty e^{-(\alpha +c(Y_n)) s} \, ds}{\nu}\\
&=\sexpectation{e^{(\alpha + c(Y_n)) \tau_n} c(Y_n) \frac{e^{-(\alpha + c(Y_n)) \tau_n}}{\alpha +c(Y_n)} }{\nu} = \sexpectation{\frac{c(Y_n) }{\alpha +c(Y_n)} }{\nu} \\
\end{align*}
By the above computation and the Monotone Convergence Theorem
\begin{align*}
0 &= \lim_{\alpha \to \infty} \sexpectation{\frac{c(Y_n) }{\alpha +c(Y_n)} }{\nu}  = \lim_{\alpha \to \infty} \sexpectation{e^{-\alpha (\tau_{n+1} - \tau_n)}}{\nu} \\
&\geq \lim_{\alpha \to \infty} \sexpectation{e^{-\alpha (\tau_{n+1} - \tau_n)} ; \tau_{n+1} \leq \tau_n }{\nu} \geq \sprobability{\tau_{n+1} \leq \tau_n }{\nu} 
\end{align*}
Thus we have $\tau_{n+1} > \tau_n$ $\nu$-a.s. for every initial measure $\nu$.  A similar but simpler computation shows
\begin{align*}
\sprobability{Y_{n+1} \neq Y_n} {\nu} &= 
\sexpectation{
\csprobability{\mathcal{G}_n}{Y_{n+1} \neq Y_n }{\nu} }{\nu}
=\sexpectation{
\int \characteristic{S \setminus \lbrace Y_n \rbrace}(y) \, \mu(Y_n , dy) 
}{\nu}  \\
&= \sexpectation{1 - \mu(Y_n, \lbrace Y_n \rbrace)}{\nu} = 1
\end{align*}
Lastly if we let $\probabilityop_x$ be the shorthand for $\probabilityop_{\delta_{(x,0)}}$ then
$\sprobability{\tau_0 = 0} {x} =1$ for all $x \in S$.  Thus let 
\begin{align*}
\tilde{\Omega} &= \cap_{n=0}^\infty \lbrace \tau_{n+1} > \tau_n \rbrace \cap \cap_{n=0}^\infty \lbrace Y_{n+1} > Y_n \rbrace \cap \lbrace \tau_0 = 0 \rbrace \\
&= \lbrace ((x_0, t_0), (x_1, t_1), \dotsc ) \mid t_0 =0, x_n \neq x_{n+1} \text{ and } t_n < t_{n+1} \text{ for all } n \in \integers_+ \rbrace
\end{align*}
and it follows that $\tilde{\Omega} \in \mathcal{A}$ and $\sprobability{\tilde{\Omega}}{x} = 1$ for all $x\in S$.

TODO: Add the definitions of all of the elements of the Markov family.
Now let 
\begin{align*}
\tilde{\Omega} &= \cap_{n=0}^\infty \lbrace Y_n \neq Y_{n+1} \rbrace \cap \cap_{n=0}^\infty \lbrace \tau_n < \tau_{n+1} \rbrace \cap \lbrace \tau_0 = 0 \rbrace
\end{align*}
From the above discussion we know that $\tilde{\Omega}$ is $\mathcal{A}$-measurable and $\sprobability{\tilde{\Omega}}{x} = 1$.  Thus we may define the restrict the
probability measures, the $sigma$-algebra $\mathcal{A}$ and the filtration $\tilde{\mathcal{G}}_n$ to $\tilde{\Omega}$ and $Z_n$ remains a Markov process which considered with the probability space $(\tilde{\Omega}, \tilde{\mathcal{A}})$ and the filtration $\tilde{\mathcal{G}}_n$.

\begin{clm}$\tilde{\mathcal{G}}_n$ is the $\sigma$-algebra generated by the coordinate projections $Z_j$ for $j=0, \dotsc, n$.
\end{clm}
TODO.

At this point we redefine $\Omega$ as $\tilde{\Omega}$  with a measurable point $\omega_\Delta$ adjoined.  For each $x \in S$, we extend the measure $\probabilityop_x$ by defining $\sprobability{Delta}{x} = 0$.  We define $\sprobabilityop{A}{\Delta} = \delta_\Delta(A)$.  Define $Y_n(\omega_\Delta) = \Delta$ and $\tau_n(\omega_\Delta) = \infty$ for all $n \in \integers_+$.

For $\omega \in \tilde{\Omega}$, let $\zeta(\omega) = \lim_{n \to \infty} \tau_n(\omega)$ which as a limit of measurable function is measurable (Lemma \ref{LimitsOfMeasurable}) ; define $\zeta(\omega_\Delta) = 0$.   For $\omega \in \tilde{\Omega}$, define
\begin{align*}
X_t(\omega) &= \begin{cases}
Y_n(\omega)  & \text{for $\tau_n(\omega) \leq t < \tau_{n+1}(\omega)$} \\
\Delta & \text{for $t \geq \zeta(\omega)$}
\end{cases}
\end{align*}
and $X_\infty(\omega) = \Delta$.  Define $X_t(\omega_\Delta) = \Delta$ for all $0 \leq t \leq \infty$.

$X_t$ is seen to be measurable by writing for each $A \in \mathcal{S}^\Delta$,
\begin{align*}
X_t^{-1} (A) &= (\lbrace \Delta \in A \rbrace \cap \lbrace t \geq \zeta \rbrace) \cup \cup_{n=0}^\infty (\lbrace Y_n \in A \rbrace \cap \lbrace \tau_n \leq t < \tau_{n+1} \rbrace)
\end{align*}
By definition and the fact that $\tau_n < \tau_{n+1}$ everywhere on $\Omega$ we see that $X_t$ is a step function on the interval $[0, \zeta)$ (in particular it is cadlag when $S$ is given the discrete topology).

Now introduce the following notation, let $\mu_\alpha(x,A) = \frac{c(x)}{\alpha + c(x)} \mu(x,A)$ and then recursively define the famility $\mu_\alpha^0(x, A) = \delta_x(A)$ and for $n \in \naturals$,
\begin{align*}
\mu_\alpha^n (x,A) &= \mu_\alpha \mu_\alpha^{n-1}(x,A) = \int \mu_\alpha(y, A) \, \mu_\alpha^{n-1}(x, dy)
\end{align*}
(note that $\mu_\alpha^1(x,A) = \int \mu_\alpha(y, A) \, \delta_x(dy) = \mu_\alpha(x,A)$ so the chosen notation is consistent).  In a similar way let 
\begin{align*}
\tilde{\mu}^n((x,t), A) &= \int \tilde{\mu}((y,s), A) \, \tilde{\mu}((x,t), dy, ds)
\end{align*}
and more generally for every positive measurable or integrable $f  : S \times [0,\infty) \to \reals$ we have
\begin{align*}
\int f(u,v) , \tilde{\mu}^n((x,t), du, dv) &= \iint f(u,v) \tilde{\mu}((y,s), du, dv) \, \tilde{\mu}((x,t), dy, ds)
\end{align*}

TODO: Move this to conditioning chapter or an exercise.
\begin{clm}If $\mu$ and $\nu$ are translation invariant kernels then so is $\mu \cdot \nu$.  Moreover if $f$ is measurable  and either non-negative or integrable then $\int f(s) \, \mu(x, ds) = \int f(s-y) \, \mu(x+y, ds)$.
\end{clm}
We prove the second claim first.  Note that by translation invariance 
\begin{align*}
\int \characteristic{A}(y) \, \mu(x,dy) &= \mu(x,A) = \mu(x+u, A+u) = \int \characteristic{A+u}(y) \, \mu(x+u,dy) \\
&= \int \characteristic{A}(y-u) \, \mu(x+u,dy)
\end{align*}
so the result holds for indicator functions.  Now apply the standard machinery.  The first claim follows from the second,
\begin{align*}
\mu \cdot \nu (x, A) &= \int \nu(y, A) \, \mu(x, dy) = \int \nu(y - u, A) \, \mu(x + u, dy) = \int \nu(y, A+y) \, \mu(x+u, dy) \\
&= \mu \cdot \nu (x+u, A+u)
\end{align*}


\begin{clm}$\int_0^\infty e^{-\lambda (s-t)} \tilde{\mu}^n((x,t), A, ds) = \mu_\lambda^n(x, A)$.
\end{clm}
By translation invariance we know that $\int_0^\infty e^{-\lambda (s-t)} \tilde{\mu}^n((x,t), A, ds) = \int_0^\infty e^{-\lambda s} \tilde{\mu}^n((x,0), A, ds)$ so it suffices to use the
right hand side.  For $n=0$ and $n=1$ we compute directly,
\begin{align*}
\int_0^\infty e^{-\lambda s} \tilde{\mu}^0((x,0), A, ds) = \int_0^\infty e^{-\lambda s} \characteristic{A}(y) \delta_{(x,0)} (dy, ds) = \characteristic{A}(x) = \mu_\alpha^0(x,A)
\end{align*}
and
\begin{align*}
\int_0^\infty e^{-\lambda s} \tilde{\mu}((x,0), A, ds) = \mu(x,A) c(x) \int_0^\infty e^{-s (\lambda + c(x)} \, ds = \mu(x,A) \frac{c(x)}{\lambda + c(x)} = \mu_\lambda(x,A)
\end{align*}
For general $n$ we use an induction argument,
\begin{align*}
\int_0^\infty e^{-\lambda s} \tilde{\mu}^{n+1}((x,0), A, ds) 
&=\iint e^{-\lambda s} \tilde{\mu}^{n}((u,v), A, ds) \tilde{\mu}((x,0), du, dv) \\
&=\iint e^{-\lambda v} \mu_\lambda^n(u, A) \tilde{\mu}((x,0), du, dv) \\
&=\int \mu_\lambda^n(u, A) c(x) \left[ \int_0^\infty e^{-v(\lambda + c(x))} dv \right ] \mu(x, du)\\
&=\int \mu_\lambda^n(u, A) \frac{c(x)}{\lambda + c(x)} \mu(x, du)\\
&=\int \mu_\lambda^n(u, A) \mu_\lambda(x, du) = \mu_\lambda^{n+1}(x,A) \\
\end{align*}

\begin{clm}\label{PureJumpTypeConstruction:Computation1}Let $g : S \to [0,\infty)$ be measurable and $\lambda \geq 0$ then for all $m \geq n$,
\begin{align*}
\csexpectationlong{\mathcal{G}_n}{g(Y_m) (e^{-\lambda \tau_m} - e^{-\lambda \tau_{m+1}})}{x} 
&=\lambda e^{-\lambda \tau_n} \int  \frac{g(y)}{\lambda + c(y)} \, \mu_\lambda^{m-n}(Y_n, dy)
\end{align*}
\end{clm}
By the Markov property we know that $\cprobability{\mathcal{G}_n}{(Y_m, \tau_m) \in A} = \tilde{\mu}^{m-n}((Y_m, \tau_m), A)$ and therefore
by Theorem \ref{Disintegration} $\cexpectationlong{\mathcal{G}_n}{f(Y_m, \tau_m, Y_n, \tau_n)} = \int f(y,s, Y_n, \tau_n) \, \tilde{\mu}^{m-n}((Y_n,\tau_n), dy, ds)$.  We compute using
two applications of this observation, the tower and pullout properties of conditional expectation and the previous claim to see
\begin{align*}
&\csexpectationlong{\mathcal{G}_n}{g(Y_m) (e^{-\lambda \tau_m} - e^{-\lambda \tau_{m+1}})}{x} 
= \csexpectationlong{\mathcal{G}_n}{g(Y_m) e^{-\lambda \tau_m} \csexpectationlong{\mathcal{G}_m} {1 - e^{-\lambda ( \tau_{m+1} - \tau_m)}}{x}}{x} \\
&= \csexpectationlong{\mathcal{G}_n}{g(Y_m) e^{-\lambda \tau_m} \left ( 1 - \int_{\tau_m}^\infty  e^{-\lambda(s - \tau_m)} c(Y_m) e^{-c(Y_m)(s - \tau_m)} \, ds \right ) }{x} \\
&= \csexpectationlong{\mathcal{G}_n}{g(Y_m) e^{-\lambda \tau_m} \left ( 1 -  c(Y_m)  e^{\tau_m(\lambda + c(Y_m))} \int_{\tau_m}^\infty  e^{-s(\lambda + c(Y_m))} \, ds \right ) }{x} \\
&= \csexpectationlong{\mathcal{G}_n}{g(Y_m) e^{-\lambda \tau_m} \left ( 1 -  c(Y_m)  e^{\tau_m(\lambda + c(Y_m))} \frac{e^{-\tau_m (\lambda + c(Y_m))}}{\lambda + c(Y_m)} \right ) }{x} \\
&= \csexpectationlong{\mathcal{G}_n}{g(Y_m) e^{-\lambda \tau_m} \frac{\lambda}{\lambda + c(Y_m)} }{x} \\
&=\iint g(y) e^{-\lambda s} \frac{\lambda}{\lambda + c(y)} \, \tilde{\mu}^{m-n}((Y_n, \tau_n), dy, ds) \\
&=\lambda e^{-\lambda \tau_m} \iint e^{-\lambda (s- \tau_m)} \frac{g(y)}{\lambda + c(y)} \, \tilde{\mu}^{m-n}((Y_n, \tau_n), dy, ds) \\
&=\lambda e^{-\lambda \tau_m} \int \frac{ g(y)}{\lambda + c(y)} \, \mu_\lambda^{m-n}(Y_n, dy) \\
\end{align*}

We also need the following similar computation
\begin{clm}\label{PureJumpTypeConstruction:Computation2}Let $g : S \to [0,\infty)$ be measurable and $\lambda \geq 0$ then for all $n \geq 0$,
\begin{align*}
\csexpectationlong{\mathcal{G}_n}{ e^{-\lambda \tau_{n+1}} g(Y_{n+1}) ; \tau_{n+1} > t }{x} &= e^{c(Y_n) \tau_n - (\lambda + c(Y_n))(t \maxop \tau_m)} \int g(y) \mu_\alpha(Y_n, dy)
\end{align*}
\end{clm}
This is just an application of the Disintegration Theorem as above
\begin{align*}
&\csexpectationlong{\mathcal{G}_n}{ e^{-\lambda \tau_{n+1}} g(Y_{n+1}) ; \tau_{n+1} > t }{x} \\
&=\iint e^{-\lambda s} g(y) \characteristic{(t,\infty)}(s) \characteristic{[\tau_n, \infty)}(s) c(Y_n) e^{-c(Y_n)(s - \tau_n)} \, ds \, \mu(Y_n, dy) \\
&=\int g(y)  c(Y_n)  e^{c(Y_n) \tau_n} \left [ \int_{t \maxop \tau_n}^\infty  e^{-(\lambda +c(Y_n)) s} \, ds \right ] \, \mu(Y_n, dy) \\
&=\int g(y)  e^{c(Y_n) \tau_n} \frac{c(Y_n)}{\lambda +c(Y_n)} e^{-(\lambda +c(Y_n)) t \maxop \tau_n}  \, \mu(Y_n, dy) \\
&= e^{c(Y_n) \tau_n-(\lambda +c(Y_n)) t \maxop \tau_n}  \int g(y) \, \mu_\lambda(Y_n, dy) \\
\end{align*}

\begin{clm}\label{PureJumpTypeConstruction:ResolventIdentity} Let $f : S \to [0,\infty)$ be Borel measurable and extend to $S^\Delta$ by defining $f(\Delta) = 0$ then if we define
\begin{align*}
R_\lambda f (x) &= \int_0^\infty e^{-\lambda s} T_s f(x) \, ds = \int_0^\infty e^{-\lambda s} \sexpectation{f(X_s)}{x}  \, ds = \sexpectation{\int_0^\infty e^{-\lambda s} f(X_s) \, ds }{x} 
\end{align*}
then we have 
\begin{align*}
R_\lambda f(x) &= \sum_{n=0}^\infty \int \frac{f(y)}{\lambda + c(y)} \, \mu_\lambda^n(x, dy) = \frac{f(x)}{\lambda + c(x)} + \int R_\lambda f(y) \mu_\lambda(x,dy)
\end{align*}
\end{clm}
Using $f(\Delta) = 0$ and the fact that $X_t=\Delta$ on $[\zeta, \infty]$ we have 
\begin{align*}
\int_0^\infty e^{-\lambda s} f(X_s) \, ds 
&=\int_0^\zeta e^{-\lambda s} f(X_s) \, ds 
= \sum_{n=0}^\infty \int_{\tau_n}^{\tau_{n+1}} e^{-\lambda s} f(Y_n) \, ds \\
&= \sum_{n=0}^\infty f(Y_n) \frac{e^{-\lambda \tau_n} - e^{-\lambda \tau_{n+1}}}{\lambda}
\end{align*}
So taking expectations using two applications of this fact, Lemma \ref{TonelliIntegralSum}, the Claim \ref{PureJumpTypeConstruction:Computation1} and the definition of $\mu_\lambda^n$ we get
\begin{align*}
R_\lambda f(x) 
&= \sexpectation{\sum_{n=0}^\infty f(Y_n) \frac{e^{-\lambda \tau_n} - e^{-\lambda \tau_{n+1}}}{\lambda}}{x} \\
&= \sum_{n=0}^\infty \int \frac{f(y)}{\lambda + c(y)} \, \mu_\lambda^n(x, dy) \\
&= \frac{f(x)}{\lambda + c(x)} + \sum_{n=1}^\infty \int \frac{f(y)}{\lambda + c(y)} \, \mu_\lambda^n(x, dy) \\
&= \frac{f(x)}{\lambda + c(x)} + \sum_{n=1}^\infty \int \left [ \int \frac{f(y)}{\lambda + c(y)} \, \mu_\lambda^{n-1}(w, dy) \right ] \, \mu_\lambda(x,dw) \\
&= \frac{f(x)}{\lambda + c(x)} + \int R_\lambda f (y) \, \mu_\lambda(x,dy) \\
\end{align*}

\begin{clm}Given $A \in \mathcal{F}^0_t$ then for all $n \in \integers_+$ there exists $A_n \in \mathcal{G}_n$ such that 
\begin{align*}
A \cap \lbrace \tau_n \leq t < \tau_{n+1} \rbrace 
&= A_n \lbrace t < \tau_{n+1} \rbrace
\end{align*}
\end{clm}
Let $\mathcal{C}$ be the set of sets in $\mathcal{A}$ satisfying the criteria of the claim; we show that $\mathcal{C}$ is a $\sigma$-algebra and that each $X_s$ for $s \leq t$ is 
$\mathcal{C}$-measurable.  First we show that $\mathcal{C}$ is a $\sigma$-algebra.  Let $A \in \mathcal{C}$ and $n \in \integers_+$.  Pick $A_n \in \mathcal{G}_n$ such that
$A \cap \lbrace \tau_n \leq t < \tau_{n+1} \rbrace = A_n \lbrace t < \tau_{n+1} \rbrace$.  Let $B_n = A_n^c \cap \lbrace \tau_n \geq t \rbrace \in \mathcal{G}_n$.    If $\omega \in A^c \cap \lbrace \tau_n \leq t < \tau_{n+1} \rbrace$ then it must be that 
\begin{align*}
\omega &\notin B_n \cap \lbrace t < \tau_{n+1} \rbrace   
= A^c_n \cap \lbrace \tau_n \geq t \rbrace \cap \lbrace t < \tau_{n+1} \rbrace 
= A^c_n  \cap \lbrace t < \tau_{n+1} \rbrace
\end{align*} 
since otherwise we would have $\omega \in A$.  Since $\omega \in  \lbrace \tau_n \geq t \rbrace \cap \lbrace t < \tau_{n+1} \rbrace = \lbrace \tau_n \leq t < \tau_{n+1}\rbrace$ it follows that $A^c \cap \lbrace \tau_n \leq t < \tau_{n+1} \rbrace \subset B_n \cap \lbrace t < \tau_{n+1} \rbrace$.  On the other hand, if $\omega \in B_n \cap \lbrace t < \tau_{n+1} \rbrace = A^c_n \cap \lbrace \tau_n \leq t < \tau_{n+1} \rbrace$ then $\omega \notin A$ since otherwise we would get that $\omega \in A \cap \lbrace \tau_n \leq t < \tau_{n+1} \rbrace = A_n \cap \lbrace t < \tau_{n+1} \subset A_n$ which contradicts $\omega \in B_n = \lbrace t \geq \tau_n \rbrace \setminus A_n$.  Therefore we conclude $A^c \cap \lbrace \tau_n \leq t < \tau_{n+1} \rbrace = B_n \cap \lbrace t < \tau_{n+1} \rbrace$ and therefore $A^c \in \mathcal{C}$.  Now let $A_1, A_2, \dotsc \in \mathcal{C}$ and $n \in \integers_+$ and $A_{j,n} \in \mathcal{G}_n$ be chosen so that $A_j \cap \lbrace \tau_n \leq t < \tau_{n+1} \rbrace = A_{j,n} \cap \lbrace t \geq \tau_n \rbrace$.  Now we have 
\begin{align*}
\left ( \cap_{j=1}^\infty A_j  \right ) \cap \lbrace \tau_n \leq t < \tau_{n+1} \rbrace 
&= \cap_{j=1}^\infty \left ( A_j \cap \lbrace \tau_n \leq t < \tau_{n+1} \rbrace \right) 
= \cap_{j=1}^\infty \left( A_{j,n} \cap \lbrace t < \tau_{n+1} \rbrace \right) \\
&= \left( \cap_{j=1}^\infty A_{j,n} \right) \cap \lbrace t < \tau_{n+1} \rbrace
\end{align*}
and since $\cap_{j=1}^\infty A_{j,n} \in \mathcal{G}_n$ we see that $\cap_{j=1}^\infty A_j \in \mathcal{C}$.  Clearly $\emptyset \in \mathcal{C}$ and it follows that $\mathcal{C}$ is a $\sigma$-algebra.  To see that $X_s$ is $\mathcal{C}$-measurable for $s \leq t$, let $A \in \mathcal{S}$ and note that 
\begin{align*}
\lbrace X_s \in A \rbrace \cap \lbrace \tau_n \leq t < \tau_{n+1} \rbrace 
&= \cup_{j=0}^n \left (\lbrace X_s \in A \rbrace \cap \lbrace \tau_j \leq s < \tau_{j+1} \rbrace \right ) \cap \lbrace \tau_n \leq t < \tau_{n+1} \rbrace  \\
&= \cup_{j=0}^n \left (\lbrace Y_j \in A \rbrace \cap \lbrace \tau_j \leq s < \tau_{j+1} \rbrace \right ) \cap \lbrace \tau_n \leq t < \tau_{n+1} \rbrace  \\
&= \left (\lbrace Y_n \in A ; \tau_n \leq s \rbrace \cup \cup_{j=0}^{n-1} \lbrace Y_j \in A ; \tau_j \leq s < \tau_{j+1} \rbrace \right ) \cap \lbrace \tau_n \leq t \rbrace 
\cap \lbrace \leq t < \tau_{n+1} \rbrace  \\
\end{align*}
which shows $\lbrace X_s \in A \rbrace \in \mathcal{C}$ hence $\mathcal{F}^0_t \subset \mathcal{C}$ and the claim is proven.


The following claim is essentially the Laplace Transform version of
the Markov property that we seek to prove.  In terms of financial
ideas, suppose we have a fixed interest rate $\lambda$.  The next claim says the that the expected net present value of the cash flow $f(X_s)$ starting at $t$ 
is obtained by taking the expected net present value starting at $s=0$ assuming that we start at $X_t$ and then discounting that amount by $e^{-t\lambda}$.
\begin{clm}\label{PureJumpTypeConstruction:ConditionalExpectationOfExponentials}Let $f : S \to [0, \infty)$, $t \geq 0$, $\lambda \geq 0$  and $x \in S$.  If we extend $f$ to $S^\Delta$ by defining $f(\Delta) = 0$ then it follows that
\begin{align*}
\csexpectationlong{\mathcal{F}^0_t}{\int_t^\infty e^{-\lambda u} f(X_u) \, du}{x} 
&= e^{-\lambda t} R_\lambda f(X_t)
\end{align*}
\end{clm}
Let $A \in \mathcal{F}^0_t$.  Since $f(\Delta) = 0$ and $X_t = \Delta$ for $t \geq \zeta$ it follows that 
\begin{align*}
\sexpectation{\int_t^\infty e^{-\lambda u} f(X_u) \, du ; A \cap \lbrace t \geq \zeta \rbrace}{x} &= 0
\end{align*}
and moreover
\begin{align*}
\sexpectation{e^{-\lambda t} R_\lambda f(X_t); A \cap \lbrace t \geq \zeta \rbrace}{x}
&=\sexpectation{e^{-\lambda t} R_\lambda f(\Delta); A \cap \lbrace t \geq \zeta \rbrace}{x} \\
&=\sexpectation{e^{-\lambda t} \int_0^\infty e^{-\lambda s} \sexpectation{f(X_s)}{\Delta} \, ds; A \cap \lbrace t \geq \zeta \rbrace}{x} \\
&=\sexpectation{e^{-\lambda t} \int_0^\infty e^{-\lambda s} f(\Delta)\, ds; A \cap \lbrace t \geq \zeta \rbrace}{x} = 0 \\
\end{align*}
Therefore we may assume that $A \subset \lbrace t < \zeta \rbrace$ and in particular by linearity we may assume that there exists $n \in \integers_+$ such that
$A \subset \lbrace \tau_n \leq t < \tau_{n+1}\rbrace$.  By the previous claim, we may choose an $A_n \in \mathcal{G}_n$ such that $A_n \subset \lbrace \tau_n \leq t \rbrace$ and $A = A_n \cap \lbrace t < \tau_{n+1} \rbrace$.  Therefore we can decompose the integral
\begin{align*}
&\sexpectation{\int_t^\infty e^{-\lambda u} f(X_u) \, du ; A_n \cap \lbrace t < \tau_{n+1} \rbrace}{x} \\
&= \sexpectation{\int_t^{\tau_{n+1}}  e^{-\lambda u} f(X_u) \, du ; A_n \cap \lbrace t < \tau_{n+1} \rbrace}{x} + \\
&\qquad \sum_{k=n+1}^\infty \sexpectation{\int_{\tau_k}^{\tau_{k+1}}  e^{-\lambda u} f(X_u) \, du ; A_n \cap \lbrace t < \tau_{n+1} \rbrace}{x}
\end{align*}
We compute in each of the two cases on the right hand side.  For the first term using Claim \ref{PureJumpTypeConstruction:Computation2} and recalling that $A_n \subset \lbrace \tau_n \leq t \rbrace$,
\begin{align*}
&\sexpectation{\int_t^{\tau_{n+1}}  e^{-\lambda u} f(X_u) \, du ; A_n \cap \lbrace t < \tau_{n+1} \rbrace}{x} \\
&=\sexpectation{ f(Y_n) \lambda^{-1} (e^{-\lambda t} - e^{-\lambda \tau_{n+1}}) ; A_n \cap \lbrace t < \tau_{n+1} \rbrace}{x} \\
&=\lambda^{-1}\sexpectation{ f(Y_n) ; A_n ; \csexpectationlong{\mathcal{G}_n}{(e^{-\lambda t} - e^{-\lambda \tau_{n+1}}) ; t < \tau_{n+1}}{x}}{x} \\
&=\lambda^{-1}\sexpectation{ f(Y_n) ; A_n ; e^{-c(Y_n)(t - \tau_{n})} e^{-\lambda t} (\mu_0(Y_n, S) - \mu_\lambda(Y_n, S))}{x} \\
&=\lambda^{-1}\sexpectation{ f(Y_n) ; A_n ; e^{-c(Y_n)(t - \tau_{n})} e^{-\lambda t} (1 -\frac{c(Y_n)}{\lambda + c(Y_n)})}{x} \\
&=e^{-\lambda t} \sexpectation{ A_n ; e^{-c(Y_n)(t - \tau_{n})} \frac{f(Y_n)}{\lambda + c(Y_n)}}{x} \\
\end{align*}
For the summands of the second term for $k \geq n+1$ we use two applications of Claim \ref{PureJumpTypeConstruction:Computation1}
\begin{align*}
&\sexpectation{\int_{\tau_k}^{\tau_{k+1}}  e^{-\lambda u} f(X_u) \, du ; A_n \cap \lbrace t < \tau_{n+1} \rbrace}{x} \\
&=\sexpectation{ f(Y_k) \lambda^{-1} (e^{-\lambda \tau_k} - e^{-\lambda \tau_{k+1}}) ; A_n \cap \lbrace t < \tau_{n+1} \rbrace}{x} \\
&=\lambda^{-1} \sexpectation{ \csexpectationlong{\mathcal{G}_{n+1}}{f(Y_k) (e^{-\lambda \tau_k} - e^{-\lambda \tau_{k+1}})}{x} ; A_n \cap \lbrace t < \tau_{n+1} \rbrace}{x} \\
&=\sexpectation{ e^{-\lambda \tau_{n+1}} \int \frac{f(y)}{\lambda + c(y)} \,  \mu^{k-n-1}_\lambda(Y_{n+1}, dy)  ; A_n \cap \lbrace t < \tau_{n+1} \rbrace}{x} \\
&=\sexpectation{ A_n ; \csexpectationlong{\mathcal{G}_{n}} {e^{-\lambda \tau_{n+1}} \int \frac{f(y)}{\lambda + c(y)} \,  \mu^{k-n-1}_\lambda(Y_{n+1}, dy) ;\cap \lbrace t < \tau_{n+1} \rbrace}{x}}{x} \\
&=\sexpectation{ A_n ; e^{-c(Y_n)(t - \tau_n)} e^{-\lambda t} \int \left [ \int \frac{f(y)}{\lambda + c(y)} \,  \mu^{k-n-1}_\lambda(u, dy) \right] \, \mu_\lambda(Y_n, du)}{x} \\
&=e^{-\lambda t} \sexpectation{ A_n ; e^{-c(Y_n)(t - \tau_n)} \int \frac{f(y)}{\lambda + c(y)} \,  \mu^{k-n}_\lambda(Y_n, dy)}{x} \\
\end{align*}
Putting this all together we get using Claim \ref{PureJumpTypeConstruction:ResolventIdentity}, Claim \ref{PureJumpTypeConstruction:Computation2} and the fact that $A_n \subset \lbrace \tau_n \leq t \rbrace$ 
\begin{align*}
&\sexpectation{\int_t^\infty e^{-\lambda u} f(X_u) \, du ; A_n \cap \lbrace t < \tau_{n+1} \rbrace}{x}  \\
&= e^{-\lambda t} \sexpectation{ A_n ; e^{-c(Y_n)(t - \tau_{n})} \frac{f(Y_n)}{\lambda + c(Y_n)}}{x} +\\
&\qquad \sum_{k=n+1}^\infty e^{-\lambda t} \sexpectation{ A_n ; e^{-c(Y_n)(t - \tau_n)} \int \frac{f(y)}{\lambda + c(y)} \,  \mu^{k-n}_\lambda(Y_n, dy)}{x} \\
&=e^{-\lambda t}\sexpectation{ A_n ; e^{-c(Y_n)(t - \tau_n)} ; \sum_{k=0}^\infty \int \frac{f(y)}{\lambda + c(y)} \,  \mu^{k}_\lambda(Y_n, dy)}{x}\\
&=e^{-\lambda t} \sexpectation{A_n ; R_\lambda f(Y_n) ; e^{-c(Y_n)(t - \tau_n)} }{x} \\
&=e^{-\lambda t} \sexpectation{A_n  ; R_\lambda f(Y_n); \csprobability{\mathcal{G}_n}{t < \tau_{n+1} }{x}}{x} \\
&=\sexpectation{e^{-\lambda t} R_\lambda f(Y_n); A_n \cap \lbrace t < \tau_{n+1} \rbrace}{x} \\
&=\sexpectation{e^{-\lambda t} R_\lambda f(X_t); A_n \cap \lbrace t < \tau_{n+1} \rbrace}{x} \\
\end{align*}
and the claim follows.

Now we show the Markov property for $X_t$.  

\begin{clm}Suppose $f : S \to [0,\infty)$ is a bounded measurable function and $\varphi : [0,\infty) \to \reals$ be a continuous function such that $\lim_{t \to \infty} \varphi(t) = 0$ then
\begin{align*}
\csexpectationlong{\mathcal{F}^0_t}{\int_t^\infty \varphi(u) f(X_u) \, du}{x} 
&= \sexpectation{\int_0^\infty \varphi(u+t) R_\lambda f(X_u) \, du}{X_t}
\end{align*}
\end{clm}
Then for any $\varphi(t) = \sum_{j=1}^n a_j e^{-\lambda_j t}$ with $\lambda_j \geq 0$ and $-\infty < a_j < \infty$ use Claim \ref{PureJumpTypeConstruction:ConditionalExpectationOfExponentials}  and linearity of expecations to see that $\csexpectationlong{\mathcal{F}^0_t}{\int_t^\infty \varphi(u) f(X_u) \, du}{x} = \sexpectation{\int_0^\infty \varphi(u+t) R_\lambda f(X_u) \, du}{X_t}$.  Now let $\varphi : [0,\infty) \to \reals$ be a continuous function such that $\lim_{t \to \infty} \varphi(t) = 0$ then we may extend $\varphi$ to a continuous function $\varphi : [0,\infty] \to \reals$ by defining $\varphi(\infty) = 0$.  By the Stone Weierstrass Theorem \ref{StoneWeierstrassApproximation} 
we may therefore approximate $\varphi$ uniformly by finite linear combinations of exponentials.  TODO: Show that the uniform approximation allows us to extend to such $\varphi$; to be honest I don't see this yet given that the domain of integration is infinite...

Now for any $s \geq 0$ we may choose continuous compactly supported $\varphi_n$ such that $\varphi_n$ approximate the delta function at $t+s$ e.g.
\begin{align*}
\varphi_n(u) = 
\begin{cases}
n(u - t - s) & \text{for $t+s \leq u \leq t+s+ \frac{1}{n}$} \\
n(t+s - u) + 2 & \text{for $t+s+ \frac{1}{n} \leq u \leq t+ s+ \frac{2}{n}$} \\
0 & \text{otherwise}
\end{cases}
\end{align*}
By the right continuity of $X_t$ for almost all $\omega \in \Omega$ there is an $N_\omega$ such that $\varphi_n(u) f(X_u) = f(X_{t+s}) \varphi_n(u)$ and $\varphi_n(u+t) f(X_u) = \varphi_n(u+t) f(X_s)$ for all $n \geq N_\omega$.  In particular $\int_t^\infty \varphi_n(u) f(X_u) \, du \toas f(X_{t+s})$  and $\int_0^\infty \varphi_n(u+t) f(X_u) \, du \toas f(X_s)$ and by bounded 
convergence we get
\begin{align*}
\csexpectationlong{\mathcal{F}^0_t}{f(X_{t+s})}{x} 
&=\lim_{n \to \infty} \csexpectationlong{\mathcal{F}^0_t}{\int_t^\infty \varphi_n(u) f(X_u) \, du}{x} \\
&=\lim_{n \to \infty} \sexpectation{\int_0^\infty \varphi_n(u+t) f(X_u) \, du}{X_t} \\
&=\sexpectation{f(X_s)}{X_t} \\
\end{align*}

Note that to extend the above construction to account for absorbing states we need to change the definition of our probability space.
Instead of constructing a probability measure on the set of sequences $((x_0, t_0), (x_1, t_1), \dotsc) \in S \times [0,\infty)$ as our starting point
we need to account for the fact that we may have only finitely many jumps.  This is done by allowing $t_n$ to be infinite with the provisions that
when $t_n$ is infinite we must have $x_n = \Delta$ and that $t_{m}$ must be infinite for all $m \geq n$.  With this definition we should be able to relax
the restriction that $c(x) > 0$.  We should look to see if we can prove that $\probability {\tau_n = \infty, Y_n = \Delta} = \probability{\tau_n = \infty}$ and
$\probability {\tau_n = \infty, \tau_{n+1} = \infty} = \probability{\tau_n = \infty}$ after applying Ionescu-Tulcea.
\end{proof}

TODO: Kolmogorov Backward equation.

Let $X$ be a pure jump-type Markov process on state space $S$, $\tau$
be the first jump time and let $\sigma = \tau \wedge t$ for some $0
\leq t < \infty$.  Now by the Strong Markov Property, the independence
of $X_\tau$ and $\tau$ and the disintegration Lemma \ref{DisintegrationIndependentLaws}
\begin{align*}
T_tf(x) 
&=\sexpectation{f(X_t)}{x} 
=\sexpectation{f((\theta_\sigma X)_{t - \sigma})}{x} 
=\sexpectation{\cexpectationlong{\mathcal{F}_\sigma}{f((\theta_\sigma  X)_{t - \sigma})}}{x} \\
&=\sexpectation{\sexpectation{f(X_{t - \sigma})}{X_\sigma}}{x} 
=\sexpectation{T_{t - \sigma}f(X_\sigma)}{x} \\
&=\sexpectation{T_{t - \sigma}f(X_\sigma) ; \tau > t}{x} +
  \sexpectation{T_{t - \sigma}f(X_\sigma); \tau \leq t}{x} \\
&= \sexpectation{T_{0}f(X_0) ; \tau > t}{x}  +
  \sexpectation{T_{t - \tau}f(X_\tau); \tau \leq t}{x} \\
&=f(x) \sprobability{\tau > t}{x} +  \int_0^t \int c(x) e^{-sc(x)} T_{t - s}f(y) \,  \mu(x, dy) ds \\
&=e^{-tc(x)}   \left( f(x) +  \int_0^t \int c(x) e^{-sc(x)} T_{s}f(y) \,  \mu(x, dy) ds \right )\\
\end{align*}
TODO: There is some measurability subtlety in applying disintegration; we need to know that $T_tf(x)$ is jointly measurable in $(t,x)$
and then apply it to $(\tau, X_\tau)$ and use the fact that $X_\tau$ is progressively measurable by right continuity and thus $X_\tau$ is measurable
by Lemma \ref{StoppedProgressivelyMeasurableProcess}.  TODO: How do the measurability considerations in the definition of Markov family enter into the picture here.

\begin{clm}Let $(\Omega, \mathcal{A}, \mathcal{F}_t, X_t, \theta_t, P_x)$ be a Markov family with $X$ jointly measurable, let $f$ be a bounded or non-negative measurable function and define $T_tf(x) = \sexpectation{f(X_t)}{x}$ then $T_tf(x)$ is a jointly measurable function $[0,\infty) \times S \to \reals$.
\end{clm}
To see the claim we just use the kernel property of $P_x$ and write $T_tf(x) = \sexpectation{f(X_t)}{x} = \int X(t, \omega) \, P_x(d\omega)$ and apply Exercise \ref{MeasurabilityKernelExtraParameter}.  TODO:  Since $X$ is $\mathcal{B}([0,\infty)) \otimes \mathcal{F}^X_\infty$ measurable I don't think the universal measurability issues enter into the discussion here.


