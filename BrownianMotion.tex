\section{Brownian Motion}
We begin by studying the one dimensional version of Brownian motion.
\begin{defn}A real-valued stochastic process $B_t$ on $[0, \infty)$ is said to be a
  \emph{Brownian motion} at $x \in \reals$ if 
\begin{itemize}
\item[(i)]$B(0) = x$
\item[(ii)]For all times $0 \leq t_1 \leq t_2 \leq \cdots \leq t_n$
  the increments $B_{t_2} - B_{t_1}, B_{t_3} - B_{t_2}, \dots, B_{t_n}
  - B_{t_{n-1}}$ are independent random variables
\item[(iii)]For all $0 \leq s < t$, the increment $B_t - B_s$ is normally distributed with
  expectation zero and variance $t - s$.
\item[(iv)]Almost surely the sample path $B(t)$ is continuous.
\end{itemize}
\end{defn}

The existence of Brownian motion is a non-trivial fact that was first
proved by Norbert Weiner.  Here we present a construction by Paul Levy
whose details are worth understanding because many properties of
Brownian motion follow from them.
\begin{thm}Standard Brownian motion exists.
\end{thm}
\begin{proof}
Before we construct Brownian motion on the entire real line, we
construct it on the interval $[0,1]$ (that is to say we only construct
the values $B(t)$ for $t \in [0,1]$).  
To motivate the construction of Brownian motion, we take as our
driving goals the fact that we have to construct a continuous random
path $B(x)$ for which the distribution of $B(x)$ for fixed $x \in
[0,1]$ is $N(0, x)$.  The approach to the construction is to proceed
iteratively such that at stage $n$ of the iteration we have a
piecewise linear approximation $B_n(x)$ with the distribution of $B_n(x)$ being
$N(0,x)$ at the points $x = 0, 1/2^n, \dots, 1$. The set of rational
  numbers of the form $\frac{k}{2^n}$ for $n \geq 0$ and $0 \leq k
  \leq 2^n$ is known as the \emph{dyadic rationals} in $[0,1]$.  We will sometime
  have need for the notation
\begin{align*}
\mathcal{D}_n = \lbrace \frac{k}{2^n} \mid 0 \leq k \leq 2^n \rbrace
\end{align*}
and $\mathcal{D} = \cup_{n=0}^\infty \mathcal{D}_n$ when discussing
the dyadic rationals.  To support the construction, we need a
probability space which we assume to be $([0,1], \mathcal{B}([0,1]),
\lambda)$.  As a concrete source of randomness, for
each $d \in \mathcal{D}$ let $Z_d$ be an $N(0,1)$ random variable
with the $Z_d$ independent (we may do this by Lemma
\ref{ExistenceCountableIndependentRandomVariables}).

It is worth walking through the first couple of iterations in rather
gory detail to reinforce the idea and to convince the reader that the
construction really is determined by the vague prescription given
above.  So our first goal is to construct a random piecewise linear
path that is constant at $x=0$ and has distribution $N(0,1)$ at
$x=1$.  The simplest idea turns out to be the right one to get
started: define $B_0(x) = x Z_1$.  Then $\variance{B_0(x)} = x^2$
which is correct for $x \in \lbrace 0,1\rbrace $ but nowhere in between.  The critical
point is the $x^2 < x$ for all $x \in (0,1)$ so we have \emph{too
  little} variance.  Getting a bit more variance is easy whereas we'd
be rather doomed if we already had too much.  

So recall the next step was to get the correct variance at the points
$\lbrace 0, 1/2, 1\rbrace$ not just at the points $\lbrace 0,1\rbrace$.  By the above,
$\variance{B_0(1/2)} = 1/4$ but we require that $B_1(1/2) = 1/2$ so we
need to add a random variable with distribution $N(0, 1/4)$ at $x=1/2$
satisfy our goal.
But since we had the
correct variance at ${0,1}$ we have make sure not to add any more
at either of those points.  This motivates the introduction of the function
\begin{align*}
\Delta(x) &= \begin{cases}
2x & \text{for $0 \leq x \leq \frac{1}{2}$} \\
2 - 2x & \text{for $\frac{1}{2} < x \leq 1$} \\
0 & \text{otherwise} \\
\end{cases}
\end{align*}
Now if we define $B_1(x) = B_0(x) + \frac{1}{2}\Delta(x) Z_{1/2}$
the we see that $B_1(1/2)$ is a sum of two $N(0,1/4)$ random variables
hence is $N(0,1/2)$ as desired.  Because $\Delta(0) = \Delta(1) =
0$, we have $B_1(0) = B_0(0)$ and $B_1(1) = B_0(1)$ so these two are
still in good shape.  

TODO: Make the following into an exercise.
Just to turn the crank one more time, by the definition of $B_1(x)$ we
can easily see that since in general $B_1(x)$ is an $N(0, x^2 +
\frac{1}{2}\Delta_{0,0}(x))$ random variable,
\begin{align*}
\variance{B_1(1/4)} &= \frac{1}{16} + \frac{1}{16} = 1/8 = 1/4 - 1/8 \\
\variance{B_1(3/4)} &= \frac{9}{16} + \frac{1}{16} = 5/8 = 3/4 - 1/8 \\
\end{align*}
so in both cases we need to add a variance of $1/8$ at the points
$\lbrace 1/4, 3/4 \rbrace$ without changing things at $\lbrace 0, 1/2,
1\rbrace$.  Mimicing what we have already done, we now need a ``double
sawtooth'' to modify $B_1(x)$ into $B_2(x)$.  For reasons that we'll
explain later we actually break the modification into two pieces: one
for the interval $(0, 1/2)$ and one for the interval $(1/2, 1)$.  So
define,
\begin{align*}
\Delta_{1,0} (x) &= \Delta(2x) = \begin{cases}
4x & \text{for $0 \leq x \leq \frac{1}{4}$} \\
2 - 4x & \text{for $\frac{1}{4} < x \leq \frac{1}{2}$} \\
0 & \text{otherwise} \\
\end{cases}
\end{align*}
and
\begin{align*}
\Delta_{1,1} (x) &= \Delta(2x - 1) = \begin{cases}
4x -2 & \text{for $\frac{1}{2} \leq x \leq \frac{3}{4}$} \\
4 - 4x & \text{for $\frac{3}{4} < x \leq 1$} \\
0 & \text{otherwise} \\
\end{cases}
\end{align*}
Now if we define $B_2(x) = B_1(x) + \frac{1}{\sqrt{8}}
(\Delta_{1,0}(x) Z_{/1/4} + \Delta_{1,1}(x) Z_{3/4})$, then we have
  added the appropriate variance of $1/8$ at $x=1/4$ and $x=3/4$.

To state the general construction, we first generalize the definition
of our sawtooth functions.  For $n > 0$ and $k=0, \cdots, 2^n -1$, we
define 
\begin{align*}
\Delta_{n,k} (x) &= \Delta(2^nx- k) = \begin{cases}
2^{n+1}x -2k & \text{for $\frac{2k}{2^{n+1}} \leq x \leq \frac{2k+1}{2^{n+1}}$} \\
2k + 2 - 2^{n+1}x & \text{for $\frac{2k+1}{2^{n+1}} < x \leq \frac{2k+2}{2^{n+1}}$} \\
0 & \text{otherwise} \\
\end{cases}
\end{align*}
With the definition we can complete the induction definition.  So our definition
of $B_n(x)$ can be completed.  We point out that $\Delta_{0,0}(x) =
\Delta(x)$ so the definition below is compatible with our definition
of $B_1(x)$ and $B_2(x)$ above:
\begin{align*} 
B_0(x) &= x Z_1 \\
B_n(x) &= B_{n-1}(x) + \frac{1}{\sqrt{2^{n+1}}} \sum_{k=0}^{2^{n-1} -1}
\Delta_{n-1,k}(x) Z_{\frac{2k+1}{2^{n}}} \\
&=B_0(x) + \sum_{j=0}^{n-1}\frac{1}{\sqrt{2^{j+2}}} \sum_{k=0}^{2^j -1}
\Delta_{j,k}(x) Z_{\frac{2k+1}{2^{j+1}}} & & \text{for $n > 0$} 
\end{align*}
We will sometimes find it convenient to use the definition
\begin{align*}
F_n(x) &= \frac{1}{\sqrt{2^{n+2}}} \sum_{k=0}^{2^n -1}
\Delta_{n,k}(x) Z_{\frac{2k+1}{2^{n+1}}} 
\end{align*}
so that we may write 
\begin{align*}
B_n(x) &= B_0(x) + \sum_{j=0}^{n-1} F_j(x) \\
B(x) &= B_0(x) + \sum_{j=0}^\infty F_j(x)
\end{align*}

There are host of important facts about the $B_n(x)$ and $B(x)$ that
proceed to prove.  No individual fact is difficult to prove but there
are many of them to keep track of.

\begin{lem}The following are true:
\begin{itemize}
\item[(i)] $B_n(x)$ is linear on every interval $[\frac{k}{2^n},
\frac{k+1}{2^n}]$ for $k=0,\dots,2^n -1$.
\item[(ii)]For every $n \geq 0$, and $0 < 2k+1 < 2^n$, 
\begin{align*}
B(\frac{2k+1}{2^n}) = \frac{1}{2} (B(\frac{2k}{2^n}) +
B(\frac{2k+2}{2^n})) + \frac{1}{\sqrt{2^{n+1}}} Z_{\frac{2k+1}{2^n}}
\end{align*}
\item[(iii)]For every $n \geq 0$ and every pair $0 \leq j < k \leq 2^n$,
$B(k/2^n) - B(j/2^n)$ is an $N(0, (k-j)/2^n)$ random variable.
Furthermore for $0 \leq j < k \leq l < m \leq 2^n$, the increments
$B(k/2^n) - B(j/2^n)$ and $B(m/2^n) - B(l/2^n)$ are independent.
\end{itemize}
\end{lem}
\begin{proof}
FIrst we prove (i).  This follows from a simple induction.  It is clear for $B_0(x)$.  For
$B_{n+1}(x)$ we are adding multiples of the functions $\Delta_{n,k}(x)$ each of which is linear on intervals of the form $[\frac{k}{2^{n+1}},
\frac{k+1}{2^{n+1}}]$.

Next we prove (ii).  This follows from the fact that
$B(\frac{2k+1}{2^n})=B_n(\frac{2k+1}{2^n})$, the definition of
$B_n(x)$ and the linearity of $B_{n-1}(x)$ on the interval
$[\frac{k}{2^{n-1}}, \frac{k+1}{2^{n-1}}]$.

To see (iii) first note that it suffices to prove this for increments $j+1=k$ and
$l+1=m$.  For if we have proven that then we can write a general
increment as a sum of independent increments of the former form.  
We proceed by induction on $n$.  The case $n=0$ is trivial
because the only non-trivial increment is the $N(0,1)$ random variable
$B(1) - B(0) = Z_1$.  Now consider the case for $n > 0$.  
To see this first we consider ``adjacent'' increments of the form
$B((2k+1)/2^n) - B(2k/2^n)$ and $B((2k+2)/2^n) - B((2k+1)/2^n)$.  Here
we use the formula $B((2k+1)/2^n) = \frac{B((2k+2)/2^n) +
  B(2k/2^n)}{2} + \frac{1}{\sqrt{2^{n+1}}} Z_{(2k+1)/2^n}$ to see 
\begin{align*}
B((2k+1)/2^n) - B(2k/2^n) &= \frac{B((2k+2)/2^n) -
  B(2k/2^n)}{2} + \frac{1}{\sqrt{2^{n+1}}} Z_{(2k+1)/2^n} \\
B((2k+2)/2^n) - B((2k+1)/2^n) &= \frac{B((2k+2)/2^n) -
  B(2k/2^n)}{2} - \frac{1}{\sqrt{2^{n+1}}} Z_{(2k+1)/2^n} \\
\end{align*}
The random variables $B((2k+2)/2^n)$ and $B(2k/2^n)$ only depend on
the $Z_d$ for $d \in \mathcal{D}_{n-1}$ and therefore $Z_{(2k+1)/2^n}$ is
independent of both.  The induction hypothesis is that $B((2k+2)/2^n) -
  B(2k/2^n)$ is an $N(0, \frac{1}{2^{n-1}})$ random variable therefore $ \frac{B((2k+2)/2^n) -
  B(2k/2^n)}{2}$ is $N(0, \frac{1}{2^{n+1}})$.  But both $\pm
\frac{1}{\sqrt{2^{n+1}}} Z_{(2k+1)/2^n}$ are also $N(0,
\frac{1}{2^{n+1}})$ so we've expressed the increments as a sum of two independent
$N(0, \frac{1}{2^{n+1}})$ random variable proving that each is $N(0,
\frac{1}{2^n})$.  Furthermore the increments are independent.  Because
we know they are normal it suffices to show they are uncorrelated
which is a simple computation using the formulae above and the induction hypothesis
\begin{align*}
&\expectation{(B((2k+1)/2^n) - B(2k/2^n))(B((2k+2)/2^n) -
  B((2k+1)/2^n))} \\
&= \expectation{(\frac{B((2k+2)/2^n) -
  B(2k/2^n)}{2} + \frac{1}{\sqrt{2^{n+1}}} Z_{(2k+1)/2^n} ) (\frac{B((2k+2)/2^n) -
  B(2k/2^n)}{2} - \frac{1}{\sqrt{2^{n+1}}} Z_{(2k+1)/2^n})} \\
&=\frac{1}{4}\expectation{(B((2k+2)/2^n) -  B(2k/2^n))^2} -
\frac{1}{2^{n+1}} \\
&=\frac{1}{4}\frac{1}{2^{n-1}} -  \frac{1}{2^{n+1}}  = 0
\end{align*}

It remains to show the independence of increments 
$B((k+1)/2^n) - B(k/2^n)$ and $B((j+1)/2^n) - B(j/2^n)$ with $0 \leq j
< k \leq 2^n$.  In a similar way to the case above we know that
by using the result (ii) we can see that for $0 \leq k < 2^n$,
\begin{align*}
B((k+1)/2^n) - B(k/2^n) &= \begin{cases}
\frac{B((k+1)/2^n) - B((k-1)/2^n)}{2} - \frac{1}{\sqrt{2^{n+1}}}
Z_{k/2^n} & \text{$k$ is odd} \\
\frac{B((k+2)/2^n) - B(k/2^n)}{2} + \frac{1}{\sqrt{2^{n+1}}}
Z_{(k+1)/2^n} & \text{$k$ is even} \\
\end{cases}
\end{align*}
If we assume that we are not in the case already proven then we are either
assuming that $j+1 \neq k$ or $k$ is even. The upshot is that we can
write each increment of length $\frac{1}{2^n}$ as a sum of an increment of
length $\frac{1}{2^{n-1}}$ and an independent $N(0, \frac{1}{2^{n+1}})$
 random variable.  The increments of length $\frac{1}{2^{n-1}}$ are
 independent by the induction hypothesis and therefore the original
 increments are seen to be independent.  TODO: Make this more precise.
\end{proof}

We make the following claim about $B_n(x)$: for $\frac{k}{2^n}
\leq x \leq \frac{k+1}{2^n}$ and $0 \leq k < 2^n$, we have
$\variance{B_n(x)} = 2^n(x - \frac{k}{2^n})^2 + \frac{k}{2^n}$.  We
use an induction to prove the claim.  Note
that the claim is easily seen to be true for $n=0$ (it reduces to
earlier observation that $\variance{B_0(x)} = x^2$).  Now assuming that
it is true for $n$ we extend to $n+1$.  Pick an interval
$[\frac{k}{2^{n}}, \frac{k+1}{2^{n}}]$ and consider passing from
$B_n(x)$ to $B_{n+1}(x)$ on the interval.  There are two subcases
corresponding to the subinteval $[\frac{k}{2^{n}},
\frac{2k+1}{2^{n+1}}]$ and the subinterval $[\frac{2k+1}{2^{n+1}},
\frac{k+1}{2^{n}}]$.

On the first subinterval, by the definition of $B_{n+1}(x)$ we are
adding to $B_n(x)$ a normal random variable with variance
$\left ( \frac{1}{\sqrt{2^{n+2}}} \Delta_{n,k}(x) \right)^2 = 2^n(x-\frac{k}{2^n})^2$.  So at such an $x$, $B_{n+1}(x)$ is normal
with variance 
\begin{align*}
\variance{B_{n+1}(x)} &= \variance{B_{n}(x)}  +
2^n(x-\frac{k}{2^n})^2  \\
&= 2^n(x-\frac{k}{2^n})^2 + \frac{k}{2^n} + 2^n(x-\frac{k}{2^n})^2 \\
&=
2^{n+1} (x-\frac{k}{2^n})^2 + \frac{k}{2^n}
\end{align*}

On the second subinterval, by the definition of $B_{n+1}(x)$ we are
adding to $B_n(x)$ a normal random variable with variance
$2^n(x-\frac{k+1}{2^n})^2$.  So at such an $x$, $B_{n+1}(x)$ is normal
with variance 
\begin{align*}
\variance{B_{n+1}(x)} &= \variance{B_{n}(x)}  +
2^n(x-\frac{k+1}{2^n})^2  \\
&= 2^n(x-\frac{k}{2^n})^2 + \frac{k}{2^n} + 2^n(x-\frac{k+1}{2^n})^2
\\
&=2^n \left[(x-\frac{2k+1}{2^{n+1}})^2 + \frac{1}{2^{n+1}}(x -
  \frac{2k+1}{2^{n+1}}) + \frac{1}{2^{2n+2}}\right] + \\
&2^n \left[(x-\frac{2k+1}{2^{n+1}})^2 - \frac{1}{2^{n+1}}(x -
  \frac{2k+1}{2^{n+1}}) + \frac{1}{2^{2n+2}}\right] + \frac{k}{2^n} \\
&=2^{n+1} (x-\frac{2k+1}{2^{n+1}})^2 + \frac{2k+1}{2^{n+1}}
\end{align*}
which verifies the claim.

We reiterate the importance of this fact is that the approximate path
$B_n(x)$ has the variance $x$ (the ``correct'' variance for a Brownian
path) at all $x = 0, \frac{1}{2^n}, \dots,1$, so that as $n$ increases
$B_n(x)$ has the correct variance on an increasing fine grid in
$[0,1]$.  In between the points of the grid, the variance of $B_n(x)$
is a quadratic function of $x$ that is strictly less than $x$.

Having defined the series expansion of our candidate Brownian motion,
the first order of business is to validate that it converges almost
surely.  To show convergence we need to make sure
that the increments we add at each $n$ get small fast enough; these increments are
multiples of independent standard normal random variables.
Convergence will follow if we can get an appropriate almost sure bound
on a random sample from a sequence of independent standard normals.

To see this we start with a tail bound for an $N(0,1)$ distribution.  
\begin{align*}
\probability{\abs{Z_d} \geq \lambda} &= \frac{2}{\sqrt{2\pi}} \int_\lambda^\infty
e^{\frac{-u^2}{2}} \, du \\
&\leq  \frac{2}{\sqrt{2\pi}} \int_\lambda^\infty
\frac{u}{\lambda} e^{\frac{-u^2}{2}} \, du \\
&= \frac{1}{\lambda\sqrt{2\pi}} e^{\frac{-\lambda^2}{2}}
\end{align*}
so if we pick any constant $c > 1$ and $n > 0$, then 
\begin{align*}
\probability{\abs{Z_d} \geq c \sqrt{n}} &\leq \frac{1}{c\sqrt{2\pi n}}
e^{\frac{-c^2 n}{2}} \leq e^{\frac{-c^2 n}{2}}
\end{align*}
Now using this bound, we see that
\begin{align*}
\sum_{n=0}^\infty \probability{ \text {there exists $d \in
    \mathcal{D}_n$ such that $\abs{Z_d} \geq c \sqrt{n}$}} &\leq
\sum_{n=0}^\infty  \sum_{d \in \mathcal{D}_n} \probability{\abs{Z_d}
  \geq c \sqrt{n}} \\
&\leq \sum_{n=0}^\infty 2^n e^{\frac{-c^2 n}{2}} \\
&= \sum_{n=0}^\infty
e^{-n(c^2 - 2\ln2)/2} 
\end{align*}
which converges if $c > \sqrt{2\ln2}$.  Picking such a $c$, we apply
the Borel Cantelli Theorem to conclude that 
\begin{align*}
\probability{ \text {there exists $d \in
    \mathcal{D}_n$ such that $\abs{Z_d} \geq c \sqrt{n}$ i.o.}} &= 0
\end{align*}
and therefore for almost all $\omega \in \Omega$ there exists
$N_\omega > 0$ such that $\abs{Z_d} < c \sqrt{n}$ for all $n >
N_\omega$ and $d \in \mathcal{D}_n$.  Using this result with the definition of
$F_n(x)=\sum_{k=0}^{2^n-1}
\frac{1}{\sqrt{2^{n+2}}}Z_{\frac{2k+1}{2^{n+1}}} \Delta_{n,k}(x)$, the
disjointness of the support of $\Delta_{n,k}(x)$ for fixed $n$ and the
fact that $\abs{\Delta_{n,k}(x)} \leq 1$ we
have $\norm{F_n}_\infty \leq 2^{-(n+2)/2} c \sqrt{n+1}$ which shows that
$\sum_{n=0}^\infty F_n(x)$ converges absolutely and uniformly in $x$.
Because each $F_n(x)$ is a continuous function, uniform convergence of
the series implies $B(x) = B_0(x) + \sum_{n=0}^\infty F_n(x)$ is continuous as
well (Theorem \ref{UniformLimitContinuousFunctionsIsContinuous}).

TODO: Show that for every $x \in [0,1]$, $B(x)$ is integrable and
has finite variance.  Not sure we need this because we'll prove a
stronger statement below.

The next step is to validate that $B(x)$ has independent Gaussian increments.
TODO: Show that we have Gaussian increments, independent increments,
zero mean and proper variance/covariance.  The first step is to note
that we have already proven that increments at dyadic rational numbers
are independent and Gaussian.  But we have also shown that $B(x)$ is
almost surely continuous so we may approximate arbitrary increments by
those at dyadic rationals.

Suppose we are given $0 \leq x_1 < x_2 < \cdots < x_n \leq 1$.  By the
density of the dyadic rationals we can find sequences $x_{j,m}$ of
dyadic rationals with $x_{j-1} < x_{j,m} \leq x_j$ such that $\lim_{m
  \to \infty} x_{j,m}= x_j$ (in the case $j=1$, we only require $0
\leq x_{1,m} \leq x_1$).  By almost sure continuity of $B(x)$ we know
that $B(x_{j,m}) - B(x_{j-1,m})$ converges to $B(x_j) - B(x_{j-1})$
for $1 < j \leq n$.  Moreover we know that 
\begin{align*}
\lim_{m \to \infty} \expectation{B(x_{j,m}) - B(x_{j-1,m})} &= 0
\end{align*}
and 
\begin{align*}
\lim_{m \to \infty} \textbf{Cov}\left ( B(x_{j,m}) - B(x_{j-1,m}), B(x_{i,m})
  - B(x_{i-1,m}) \right) &= \delta_{i,j} \lim_{m \to \infty} (x_{i,m} -
x_{i-1,m}) \\
&= \delta_{i,j}  (x_i - x_{i-1})
\end{align*}
and therefore by Lemma \ref{LimitOfGaussianRandomVectors} we know that
the $B(x_j) - B(x_{j-1})$ are independent $N(0, x_j - x_{j-1})$ random
variables and we are done.

Note that we have ignored measurability considerations up to this
point and it is worth filling in that gap so that we have verified our
construction defines a proper stochastic process.  Since we have
defined $B$ as an almost sure limit of the $B_n$ it suffices to show
that each $B_n$ is measurable  (Lemma
\ref{LimitsOfMeasurable}).  Now each $B_n$ is a sum of terms each of
which is a random variable times a deterministic function so by Lemma
\ref{ArithmeticCombinationsOfMeasurableFunctions} it suffices to show
each such term is measurable.  So let $\xi$ be a random variable and
let $g(x)$ an element of $\reals^{[0,1]}$.  If we pick $0 \leq x \leq
1$ and $A \in \mathcal{B}(\reals)$ then $\lbrace \xi g(x) \in A
\rbrace = \lbrace \xi \in A\cdot  1/g(x) \rbrace$ which is measurable
because $\xi$ is (here we have ignored the case in which $g(x) = 0$;
in that case the set is either $\emptyset$ or $\Omega$ so is
measurable).  Since sets of the form $\lbrace f (t) \in A \rbrace$
generate the $\sigma$-algebra on $\reals^{[0,1]}$ we have shown that
$\xi g(x)$ is measurable (Lemma \ref{MeasurableByGeneratingSet}).
\end{proof}
TODO: Note the connection of the construction to wavelets.  What we
are doing here is expressing the Brownian motion as a linear
combination of integrals of the Haar wavelet basis (in some sense we
are integrating ``white noise'' which is called an \emph{isonormal
  process} in the mathematical literature these days).  Note that the
such a form for a Brownian motion can be anticipated by examining the
covariance of Brownian motion (see Steele).

TODO: Modulus of continuity of Brownian paths; Holder continuity and
nowhere differentiability.

TODO: Some of these proofs use the specifics of the Levy construction
of Brownian motion and not just the defining properties of Brownian
motion.  In what way is this justified; i.e. to what extent is the
Levy construction unique?  The answer to this question is that Wiener
measure on $C[0,\infty)$ is uniquely defined by its finite dimensional
distributions.

\begin{defn}A function $f : (S, d) \to (T, d^\prime)$ between metric
  spaces is said to be \emph{H\"older continuous} with exponent
  $\alpha$ if there exists a constant $C > 0$ such that
  $d^\prime(f(x), f(y)) \leq C d(x,y)^\alpha$ for all $x, y \in S$.  
\end{defn}
\begin{lem}\label{HaarWaveletCoefficientHolderContinuity}Let $f : [0,1] \to \reals$ be continuous with $f(x) = c_0 +
  \sum_{n=0}^\infty \sum_{k=0}^{2^n -1} c_{n,k} \Delta_{n,k}(x)$.  Suppose $\abs{c_{n,k}} \leq
  2^{-\alpha n}$ for some $0 < \alpha < 1$ then $f \in C^{\alpha}[0,1]$.
\end{lem}
\begin{proof}
Since the condition for H\"older continuity only depends on
differences between a function we may assume that $c_0 = 0$.  Pick
$s,t \in [0,1]$ and use the triangle inequality to conclude 
\begin{align*}
\abs{f(s) - f(t)} &\leq \sum_{n=0}^\infty \abs{\sum_{k=0}^{2^n -1}
  c_{n,k} \left ( \Delta_{n,k}(s) - \Delta_{n,k}(t) \right) }
\end{align*}
To clean up our notation a bit we define 
\begin{align*}
D_n(s,t) &= 
\sum_{k=0}^{2^n -1}  c_{n,k} \left ( \Delta_{n,k}(s) - \Delta_{n,k}(t) \right)
\end{align*}
for $n\geq 0$ and we work on getting a bound on $\abs{D_n}$.  Since we have a very
concrete description of the $\Delta_{n,k}$ elementary (but detailed)
tools can be used.  Because the support of $\Delta_{n,k}$ for fixed
$n$ are disjoint $\Delta_{n,k}(s)$ is non-zero for at most one $k$ and
similarly with $\Delta_{n,k}(t)$.  Let $0 \leq k_s < 2^n$ be an
integer such that $k_s/2^{n} \leq s \leq (k_s+1)2^n$ and similarly with
$k_t$ (there is ambiguity in the choice for $s,t = k/2^n$ but it
doesn't matter since the $\Delta_{n,k}$ all vanish at such points);
with these choices, $D_n(s,t) = c_{n,k_s} \Delta_{n,k_s}(s) - c_{n, k_t}
  \Delta_{n,k_t}(t)$.  Each function $\Delta_{n,k}$ is
piecewise linear and comprises two line segments with slope $\pm
2^{n+1}$ and it is geometrically clear that $\Delta_{n,k_s}(s)$ and
$\Delta_{n,k_t}(t)$ can be no farther than if they are on the same
such line : hence $\abs{\Delta_{n,k_s}(s) - \Delta_{n,k_t}(t)} \leq
\abs{s-t} 2^{n+1}$ and by the bounds we have on the coefficients
$c_{n,k}$ we get
\begin{align*}
\abs{D_n(s,t)} &\leq \left( \abs{c_{n,k_s}} \vee \abs{c_{n,k_t}}
\right) \abs{\Delta_{n,k_s}(s) - \Delta_{n,k_t}(t)}  \leq 2^{-\alpha
  n} \abs{s-t} 2^{n+1}
\end{align*}
This is a good bound when $s,t$ are close (in fact it is a tight bound
when $k_s=k_t$ and $s,t$ are on the same line segment).  However, as
$s,t$ get farther apart we can do better just by using the fact that
$0 \leq \Delta_{n,k} \leq 1$.  Indeed by the triangle inequality
\begin{align*}
\abs{D_n(s,t)} &= \abs{c_{n,k_s} \Delta_{n,k_s}(s)} + \abs{c_{n,k_t}
  \Delta_{n,k_t}(t)} \leq \abs{c_{n,k_s}} + \abs{c_{n,k_t}} \leq
2^{-\alpha n + 1}
\end{align*}
and therefore we have the two bounds
\begin{align*}
D_n(s,t) &\leq 2^{-\alpha  n} \abs{s-t} 2^{n+1} \wedge 2^{-\alpha n + 1}
\end{align*}
As mentioned, the first of these bounds is a better estimate when $s,t$ are closer
that $2^{-n}$ and the latter is better otherwise.  So with $s,t$
given pick $N \geq 0$ such that $2^{-N -1} \leq \abs{s -t} < 2^{-N}$
and use the appropriate mix of the two estimates
\begin{align*}
\abs{f(s) - f(t)} &\leq \sum_{n=0}^N \abs{\sum_{k=0}^{2^n -1}
  c_{n,k} \left ( \Delta_{n,k}(s) - \Delta_{n,k}(t) \right) } + \sum_{n=N+1}^\infty \abs{\sum_{k=0}^{2^n -1}
  c_{n,k} \left ( \Delta_{n,k}(s) - \Delta_{n,k}(t) \right) } \\
&\leq \sum_{n=0}^N 2^{-\alpha  n} \abs{s-t} 2^{n+1} + 
\sum_{n=N+1}^\infty 2^{-\alpha n + 1} \\
&= 2 \abs{s - t} \frac{2^{(1 -\alpha)(N+1)} - 1}{2^{1 - \alpha} -1} + 2 \cdot 2^{-\alpha(N+1)} \cdot \frac{1}{1 - 2^{-\alpha}} \\
&\leq \frac{2}{2^{1 - \alpha} -1} \abs{s-t}^\alpha - \frac{2}{2^{1 -
    \alpha} -1}\abs{s-t} + \frac{2}{1 - 2^{-\alpha}} \abs{s - t}^\alpha \\
&\leq \left( \frac{2}{2^{1 - \alpha} -1} + \frac{2}{1 - 2^{-\alpha}}
\right ) \abs{s-t}^\alpha
\end{align*}
where we have used the assumption that $0 < \alpha < 1$ to determine
the sign of coefficients in the estimates (e.g. to conclude
that $\frac{2}{2^{1 - \alpha} -1}\abs{s-t} > 0$ so that this term may
be dropped from the estimate).
\end{proof}

A corollary of this result and our construction of Brownian motion is
the fact that Brownian paths are H\"older continuous with any exponent
less that $1/2$.
\begin{thm}[H\"older Continuity of Brownian
  Paths]\label{BrownianHolderContinuous}Let $B_t$ be a standard
  Brownian motion then almost surely $B_t$ is H\"older continuous for
  any exponent $\alpha < 1/2$.  Furthermore there exists a constant $C
  > 0$ (independent of $\omega$) such that almost surely there exists
  a constant $\epsilon > 0$ (depending on $\omega$) such that for all
  $0 \leq h \leq \epsilon$ and $0 \leq t \leq 1-h$ we have 
\begin{align*}
\abs{B_{t+h} - B_t} \leq C \sqrt{h \log(1/h)}
\end{align*}
\end{thm}
\begin{proof}
From our construction of Brownian motion recall that we had the
representation
\begin{align*}
B_t &= t Z_0 + \sum_{n=0}^\infty \frac{1}{\sqrt{2^{n+2}}}
\sum_{k=0}^{2^n -1} \Delta_{n,k}(t) Z_{\frac{2k+1}{2^{n+1}}}
\end{align*}
and moreover we have shown during the construction of Brownian motion
for $c > \sqrt{2 \ln 2}$ almost surely there is an $N>0$ such that 
\begin{align*}
\abs{Z_{\frac{2k+1}{2^{n+1}}}} \leq c \sqrt{n+1}
\end{align*}
for all $n \geq N$.  Note that we can ignore the leading term $t Z_0$ since is clearly
H\"older continuous, so to apply Lemma
\ref{HaarWaveletCoefficientHolderContinuity}
it suffices to observe that we have coefficients $c_{n,k} =
\frac{1}{\sqrt{2^{n+2}}}Z_{\frac{2k+1}{2^{n+1}}}$ with the bound
\begin{align*}
\abs{c_{n,k}} &\leq \frac{c\sqrt{n+1}}{\sqrt{2^{n+2}}} \leq 2^{-\alpha n}
\end{align*}
for $n$ sufficiently large.  TODO: In the previous Lemma we need to
rephrase things to note that it suffices to have the bound hold
eventually.

TODO:  Extend the estimates from the prior Lemma to yield the simple upper bound for
modulus of continuity.  Following the proof of the prior Lemma and
using our estimate on the $c_{n,k}$ directly instead of the derived
bound $\abs{c_{n,k}} \leq 2^{-\alpha n}$ we get by picking $2^{-M-2}
< \abs{s-t} \leq 2^{-M-1}$ (so that $M+1 \leq \log_2(1/\abs{s-t})$)
\begin{align*}
\abs{B_s - B_t} &\leq \sum_{n=0}^{N-1} \max_{0 \leq k < 2^n}
\abs{c_{n,k}} \abs{s-t} 2^{n+1} +
\sum_{n=N}^M \abs{s-t} 2^{n+1} \frac{c\sqrt{n+1}}{2^{(n+2)/2}} + 
2 \sum_{n=M+1}^\infty  \frac{c\sqrt{n+1}}{2^{(n+2)/2}}
\end{align*}
For the first term, we use the fact that $\lim_{\epsilon \to 0^+}
\epsilon/\sqrt{\epsilon \log(1/\epsilon)} = 0$ to find $\epsilon$ 
(depending on $\omega$) sufficiently small so that provided $\abs{s-t} \leq
\epsilon$ we have 
\begin{align*}
\sum_{n=0}^{N-1} \max_{0 \leq k < 2^n}
\abs{c_{n,k}} \abs{s-t} 2^{n+1} \leq \sqrt{\abs{s-t}
  \log(1/\abs{s-t})}
\end{align*}
For the second term, by choice of $M$ we get
\begin{align*}
\sum_{n=N}^M \abs{s-t} 2^{n+1} \frac{c\sqrt{n+1}}{2^{(n+2)/2}} 
&\leq c \abs{s-t} \sum_{n=0}^M 2^{n/2} \sqrt{n+1} \\
&\leq  c \abs{s-t}   \sqrt{M+1} \frac{2^{(M+1)/2}-1}{\sqrt{2}-1}\\
&\leq \frac{c}{\sqrt{2}-1}
  \sqrt{\abs{s-t} \log_2(1/\abs{s-t})}
\end{align*}
For the third term by choice of $M$ we get
\begin{align*}
2 \sum_{n=M+1}^\infty  \frac{c\sqrt{n+1}}{2^{(n+2)/2}}
&\leq \sqrt{M+1} \frac{c}{2^{(M+1)/2}} \sum_{n=0}^\infty
\sqrt{\frac{n+M+1}{M+1}} \frac{1}{2^{n/2}} \\
&\leq \sqrt{M+1} \frac{c}{2^{(M+1)/2}} \sum_{n=0}^\infty
\sqrt{n+1} \frac{1}{2^{n/2}} \\
&\leq C_2 \sqrt{\abs{s-t}\log_2(1/\abs{s-t})}
\end{align*}
where the constant $C_2$ depends only on the value of the convergent
series and the choice of $c$.
\end{proof}

TODO: Levy's modulus of continuity Lemmas
\begin{thm}Almost surely 
\begin{align*}
\limsup_{h \downarrow 0} \sup_{0 \leq t \leq 1-h} \frac{\abs{B_{t+h} -
    B_t}}{\sqrt{2h\log(1/h)}} &= 1
\end{align*}
(TODO: Is this $\log_e$ or $\log_2$?)
\end{thm}
\begin{proof}
My notes on the proof from Peres and Morters
Fix a $c > \sqrt{2}$ and pick $0 < \epsilon < 1/2$.  For this $\epsilon$,
by Lemma ? we pick $m>0$ such that for every $[s,t] \subset [0,1]$ we
get $[s^\prime,t^\prime] \in \Lambda(m)$ such that $\abs{t - t^\prime}
< \epsilon (t -s)$ and  $\abs{s - s^\prime}
< \epsilon (t -s)$.  Now by Lemma ? we choose $N > 0$ such that for
all $n \geq N$, almost surely for every $[s^\prime, t^\prime] \in
\Lambda_n(m)$
\begin{align*}
\abs{B_{t^\prime} - B_{s^\prime}} &\leq c \sqrt{(t^\prime - s^\prime)
  \log(1/(t^\prime -s^\prime))}
\end{align*}
(we want this to be true for the approximating $[s^\prime, t^\prime]$:
how do we know that $[s^\prime, t^\prime] \in \Lambda_n(m)$ for
sufficiently large $n$; I think it is true that $\Lambda_n(m) \subset
\Lambda_{2n}(m)$?  No I think we make the assumption that $t-s < 2^{-N}$).
But we also have Theorem \ref{BrownianHolderContinuous} (TODO: Does
this work; this result gives the bound for $h$ smaller than a
\emph{random} constant but here it seems we are assuming that it is
not random) so we can
estimate
\begin{align*}
\abs{B_{t} - B_{s}} &\leq \abs{B_{t} -
  B_{t^\prime}} + \abs{B_{t^\prime} - B_{s^\prime}} +
\abs{B_{s^\prime} - B_{s}} \\
&\leq C \sqrt{\abs{t - t^\prime} \log(1/\abs{t - t^\prime})} + 
c \sqrt{(t^\prime - s^\prime)  \log(1/(t^\prime -s^\prime))} +
C \sqrt{\abs{s - s^\prime} \log(1/\abs{s - s^\prime})}
\end{align*}
The function $x \log(1/x)$ is increasing for $0\leq x \leq 1/2$ (here
we are using $\log_2$; otherwise $1/e$) therefore if we assume $t -s <
\epsilon$ then $\abs{t - t^\prime} < \epsilon (t-s) < 1/4$ so we get
the estimate
\begin{align*}
C \sqrt{\abs{t - t^\prime} \log(1/\abs{t - t^\prime})} &\leq C
\sqrt{\epsilon(t-s)\log(1/\epsilon(t-s))} \\
&\leq C\sqrt{\epsilon(t-s)\log(1/(t-s)^2)} \\
&= \sqrt{2 \epsilon} C \sqrt{(t-s)\log(1/(t-s))}
\end{align*}
and similarly with the term involving $\abs{s- s^\prime}$.  As for the
middle term, we have by choice of $[s^\prime, t^\prime]$ that $(1 -
2\epsilon)(t -s) \leq (t^\prime - s^\prime) \leq (1+2 \epsilon)(t-s)$
and by assumption $\log(1/(t-s)) > 1$ therefore
\begin{align*}
c \sqrt{(t^\prime - s^\prime)  \log\frac{1}{t^\prime -s^\prime}} &\leq c
\sqrt{(1 + 2\epsilon) (t - s)  \log\frac{1}{(1-2\epsilon) (t -s)}} \\
&= c\sqrt{(1 + 2\epsilon) (t - s)  (\log\frac{1}{(1-2\epsilon) }
+ \log\frac{1}{ (t -s)})} \\
&\leq c\sqrt{(1 + 2\epsilon) (t - s)  \log\frac{1}{ (t -s)} (1- \log(1-2\epsilon))} 
\end{align*}
Now since $\epsilon > 0$ was arbitrary, we can put all three estimates
together conclude for any $0 < h < \epsilon$, 
\begin{align*}
\sup_{0 \leq t \leq 1-h} \abs{B_{t+h} - B_t} &\leq \left ( 2\sqrt{2\epsilon}C
  +  c\sqrt{(1 + 2\epsilon) (1- \log(1-2\epsilon)) } \right) \sqrt{h\log(1/h)}
\end{align*}
and thus 
\begin{align*}
\limsup_{h \downarrow 0} \sup_{0 \leq t \leq 1-h} \frac{\abs{B_{t+h} - B_t}}{\sqrt{h\log(1/h)}} &\leq  2\sqrt{2\epsilon}C
  +  c\sqrt{(1 + 2\epsilon) (1- \log(1-2\epsilon))} 
\end{align*}
Now since $0 < \epsilon < 1/2$ was arbitrary and $c > \sqrt{2}$ was
arbitrary we can let $\epsilon \downarrow 0$ and then $c \downarrow
\sqrt{2}$ to conclude the result.
\end{proof}

The approach above to studying the sample path properties of Brownian
motion is based on examing the (random) coefficients of the expression
of the Brownian motion in the Schauder basis.  This has advantages and
disadvantages.  The obvious advantage is a certain concreteness that
is appealing.  The disadvantage is that the analysis is less general
than it can be.  Here we provide a classical alternative to the
construction of Brownian motion and the analysis of sample paths that
relies on tools that are more general.  It is critical to have these
more general tools at hand when discussing larger classes of
stochastic process.

\begin{thm}[Kolmogorov-Centsov]Let $X_t$ be a stochastic process on
  $[0,T]^d$ with values in a complete metric space $(S,d)$ and suppose
  that there exist constant $C, \alpha, \beta$ such that 
\begin{align*}
\expectation{d(X_s, X_t)^\alpha} &\leq \abs{s-t}^{d + \beta} \text{
  for all $s,t \in \reals^d$}
\end{align*}
then $X_t$ has a continuous modification $\tilde{X}_t$ and furthermore
the paths of $\tilde{X}_t$ are almost surely H\"older continuous with
exponent $\gamma$ for every $0 < \gamma < \beta/\alpha$.
\end{thm}
\begin{proof}
We do the proof with $T=1$ and $d=1$.  

The basic idea of the proof is that via Markov bounding, the moment
condition controls the variations of $X_t$ pointwise; furthermore by careful
selection of constants we can extend this to uniform continuity of $X_t$ on a
countable subset of $[0,T]^d$.  By chosing a countable dense subset of
$[0,T]^d$ we will then be in position to create the modification.

For each $n \geq 0$, let $\mathcal{D}_n = \lbrace k/2^n \mid 0 \leq k
\leq 2^n \rbrace$ be the dyadic rationals with scale $n$ and consider
the behavior of $X_t$ on the grid $\mathcal{D}_n^d \subset [0,1]^d$.
To begin bound the variation on adjacent points in the grid using a
union bound and a Markov bound (TODO: Fix up the sum below for the
case $d>1$)
\begin{align*}
\probability{\max_{0 < k \leq 2^n} d(X_{k/2^n}, X_{(k-1)/2^n}) \geq  \epsilon}
&\leq \sum_{k=1}^{2^n} \probability{d(X_{k/2^n}, X_{(k-1)/2^n}) \geq \epsilon} \\
&\leq \sum_{k=1}^{2^n} 2^{-n(d+\beta)}/\epsilon^\alpha = 2^{-n\beta} \epsilon^{-\alpha}
\end{align*}
So if we pick $0 < \gamma < \beta/\alpha$ and $\epsilon=2^{-n\gamma}$
then we have the bound 
\begin{align*}
\sum_{n=0}^\infty \probability{\max_{0 < k \leq 2^n} d(X_{k/2^n}, X_{(k-1)/2^n}) \geq
  2^{-n\gamma}} \leq \sum_{n=1}^\infty  2^{-n(\beta - \gamma\alpha)} < \infty
\end{align*}
and Borel Cantelli tells us that there is an event $A \subset \Omega$
with $\probability{A}=1$ and for each $\omega \in A$ there exists an
$N(\omega)$ such that 
\begin{align*}
d(X_{k/2^n}(\omega),
X_{(k-1)/2^n}(\omega)) &< 2^{-n\gamma} \text{ for all $n \geq
  N(\omega)$ and $0 < k \leq 2^n$}
\end{align*}

We have gained some control on the behavior of $X_t$ on a sequence of
successively finer dyadic grids but what we need is to translate this into
control of $X_t$ simultaneously over the union of all grids (to see
what we are lacking at this point realise that we have an almost sure
bound on a term like $d(X_{k/2^n}, X_{(k-1)/2^n})$ with $k/2^n -
(k-1)/2^n = 1/2^n$ but we don't yet have a bound on a term like
$d(X_{(2k+1)/2^n}, X_{(2k-1)/2^n})$ with $(2k+1)/2^{n+1} -
(2k-1)/2^{n+1} = 1/2^n$).  

Claim: For every $n \geq N(\omega)$ and every $m > n$ we have 
\begin{align*}
d(X_t(\omega), X_s(\omega)) &\leq 2\sum_{k=n+1}^m 2^{-k\gamma} \text{
  for $s,t \in \mathcal{D}_m$ with $0 < \abs{s-t} < 2^{-n}$}
\end{align*}

The proof of the claim is by induction.  For $m=n+1$ the only way for
$0 < \abs{s-t} < 2^{-n}$ when $s,t \in \mathcal{D}_{n+1}$ is when
$s=(k-1)/2^{n+1}$ and $t=k/2^{n+1}$ and therefore by what we have
already shown $d(X_t(\omega), X_s(\omega)) \leq 2^{-(n+1)\gamma}$ so
the result holds in this case.  Now assume that the result holds for
all $n+1, \dotsc, m$ and we show it for $m+1$.  Assume without loss of
generality that $s < t$ define $s^* = \ceil{2^m s}/2^m$ and $t^* =
\floor{2^m t}/2^m$ (that is to say round $s$ up
to nearest point on the grid $\mathcal{D}_m$ and round $t$ down to the
nearest point on the grid $\mathcal{D}_m$).  Then the following are
easily seen to be true
\begin{itemize}
\item[(i)] $s^*, t^* \in \mathcal{D}_m$
\item[(ii)] $s \leq s^* \leq t^*\leq t$
\item[(iii)] $0 \leq s^* - s \leq 1/2^{m+1}$
\item[(iv)] $0 \leq t - t^* \leq 1/2^{m+1}$
\item[(v)] $0 \leq t^* - s^* < 1/2^n$
\end{itemize}
Now by the triangle inequality, the induction hypothesis and the result for adjacent points in
the grid $\mathcal{D}_{m+1}$ we get
\begin{align*}
d(X_t, X_s) &\leq d(X_t, X_{t^*}) + d(X_{t^*}, X_{s^*}) + d(X_{s^*},
X_s)  \\
&\leq 2^{-(m+1)\gamma} + 2 \sum_{k=n+1}^m 2^{-k\gamma} +
2^{-(m+1)\gamma}  = 2 \sum_{k=n+1}^{m+1} 2^{-k\gamma}
\end{align*} 
and we are done with the claim.

The claim establishes the local H\"older continuity of $X_t(\omega)$
on $\mathcal{D} = \cup_{n=1}^\infty \mathcal{D}_n$ (hence uniform
continuity).  To see this, pick $s,t \in \mathcal{D}$ such that
$\abs{s-t} < 2^{-N(\omega)}$ and find $n > N(\omega)$ such that
$2^{-(n+1)} \leq \abs{s-t} < 2^{-n}$, then $s,t \in \mathcal{D}_m$ for all
$m$ large enough and so
\begin{align*}
d(X_t(\omega), X_s(\omega)) &\leq 2 \sum_{k=n+1}^m 2^{-k\gamma} \leq
2^{-(n+1)\gamma} \frac{2}{1 - 2^{-\gamma}}  \leq \abs{s-t}^\gamma \frac{2}{1 - 2^{-\gamma}}
\end{align*}

Since $X_t$ is almost surely H\"older continuous on $\mathcal{D}^d$
which is a dense subset of $[0,1]^d$ we know that $X_t$ has a unique
extension $\tilde{X}_t$ to a continuous function on $[0,1]^d$ and that
the extension is H\"older continuous with the same exponent and
constant.  Define $\tilde{X}_t = 0$ for $\omega \notin A$.  

It remains to show that $\tilde{X}_t$ defined in this way is a
modification of $X_t$.
Assume $\epsilon
> 0$ and apply a Markov bound 
\begin{align*}
\probability{d(X_t, X_s) > \epsilon} &\leq \frac{\expectation{d(X_t,
    X_s)^\alpha}}{\epsilon^\alpha} \leq \frac{\abs{s-t}^{d + \beta}}{\epsilon^\alpha}
\end{align*}
which shows that for every $s \in [0,1]^d$ we have $X_t \toprob X_s$
as $t \to s$.

TODO: Finish the argument that this is a modification.
\end{proof}

The flip side of the positive results showing that Brownian paths are
H\"older continuous is the following result showing that a sea change
occurs at $\alpha = 1/2$.  As we'll note, in particular this shows
that Brownian paths are almost surely nowhere differentiable.

\begin{thm}\label{BrownianNotHolderContinuous}For every $\alpha > 1/2$ almost surely a Brownian path has
  no point that is locally H\"older continuous with exponent $\alpha$.
\end{thm}
\begin{proof}
Pick an $\alpha > 1/2$, $C > 0$, $\epsilon > 0$ and define
\begin{align*}
G(\alpha, C, \epsilon) &= \lbrace \omega \mid \text{ there exists } s
\in [0,1] \text{ such that } \abs{B_t(\omega) - B_s(\omega)} <
C\abs{t-s} \text{ for every } t \in [0,1] \text{ with }
\abs{t-s}<\epsilon \rbrace
\end{align*}
The set $G(\alpha,C, \epsilon)$ is not necessarily measurable so it
doesn't make sense to show that it has measure zero; however we will
show that it is contained in a set of a measure zero.  The trick to
doing this is the observation that the $\alpha$-H\"older continuity of
$B_s(\omega)$ from the definition of $G(\alpha,C,\epsilon)$ implies an
arbitrarily large number of independent increments to be small.  By
the Gaussian nature of the increments and a very crude tail
probability estimate we'll be able to conclude that the
probability of the increments all being small can be sent to zero.  At
the risk of being pedantic, note that while the positive results on
H\"older continuity relied on bounds showing it is unlikely that a
collection of independent Gaussians will simulaneously be large, this
result requires a bound showing it is unlikely that a collection of
independent Gaussians will simultaneously be small.

To make this precise, pick an $\omega \in G(\alpha, C,\epsilon)$ and
let $s \in [0,1]$ be an appropriate H\"older continuous point.  Now
define $U = [0,1] \cap (s - \epsilon, s + \epsilon)$ so that the
diameter is at least $\epsilon$.  Now for any $m >0$ there is an
$N_{m,\epsilon}$ (roughly speaking $N_{m,\epsilon} = 2m/ \epsilon$)
such that for all $n \geq N_{m,\epsilon}$ there exists a $k$ with $0
\leq k < n-m$ such that for all $0\leq i < m$, $[\frac{k+i}{n},
\frac{k+i+1}{n}] \subset U$ and either $s \in [\frac{k}{n},
\frac{k+1}{n}]$ or $s \in [\frac{k+m-1}{n},\frac{k+m}{n}]$ (we only
need the last option when $s =1$).  Now using the fact that the
diameter of $U$ is less that $\epsilon$, the triangle inequality and
the H\"older continuity at $s$ we see for every $0 \leq i < m$,
\begin{align*}
\abs{B_{\frac{k+i+1}{n}}(\omega) - B_{\frac{k+i}{n}}(\omega)} &\leq
\abs{B_{\frac{k+i+1}{n}}(\omega) - B_{s}(\omega)} + \abs{B_{s}(\omega)
  - B_{\frac{k+i}{n}}(\omega)} \leq 2 C \left( \frac{m}{n} \right)^\alpha
\end{align*}
From this argument we conclude that for every $m > 0$ and every $n
\geq N_{m, \epsilon}$
\begin{align*}
G(\alpha, C, \epsilon) &\subset \cup_{k=0}^{n-m-1} \cap_{i=0}^{m-1}
\lbrace \omega \mid \abs{B_{\frac{k+i+1}{n}}(\omega) -
  B_{\frac{k+i}{n}}(\omega)} \leq 2 C \left( \frac{m}{n}
\right)^\alpha \rbrace
\end{align*}
We know that each increment $B_{\frac{k+i+1}{n}}(\omega) -
  B_{\frac{k+i}{n}}(\omega)$ is Gaussian with variance $1/n$.  Thus
 we can apply the simple bound for a $N(0,1)$ random
  variable $Z$,
\begin{align*}
\probability{\abs{Z} \leq \lambda} &= \frac{1}{\sqrt{2\pi}}
\int_{-\lambda}^\lambda e^{-x^2/2} \, dx \leq \frac{1}{\sqrt{2\pi}}
\int_{-\lambda}^\lambda \, dx = \frac{2\lambda}{\sqrt{2\pi}}
\end{align*}
to conclude 
\begin{align*}
\probability{\abs{B_{\frac{k+i+1}{n}}(\omega) -
  B_{\frac{k+i}{n}}(\omega)} \leq 2 C \left( \frac{m}{n}
\right)^\alpha} &\leq \frac{4 C \sqrt{n}}{\sqrt{2\pi}}\left( \frac{m}{n}
\right)^\alpha
\end{align*}
By a union bound and the independence of Brownian increments we know
that 
\begin{align*}
&\probability{\cup_{k=0}^{n-m-1} \cap_{i=0}^{m-1}
\lbrace \omega \mid \abs{B_{\frac{k+i+1}{n}}(\omega) -
  B_{\frac{k+i}{n}}(\omega)} \leq 2 C \left( \frac{m}{n}
\right)^\alpha \rbrace} \\
&\leq n \left( \frac{4 C \sqrt{n}}{\sqrt{2\pi}}\left( \frac{m}{n}
\right)^\alpha\right)^m =\left( \frac{4 C m^\alpha}{\sqrt{2\pi}}\right)^m n^{1 + (\frac{1}{2}-\alpha)m}
\end{align*}
The important point is if we choose any value of $m > \frac{1}{\alpha
  - 1/2}$ (possible since $\alpha > 1/2$) then the exponent $1 +
(\frac{1}{2}-\alpha)m < 0$ and taking the limit as $n \to \infty$ we
see that $G(\alpha, C, \epsilon)$ is contained in a set of measure
zero.

The proof of the Theorem is completed by taking the countable union 
over all rational $C$ and rational $\epsilon$ and noting that this is
also contained in a set of measure zero.
\end{proof}

\begin{cor}[Nondifferentiability of Brownian Motion]Almost sure a
  Brownian path is nowhere differentiable.  
\end{cor}
\begin{proof}Take $\alpha = 1$ in the Theorem \ref{BrownianNotHolderContinuous}
\end{proof}

\begin{thm}[Markov Property of Brownian motion]\label{BrownianMarkovProperty}Let $B_t$ be a Brownian motion starting at $x$ and let $s
  \geq 0$.  Then $B_{t+s} - B_s$ is a Brownian motion starting at $0$
  that is independent of $B_t$ for $0 \leq t \leq s$.
\end{thm}
\begin{proof}
The fact that $B_{t+s} - B_s$ is a Brownian motion follows from the
fact that increments of the translated process are increments of the
original Brownian motion.  More precisely if we select $t_1 \leq
\cdots \leq t_n$ then each $(B_{t_{i+1}+s} - B_s) - (B_{t_{i}+s} -
B_s) = B_{t_{i+1}+s} - B_{t_i + s}$ and therefore they are we can
conclude they are jointly independent Gaussian with variance $(t_{i+1}
- s) - (t_i - s) = t_{i+1} - t_i$.

The independence of the Brownian motion $B_{t+s} - B_s$ and $B_t$ for
$0 \leq t \leq s$ follows from the property of independent
increments.  Specifically, by the montone class argument of Lemma \ref{IndependenceFinitary} we know that it is sufficient
to show independence for finite sets $\lbrace B_{{t_1}+s} - B_s,
\dots ,B_{{t_n}+s} - B_s \rbrace$ and $\lbrace B_{s_1}, \dots,
B_{s_m}\rbrace$ for all finite sequence of times $s_1 \leq \cdots \leq s_m \leq s$ and $0 \leq t_1 \leq \cdots \leq
t_n$.  Observe that for any measurable random vectors $\xi_1, \dots ,
\xi_n$ we have $\sigma(\xi_1, \xi_2 - \xi_1, \dots,\xi_n - \xi_1) =
\sigma(\xi_1, \xi_2 - \xi_1, \dots,\xi_n - \xi_{n-1})$ (to see this
note that every term on the left is a sum of terms on the right and
vice versa).  In particular by independence of increments and Lemma
\ref{IndependenceGrouping} we know that $\sigma(B_{{t_1}+s} - B_s,
\dots ,B_{{t_n}+s} - B_{t_{n-1}})$ and $\sigma( B_{s_1} - B_0, \dots,
B_{s_m} - B_{s_{m-1}})$ are independent
which establishes the result by applying the previous observation.
\end{proof}

\subsection{Skorohod Embedding and Donsker's Theorem}
TODO: Clarify what we mean when we say a Brownian motion is
independent of a $\sigma$-algebra.  ANSWER: Independence of a Brownian
motion and $\sigma$-algebra is interpreted by thinking of the Brownian
motion as a stochastic process.  Because the $\sigma$-algebra on
$\reals^{\reals_+}$ (or $\reals^{[0,1]}$) is generated by
projections/evaluation maps $\pi_t(f) = f(t)$ we can check
independence by checking independence on finite dimensional
projections $\lbrace (B_{t_1}, \dotsc, B_{t_n}) \in A \rbrace$ by
monotone classes.  Up till this point we have been treating
independence in a slightly different (though I expect equivalent) way
of saying that the $\sigma$-algebra $\sigma(B_t)$ is the basis of independence.

TODO: Introduce the right continuous filtration $\mathcal{F}^+_t$

TODO: Extend the Markov property of Brownian motion to the filtration $\mathcal{F}^+_t$.

TODO: Define $\mathcal{F}$-Brownian motion

\begin{thm}[Markov  Property]\label{MarkovPropertyBrownianMotion}Let $B_t$ be a
  Brownian motion then for any $s \geq 0$ the process $B^*_t = B_{t + s} -
  B_s$ is a standard Brownian motion independent of $\lbrace B_t \mid
  0 \leq t \leq s \rbrace$.
\end{thm}
\begin{proof}
We simply walk through the defining properties of Brownian motion:
\begin{itemize}
\item[(i)] Clearly $B^*_0 = B_s - B_s = 0$.
\item[(ii)] For any $0 \leq t_1 \leq \cdots \leq t_n$ the increment
  $B^*_{t_j} - B^*_{t_{j-1}} = B^*_{s+ t_j} - B^*_{s+ t_{j-1}}$
  therefore the independence of the increments $B^*_{t_2} -
  B^*_{t_1}, \dotsc, B^*_{t_n} - B^*_{t_{n-1}}$ follows from the fact
  that $B_t$ is a Brownian motion
\item[(iii)]By the same argument as in (ii), for any $t_1 < t_2$ we
  have $B^*_{t_2} -  B^*_{t_1} = B_{s+t_2} -  B_{s+t_1}$ is normallly
  distributed with mean $0$ and variance $(s + t_2) - (s+t_1) = t_2 -
  t_1$.
\item[(iv)]The paths $B^*_t = B_{s+t}$ are almost surely continuous
  because $B_t$ is a Brownian motion
\end{itemize}

To see the independence statement pick $0 \leq t_1 \leq \cdots \leq
t_n$ and $0 \leq s_1 \leq \cdots \leq s_m \leq s$ 

TODO: Finish
\end{proof}

The Markov property of Brownian motion can be extended to a slightly
stronger statement.
\begin{thm}\label{ExtendedMarkovPropertyBrownianMotion}Let $B_t$ be a
  Brownian motion then for any $s \geq 0$ the process $B^*_t = B_{t + s} -
  B_s$ is a standard Brownian motion independent of $\mathcal{F}^+_s$.
\end{thm}
\begin{proof}
Suppose $s \geq 0$ is chosen.  We have already shown that $B^*_t$ is a standard Brownian motion
independent of $\mathcal{F}^0_s$; we only need to extend the independence
statement to the larger filtration $\mathcal{F}^+_s = \cap_{t > s}
\mathcal{F}^0_t$.  Let $s_n$ be a sequence of real numbers such that
$s_n \downarrow s$ and define for each $n \in \naturals$ the process $B^n_t
= B_{t+s_n} - B_{s_n}$ which by the Markov Property Theorem
\ref{MarkovPropertyBrownianMotion} is a Brownian motion independent of
$\mathcal{F}^0_{s_n} \supset \mathcal{F}^+_{s} $.  By almost sure continuity of $B_t$ we know
that almost surely $\lim_{n \to \infty} B^n_t = B^*_t$ and therefore
$B^*_t$ is also independent of $\mathcal{F}^+_{s}$.  TODO: We make
this last argument several times in this section (see proof of the
Strong Markov Property below) so we should factor it out for easy reference.
\end{proof}

\begin{thm}[Strong Markov Property]\label{StrongMarkovPropertyBrownianMotion}Let $B_t$ be a
  Brownian motion and let $\tau$ be an almost surely finite $\mathcal{F}^+$-optional
  time, then $B^*_t = B_{\tau + t} - B_\tau$ is a standard Brownian
  motion independent of $\mathcal{F}^+_\tau$.
\end{thm}
\begin{proof}
First suppose that $\tau$ has a countable range $S \subset \reals_+$
and for each $s \in S$ define $B^s_t = B_{s+t} - B_s$.
Let $A$ be a
measurable subset of $\reals^{[0,\infty)}$ and let $E \in
\mathcal{F}^+_\tau$.  Now using the fact that $\tau$ is
$\mathcal{F}^+$-optional and the Markov property of $B^s_t$ (Theorem
\ref{ExtendedMarkovPropertyBrownianMotion}) we get
\begin{align*}
\probability{\lbrace B^*_t \in A \rbrace \cap E} &= \sum_{s \in S}
\probability{ \lbrace B^s_t
  \in A \rbrace \cap E \cap \lbrace \tau = s \rbrace } \\
&=\sum_{s \in S} \probability{ B^s_t
  \in A } \probability{ E \cap \lbrace \tau = s \rbrace } \\
&=\probability{ B_t
  \in A } \sum_{s \in S} \probability{ E \cap \lbrace \tau = s \rbrace } \\
&= \probability{ B_t  \in A } \probability{ E } 
\end{align*}
which shows that the process $B^*_t$ is independent of
$\mathcal{F}^+$.  
It is clear that $B^*_t$ is almost surely continuous and $B^*_0 = 0$;
furthermore taking $E = \Omega$ and $A = (\pi_{t_1}, \dotsc,
\pi_{t_d})^{-1}(C)$ for some $C \in \mathcal{B}(\reals^d)$ in the above calculation and we see
that $B^*_t$ has independent Gaussian increments, thus $B^*_t$ is a
standard Brownian motion.  TODO:  Is there anything that needs to be
done to show that $B^*_t$ is measurable?

It remains to extend the result to arbitrary $\mathcal{F}^+$-optional times.  It is clear that $B^*_t$ is
almost surely continuous and $B^*_0 = 0$.  Given
$\tau$ an $\mathcal{F}^+$-optional time let $\tau_n = \frac{1}{2^n}
\floor{2^n \tau + 1}$ so that $\tau_n \downarrow \tau$ (Lemma
\ref{DiscreteApproximationOptionalTimes}) and by definition each
$\tau_n$ is $\mathcal{F}^+$-optional.  For each $n \in \naturals$ define $B^n_t = B_{\tau_n + t} -
B_{\tau_n}$ and apply the result for countably valued optional times
to conclude $B^n_t$ is a standard Brownian motion independent of
$\mathcal{F}^+_{\tau_n} \supset \mathcal{F}^+_{\tau} $.  Since $B_t$ is almost surely
continuous we know that almost surely $B^*_t = \lim_{n \to \infty}
B^n_t$.  Now by Dominated Convergence and the fact that $B^n_t$ is
independent of $\mathcal{F}^+_{\tau}$ for all $n$ we get for all $E
\in \mathcal{F}^+_{\tau}$, all
bounded continuous functions $f : \reals^d \to \reals$ and $0 \leq t_1 \leq
\dotsb \leq t_d < \infty$,
\begin{align*}
\expectation{f(B^*_{t_1}, \dotsc, B^*_{t_d}) ; E} &= 
\lim_{n \to  \infty} \expectation{f(B^n_{t_1}, \dotsc, B^n_{t_d}) ; E}
= \lim_{n \to  \infty} \expectation{f(B^n_{t_1}, \dotsc, B^n_{t_d})} \probability{E}
=\expectation{f(B^*_{t_1}, \dotsc, B^*_{t_d})} \probability{E} 
\end{align*}
Given an arbitrary open set $U \subset \reals^d$ we define $f_n(x) = n
d(x, U^c) \wedge 1$ so that $f_n$ is bounded and continuous and $f_n
\downarrow \characteristic{U}$ so that by Monotone Convergence we get
\begin{align*}
\probability{ \lbrace (B^*_{t_1}, \dotsc, B^*_{t_d}) \in U \rbrace
  \cap E} = \probability{ (B^*_{t_1}, \dotsc, B^*_{t_d}) \in U } \probability{E}
\end{align*}
and because sets of the form $\lbrace (B^*_{t_1}, \dotsc, B^*_{t_d})
\in U \rbrace$ are a $\pi$-system generating $B^*_t \in E$, we get 
\begin{align*}
\probability{ \lbrace B^*_t \in A \rbrace
  \cap E} = \probability{ B^*_t \in A } \probability{E}
\end{align*}
Now arguing exactly as in the countable case this shows that $B^*_t$
is a standard Brownian motion independent of $\mathcal{F}^+_\tau$.
\end{proof}

The following corollary of the strong Markov property turns out to be
a very useful tool in calculating the distributions of various
functions of Brownian motion.  It is called the reflection principle
because it shows that if one runs a Brownian motion up to an optional
time $\tau$ and then reverses the sign of all subsequent increments
(reflecting the graph of the Brownian motion with respect to the line
$y=\tau$) then the resulting process has same distribution.  TODO: Draw a picture illustrating the
geometry of reflection.
\begin{lem}[Reflection Principle]\label{ReflectionPrinciple}Let $B_t$ be a Brownian motion and let $\tau$ be an
  optional time then 
\begin{align*}
B^\prime_t &= B_{\tau \wedge t} - (B_t - B_{\tau \wedge t}) = \begin{cases}
B_t & \text{when $t \leq \tau$} \\
2 B_\tau - B_t & \text{when $t > \tau$}
\end{cases}
\end{align*}
is a Brownian motion with the same distribution as $B_t$.
\end{lem}
\begin{proof}
First assume that $\tau$ is almost surely finite.  Define $B^\tau_t =
B_{\tau \wedge t}$ and $B^*_t = B_{\tau + t} - B_{\tau}$.  Because
$B_t$ is continuous we know that $B_t$ is progressively measurable
(Lemma \ref{ContinuityAndProgressiveMeasurability}) and therefore $B^\tau$ is
$\mathcal{F}_\tau$-measurable (Lemma \ref{StoppedProgressivelyMeasurableProcess}).  By the Strong Markov Property (Theorem
\ref{StrongMarkovPropertyBrownianMotion}) we know that $B^*$ is a standard Brownian
motion independent of $\mathcal{F}^+_\tau$ hence independent of $\tau$
and $B^\tau$; the same is true of $-B^*$.  Combining independence and
the equality of the marginal distributions we know $(\tau, B^\tau,
B) \eqdist (\tau, B^\tau, -B^*)$ (Lemma
\ref{IndependenceProductMeasures}).  Now define $G : \reals \times
\reals^{[0,\infty)} \times \reals^{[0,\infty)} \to
\reals^{[0,\infty)}$ by $G(t,f,g)(s) = f(s) + g((s-t)_+)$ and note
that $B_t = G(\tau, B^\tau, B^*)$ and $B^\prime_t = G(\tau, B^\tau,
-B^*)$ so the result follows once we verify that $G$ is measurable.

Unfortunately in the generality we've defined it, $G$ is not
measurable.  However all we really need is the fact that restriction
of $G$ to $C([0,\infty), \reals)$ is measurable.  Here we peek ahead
to use the fact that the $\sigma$-algebra induced on $C([0,\infty),
\reals)$ from the product $\sigma$-algebra is the Borel
$\sigma$-algebra coresponding to the topology of uniform convergence
on compact sets (see Lemma \ref{BorelGeneratedByProjections}).  It is
easy to see that $G$ is continuous hence measurable.

TODO: Can we avoid appealing to the continuity argument and see the
measurability directly?
\end{proof}

\begin{lem}Let $B_t$ be a standard Brownian motion, $0 \leq x < b$ and
  $\tau = \inf \lbrace t \mid B_t \geq b\rbrace$.  Then 
\begin{align*}
\probability{\sup_{0 \leq s \leq t} B_t \geq b ; B_t < x} =
\probability{B_t > 2b - x }
\end{align*}
\end{lem}
\begin{proof}
A general fact seems to be that many of the consequences of the Strong
Markov Property can be shown without making a direct appeal to the
Strong Markov Property.  Here is a proof of the reflection principle that doesn't use the
Strong Markov Property directly but instead replays key parts of the proof
Strong Markov Property for Brownian motion.

Define 
$\tau_n = \frac{1}{2^n}
\floor{2^n \tau + 1}$ so that $\tau_n \downarrow \tau$ (Lemma
\ref{DiscreteApproximationOptionalTimes}).  First consider
\begin{align*}
\probability{\tau_n \leq t; B_t - B_{\tau_n} < x - b} &=
\sum_{k=0}^{\floor {2^n t}} \probability{\tau_n = k/2^n; B_t -
  B_{k/2^n} < x - b} \\
&= \sum_{k=0}^{\floor {2^n t}} \probability{\tau_n = k/2^n} \probability{ B_t -
  B_{k/2^n} < x - b} \\
&= \sum_{k=0}^{\floor {2^n t}} \probability{\tau_n = k/2^n} \probability{ 
B_t -  B_{k/2^n} > b - x} \\
&= \probability{\tau_n \leq t ;
  B_{t} - B_{\tau_n} > b - x} \\
\end{align*}
where we have used the fact that $B_t - B_{k/2^n}$ is independent of
$\mathcal{F}_{k/2^n}$ and $\tau_n = k/2^n \in \mathcal{F}_{k/2^n}$
since $\tau_n$ is an optional time and the fact that a Gaussian
distribution is symmetric about $0$.

Now because $B_t$ is almost surely continuous we have that $B_\tau =
b$ almost surely and therefore $\lbrace \tau = t  \rbrace \subset
\lbrace B_t = b \rbrace$ and because $B_t$ is a Gaussian random
variable we know that $\probability{\tau = t} = \probability{B_t = b}
= 0$.  Similarly, because the increment $B_t - B_\tau$ is Gaussian we
know that $\probability{B_t - B_\tau = b -x } = \probability{B_t -
  B_\tau = x-b} = 0$.  Therefore both $(-\infty, t] \times (b-x,\infty)$ and $(-\infty, t] \times (-\infty,x-b)$ are
$\mathcal{L}(\tau,B_t - B_\tau)$-continuity sets and by the
Portmanteau Theorem \ref{PortmanteauTheorem} we get
\begin{align*}
\probability{\tau \leq t; B_t - B_\tau < x - b}
&= \lim_{n \to \infty} \probability{\tau_n \leq  t; B_t - B_{\tau_n} < x - b} \\
&= \lim_{n \to \infty} \probability{\tau_n \leq t; B_t - B_{\tau_n} >  b - x} \\
&=\probability{\tau \leq t; B_t - B_\tau > b - x}
\end{align*}
Using the fact that $B_\tau = b$ we can rewrite the equality as
\begin{align*}
\probability{\tau \leq t; B_t < x }
&=\probability{\tau \leq t; B_t > 2b - x}
\end{align*}
and by the continuity of $B_t$ we know that $\lbrace \tau \leq t
\rbrace = \lbrace \sup_{0 \leq s \leq t} B_s \geq b \rbrace$ and $
\lbrace \sup_{0 \leq s \leq t} B_s \geq b \rbrace \subset \lbrace B_t
> 2b - x\rbrace$ and therefore we get
\begin{align*}
\probability{\sup_{0 \leq s \leq t} B_s \geq b ; B_t < x }
&=\probability{B_t > 2b - x}
\end{align*}
\end{proof}

\begin{lem}\label{BrownianMaximumProcessLaw}Let $M_t = \sup_{0 \leq s \leq t}
  B_s$ be the maximal process associated with a standard Brownian
  motion then $M_t \eqdist \abs{B_t}$.
\end{lem}
\begin{proof}
Suppose $x > 0$ then we can calculate using continuity of measure, the Reflection Principle
and the fact that $\probability{B_t = x} = 0$,
\begin{align*}
\probability{ \sup_{0 \leq s \leq t} B_s \geq x}
&=\probability{ \sup_{0 \leq s \leq t} B_s \geq x, B_t < x} +
\probability{ \sup_{0 \leq s \leq t} B_s \geq x, B_t \geq x} \\
&=\lim_{n \to \infty} \probability{ \sup_{0 \leq s \leq t} B_s \geq x,
  B_t < x- 1/n} + 
\probability{B_t \geq x} \\
&=\lim_{n \to \infty} \probability{ B_t > x + 1/n} + 
\probability{B_t \geq x} \\
&= \probability{B_t > x} + \probability{B_t \geq x} = \probability{\abs{B_t} \geq x}\\
\end{align*}
From this it follows that $\probability{ \sup_{0 \leq s \leq t} B_s
  \geq 0} = 1$ so in addition we have $\probability{ \sup_{0 \leq s \leq t} B_s
  \leq x} = 0$ for all $x \leq 0$ and the result is shown.
\end{proof}

\begin{lem}Let the $B_t$ be a standard Brownian motion the $B_t^2 - t$
  and $B_t^4 - 6tB_t^2 + 3t^2$ are both martingales.
\end{lem}
\begin{proof}
TODO: Prove using independent increments.
\end{proof}

TODO: Wald's Lemma seems to apply only to specific filtrations.
Clarify this in the statement.

\begin{lem}[Wald's Lemma]\label{WaldLemma}Let $B_t$ be a standard
  Brownian motion and let $\tau$ be an $\mathcal{F}$-optional time
 such that $B_\tau$ is bounded then
\begin{itemize}
\item[(i)]$\expectation{B_\tau} = 0$
\item[(ii)]$\expectation{B_\tau^2} = \expectation{\tau}$
\item[(iii)]$\expectation{\tau^2} \leq 4\expectation{B_\tau^4}$
\end{itemize}
\end{lem}
\begin{proof}
The idea is that the first two results are consequences of optional
stopping (e.g. to get (i) let  $\sigma = 0$  
then apply Optional Stopping to conclude $\expectation{B_\tau} =
\expectation{\cexpectationlong{\mathcal{F}_0}{B_\tau}} = B_0 = 0$; to
get (ii) one argues using the martingale $B_t^2 - t$ and to get (iii)
one argues using the martingale $B_t^4 -6tB_t^2 + 3t^2$).  The trick is
that $\tau$ is not assumed bounded so we cannot apply Theorem
\ref{OptionalStoppingContinuous}.  To fix this, pick an arbitrary $T >
0$ and argue as above to conclude that $\expectation{B_{\tau \wedge
    T}} = 0$, $\expectation{B_{\tau \wedge
    T}^2} = \expectation{\tau \wedge T}$ and $\expectation{B_{\tau
    \wedge T}^4 } + 3\expectation{(\tau \wedge T)^2} = 6
  \expectation{(\tau \wedge T) B_{\tau \wedge T}^2}$.  Now by the boundedness of
$B_\tau$, we know that $B_{\tau \wedge T}$ is bounded so we may apply
Dominated Convergence to conclude $\expectation{B_\tau} = \lim_{T \to
  \infty} \expectation{B_{\tau \wedge T}} = 0$ and $0 \leq \tau \wedge T
\uparrow \tau$ so by Dominated Convergence and Monotone Convergence we
have
\begin{align*}
\expectation{B_\tau^2} = \lim_{T \to \infty} \expectation{B_{\tau
    \wedge T}^2} = \lim_{T \to \infty} \expectation{\tau \wedge T} = \expectation{\tau}
\end{align*}

As a consequence of (ii) and the boundedness of $B_\tau$ we now that
$\expectation{\tau} < \infty$ and therefore $(\tau  \wedge T)
B_{\tau\wedge T}^2$ is bounded
by the integrable function $C^2 \tau$ where $C = \sup_{0 \leq t <
  \infty} \abs{B_t}$ and we can use Dominated Convergence
and Monontone Convergence to take limits and conclude 
\begin{align*}
\expectation{B_{\tau}^4 } + 3\expectation{\tau^2} &= \lim_{T \to
  \infty} \expectation{B_{\tau
    \wedge T}^4 } + 3\lim_{T \to \infty} \expectation{(\tau \wedge
  T)^2} \\
&= 6
\lim_{T \to \infty}  \expectation{(\tau \wedge T) B_{\tau \wedge T}^2}
=  6 \expectation{\tau B_{\tau}^2} \\
&\leq 6 (\expectation{\tau^2} \expectation{B_\tau^4})^{1/2}
\end{align*}
where in the last line we have used Cauchy Schwartz (Lemma
\ref{CauchySchwartz}).  If we divide by $\expectation{B_\tau^4}$ and
write $r = (\expectation{\tau^2} /\expectation{B_\tau^4})^{1/2}$ the
inequality we have proven is $1 + 3r^2 \leq 6 r$.  Now simple algebra
shows 
$3(r -1)^2 = 3r^2 - 6r + 3 \leq 2$
and therefore $r \leq 1 + \sqrt{2/3} < 2$. Upon backsubstituting the
definition of $r$ the inequality (iii) is proven.
\end{proof}

As a small step toward Skorohod embedding we first let $x \leq 0 \leq
y$ be two real numbers and consider the hitting time $\tau_{x,y} =
\inf \lbrace t \geq 0 \mid B_t = x \text{ or } B_t = y\rbrace$.  By
  continuity of $B_t$ and the closedness of the set $\lbrace x,y
  \rbrace$ we know from Lemma \ref{HittingTimesContinuous} that
  $\tau_{x,y}$ is an optional time and by Wald's Lemma just proven
  (TODO: Show that $\tau_{x,y}$ is almost surely finite; from this it
  follows trivially from the definition of $\tau_{x,y}$ that
  $B_{\tau_{x,y}}$ is almost surely bounded by $-x 
\vee y$) we
  know that $\expectation{B_{\tau_{x,y}}} = 0$.  The point to bring
  out is that since the distribution of $B_{\tau_{x,y}}$ is supported
  on the two points $\lbrace x,y \rbrace$ by definition the condition
  $\expectation{B_{\tau_{x,y}}} = 0$ uniquely determines the
  distribution to be $\mathcal{L}(B_{\tau_{x,y}}) = \frac{y \delta_x - x
    \delta_y}{y-x}$ and moreover tells us that every mean zero measure
  supported on two points $\lbrace x,y \rbrace$ may be represented as
  a $B_{\tau_{x,y}}$.  
Given the tools we have developed, this fact was
  quite easy to see but what is less clear is that it can pushed further to
  represent an arbitrary mean zero random variable as a stopped
  Brownian motion.

TODO:  Have we shown that the integral of the measures $\nu_{a,b}$ is
a well defined object?

\begin{lem}\label{MixtureTwoPoint}Let $\mu$ a Borel measure on $\reals$ such that $\int x \,
  d\mu = 0$ and for all $a \leq 0 \leq b \in \reals$ define the measure
\begin{align*}
\nu_{a,b} &=  \begin{cases}
\delta_0 & \text{if $a=0$ or $b=0$} \\
\frac{b \delta_a - a \delta_b}{b - a} & \text{ if $a < 0 < b$}
\end{cases}
\end{align*}
then $\nu_{a,b} : \reals_{-} \times \reals_+ \to \mathcal{P}(\reals)$ is a probability kernel and there exists a measure $\tilde{\mu}$ on $\reals_{-}
\times\reals_+$ such that 
\begin{align*}
\mu(A) &= \int \nu_{a,b}(A) \, d\tilde{\mu}(a,b) \text{ for all $A \in \mathcal{B}(\reals)$}
\end{align*}
\end{lem}
\begin{proof}
To see that $\nu_{a,b}$ is a kernel, it is immediate from the
definition that for fixed $(a,b) \in \reals_{-} \times \reals_+$
$\nu_{a,b}$ is a probability measure.  For fixed $A \in
\mathcal{B}(\reals)$ we have
\begin{align*}
\nu_{a,b}(A) &= \begin{cases}
\characteristic{A}(0) & \text{if $a=0$ or $b=0$} \\
\frac{b}{b-a} \characteristic{A}(a) -
\frac{a}{b-a}\characteristic{A}(b) & \text{if $a < 0 < b$}
\end{cases}
\end{align*}
which is a measurable function of $(a,b)$ by measurability of the sets
$A$, $\lbrace (a,b) \in \reals^2 \mid a=0
\text { or } b=0\rbrace$ and $\lbrace (a,b) \in \reals^2 \mid a < 0 <
b \rbrace$.

Denote by $\mu_+$  the restriction $\mu_{(0,
  \infty)}$ of $\mu$ to the interval $(0, \infty)$ and by  $\mu_-$  the
  restriction $\mu_{(-\infty,0)}$ of $\mu$ to the interval
    $(-\infty,0)$.
Define $c = \int x \, d\mu_+$ and by the condition $\int x \, d\mu$
note that $c = -\int x \, d\mu_-$. Now let $f : \reals \to \reals_+$
be a non-negative Borel measurable function and calculate using Tonelli's Theorem (Theorem \ref{Fubini})
\begin{align*}
c \int f(y) \, d\mu(y) &= c\mu(\lbrace 0 \rbrace) f(0) + c \int f(y) \, d\mu_+(y) + c \int f(y) \,
d\mu_-(y) \\
&= c\mu(\lbrace 0 \rbrace) f(0) -\int x \, d\mu_-(x) \int f(y) \, d\mu_+(y) + \int x \, d\mu_+(x)
\int f(y) \, d\mu_-(y) \\
&= c\mu(\lbrace 0 \rbrace) f(0)  + \int \left( yf(x) - xf(y) \right) \, d(\mu_{-} \otimes\mu_+ )(x,y) \\
&= c\mu(\lbrace 0 \rbrace) f(0) + \int \left( y-x \right)\left[ \int f(z) \, d\nu_{x,y} \right] \, d(\mu_{-} \otimes\mu_+ )(x,y)
\end{align*}
where in the last line we have used the direct calculation $\int f \, d\nu_{x,y} =
\frac{yf(x) - xf(y)}{y-x}$.

We can also compute for measurable $f$ 
\begin{align*}
\int \left [ \int f \, d\nu_{x,y} \right ]\, d\delta_{0,0}(x,y) 
&= \int f \, d\nu_{0,0} = f(0)
\end{align*}

Thus if for every $\mu$ we define 
\begin{align*}
\tilde{\mu} &= \mu(\lbrace 0 \rbrace)\delta_{0,0} + \frac{y-x}{\int x
  \, d\mu_+(x)} \mu_{-} \otimes \mu_{+}
\end{align*}
we have for all non-negative Borel measurable $f$, $\int f \, d\mu
= \int \left[ \int f \, d\nu_{a,b} \right ] \, d\tilde{\mu}(a,b)$.  In particular this holds for indicator functions.

The measurability of the map $\mu \to \tilde{\mu}$ follows by noting
it is a composition of a number of measurable maps; indeed by
definition of the $\sigma$-algebra on the space of measures we know
that $\mu \to \mu(\lbrace 0 \rbrace)$ is measurable and Lemma
\ref{MeasurableMappingsOfMeasures} shows that the restrictions $\mu
\to \mu_{\pm}$, the integral $\mu \to \int x \, d\mu_+(x)$ and the
product measure $(\mu_+, \mu_-) \to \mu_- \otimes \mu_+$ are all
measurable mappings of measures.
\end{proof}

We are now ready to show that one can represent any mean zero Borel
probability measure in the form $B_\tau$ for an appropriate optional
time $\tau$.

\begin{lem}\label{EmbeddingSingleRandomVariableInBrownian}Let 
\begin{itemize}
\item[(i)] $\mu$ be a Borel probability measure on $\reals$ with $\int x
  \, d\mu = 0$
\item[(ii)] $\tilde{\mu}$ is a Borel measure on $\reals_-
  \times \reals_+$ such that $\mu(A) = \int \nu_{x,y}(A) \,
  d\tilde{\mu}(x,y)$ for all $A \in \mathcal{B}(\reals)$
\item[(iii)] $(\alpha, \beta)$ be a random element in $\reals_-
  \times \reals_+$ such that $\mathcal{L}(\alpha, \beta) =
  \tilde{\mu}$
\item[(iv)] $B_t$ be an independent Brownian motion
\item[(v)] $\mathcal{F}$ be the filtration defined by $\mathcal{F}_t =
  \sigma( \cup_{s \leq t} \sigma(B_s) \cup \sigma(\alpha) \cup \sigma(\beta))$
\end{itemize}
Then 
\begin{align*}
\tau &= \inf \lbrace t \geq 0 \mid B_t = \alpha \text { or } B_t =
\beta \rbrace
\end{align*}
is an $\mathcal{F}$-optional time and 
\begin{align*}
\mathcal{L}(B_\tau) &= \mu & \expectation{\tau} &= \int x^2 \, d\mu(x)
& \expectation{\tau^2} &\leq 4 \int x^4 \, d\mu(x)
\end{align*}
\end{lem}
\begin{proof}
First note that by independence of the $B_t$ and $(\alpha, \beta)$ we
also know that $B_t -B_s$ is independent of $(\alpha, \beta)$ for all
$s \leq t$ and therefore $B_t$ is an $\mathcal{F}$-Brownian motion.
To see that $\tau$ is $\mathcal{F}$-optional we recast the definition
of $\tau$ slightly to make it clear that it is (almost) a hitting time
$\tau = t \geq 0 \mid \lbrace \frac{B_t - \alpha}{\beta} \in
\lbrace 0, 1\rbrace \rbrace$ (it is not a hitting time since we have
the condition $t\geq 0$ rather than $t>0$).  Pick $0 \leq t < \infty$,
there is an analogous but
simpler argument to that in 
Lemma \ref{HittingTimesContinuous}  using the the continuity of $B_t$,
closedness of $\lbrace 0, 1\rbrace$ to conclude
\begin{align*}
\lbrace \tau \leq t \rbrace &= \lbrace \alpha \neq \beta \rbrace \cap
\cap_{n=1}^\infty 
\cup_{\substack{0 \leq q \leq  t \\ q \in \rationals} }
\lbrace \frac{B_t - \alpha}{\beta - \alpha} \in (-1/n,1/n) \cup
(1-1/n, 1+1/n) \rbrace \\
&\cup  \lbrace \alpha = \beta \rbrace \cap \cap_{n=1}^\infty 
\cup_{\substack{0 \leq q \leq  t \\ q \in \rationals} }
\lbrace B_t - \alpha \in (-1/n,1/n) \rbrace 
\end{align*}
and therefore $\mathcal{F}_t$ measurability follows from the
adaptedness of $B_t$.

To calculate the distribution of $B_\tau$ we let $A \in
\mathcal{B}(\reals)$ and consider $B_\tau$ as a function of the
independent random elements $B_t$ and $(\alpha, \beta)$ applying
Fubini (specifically Lemma \ref{DisintegrationIndependentLaws}), the
Expectation Rule (Lemma \ref{ExpectationRule}) and the definition of
$\tilde{\mu}$ to get
\begin{align*}
\probability {B_\tau \in A} &=
\expectation{\probability{B_{\tau_{x,y}} \in A}\mid_{(x,y)=(\alpha,
    \beta)}} = \expectation{ \nu_{\alpha, \beta} (A)} = \int
\nu_{x,y}(A) \, d\tilde{\mu}(x,y) = \mu(A)
\end{align*}
Now we compute using Lemma \ref{DisintegrationIndependentLaws} , Wald's Lemma \ref{WaldLemma}, the
Expectation Rule and the
just proven fact that the distribution of $B_\tau$ is $\mu$ to see
\begin{align*}
\expectation{\tau} &= \expectation{\expectation{\tau_{x,y}}\mid_{(x,y)
    = (\alpha, \beta)}} = \expectation{B_\tau^2} = \int x^2 \, d\mu(x)
\end{align*}
and in the same way
\begin{align*}
\expectation{\tau^2} &=  \expectation{\expectation{\tau_{x,y}^2}\mid_{(x,y)
    = (\alpha, \beta)}} \leq 4\expectation{B^4_\tau} = 4\int x^4 \, d\mu(x)
\end{align*}
\end{proof}

We can now complete the embedding of a random walk in a suitable Brownian
motion.
\begin{thm}[Skorohod Embedding]\label{EmbeddingRandomWalkInBrownianMotion}Let $\xi, \xi_1, \xi_2, \dotsc$ be i.i.d. random variables
  such that $\expectation{\xi} =0$.  Define $S_n = \xi_1 + \dotsb + \xi_n$, there exists a
  probability space $(\Omega, \mathcal{A}, P)$ and a filtration
  $\mathcal{F}$ with a Brownian motion $B_t$ and
  $\mathcal{F}$-optional times $0=\tau_0 \leq \tau_1 \leq \dotsb $
  such that $(B_{\tau_1}, B_{\tau_2}, \dotsc) \eqdist (S_1,S_2, \dotsc)$ and
  the differences $\Delta \tau_n = \tau_n - \tau_{n-1}$ are i.i.d. and
  satisfy
  $\expectation{\Delta \tau_n} = \expectation{\xi^2}$ and
  $\expectation{(\Delta \tau_n)^2} \leq 4 \expectation{\xi^4}$.
\end{thm}
\begin{proof}
Let $\mu$ be distribution of $\xi$ and let $B_t$ be a standard
Brownian motion.  Because $\expectation{\xi}=0$ by
Lemma \ref{MixtureTwoPoint} we know there is a $\tilde{\mu}$ such that
$\mu(A) = \int \nu_{x,y}(A) \, d\tilde{\mu}$ for all $A \in
\mathcal{B}(\reals)$.  Potentially extending the probability space of
$B_t$ we can assume that there are i.i.d random vectors
$(\alpha_1, \beta_1), (\alpha_2, \beta_2), \dotsc$ with distribution
$\tilde{\mu}$ that are independent of $B_t$ (Theorem
\ref{ExistenceCountableIndependentRandomVariables} TODO: This theorem
is stated for random variables; what is necessary to extend to random
vectors?).  
Define the filtrations 
\begin{align*}
\mathcal{F}^n_t &= \sigma(\alpha_k,
\beta_k, k \leq n , B_s, 0 \leq s \leq t) \text{ for $n>0$} \\
\mathcal{G}_n &= \sigma(\alpha_k,
\beta_k, k \leq n , B) \\
\mathcal{F}_t &= \sigma(\alpha_n, \beta_n, n >
0, B_s, 0 \leq s \leq t) \\
\end{align*}

Claim: $B_t$ is an $\mathcal{F}$-Brownian motion.

This follows from independence of $B_t$ and the $(\alpha_n, \beta_n)$ (TODO: more detail
presumably referencing Lemma \ref{IndependenceGrouping}).

Define the sequence of random times $0=\tau_0 \leq \tau_1 \leq \dotsb$
recursively by the formula for $n \geq 1$
\begin{align*}
B^{n-1}_t &= B_{\tau_{n-1} + t} - B_{\tau_{n-1}} \\
\tau_n &= \inf \lbrace t \geq \tau_{n-1} \mid B_t - B_{\tau_{n-1}} \in
\lbrace \alpha_n, \beta_n\rbrace \rbrace \\
&=\tau_{n-1} + \inf \lbrace t \geq 0 \mid B^{n-1}_t \in \lbrace \alpha_n, \beta_n\rbrace \rbrace
\end{align*}

Claim: $\tau_n$ is $\mathcal{F}$-optional and
$\mathcal{G}_n$-measurable for all $n \geq 0$.

In fact we shall show the stronger result that $\tau_n$ is
$\mathcal{F}^n$-optional.  This follows using induction and the explicit formula 
\begin{align*}
\lbrace \tau_n \leq t \rbrace 
&= \cap_{n=1}^\infty \cup_{\substack{0 \leq q
    \leq t \\ q \in \rationals}} \lbrace \tau_{n-1} \leq q \rbrace \cap
  \lbrace \frac{B_q - B_{\tau_{n-1}} - \alpha_n}{\beta_n - \alpha_n} \in (-1/n,1/n)
  \cup (1-1/n,1+1/n)\rbrace \\
&= \cap_{n=1}^\infty \cup_{\substack{0 \leq q
    \leq t \\ q \in \rationals}} \lbrace \tau_{n-1} \leq q \rbrace \cap
  \lbrace \frac{B_q - B_{\tau_{n-1} \wedge t} - \alpha_n}{\beta_n - \alpha_n} \in (-1/n,1/n)
  \cup (1-1/n,1+1/n)\rbrace 
\end{align*}
The fact that $B_t$ has continuous sample paths implies that it is
progressively measurable (Lemma
\ref{ContinuityAndProgressiveMeasurability}) and therefore since
$\tau_{n-1}$ is $\mathcal{F}^{n-1}$-optional (a fortiori
$\mathcal{F}^{n}$-optional) we know
that $B_{\tau_{n-1} \wedge t}$ is $\mathcal{F}^n_t$-measurable (Lemma \ref
{StoppedProgressivelyMeasurableProcess}).  Since $\alpha_n$ and
$\beta_n$ are $\mathcal{F}^n_0$-measurable and $B_q$ is
$\mathcal{F}^n_t$-measurable for all $q \leq t$, optionality is shown.
Now since $\mathcal{F}^n_t \subset \mathcal{G}_n$ for all $t \geq 0$,
we have $\lbrace \tau_n \leq t \rbrace \in \mathcal{G}_n$ for all $t
\geq 0$ and $\mathcal{G}_n$-measurability of $\tau_n$ follows (Lemma
\ref{IntervalsGenerateBorel} and Lemma \ref{MeasurableByGeneratingSet}).

Claim: $\Delta \tau_n$ are i.i.d.,  $\expectation{\Delta \tau_n} =
\expectation{\xi^2}$ and $\expectation{(\Delta \tau_n)^2} = \expectation{\xi^4}$

On the subset $C([0,\infty), \reals) \times \reals \times \reals$, we claim that the function
\begin{align*}
\Psi (f, a, b) &= \inf \lbrace t \geq 0 \mid f(t) \in \lbrace a, b
\rbrace \rbrace
\end{align*}
is measurable.  First, define the mapping $\tau_F(f) = \inf \lbrace t \geq 0 \mid f(t)
\in F\rbrace$ for continuous $f$ and closed $F$.  The often used formula 
\begin{align*}
\lbrace \tau_F(f) \leq t \rbrace &= \cap_{n=1}^\infty
\cup_{\substack{0 \leq q \leq t \\ q \in \rationals}} \lbrace d(f(q),
F) < 1/n\rbrace \text{ for $f$ continuous and $F$ closed} 
\end{align*}
shows that $\tau_F$ is measurable on $C([0,\infty), \reals) \cap
\reals^{[0,\infty)}$.  
Then factoring 
\begin{align*}
\Psi (f, a, b) &= \characteristic{a\neq b} \tau_{\lbrace 0,1
  \rbrace}((f-a)/(b-a)) + \characteristic{a = b} \tau_{\lbrace 0
  \rbrace}(f-a)
\end{align*}
and using measurability of group operations on set functions (Lemma
\ref{MeasurableFunctionGroup}) we get the measurability of $\Psi$.
We can write $\Delta \tau_n = \Psi(B_{\tau_{n-1}+t} - B_{\tau_{n-1}}, \alpha_n,
  \beta_n)$ and by the Strong Markov Property we know that for all $n
  \geq 0$, $B_{\tau_{n-1}+t} - B_{\tau_{n-1}}$ is a standard Brownian
  motion independent of $\mathcal{F}_{\tau_{n-1}}$ (hence independent
  of $(\alpha_n, \beta_n)$) and by construction $(\alpha_n, \beta_n)$ is i.i.d. with
  distribution $\tilde{\mu}$.  Therefore
  $\mathcal{L}(B_{\tau_{n-1}+t} - B_{\tau_{n-1}}, \alpha_n,  \beta_n)
  = \mathcal{L}(B_t) \otimes \tilde{\mu}$.  Since we have expressed
  $\Delta \tau_n$ as a function $ \Psi(B_{\tau_{n-1}+t} - B_{\tau_{n-1}}, \alpha_n,
  \beta_n)$ it follows from the Expectation Rule (Lemma
  \ref{ExpectationRule}) that $\probability{ \Delta \tau_n \in A} =
  \int \Psi(x,y,z) \, d \mathcal{L}(B_t)(x) \otimes \tilde{\mu}(y,z)$ and is the same for all $n \geq 0$.
Independence follows in a similar way.  By Lemma
\ref{IndependenceByPairwiseIndependence} it suffices for us to show
that $\cindependent{(\Delta \tau_0, \dotsc, \Delta \tau_{n}) }{\Delta
  \tau_{n+1}}{}$ for all $n\geq 0$.  In fact we shall prove something
a bit stronger.  Define $\mathcal{H}_n = \sigma(\tau_k, B_{\tau_k},
k\leq n)$; we shall show $\cindependent{\mathcal{H}_n}{\Delta
  \tau_{n+1}}{}$.  Applying Lemma
\ref{IndependenceByPairwiseIndependence} to the sequence of
$\sigma$-algebras $\sigma(B), \sigma(\alpha_1,\beta_1), \dotsc$ we know that
$\cindependent{(\alpha_{n+1}, \beta_{n+1})}{\mathcal{G}_n}{}$ for all
$n \geq 1$.  We have shown that $\tau_n$ is $\mathcal{G}_n$-measurable
and moreover $\tau_n$ is $\mathcal{F}^n$-optional therefore
$B_{\tau_n}$ is $\mathcal{F}^n_{\tau_n}$-measurable hence
$\mathcal{G}_n$-measurable.  Therefore $\sigma(B^n, \mathcal{H}_n)
\subset \mathcal{G}_n$ and we conclude $\cindependent{(\alpha_{n+1},
  \beta_{n+1})}{(B^n, \mathcal{H}_n)}{}$ for all
$n \geq 1$.  Now on the other hand, by the Strong Markov Property
Theorem \ref{StrongMarkovPropertyBrownianMotion} we know that $B^n$ is independent
of $\mathcal{F}_{\tau_n}$.  By $\mathcal{F}_{\tau_k}$-measurability
of $\tau_k$ and $B_{\tau_k}$ and the fact that $\tau_k \leq \tau_n$
for $k \leq n$ we know that $\mathcal{H}_n \subset
\mathcal{F}_{\tau_n}$ and we conclude that
$\cindependent{\mathcal{H}_n}{B^n}{}$ and therefore $\cindependent{(\alpha_{n+1},
  \beta_{n+1}, B^n)}{\mathcal{H}_n}{}$ for all
$n \geq 1$ by Lemma \ref{ConditionalIndependenceChainRule}.  Now we
have expressed $\Delta \tau_{n+1} = \Psi(B^n, \alpha_{n+1},
\beta_{n+1})$ and therefore $\cindependent{\Delta \tau_{n+1}}{\mathcal{H}_n}{}$ for all
$n \geq 1$ by Lemma \ref{IndependenceComposition}.  The fact that
$\expectation{\Delta \tau_n} = \expectation{\xi^2}$ and
$\expectation{(\Delta \tau_n)^2} = \expectation{\xi^4}$ follows from Lemma
\ref{EmbeddingSingleRandomVariableInBrownian} applied to the standard
Brownian motion $B^{n-1}$.

Claim: The $B^n_{\Delta \tau_{n+1}}$ are i.i.d. with $B^n_{\Delta \tau_{n+1}}
\eqdist \xi$.

The fact that $B^n_{\Delta \tau_{n+1}} \eqdist \xi$ follows from Lemma
\ref{EmbeddingSingleRandomVariableInBrownian} applied to the standard
Brownian motion $B^n_t$ using the facts that $\Delta
\tau_{n+1} = \inf \lbrace t \geq 0 \mid B^n_t \in \lbrace
\alpha_{n+1}, \beta_{n+1}\rbrace \rbrace$ and
$\mathcal{L}(\alpha_{n+1},\beta_{n+1}) = \tilde{\mu}$.  To see that
the $B^n_{\Delta \tau_{n+1}}$ are independent it suffices to show that 
$\cindependent{B^n_{\Delta \tau_{n+1}}}{(B^0_{\Delta \tau_1}, \dotsc,
  B^{n-1}_{\Delta \tau_n})}{}$ for each $n > 0$ (Lemma \ref{IndependenceByPairwiseIndependence}).
It follows from Lemma \ref{EmbeddingSingleRandomVariableInBrownian}  that $\Delta \tau_n$ is $\sigma(\alpha_n,
\beta_n, B^{n-1}_s ; s\leq t)$-optional and therefore by Lemma
\ref{StoppedProgressivelyMeasurableProcess} we know that
$B^{n}_{\Delta \tau_{n+1}  \wedge t}$ is $\sigma(\alpha_{n+1}, \beta_{n+1},
B^{n})$-measurable for all $t \geq 0$.  Taking the limit as $t$ goes
to infinity we conclude that $B^{n}_{\Delta \tau_{n+1}}$ is $\sigma(\alpha_{n+1}, \beta_{n+1},
B^{n})$-measurable.  On the other hand, since 
$B^{n-1}_{\Delta  \tau_n} = B_{\tau_n} - B_{\tau_{n-1}}$ we know that 
$B^{k-1}_{\Delta  \tau_k}$ is $\sigma(\tau_k, B_{\tau_k}, k \leq
n)=\mathcal{H}_{n}$-measurable for all $k \leq n$.  Having shown that
$\cindependent{(\alpha_{n+1}, \beta_{n+1}, B^{n})}{\mathcal{H}_{n}}{}$
in the proof of the prior claim 
we are done with this claim.

The last part of the result to show is that $(B_{\tau_1},B_{\tau_2},
\dotsc) \eqdist (S_1, S_2, \dotsc)$;
this follows from writing $B_{\tau_n}$ as a telescoping sum
\begin{align*}
B_{\tau_n} = \sum_{k=1}^n B_{\tau_k} - B_{\tau_{k-1}} = \sum_{k=1}^n
B^{k-1}_{\Delta \tau_k}
\end{align*}
and using the previous claim to see that 
\begin{align*}
(B_{\tau_1},B^1_{\Delta \tau_2}, B^2_{\Delta \tau_3}, \dotsc) \eqdist (\xi_1,
\xi_2, \xi_3, \dotsc)
\end{align*} and then applying the measurable mapping $g(t_1, t_2,t_3,
\dotsc) = (t_1, t_1+t_2, t_1+t_2+t_3, \dotsc)$.
\end{proof}

The Skorohod Embedding shows that a Brownian motion and associated
optional times can be constructed
to represent any unbiased random walk up to distribution.  However it
says a bit more than that in that it shows the optional times used in
the embedding are i.i.d. sums.  By the Law of Large Numbers we should
therefore expect that almost surely in the large time limit the
optional times should approach deterministic times so that, up to some
error terms, if we sample the Brownian motion at these deterministic
times it should be a random walk and we can dispense with the optional
times altogether.  This intuition turns out to be true and in fact a
bit more is true; once we consider approximating a random walk with a
Brownian motion sampled at deterministic times we are also in a
position to get an almost sure approximation (as opposed to an
approximation in distribution only).

To begin with we need the following result that is really a corollary
of the proof of the Law Of Iterated Logarithm (Theorem \ref{LILBrownianMotion}).
\begin{lem}\label{RateOfContinuityBrownianMotion}Let $B_t$ be a standard Brownian motion then
\begin{align*}
\lim_{r \downarrow 1} \limsup_{t \to \infty} \sup_{t \leq u \leq rt}
\frac{\abs{B_u - B_t}}{\sqrt{2t \log \log t}} = 0 \text{ a.s.}
\end{align*}
\end{lem}
\begin{proof}
To clean up the notation a little define $\psi(t) = \sqrt{2t\log\log
  t}$.

First note that $\limsup_{t \to \infty} \sup_{t \leq u \leq rt}
\frac{\abs{B_u - B_t}}{\sqrt{2t \log \log t}}$ is a decreasing
function of $r$ and therefore to show the result it suffices to
restrict ourselves to showing 
\begin{align*}
\lim_{n \to \infty} \limsup_{t \to \infty} \sup_{t \leq u \leq r_n t}
\frac{\abs{B_u - B_t}}{\sqrt{2t \log \log t}} = 0 \text{ a.s.}
\end{align*}
where $r_n$ is any sequence such that $r_n \downarrow 1$.  In
particular the result holds if we restrict $r \in \rationals$ and
interpret the limit in $r$ as being over rational $r > 1$.

Claim: It suffices to show 
\begin{align*}
\lim_{r \downarrow 1} \limsup_{n \to
  \infty} \sup_{r^n \leq u \leq r^{n+1}} \frac{\abs{B_u - B_{r^n}}}{\psi(r^n)} = 0 \text{ a.s.}
\end{align*}
This is a general fact; if $f(t)$ is a function with $t \geq 0$ then
given any sequence $t_n \to \infty$ we have $\limsup_{n \to \infty}
f(t_n) \leq \limsup_{t \to \infty} f(t)$ (any limit point of $f(t_n)$
is also a limit point of $f(t)$).

Now let $r > 1$, $n > 0$ and $c > 0$ be fixed for the moment and define
\begin{align*}
A_n &= \lbrace \sup_{r^n \leq u \leq r^{n+1}} \abs{B_u - B_{r^n}} \geq
  c \psi(r^n) \rbrace
\end{align*}
Now applying the Markov Property of Brownian motion to conclude that $B_u
- B_{r^n}$ is a standard Brownian motion,   applying Lemma
\ref{BrownianMaximumProcessLaw} to get the distribution of the maximal
process, normalizing to a standard normal variable $Z$ and using the
tail bound Lemma \ref{GaussianTailsElementary} and some algebra we get
\begin{align*}
\probability{A_n} 
&= \probability{\sup_{0\leq u \leq r^{n+1} - r^n}
    \abs{B_{u}} \geq  c \psi(r^n)} \\
&= \probability{\abs{B_{r^{n+1} - r^n}} \geq  c \psi(r^n)} \\
&= \probability{\abs{Z} \geq  \frac{c
    \psi(r^n)}{\sqrt{r^{n+1} - r^n}}} \\
&\leq \frac{\sqrt{r^{n+1} - r^n}}{\sqrt{2 \pi} c\psi(r^n)} e^{-c^2
  \psi^2(r^n)/2(r^{n+1} -r^n)} \\
&= \frac{\sqrt{r - 1}}{c \sqrt{4 \pi (\log n + \log \log r) } } e^{-c^2
  \log \log r^n/2(r -1)} \\
&= \frac{\sqrt{r - 1}}{c \sqrt{4 \pi (\log n + \log \log r) } } (n
\log r)^{-c^2/2(r -1)} \\
&\leq C_{r,c} n^{-c^2/2(r -1)}
\end{align*}
where we have selected a constant $C_{r,c}$ depending only on $r>1$
and $c > 0$.  Now for any $c > \sqrt{2(r-1)}$ we see that
$\sum_{n=1}^\infty \probability{A_n} < \infty$ and therefore we may
apply Borel Cantelli (Lemma \ref{BorelCantelli}) to conclude that
almost surely there exists a random $N$ (depending on $r$ and $c$) such that $\sup_{r^n \leq u \leq r^{n+1}} \abs{B_u - B_{r^n}} <
  c \psi(r^n)$ for all $n \geq N$.  Therefore in particular if we
  choose $c = 2\sqrt{r-1}$ we conclude that
  almost surely
  $\limsup_{n \to \infty} \sup_{r^n \leq u \leq r^{n+1}}
  \frac{\abs{B_u - B_{r^n}} }{\psi(r^n)} \leq 2\sqrt{r-1}$.  Taking
  the countable intersection of events of probability $1$  we get the
  bound almost surely for all $r \in
  \rationals$.  Let $r \downarrow 1$ over $r \in \rationals$ and we are done.
\end{proof}

Now we are ready to state the results that an Brownian motion
asymptotically approximates a random walk.
\begin{thm}\label{RandomWalkApproximatesBrownianMotion}Let $\xi, \xi_1, \xi_2, \dotsc$ be an i.i.d. sequence of
  random variables with $\expectation{\xi} = 0$ and
  $\expectation{\xi^2} = 1$ and let $S_n = \xi_1 + \dotsb + \xi_n$.  There exists a Brownian motion $B$ such
  that 
\begin{align*}
\frac{1}{\sqrt{t}}\sup_{0 \leq s \leq t} \abs{S_{\floor{s}} - B_s}
&\toprob 0 \text{ as $t \to \infty$} \\
\lim_{t \to \infty} \frac{\abs{S_{\floor{t}} - B_t}}{\sqrt{2t\log \log
    t}} &= 0 \text { a.s.}
\end{align*}
\end{thm}
\begin{proof}
The first order of business is to observe how the Skorohod embedding
may be modified to get an almost sure representation of the random
walk.  Applying Theorem \ref{EmbeddingRandomWalkInBrownianMotion} we
can conclude that there exists a Brownian motion $\tilde{B}$ and
optional times $\tilde{\tau}_n$ such that $\tilde{B}_{\tilde{\tau}_n}
\eqdist S_n$ and the $\Delta \tilde{\tau}_n = \tilde{\tau}_n - \tilde{\tau}_{n-1}$ are an
i.i.d. sequence with $\expectation{\Delta \tilde{\tau}_n} = 1$.  Now
if we define $(\tilde{B}, \Delta \tilde{\tau}_1, \Delta
\tilde{\tau}_2, \dotsc)$ as a random element in $C([0,\infty);\reals)
\times \reals_+^\infty$ and define $g : C([0,\infty);\reals)
\times \reals_+^\infty \to \reals^\infty$ by $g(f, t_1, t_2, \dotsc) =
(f(t_1), f(t_1 + t_2) , f(t_1 + t_2 + t_3), \dotsc)$ then we have on
the one hand 
\begin{align*}
g(\tilde{B}, \Delta \tilde{\tau}_1, \Delta
\tilde{\tau}_2, \dotsc) &\eqdist (S_1, S_2, \dotsc)
\end{align*}

Now we can
apply Lemma \ref{SolvingStochasticEquations} to conclude that there is
a random element $(B, \Delta \tau_1, \Delta \tau_2, \dotsc)$ such that 
\begin{align*}
(B, \Delta \tau_1, \Delta \tau_2, \dotsc) &\eqdist 
(\tilde{B}, \Delta \tilde{\tau}_1, \Delta \tilde{\tau}_2, \dotsc) \\
\intertext{and}
(B_{\Delta \tau_1}, B_{\Delta \tau_1 + \Delta \tau_2}, \dotsc) 
&= (S_1,S_2, \dotsc) \text{ a.s.}
\end{align*}  
In particular by taking marginals we know that $B \eqdist
\tilde{B}$ which shows $B$ is a Brownian motion and the $\Delta
\tau_n$ are i.i.d. with $\expectation{\Delta \tau_n} = 1$ and  if we
define random times $\tau_n = \sum_{k=1}^n \Delta \tau_k$ then we have
$B_{\tau_n} = S_n$ a.s. for all $n>0$.
(TODO: Show $g$ is measurable and justify that spaces are Borel).
Note that while $\tilde{\tau}_n$ are optional times the $\tau_n$ are not nor do
we need them to be; what matters here is only that $\tau_n$ is a sum
of i.i.d. random variables $\Delta \tau_n$.  By the Strong Law of Large Numbers (Theorem
\ref{SLLN}) we know that $\lim_{n \to \infty} \frac{\tau_n}{n} = 1$
and furthermore we know that 
$\lim_{t \to \infty} \frac{\tau_{\floor{t}}}{t} = \lim_{t \to \infty}
\frac{\tau_{\floor{t}}}{\floor{t}}\frac{\floor{t}}{t} = 1$ a.s.

Claim: $\lim_{t \to \infty} \frac{\abs{S_{\floor{t}} - B_t}}{\sqrt{2t\log \log t}} = 0 \text { a.s.}$

As usual we define $\psi(t) = \sqrt{2t\log \log t}$.  Since $\lim_{t
  \to \infty} \frac{\tau_{\floor{t}}}{t} = 1$ a.s. we know that almost
surely for every $r > 1$ there is a random $T > 0$ such that $1/r <
\frac{\tau_{\floor{t}}}{t} < r$ for all $t \geq T$.  This implies that
either $t \leq \tau_{\floor{t}} \leq rt$ or $\tau_{\floor{t}} \leq t
\leq r \tau_{\floor{t}}$ and consequently for every $r > 1$
\begin{align*}
\frac{\abs{S_{\floor{t}} - B_t}}{\psi(t)} &=
\frac{\abs{B_{\tau_{\floor{t}}} - B_t}}{\psi(t)} \\
&\leq \sup_{t \leq u \leq rt} \frac{\abs{B_u - B_t}}{\psi(t)} \wedge 
\sup_{\tau_{\floor{t}} \leq u \leq r \tau_{\floor{t}}} \frac{\abs{B_u - B_{\tau_{\floor{t}}}}}{\psi(t)} 
\end{align*}
for sufficiently large $t > 0$ (depending on $r$).  Now taking the
limit as $t \to \infty$ and using the fact that $\lim_{t \to \infty}
\tau_{\floor{t}}/t =1$ implies $\lim_{t \to \infty}
\psi(\tau_{\floor{t}})/\psi(t) -1 $ and the general fact that if $\lim_{t
  \to \infty} g(t)/t = 1$ implies  $\limsup_{t \to \infty} f(g(t))
\leq \limsup_{t \to \infty} f(t)$ we get that almost surely for every
$r > 1$,
\begin{align*}
\limsup_{t \to \infty} \frac{\abs{S_{\floor{t}} - B_t}}{\psi(t)} &\leq
\limsup_{t \to \infty} \sup_{t \leq u \leq rt} \frac{\abs{B_u - B_t}}{\psi(t)}
\end{align*}
Now taking the limit as $r \downarrow 1$, using Lemma
\ref{RateOfContinuityBrownianMotion} and the positivity of
$\frac{\abs{S_{\floor{t}} - B_t}}{\psi(t)}$ we conclude $\lim_{t \to
  \infty}  \frac{\abs{S_{\floor{t}} - B_t}}{\psi(t)} = 0$ a.s.

To get the next limit first we need a simple fact about deterministic
sequences

Claim: If $\lim_{n \to \infty} a_n/n = 1$ then $\lim_{t \to \infty}
\sup_{0 \leq s \leq t} \abs{a_{\floor{s}} - s}/t = 0$.

Let $\epsilon > 0$ be arbitrary and pick $N > 0$ such that $1-\epsilon
< a_n/n < 1 + \epsilon$ for all $n \geq N$; we use this in the form
$\abs{a_n -n} < n\epsilon$.  Now for every $t \geq N$ we use this
bound to conclude
\begin{align*}
\sup_{0 \leq s \leq t} \abs{a_{\floor{s}} - s} &\leq 
\sup_{0 \leq s  < N} \abs{a_{\floor{s}} - s} + 
\sup_{N \leq s \leq t} \abs{a_{\floor{s}} - \floor{s}} + 1 
\leq 
\sup_{0 \leq s  < N} \abs{a_{\floor{s}} - s} + 
t \epsilon + 1 
\end{align*}
so dividing by $t$ and taking the limit we get 
$\lim_{t \to \infty} \sup_{0 \leq s \leq t} \abs{a_{\floor{s}} - s}/t
\leq \epsilon$.  Now let $\epsilon$ go to zero.

Now we define $\delta_t = \sup_{0 \leq s \leq t}\abs{\tau_{\floor{s}}
  -s}$ and conclude from the previous claim that $\delta_t /t \toas 0$
as $t \to \infty$.  Recall the definition of the modulus of continuity
\begin{align*}
w(f,t,h) &= \sup_{\substack{0 \leq r,s \leq t\\ \abs{r-s} < h}} \abs{f(r) - f(s)}
\end{align*}
and the fact that $\lim_{h \to \infty} w(f,t,h) = 0$ if and only if
$f$ is continuous (hence uniformly continuous) on $[0,t]$.  With this
notation in hand, let $\epsilon > 0$ and $h > 0$ be given and use a union bound and a
rescaling of the Brownian motion by the factor $\sqrt{t}$ to bound
\begin{align*}
\probability{\frac{1}{\sqrt{t}} \sup_{0 \leq s \leq t}
  \abs{B_{\tau_{\floor{s}}} - B_s} > \epsilon }
&\leq 
\probability{\delta_t \geq  h t} +
\probability{w(B, t + h t, h t) > \sqrt{t} \epsilon} \\
&=\probability{\delta_t \geq  h t} +
\probability{w(B, 1 + h 1, h) > \epsilon} \\
\end{align*}
Since $\delta_t /t \toas 0$ we know that $\delta_t /t \toprob 0$ and
taking the limit as $t \to \infty$ we get $\lim_{t \to \infty}
\probability{\delta_t \geq  h t}  =0$.  Then because Brownian motion
is almost surely continuous hence almost surely uniformly continuous
on every finite interval and we know that $w(B, T, h) \toprob 0$ as $h
\to 0$ for every fixed $T > 0$ so we get $\lim_{h \to 0}
\probability{w(B, 1 + h 1, h) > \epsilon} = 0$ and thus we conclude 
\begin{align*}
\lim_{t \to \infty} \probability{\frac{1}{\sqrt{t}} \sup_{0 \leq s \leq t}
  \abs{B_{\tau_{\floor{s}}} - B_s} > \epsilon } =0
\end{align*}
and the result is proven.

In the above proof we glossed over a measurability question that we
backfill for completeness.

Claim: For fixed $t$ and $h$, $w(f,t,h)$ is a measurable function of
$f$ on $C([0,\infty); \reals) \cap \reals^{[0,\infty)}$.

The basic point is that the supremum in the definition of the modulus
of continuity can be restricted to the rationals without changing the
definition.  Let 
\begin{align*}
w^{\rationals}(f,t,h) &= \sup_{\substack{0 \leq r,q \leq t; r,q \in
    \rationals \\ \abs{r - q} < h}} \abs{f(r) - f(q)}
\end{align*}
and we clearly have $w^{\rationals}(f,t,h) \leq w(f,t,h)$.  In the
other direction let $\epsilon > 0$ be given and pick $x,y$ be such
that $\abs{f(x) - f(y)} > w(f,t,h) - \epsilon$.  Now by density of
rationals and continuity of $f$ we can find rational numbers $r,q$
such that $\abs{r-q} < h$ and 
\begin{align*}
\abs{f(r) - f(q)} \geq \abs{f(x) -
  f(y)} - \abs{f(r) - f(x)} - \abs{f(q)-f(y)} > w(f,t,h) - \epsilon/2
\end{align*}
Since $\epsilon>0$ was arbitrary we conclude that
$w^{\rationals}(f,t,h) = w(f,t,h)$.
Now for any $v \geq 0$ we have 
\begin{align*}
\lbrace w^{\rationals}(f,t,h) \leq v \rbrace = \cap_{\substack{0 \leq r,q \leq t; r,q \in
    \rationals \\ \abs{r - q} < h}} \lbrace \abs{f(r) - f(q)} \leq v \rbrace
\end{align*}
and each $\lbrace \abs{f(r) - f(q)} \leq v \rbrace$ is easily seen to
be measurable as it depends on evaluation of $f$ at a finite number of points.
\end{proof}

The approximation result just proven can be turned into a weak convergence
result if we put a little bit of work into defining the function
spaces in which the convergence occurs.  There are several choices one
may make about how to do this.  For the result we are to prove, we
consider a random walk to be a piecewise constant function and
therefore we look for convergence in a space of discontinuous
functions.

We define the space of functions that have left limits and are
continuous from the right (cadlag functions)
\begin{align*}
D[0,1] &= \lbrace f: [0,1] \to \reals \mid \lim_{x \to a^+} f(x) = f(a)
\text{ and } \lim_{x \to a^-} f(x) \text{ exists for all $x \in [0,1]$} \rbrace
\end{align*}
and provide it with the supremum norm $\norm{f}_\infty = \sup_{0 \leq x \leq
  1} \abs{f(x)}$. We take the $\sigma$-algebra on $D[0,1]$ generated
by the evaluations $\pi_t(f) = f(t)$ (i.e. we consider $D[0,1] \cap
\reals^{[0,1]}$ as required in the definition of a stochastic
process).  We note that since each $\pi_t$ is continuous in the sup
norm, this $\sigma$-algebra is a sub-algebra of the Borel
$\sigma$-algebra.  It is in fact true that this $\sigma$-algebra is a
proper subalgebra of the Borel $\sigma$-algebra so we have defined a
setting in which we cannot apply our notions of convergence in
distribution and therefore we have to be a bit barehanded about how we
phrase and prove the desired result.

The key remaining technical lemma is the following one which can be
regarded as a combination of Slutsky's Lemma and the Continuous
Mapping Theorem tailor made for our scenario.

\begin{defn}Let $X : (\Omega, \mathcal{A}) \to D[0,1]$ be a process with paths in $D[0,1]$ and let $\phi : D[0,1] \to
  \reals$ be a function.  We say $\phi$ is almost surely continuous at
  $X$ if 
\begin{align*}
\sup_{\substack{A \in \mathcal{A}\\
    X(A) \cap D_{\phi} = \emptyset}} \probability{A} &= 1
\intertext{where}
D_\phi &= \lbrace f \in D[0,1] \mid \phi \text{ is not continuous at } f\rbrace
\end{align*}
\end{defn}

\begin{lem}\label{ConvergenceAndApproximationCadlag}Let $X^1, X^2, \dotsc$ and $Y,Y^1, Y^2, \dotsc$ be cadlag
  processes in $D[0,1]$ such that $Y^n \eqdist Y$ for all $n>0$ and
  $\norm{X^n - Y^n}_\infty \toprob 0$.  Let $\phi : D[0,1] \to \reals$ be
  measurable and almost surely continuous at $Y$, then $\phi(X^n) \todist \phi(Y)$.
\end{lem}
\begin{proof}
Let $T = [0,1] \cap \rationals$ and note that $\reals^T$ with the
product $\sigma$-algebra is a Borel space (TODO: where do we prove this).  Therefore using Lemma \ref{Transfer}
we can construct a sequence processes $\overline{X}^n$ on $T$ such that 
\begin{itemize}
\item[(i)]$\overline{X}^n \eqdist X^n$ for all $n >0$
\item[(ii)]$(\overline{X}^n, Y) \eqdist (X^n, Y^n)$ for all $n > 0$
\end{itemize}

The first order of business is to verify that $\overline{X}^n$ can be
extended to processes with paths in $D[0,1]$.

Claim: For every $n > 0$, $\overline{X}^n$ is almost surely bounded
and has finitely many upcrossings on every finite interval.

The point is that these properties follow from the distribution of
$\overline{X}^n$ and since they are true of $X^n$ they hold for
$\overline{X}^n$.  Specifically as for almost sure boundedness
\begin{align*}
\probability{\cap_{m=1}^\infty \norm{\overline{X}^n}_\infty > m} &=
\lim_{m \to \infty} \probability{\norm{\overline{X}^n}_\infty > m} =
\lim_{m \to \infty} \probability{\norm{X^n}_\infty > m} =
\probability{\cap_{m=1}^\infty \norm{X^n}_\infty > m} = 0
\end{align*}
TODO: Do the details on the upcrossings using the defintions from Doob
Upcrossing.

Based on the previous claim, we see that we can define
\begin{align*}
\tilde{X}^n_t &= \lim_{s \to t^+} \overline{X}^n_s
\end{align*}
and since $\lim_{s \to t^-} \tilde{X}^n_t$ exists for every $t \in
[0,1]$ we know that $\tilde{X}^n$ is a process with paths in $D[0,1]$ (measurability
of $\tilde{X}^n_t$ follows from the fact that it is defined as a limit
of measurable functions (Lemma \ref{LimitsOfMeasurable})).

Claim: $(\tilde{X}^n, Y) \eqdist (X^n, Y^n)$ for all $n > 0$.

We know from construction that $\probability{(\tilde{X}^n, Y) \in A} =
\probability{(X^n,Y^n) \in A}$ for all $A \in \mathcal{B}(\reals)^T$
so it suffices to show that $\mathcal{B}(\reals)^T$ generates $D[0,1]
\cap \mathcal{B}(\reals)^{[0,1]}$ (Lemma \ref{UniquenessOfMeasure}).
This follows from right continuity of members of $D[0,1]$ as for any
$t \in [0,1]$ we have $\pi_t =\lim_{n \to \infty} \pi_{q_n}$ where
$q_n$ is an sequence of elements of $T$ such that $q_n \downarrow t$.

Claim: $\norm {\cdot}_\infty$ is measurable on $D[0,1]$ and
subtraction $(f,g) \to f - g$ is measurable on $D[0,1] \times D[0,1]
\to D[0,1]$.

By right continuity and density of $\rationals$, 
\begin{align*}
\lbrace \norm{f}_\infty \leq x \rbrace &= \lbrace \sup_{t \in T}
\abs{f(t)} \rbrace = \cap_{t \in T} \lbrace f(t) \leq x \rbrace
\end{align*}

As for subtraction, again using right continuity and density of
$\rationals$,
\begin{align*}
\lbrace f(t) - g(t) \leq x \rbrace = \cap_{\substack{q \geq t \\ q \in
    \rationals}} \lbrace f(q) \leq x - q \rbrace \times \lbrace g(q) \leq q \rbrace
\end{align*}

From the two previous claims we know that $\norm{\tilde{X}^n -
  Y}_\infty \eqdist \norm{X^n - Y^n}_\infty$.

Claim: $\phi(\tilde{X}^n) \toprob \phi(Y)$.

First, since $\norm{X^n - Y^n}_\infty \toprob 0$ we have  $\norm{X^n -
  Y^n}_\infty \todist 0$ and since $\norm{\tilde{X}^n -
  Y}_\infty \eqdist \norm{X^n - Y^n}_\infty$ we conclude that $\norm{\tilde{X}^n -
  Y}_\infty \todist 0$; as the weak limit is a deterministic constant by Lemma
\ref{ConvergenceInDistributionToConstant} we get $\norm{\tilde{X}^n -
  Y}_\infty \toprob 0$.

We know that $\norm{\tilde{X}^n -  Y}_\infty \toprob 0$ if and only if
every subsequence has a further subsequence that converges almost
surely (Lemma \ref{ConvergenceInProbabilityAlmostSureSubsequence}).
Let $N$ be a subsequence of $\abs{\phi(\tilde{X}^n) - \phi(Y)}$ and
select a further subsequence $N^\prime \subset N$ such that
$\norm{\tilde{X}^n - Y}_\infty \toas 0$ along $N^\prime$.  By the
almost sure continuity of $\phi$ at $Y$ we conclude that
$\abs{\phi(\tilde{X}^n) - \phi(Y)} \toas 0$ along $N^\prime$ hence we
conclude $\phi(\tilde{X}^n) \toprob \phi(Y)$.  Since $\phi$ was
assumed measurable we have $\phi(\tilde{X}^n) \eqdist \phi(X^n)$ and
therefore we conclude $\phi(X^n) \todist \phi(Y)$.
\end{proof}

\begin{thm}[Donsker's Invariance Principle]\label{DonskersTheorem}Let
  $\xi_1, \xi_2, \dotsc$ be an i.i.d. sequence of random variable with
  mean 0 and variance 1 and define for all $t \in [0,1]$ and $n \in \naturals$,
\begin{align*}
S_n &= \sum_{k=1}^n \xi_k \\
X^n_t &= \frac{1}{\sqrt{n}} \sum_{k=1}^{\floor{nt}} \xi_k =
\frac{1}{\sqrt{n}} S_{\floor{nt}}
\end{align*}
Let $B$ be a Brownian motion on $[0,1]$ and let $\phi : D[0,1] \to
\reals$ be measurable and almost surely continuous at $B$, then
$\phi(X^n) \todist \phi(B)$.
\end{thm}
\begin{proof}
Define $Y^n_t = \frac{1}{\sqrt{t}} B_{nt}$ and note that by scaling we
have $Y^n \eqdist B$ for all $n \in \naturals$.  Note that 
\begin{align*}
\norm{X^n - Y^n}_\infty &= 
\frac{1}{\sqrt{n}} \sup_{0 \leq t \leq 1} \abs{S_{\floor{nt}} - B_{nt}} = 
\frac{1}{\sqrt{n}} \sup_{0 \leq t \leq n} \abs{S_{\floor{t}} - B_{t}} 
\end{align*}
and therefore by Theorem \ref{RandomWalkApproximatesBrownianMotion} we
conclude $\norm{X^n - Y^n}_\infty \toprob 0$.  Now we apply the
previous Lemma \ref{ConvergenceAndApproximationCadlag} to conclude
$\phi(X^n) \todist \phi(B)$ and we are done.
\end{proof}

\begin{thm}[Law of Iterated Logarithm]\label{LILBrownianMotion}Let
  $B_t$ be a standard Brownian motion then 
\begin{align*}
\limsup_{t \to \infty} \frac{B_t}{\sqrt{2t\log \log t}} &= 1 \text{ a.s.}
\end{align*}
\end{thm}
\begin{proof}
The basic idea of the proof is to examine the behavior of Brownian
paths sampled along the values of a geometric sequence $q^n$ for some
number $q > 1$.  Because we need to interpolate between sampling
points we must consider segments of the Brownian path between sampling
points.

To get started pick a number $q \in \rationals$ such that $q >1$ and
pick an $\epsilon > 0$ that we will later send to zero.  To clean up
the notation a bit define $\psi(t) = \sqrt{2t\log \log t}$ and let
$A_n = \lbrace \sup_{0 \leq t \leq q^n} B_t \geq (1+\epsilon)\psi(q^n)
\rbrace$.  Using the distribution of the Brownian maximum process (Lemma \ref{BrownianMaximumProcessLaw}), rescaling to a
standard normal random variable and the Gaussian tail bounds from
Lemma \ref{GaussianTailsElementary} we know that 
\begin{align*}
\probability{A_n} &= 
\probability{\abs{B_{q^n}} \geq (1 + \epsilon)  \psi(q^n)} \\
&=\probability{\frac{\abs{B_{q^n}}}{\sqrt{q^n}} \geq \frac{(1 + \epsilon)  \psi(q^n)}{\sqrt{q^n}}} \\
&\leq \frac {\sqrt{q^n}}{(1 + \epsilon)  \psi(q^n)} e^{-(1+\epsilon)^2
  \psi^2(q^n)/2q^n} \\
&=\frac {1}{(1 + \epsilon)  \sqrt{2\log \log (q^n)}} e^{-(1+\epsilon)^2
  \log \log (q^n)} 
\end{align*}
and there exists an $N_q$ depending only on $q$ such that the leading
constant is less than $1$ for $n \geq N_q$, so we have
\begin{align*}
\probability{A_n} &\leq \frac{1}{(n \log q)^{(1+\epsilon)^2}} 
\text{ for $n \geq N_q$}
\end{align*}
which shows that $\sum_{n=1}^\infty \probability{A_n} < \infty$. The
Borel Cantelli Theorem implies that almost surely at most finitely
many $A_n$ occur.  Thus almost surely there is an $N_\omega$ such that
$\abs{B_{t}} < (1 + \epsilon)  \psi(q^n)$ for all $n \geq N_\omega$
and all $0 \leq t \leq q^n$.  We now have to provide a bound using
$\psi(t)$ rather than $\psi(q^n)$.  For any $t \geq 1$ pick $n \geq 1$
such that $q^{n-1} \leq t < q^n$, and use the fact that $\psi(t)/t =
\sqrt{\frac{2\log\log t}{t}}$ is
a decreasing function of $t$ for large $t$ (for example $t \geq e^e$ works
since $\frac{d}{dt} \psi(t)/t = \frac{\frac{1}{\log t} - \log \log t}{t^2}$) to bound
\begin{align*}
\frac{B(t)}{\psi(t)} &=
\frac{B(t)}{\psi(q^n)}\frac{\psi(q^n)}{q^n}\frac{t}{\psi(t)}\frac{q^n}{t}
\leq (1+\epsilon) q
\end{align*}
for $t > q^{N_\omega} \wedge e^e$ and therefore $\limsup_{t \to \infty}
\frac{B(t)}{\psi(t)} \leq (1+\epsilon)q$.  Since $\epsilon > 0$ and $q
> 1$  were
arbitrary we conclude $\limsup_{t \to \infty}\frac{B(t)}{\psi(t)} \leq 1$.


Now for the other direction, again pick $q > 1$ and consider the
events
\begin{align*}
D_n &= \lbrace B_{q^n} - B_{q^{n-1}} \geq \psi(q^n - q^{n-1}) \rbrace
\end{align*}
We know that since $q \leq q^2 \leq \cdots$ so the $D_n$ are
independent events and $(B_{q^n} - B_{q^{n-1}})/\sqrt{q^n - q^{n-1}}$ is
$N(0,1)$ so we can apply Lemma \ref{GaussianTailsElementary} to see
that
for any $x \geq x_0$ we have
\begin{align*}
\probability{(B_{q^n} - B_{q^{n-1}})/\sqrt{q^n -  q^{n-1}} \geq x} \geq
\frac{x}{x^2+1} e^{-x^2/2} \geq \frac{x_0^2}{x_0^2+1} \frac{1}{x} e^{-x^2/2} 
\end{align*}
so if we let $c_1 =
\frac{2 \log \log q}{2 \log \log q+1}$ then 
\begin{align*}
\probability{D_n} &= \probability{(B_{q^n} - B_{q^{n-1}})/\sqrt{q^n -
  q^{n-1}} \geq \psi(q^n - q^{n-1})/\sqrt{q^n - q^{n-1}} } \\
&\geq c_1 \frac{e^{-\log \log (q^n - q^{n-1})}}{\sqrt{2 \log \log (q^n -
    q^{n-1})}}
\geq c_1 \frac{e^{-\log \log q^n}}{\sqrt{2 \log \log q^n}} \geq
\frac{c_2}{n \log n}
\end{align*}
so by the integral test we see that $\sum_{n=1}^\infty
\probability{D_n} = \infty$.  By Borel Cantelli we know that almost
surely there exists $N_1$ such that $B_{q^n} \geq B_{q^{n-1}} + \psi(q^n
- q^{n-1})$ for all $n \geq N_1$ (where $N_1$ depends on $q$ and $\omega
\in \Omega$).  To turn this into a lower bound on $B_{q^n}$ alone we
use the fact $-B_t$ is also a Brownian motion so we know from the
upper bound that we have already proven
\begin{align*}
\liminf_{t \to \infty} \frac{B_t}{\psi(t)} &= -\limsup_{t \to \infty}
\frac{-B_t}{\psi(t)} \geq -1 \text{ a.s.}
\end{align*}
If we pick an arbitrary $\epsilon > 0$ then almost surely there exists
$N_2$ such that for all $n \geq N_2$
$B_{q^n} \geq -(1 + \epsilon) \psi(q^n)$.  Therefore we have for all
$n \geq N_1 \wedge N_2$,
\begin{align*}
\frac{B_{q^n}}{\psi(q^n)} \geq \frac{B_{q^{n-1}} + \psi(q^n
- q^{n-1}) }{\psi(q^n)}
\geq \frac{-(1 + \epsilon) \psi(q^{n-1}) + \psi(q^n- q^{n-1})}{\psi(q^n)}
\end{align*}
Now we can provide lower bounds for $\psi(t)$ in the expressions
above.  Using the fact that $\psi(t)/\sqrt{t} = \sqrt{2 \log \log(t)}$ is increasing we have
\begin{align*}
\frac{\psi(q^{n-1})}{\psi(q^n)} &=
\frac{\psi(q^{n-1})}{\sqrt{q^{n-1}}}
\frac{\sqrt{q^n}}{\psi(q^n)}\frac{1}{\sqrt{q}} \leq \frac{1}{\sqrt{q}} 
\end{align*}
and using the fact that $\psi(t)/t$ is decreasing for large $t$ we have 
\begin{align*}
\frac{\psi(q^n- q^{n-1})}{\psi(q^n)} &\geq \frac{q^n - q^{n-1}}{q^n} =
1 - \frac{1}{q}
\end{align*}
for sufficiently large $n$ so putting these facts together we get 
\begin{align*}
\limsup_{t \to \infty} \frac{B_t}{\psi(t)} &\geq \limsup_{n \to
  \infty} \frac{B_{q^n}}{\psi(q^n)} \geq
\frac{-(1+\epsilon)}{\sqrt{q}} + 1 - \frac{1}{q} \text{ a.s.}
\end{align*}
Now taking the intersection of countably many events of probability
$1$ over all $q \in \rationals$ this bound exists almost surely for
all rational numbers $q > 1$ so we may take the limit as $q \to
\infty$ and conclude that $\limsup_{t \to \infty} \frac{B_t}{\psi(t)}
\geq 1$.
\end{proof}
An additional scaling argument allows us to get a Law of Iterated
Logarithm for the limit as $t \to 0$,
\begin{cor}Let $B_t$ be a standard Brownian motion then 
\begin{align*}
\limsup_{t \to 0} \frac{B_t}{\sqrt{2 t \log \log (1/t)}} &= 1
\end{align*}
\end{cor}
\begin{proof}
We know that the rescaled process $X_t = t B_{1/t}$ for $t > 0$ is a
standard Brownian motion.  Therefore letting $h = 1/t$,
\begin{align*}
1 &= \limsup_{h \to \infty} \frac{X_h}{\sqrt{2 h \log \log (h)}} =
\limsup_{t \to 0} \frac{X_{1/t}}{\sqrt{2/t \log \log (1/t)}} =
\limsup_{t \to 0} \frac{B_t}{\sqrt{2 t \log \log (1/t)}}
\end{align*}
\end{proof}

Donsker's Theorem states roughly that Brownian motion can be
approximated in distribution by a suitably rescaled random walk.
Moreover it states that essentially all possible random walks that one
might expect could approximate Brownian motion in fact do.  This fact
shows that Brownian motion is analogous to standard normal
distributions and Donsker's Theorem is often referred to as the
Functional Central Limit Theorem.

\begin{thm}
Suppose we are given an i.i.d. sequence of random variables $\xi_1,
\xi_2, \dotsc$ such that $\expectation{\xi_n}=0$ and
$\variance{\xi_n}=1$ for all $n \in \naturals$.  Define the random
walk 
\begin{align*}
S_n &= \sum_{j=1}^n \xi_j
\end{align*}
its linear interpolation
\begin{align*}
S(t) &= S_{\floor{t}} + (t - \floor{t})(S_{\floor{t}+1} - S_{\floor{t}})
\end{align*}
and its rescaling from the interval $[0,n]$ to $[0,1]$
\begin{align*}
S_n^*(t) &= \frac{1}{\sqrt{n}} S(nt) & & \text{for $t\in [0,1]$}
\end{align*}
On the space $C[0,1]$ with the uniform norm, the sequence $S^*_n(t)$
converges in distribution to the standard Brownian motion.
\end{thm}

\begin{proof}
TODO
\end{proof}

TODO: Extension of Donsker's Theorem to convergence of errors of
empirical distributions to Brownian bridge.  This may be harder
because the convergence takes place not in the separable space
$C[0,1]$ but rather the space of cadlag functions (which is only
separable under the Skorohod topology).  The alternative here is
presumably to use the generalized form of weak convergence from
empirical process theory.