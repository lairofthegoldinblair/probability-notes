\chapter{More Real Analysis}
Holding area for more advanced topics in real analysis that are
eventually required (and in some cases there may be some topics that I
am just interested in).
\section{Metric Spaces}

\section{Topological Spaces}
\begin{lem}\label{OpenAlternative}A set $U \subset X$ is open if and only if for every $x \in
  U$ there is an open set $V \subset U$ such that $x \in V$.
\end{lem}
\begin{proof}
Suppose $U$ is open and $x \in U$, then let $V = U$.

Suppose for every $x \in U$ there exist an open set $V_x$ such that $x
\in V_x \subset U$.  Note that $\cup_x V_x \subset U$ because each
$V_x \subset U$ and on the other hand $\cup_x V_x \supset U$ since
every $x \in U$ satisfies $x \in V_x$.  Thus $U = \cup_x V_x$ which
shows that $U$ is open.
\end{proof}
\begin{defn}A mapping $f : X \to Y$ between topological spaces is said
  to be \emph{continuous} if and only if $f^{-1}(V)$ is open in $X$
  for every $V$ open in $Y$.
\end{defn}
\begin{defn}A mapping $f : X \to Y$ between topological spaces is said
  to be \emph{continuous at x} if and only if for every $V$ open in
  $Y$ such that $f(x) \in V$, there exists an open set $U$ in $X$ with $x \in U$ and $f(U)
  \subset V$.
\end{defn}
\begin{lem}A mapping $f : X \to Y$ between topological spaces is
  continuous if and only if it is continuous at $x$ for every $x \in X$.
\end{lem}
\begin{proof}
Suppose $f$ is continuous and let $x \in X$ and $V$ be open in $Y$
with $f(x) \in V$.  By continuity of $f$, we know that $f^{-1}(V)$ is
open in $X$ and $x \in f^{-1}(V)$.  By Lemma \ref{OpenAlternative} we
can pick an open set $U$ such that $x \in U$ and $U \subset
f^{-1}(V)$.  It follows that $f(U) \subset V$.

Now suppose $f$ is continuous at every $x \in X$ and let $V$ be open
in $Y$.  If $x \in f^{-1}(V)$ then $f$ is continuous at $x$ hence
there exists and open $U$ such that $x \in U$ and $f(U) \subset V$.
It follows that $U \subset f^{-1}(V)$ and by Lemma
\ref{OpenAlternative}  we have shown that $f^{-1}(V)$ is open.
\end{proof}

\begin{defn}A \emph{base} of a topology $\mathcal{T}$ at a point $x
  \in X$ is a collection
  of sets $\mathcal{B}$ such that for every open set $U \in
  \mathcal{T}$ such that $x \in U$ there exists a $B \in \mathcal{B}$ such
  that $x \in B \subset U$.  A base of a topology is a collection of
  sets that is a base at all points $x \in X$.
\end{defn}

\begin{lem}A set $\mathcal{B}$ of sets $B \subset X$ is a base of a
  topology if and only if for every $x \in X$ there exists $B \in
  \mathcal{B}$ such that $x \in B$ and for every $A, B \in
  \mathcal{B}$ and $x \in A \cap B$ there exists $C \in \mathcal{B}$
  such that $x \in C \subset A \cap B$.
\end{lem}
\begin{proof}
Suppose $\mathcal{B}$ satisfies the hypothesized conditions and let
\begin{align*}
\tau &= \lbrace U \subset X \mid \text { for every } x \in U \text{
  there exists } B \in \mathcal{B} \text{ such that } x \in B \subset
U \rbrace
\end{align*}
It is certainly the case that $\mathcal{B}\subset \tau$ and we claim that $\tau$ is a topology.  Certainly $\emptyset \in \tau$.
Let $U_\alpha$ for $\alpha \in \Lambda$ are sets in $\tau$.
Then if $x \in \cup_{\alpha \in \Lambda} U_\alpha$ there exists an
$\alpha \in \Lambda$ such that $x \in U_\alpha$ and by hypothesis we
pick $B$ such that $x \in B \subset U_\alpha \subset  \cup_{\alpha \in
  \Lambda} U_\alpha$.  If $U_1, \dotsc, U_n \in \tau$ and $x \in U_1
\cap \dotsc \cap U_n$ then there exists $B_1, \dotsc, B_n$ such that
$x \in B_j \subset U_j$ for $j = 1, \dotsc, n$ and therefore $x \in
B_1 \cap \dotsc \cap B_n \subset U_1 \cap \dotsc \cap U_n$.  A simple
induction on the hypothesis shows that $B_1 \cap \dotsc \cap B_n \in \mathcal{B}$.
Because $\mathcal{B}$ is cover of $X$ we have $X = \cup_{B \in
  \mathcal{B}} B \in \tau$ and therefore $\tau$ is a topology.  By the
definition of $\tau$ it is immediate that $\mathcal{B}$ is a base of
the topology.
\end{proof}


\begin{defn}
\begin{itemize}
\item[(i)]A topological space is said to be \emph{separable} if and
  only if it has a countable dense subset.
\item[(ii)]A topological space is said to be \emph{first countable} if and
  only if every point has a countable local base.
\item[(ii)]A topological space is said to be \emph{second countable} if and
  only if every the topology has a countable base.
\end{itemize}
\end{defn}
\begin{lem}A metric space is separable if and only if it is second countable.
\end{lem}
\begin{proof}
TODO:
outline of proof is to pick a countable dense subset $\lbrace x_n
\rbrace$ and then pick the open balls $B(x_n; \frac{1}{m})$ for $m \in
\naturals$.  Show this is a base of the topology.
\end{proof}

TODO:  Fun fact, there is a non-first countable topological space
which is compact but not sequentially compact!  In first countable
spaces sequential compactness is equivalent to 

TODO: The goal of the next set of results is to show that separable
complete metric spaces (actually Polish spaces which are those with
a separable topology which can be metrized by a complete metric) are Borel.


The following appears in Royden as Theorem 8.11 (with proof delgated
to exercises)
\begin{lem}Let $X$ be a Hausdorff topological space, $Y$ be a
  complete metric space and $Z \subset X$ be a dense subset.  If $f :
  Z \to X$ is a homeomorphism then $Z$ is a countable intersection of
  open sets.
\end{lem}
\begin{proof}
For each $n$ let 
\begin{align*}
O_n &= \lbrace x \in X \mid \text{there exists $U$
  open with $x \in U$ and $\diam(f(U \cap Z)) < \frac{1}{n}$} \rbrace
\end{align*}
Note that $O_n$ is open because for any $x \in O_n$ by definition we
have the open set $U$ that provides the evidence that $x \in O_n$;
$U$ also provides the evidence that proves that every $y \in U$
belongs to $O_n$.  Also
note that $Z \subset O_n$ since for any $n$, by continuity of $f$ at $x \in Z$ and Lemma
\ref{OpenAlternative}  we
can find an open $U \subset X$ such that $x \in U \cap Z$ and $f(U \cap Z) \subset B(f(x),
\frac{1}{2n})$ (sets of the form $U \cap Z$ being precisely the open
sets in $Z$).

Now define $E = \cap_n O_n$.  As noted we know $Z \subset E$ so we
will be done if we can show $E
\subset Z$ as well.  Let $x \in E$; we will construct $z \in Z$
such that $x = z$.  For each $n$ pick $U_n$ such $x \in U_n$ and $\diam(f(U_n \cap Z)) <
\frac{1}{n}$ and let $x_n$ be an arbitrary point in $\cap_{j=1}^n
U_j \cap Z$ (the intersection is non-empty because $Z$ is dense in
$X$).  
For every $n$ and $m \geq n$ we have by construction that $x_n
\in U_n$ and $x_m \in U_n$ hence $d(f(x_n), f(x_m)) < \frac{1}{n}$.
Therefore $f(x_n)$ is Cauchy in
$Y$ and by completeness of $Y$ we know that $f(x_n)$ converges to a
value $y \in Y$ with $d(y, f(x_n)) \leq \frac{1}{n}$.  
Because $f$ is a homeomorphism we know that 
there is a unique $z \in Z$ such that $f(z) = y$; we claim that $x =
z$.  Suppose that $x
\neq z$, then by the Hausdorff property on $X$ we can pick open sets $U$ and
$V$ such that $U \cap V = \emptyset$, $x \in U$ and $z \in V$.  Since
$f$ is a homeomorphism, we know $f(Z \cap V)$ is open and contains
$f(z)$ hence for sufficiently large $n$, $f^{-1}(B(f(z), \frac{1}{n}))
\subset Z \cap V \subset V$.  On
the other hand, by the definition of $x$ we have $U_{2n}$ open such that
$x \in U_{2n}$ and $\diam(f(Z \cap U_{2n})) < \frac{1}{2n}$.  By openness of
$U \cap U_{2n}$ and density of $Z$ we know there is a $w \in U \cap
U_{2n} \cap Z$.  Putting these observations together we have
\begin{align*}
d(f(w), f(z)) &\leq  d(f(w), f(x_{2n})) + d(f(x_{2n}), f(z)) 
< \frac{1}{2n} + \frac{1}{2n} = \frac{1}{n}
\end{align*}
which implies $w \in V$ providing a contradiction of $U \cap V =
\emptyset$ hence we conclude $x = z$.
\end{proof}

\begin{thm}[Tychonoff's Theorem]\label{Tychonoff}Let $I$ be index set
  and let $(X_i,
  \mathcal{T}_i)$ be a topological space for each $i \in I$, the
  cartesian product $\prod_{i \in I} X_i$ with the product topology is
  compact.
\end{thm}
\begin{proof}
TODO:
\end{proof}

\begin{thm}\label{PolishImpliesBorel}A Polish space is Borel.
\end{thm}
\begin{proof}
TODO
\end{proof}

Separation axioms tells us that we have enough open sets in a topology
to distinguish features of the the underlying set (e.g. distinguishing
points from points or closed sets from closed sets).  Another way of
thinking about the size of a topology is by considering the number of
continuous functions that the topology allows.  The following theorem
shows that in normal topological spaces we have enough continuous
functions to approximate indicator functions of closed sets.

\begin{thm}[Uryshon's Lemma]\label{UrysohnsLemma}Let $X$ be a
  topological space, then following are equivalent
\begin{itemize}
\item[(i)]$X$ is normal
\item[(ii)]Given a closed set $F \subset X$ and an open neighborhood
  $F \subset U$ there is an open set $V$ such that $F \subset V
  \subset \overline{V} \subset U$.
\item[(iii)]Given disjoint closed sets $F$ and $G$ there exists a
  continuous function $f : X \to [0,1]$ such that $f \equiv 1$ on $F$
  and $f \equiv 0$ on $G$.
\item[(iv)]Given a closed set $F$ with an open neighborhood $U$ there
  is a continuous function $f$ such that $\characteristic{F}(x) \leq
  f(x) \leq \characteristic{U}(x)$ for all $x \in X$.
\end{itemize}
\end{thm}
\begin{proof}
(i) $\implies$ (ii): Since $U^c$ is and $F \cap U^c = \emptyset$ we
use normality to find disjoint open sets $V$ and $O$ such that $F \subset V$
and $U^c \subset O$.  Note that $\overline{V} \cap U^c = \emptyset$; if $x \in U^c$ then $O$ is an open neighborhood $x$ such that
$O \cap V$ which implies $x \notin \overline{V}$. Therefore we have $F
\subset V \subset \overline{V} \subset U$.

(ii) $\implies$ (i): Let $F$ and $G$ be closed subsets of $X$, it
follows that $G^c$ is open and $F \subset G^c$.  Find an open set $V$
such that $F \subset V \subset \overline{V} \subset G^c$ and observe
that if we define $U = \overline{V}^c$ then we have $V \cap U =
\emptyset$ and $F \subset V$ and $G \subset U$.

(iii) $\implies$ (iv): Construct continuous $f : X \to [0,1]$ such
that $f$ equals $1$ on $F$ and $f$ equals 0 on $U^c$.  Clearly
$\characteristic{F} \leq f$ and $\characteristic{U^c} \leq 1 -f$.  The
latter is equivalent to $f \leq \characteristic{U}$ since
$\characteristic{U^c} = 1 - \characteristic{U}$.

(iv) $\implies$ (iii):  Note that $F \subset G^c$ and construct $f$
such that $\characteristic{F} \leq f \leq \characteristic{G^c}$.  The
first inequality implies that $f \equiv 1$ on $F$ while the second
implies that $f \equiv 0$ on $(G^c)^c = G$.

(iii) $\implies$ (i):  Given $F$ and $G$ and a continuous function $f
: X \to [0,1]$ such that $F \subset f^{-1}(1)$ and $G \subset
f^{-1}(0)$, simply define $U =  f^{-1}(2/3,1]$ and $V = f^{-1}[0,1/3)$
and note that by continuity of $f$ both $U$ and $V$ are open.  

(ii) $\implies$ (iv):  We construct $f$ as a limit of (discontinuous)
indicator functions.  Suppose that $F$ and $U$ are given as in the
hypothesis in (iv).  Define $F_1 = F$ and $U_0 = U$.  Using (ii) we
find an open neighborhood $V$ such that $F_1 \subset V \subset
\overline{V} \subset U$.  Define $F_{1/2} = \overline{V}$ and $U_{1/2}
= V$ so we may rewrite our inclusions as 
\begin{align*}
F_1 &\subset U_{1/2} \subset F_{1/2} \subset U_{0}
\end{align*}
Now we iterate this construction.  To make it clear and to set the
notation for the iteration we turn the crank one more time we apply
(ii) to the pair $F_1 \subset U_{1/2}$ to construct an open set $U_{3/4}$
and closed set $F_{3/4}$ and to the pair $F_{1/2} \subset U_{0}$ to
construct an open set $U_{1/4}$
and closed set $F_{1/4}$ yielding the inclusions
\begin{align*}
F_1 &\subset U_{3/4} \subset F_{3/4} \subset U_{1/2} \subset F_{1/2} \subset U_{1/4} \subset F_{1/4} \subset U_{0}
\end{align*}
Now we induct over the dyadic rationals $\mathcal{D} = \lbrace a/2^n
\mid a \in \naturals \text{ and } n \in \naturals \rbrace \cap (0,1)$ so that we create a sequence
of open and closed sets $U_q$ and $F_q$ satisfying
\begin{itemize}
\item[(i)] $U_q \subset F_q$ for all $q \in \mathcal{D}$
\item[(i)] $F_r \subset U_q$ for all $r,q \in \mathcal{D}$ with $r > q$.
\end{itemize}
Now let $f(x) = \inf \lbrace q \mid x \in U_q \rbrace$.  
TODO: Show that $f$ works...
\end{proof}

\begin{thm}[Tietze's Extension
  Theorem]\label{TietzeExtensionTheorem}Let $F$ be a closed subset of
  a normal topological space, let $a < b$ be real numbers and let $f :
  F \to [a,b]$ be a continuous function.  There exists a continuous
  function $g : X \to [a,b]$ such that $g\mid_F = f$.  If $f : F \to
  \reals$ is a continuous function then there exists a continuous
  function $g: X \to \reals$ such that $g \mid_F = f$.
\end{thm}
\begin{proof}
We begin with the case of $f$ with bounded range.  We construct $g$
via an iterative procedure.  
TODO:
\end{proof}

\begin{defn}Given a topological space $(X, \mathcal{T})$ the Baire
  $\sigma$-algebra is smallest $\sigma$-algebra for which all bounded
  continuous functions are measurable.  Equivalently 
\begin{align*}
Ba(X,\mathcal{T}) &= \sigma(\lbrace f^{-1}(U) \mid U \subset \reals
\text{ is open; } f \in C_b(X,\reals)\rbrace)
\end{align*}
\end{defn}
\begin{lem}For every topological space $(X, \mathcal{T})$, $Ba(X)
  \subset \mathcal{B}(X)$.  For a metric space $(S,d)$, $Ba(S) = \mathcal{B}(S)$.
\end{lem}
\begin{proof}
To see the inclusion $Ba(X)
  \subset \mathcal{B}(X)$, note that by continuity of $f \in
  C_b(X;\reals)$, every set $f^{-1}(U)$ is open.

Now suppose $(S,d)$ is a metric space.  To show $\mathcal{B}(S)
\subset Ba(S)$, it suffices if we show every closed set $F \subset S$
can be written as $f^{-1}(G)$ where $G \subset \reals$ is closed and
$f \in C_b(S; \reals)$.  By the triangle inequality (see e.g. Lemma
\ref{DistanceToSetLipschitz}) we know
that $g(x) = d(x, F)$ is continuous (in fact Lipschitz) and by Lemma
\ref{MaxMinOfLipschitz} we know that $f(x) = d(x, F) \wedge 1$ is also
Lipschitz and therefore $f(x) \in C_b(S; \reals)$.  Because $F$ is
closed we also know that $F = f^{-1}(\lbrace 0 \rbrace)$ and we are done.
\end{proof}

The theory of probability measures on separable metric spaces is simpler in many ways than its general counterpart 
for non-separable metric spaces. The simplicity derives from the fact that a separable metric space is not too far being compact
so in a sense doesn't have too many unbounded continuous functions with respect to which probability measures can misbehave.

One way in which to understand the way in which a separable metric space is close to being compact is to consider the real line.  Through any number of homeomorphisms, the real line is homeomorhpic to the open unit interval $(0,1)$.  In this way, distances on the real line may be rescaled so as to make the real line bounded.  Then by completing the open interval we see that real line is a couple of points away from being compact.

\begin{defn}Let $(S,d)$ be a metric space then we denote by $U^d(S)$ the set of uniformly continuous functions from $S$ to $\reals$ and by $U_b^d(S)$ the set of bounded uniformly continuous functions from $S$ to $\reals$.
\end{defn}
TODO: Show that $U^d_b(S)$ is a Banach space under the supremum norm.

\begin{lem}\label{SeparabilityOfBoundedUniformlyContinuous}Let $(S,d)$ be a separable metric space, then $X$ is
  homeomorphic to a subset of $[0,1]^{\integers_+}$ and furthermore
\begin{itemize}
\item[(i)]$S$ has a metric making it totally bounded
\item[(ii)]If $S$ is compact then $C(S; \reals)$ with the uniform
  topology is separable.
\item[(iii)]If $\hat{d}$ is a totally bounded metric on $S$ then $U^{\hat{d}}(S) = U^{\hat{d}}_b(S)$ and 
  $U^{\hat{d}}_b(S)$ is separable
\end{itemize}
\end{lem}
\begin{proof}
Let $\rho$ be the product metric $\rho(x,y) = \sum_{n=1}^\infty
\frac{\abs{x_n-y_n}}{2^n}$ on the space $[0,1]^{\integers_+}$. 
 Pick a countable dense subset $x_1, x_2, \dotsc$ of $S$ and define 
$f : S \in [0,1]^{\integers_+}$ by 
\begin{align*}
f(x) &= \left ( \frac{d(x_1, x)}{1 + d(x_1, x)}, \frac{d(x_2, x)}{1 +
    d(x_2, x)}, \dotsc \right )
\end{align*}
 
\begin{clm}$f(x)$ is continuous.
\end{clm}

By definition of the product topology $f(x)$ is continuous if and only
if each coordinate is.  For any given fixed $x_j$, we know that
$d(x_j, x)$ is continuous (in fact Lipschitz by Lemma
\ref{DistanceToSetLipschitz}) and thus the result follows from the
continuity of $x/(1+x)$ on $\reals_+$.

\begin{clm}$f(x)$ is injective.
\end{clm}

For any $z \neq y$ we find $\epsilon > 0$ such that $B(z ; \epsilon)
\cap B( y ; \epsilon) = \emptyset$ and then using density of $x_1,
x_2, \dotsc$ to pick an $x_n$ such that $d(z,x_n) < \epsilon$ and
$d(y, x_n) \geq \epsilon$ showing $f(z) \neq f(y)$.  

\begin{clm}The inverse of $f(x)$ is continuous.
\end{clm}

Fix an $x \in S$ and let $\epsilon >0$ be given.  Pick $x_n$ such that
$d(x_n, x) < \epsilon/2$.  If we let $g(x) : [0,1) \to \reals_+$ be
defined by $g(x) = x/(1-x)$ then $g(x)$ is the inverse of $x/(1+x)$ 
and by continuity of $g(x)$ at the point $\frac{d(x_n, x)}{1+d(x_n,x)}$ we know that there exists a $\delta > 0$
such that $\abs{\frac{d(x_n, x)}{1+d(x_n,x)} - 
\frac{d(x_n,  y)}{1+d(x_n,y)}}< \delta$ implies $\abs{d(x_n,x) - d(x_n,y)} <
  \epsilon/2$.
Then if $f(y) \in B(f(x), \frac{\delta}{2^n})$ we have
\begin{align*}
\abs{\frac{d(x_n, x)}{1+d(x_n,x)} - 
\frac{d(x_n,  y)}{1+d(x_n,y)}} &\leq 2^n \rho(f(x), f(y)) < \delta
\end{align*}
$d(x,y) \leq d(x_n,x) + \abs{d(x_n,x) - d(x_n,y)} < \epsilon$.

Now to see (i) we simply pull back the metric $\rho$ via the embedding
$f(x)$ and use the facts that $\rho$ generates the product topology,
$[0,1]^{\integers_+}$ is compact in product topology
(by Tychonoff's Theorem \ref{Tychonoff}; alternatively one can avoid
the use of Tychonoff's Theorem for it is easy to
see with a diagonal subsequence argument that a countable product of
sequentially compact metric spaces is sequentially compact) hence totally bounded (Theorem
\ref{CompactnessInMetricSpaces}).

Here is the argument that $\rho$ generates the product topology; TODO:
put this in a separate lemma.  To see that the topology generated by
$\rho$ is finer than the product topology, suppose $U$ is open in the
topology generated by $\rho$.  Pick $x \in U$ and select $N > 0$ such
that $B(x,\epsilon) \subset U$.  Then pick $N > 0$ such that $2^{-N-1} <
\epsilon$ and consider $B=B(x_1, \epsilon/2)
\times \dotsb \times B(x_{2^N}, \epsilon/2) \times S \times \dotsb$
which is open in the product topology.  If $y \in B$ then 
\begin{align*}
\rho(x,y) &=
\sum_{n=1}^\infty \frac{\abs{x_n-y_n}}{2^n} = \sum_{n=1}^{2^N}
\frac{\abs{x_n-y_n}}{2^n} + \sum_{n=2^N+1}^\infty \frac{\abs{x_n-y_n}}{2^n} \leq
\frac{\epsilon}{2} \sum_{n=1}^{2^N} \frac{1}{2^n} +
\sum_{n=2^N+1}^\infty \frac{1}{2^n} < \epsilon
\end{align*}
To see that the product topology is finer than the metric topology,
suppose $n >0$ is an integer, $U\subset[0,1]$ is open and consider $\pi_n^{-1}(U)$.  Let $x
\in \pi_n^{-1}(U)$ and find an $\epsilon > 0$ such that $B(x_n,
\epsilon) \subset U$.  Note that if $y \in B(x, \frac{\epsilon}{2^n})$
then $\abs{x_n - y_n} < 2^n \rho(x,y) \leq \epsilon$ and therefore
$B(x, \frac{\epsilon}{2^n}) \subset \pi_n^{-1}(B(x_n, \epsilon))
\subset U$.

To see (ii), if $S$ is compact then $f(S) \subset [0,1]^{\integers_+}$
is compact (Lemma \ref{ContinuousImageOfCompact}).  Observe that 
\begin{align*}
A &= \lbrace \Pi_{i=1}^np_i(x_i) \mid n \in \naturals \text{ and } p_i
\in \rationals[x] \rbrace
\end{align*}
is a subalgebra of $C([0,1]^{\integers_+} ; \reals)$ and $A$ separates points (
given $x \neq y \in [0,1]$, pick
$n$ such that $x_n \neq y_n$ and pick the function $g(x) = x_n$).  By
the Stone-Weierstrass Theorem \ref{StoneWeierstrassApproximation} we know that $A$ is dense in $C([0,1]^{\integers_+};
\reals)$; now pullback $A$ under $f(x)$ to a countable dense subset of
$C(S;\reals)$. 

To see (iii), suppose $\hat{\rho}$ is a totally bounded metric on
$S$.  Let $\hat{S}$ be the completion of $S$ with respect to
this metric.  

\begin{clm}$\hat{\rho}$ extends to a totally bounded
metric on $\hat{S}$.  
\end{clm}

Let $\epsilon>0$ be given and cover $S$ by ball
$B(x_i, \epsilon/2)$; we show that $B(x_i, \epsilon)$ covers
$\hat{S}$.  Biven $y \in \hat{S}$ we can find $x \in
S$ such that $\hat{\rho}(x,y) < \epsilon/2$.  Since $x \in S$ there
exists an $x_i$ such that $x \in B(x_i, \epsilon/2)$ and therefore
$\hat{\rho}(x_i,y) \leq \hat{\rho}(x,x_i) + \hat{\rho}(x_i,y) <
\epsilon$.

Because $(\hat{S},\hat{\rho})$ is complete and totally bounded we know
it is compact (Theorem \ref{CompactnessInMetricSpaces}) and we have
just shown that $C(\hat{S} ; \reals)$ has a countable dense subset.

\begin{clm}$f\vert_S : C(\hat{S} ; \reals) \to U_b^{\hat{\rho}}(S ;
\reals)$ is a well defined, continuous and surjective.
\end{clm}

Being well defined in this context means that restriction to $S$
results in a bounded uniformly continuous function.  This follows
from the fact that any continuous function of a compact set is bounded
and uniformly continuous (Theorem \ref{ContinuousImageOfCompact} and
Theorem \ref{UniformContinuityOnCompactSets} respectively) and these
properties are preserved upon restriction.  To see surjectivity, let
$g : S \to \reals$ be uniformly continuous.  We may apply Proposition
\ref{ExtensionOfUniformlyContinuousMapCompleteRange} to see that $g$ has a unique extension
to a continuous function from the closure of $S$ to $\reals$.  Since the closure of $S$ in $\hat{S}$ is
$\hat{S}$ be are done with the claim.  Note that we did not need boundedness of $g$ in order to prove the existence of the extension;
therefore we have shown $U^{\hat{d}}(S) = U^{\hat{d}}_b(S)$.

Now the continuous image of a dense set under a surjective map is also
dense.  This is easily seen by picking a point $f(x)$ in the image;
picking a sequence $x_n$ such that $x_n \to x$ and then considering
the image $f(x_n) \to f(x)$.  Thus the result is proven.
\end{proof}

\begin{lem}[Dini's Theorem]\label{DinisTheorem}Let $K$ be a compact
  topological space and let $f_n: K \to \reals$ be a sequence  of continuous
  functions such that $f_n \downarrow 0$ pointwise on $K$, then $f_n
  \to 0$ uniformly.
\end{lem}
\begin{proof}
Given $\epsilon > 0$ define $U_n = f_n^{-1}((-\infty,\epsilon))$.
Then each
$U_n$ is open, $U_1 \subset U_2 \subset \dotsb$ (since the $f_n$ are
decreasing) and the $U_n$ form an open cover of $K$.  We can extract a
finite subcover which since the $U_n$ are nested implies that $K =
U_N$ for some $N > 0$.  This is exactly the statement that $\sup_{x
  \in K} \abs{f_n(x)} < \epsilon$ for all $n \geq N$ hence the result proven.
\end{proof}

\begin{lem}\label{StoneDaniellProbability}Let $(S,d)$ be a separable metric space and let $\Lambda :
  U_b^d(S; \reals) \to \reals$ be a linear map such that 
\begin{itemize}
\item[(i)] $\Lambda$ is non-negative (i.e. if
  $f \geq 0$ then $\Lambda(f) \geq 0$) 
\item[(ii)] $\Lambda(1) = 1$
\item[(iii)] for all $\epsilon > 0$ there exists a compact set $K
  \subset S$ such that for all $f \in U_b^d(S; \reals)$,
\begin{align*}
\abs{\Lambda(f)} &\leq \sup_{x \in K} \abs{f(x)} + \epsilon \norm{f}_u
\end{align*}
\end{itemize}
then there exists a Borel probability measure $\mu$ on $S$ such that
$\Lambda(f) = \int f \, d\mu$.  Whenever such a probability measure
exists it is unique.
\end{lem}
\begin{proof}
We construct $\mu$ by use of the Daniell-Stone Theorem
\ref{DaniellStoneTheorem}.  It is clear that $U_b^d(S; \reals)$ is
closed under max and min and contains the constant functions so
$U_b^d(S; \reals)$ is a Stone Lattice.  It remains to show that
$\Lambda$ obeys the ``montone convergence'' property: if $f_n
\downarrow 0$ pointwise then $\Lambda(f_n) \downarrow 0$.  This
property is a corollary of Dini's Theorem \ref{DinisTheorem} since by that result,
if $f_n$ are continuous and $f_n \downarrow 0$ pointwise on a compact
set then the converge uniformly to $0$ on the compact set.  In
particular, pick an $\epsilon > 0$ and let $K \subset S$ be compact
as in the hypothesis.  By Dini's Theorem there exists $N > 0$ such
that $\sup_{x \in K} \abs{f_n(x)} < \epsilon$ for all $n \geq N$.  Therefore
for all $n \geq N$,
\begin{align*}
\abs{\Lambda(f_n)} &\leq \sup_{x \in K} \abs{f_n(x)} + \epsilon
\norm{f_n}_\infty \\
&\leq \epsilon(1 + \norm{f_1}_\infty)
\end{align*}
thus $\lim_{n \to \infty} \Lambda(f_n) = 0$ and we can apply Theorem \ref{DaniellStoneTheorem}. 

Uniqueness follows because a probability measure is determined by its
integrals over $U_b^d(S;\reals)$ (in fact over the subset of bounded
Lipschitz functions).  This follows because for any closed $F \subset
S$ we can define $f_n(x) = n d(x, F) \wedge 1$ so that $f_n
\downarrow \characteristic{F}$ and apply Montone Convergence (see the
proof of the Portmanteau Theorem \ref{PortmanteauTheorem} for complete
details on this argument).
\end{proof}

TODO: Apparently tight implies relatively compact does not require that $S$ be separable, find a proof for that (Ethier and Kurtz have one).

\begin{thm}[Prohorov's Theorem]\label{Prohorov}Let $(S,d)$ be a
  separable metric space, then a tight set of probability measures on
  $S$ is weakly relatively compact.  If $S$ is also complete then a
  weakly relatively compact set is tight.
\end{thm}
\begin{proof}
By the Portmanteau Theorem \ref{PortmanteauTheorem} we know that a
set of measures is tight if and only if its weak closure is tight
(compact sets are closed hence can only gain mass in a weak limit).  Thus it suffices to assume
that we have a closed tight set $M$ of measures.  Put a totally
bounded metric $\hat{d}$ on $S$ so that $U_b^{\hat{d}}(S;\reals)$ is separable
(Lemma \ref{SeparabilityOfBoundedUniformlyContinuous}); let $f_1, f_2,
\dotsc$ be a countable uniformly dense subset.  

Pick a sequence $\mu_n$ from $M$; we must show that it has a weakly
convergent subsequence.  For every fixed $f_m$
we know that $\abs{\int f_m \, d\mu_n} \leq \norm{f_m}_u < \infty$ so
there is a subsequence $N \subset \naturals$ such that $\int f_m \,
d\mu_n$ converges along $N$.  Since this is true for every $m>0$, a
diagonalization argument shows there is a subsequence $\hat{\mu}_{k}$ such that
$\lim_{k \to \infty} \int f_m \, d\hat{\mu}_{k}$ exists for every
$m>0$.  Define $\Lambda(f_m) =   \lim_{k \to \infty} \int f_m \,
d\hat{\mu}_{k}$ for every such $f_m$.  Our next goal is to extend
$\Lambda$ to all of $U_b^\rho(S;\reals)$.  Since $\Lambda$ is
uniformly continuous on a dense subset we know that a continuous
extension is defined; however we need a little bit more information.

\begin{clm}$\lim_{k \to \infty} \int f \, d\hat{\mu}_{k}$ exists for every
$f \in U_b^\rho(S;\reals)$; moreover 
\begin{align*}
\lim_{k \to \infty} \int f \,d\hat{\mu}_{k} &= \lim_{m \to \infty} \Lambda(\hat{f}_m)
\end{align*} 
where $\hat{f}_m$ is any subsequence of $f_m$ that converges uniformly to $f$.
\end{clm}

Pick a subsequence of the $f_m$ that converges to $f$.  Let that
subsequence be donoted $\hat{f}_m$ so that $\lim_{m \to \infty} \norm{\hat{f}_m -
  f}_\infty = 0$.  For every $m > 0$ we have
\begin{align*}
\int \hat{f}_m \, d \hat{\mu}_k -\norm{\hat{f}_m - f}_\infty &\leq 
\int f \, d\hat{\mu}_k \leq \int \hat{f}_m \, d \hat{\mu}_k + \norm{\hat{f}_m - f}_\infty
\end{align*}
and therefore taking limits in $k$ and using the definition of
$\Lambda$ at the points $f_m$,
\begin{align*}
\Lambda(\hat{f}_m) -\norm{\hat{f}_m - f}_\infty 
&\leq \liminf_{k \to \infty} \int f \, d\hat{\mu}_k 
\leq \limsup_{k \to \infty} \int f \, d\hat{\mu}_k 
\leq \Lambda(\hat{f}_m) + \norm{\hat{f}_m - f}_\infty
\end{align*}
Now letting $m$ go to infinity we get $\lim_{m \to \infty}
\Lambda(\hat{f}_m) = \lim_{k \to \infty} \int f \, d\hat{\mu}_k$.

As a result of the claim, we now define $\Lambda(f) = \lim_{k \to
  \infty} \int f \, d\hat{\mu}_{k}$ for every $f$  and it is clearly
linear (by linearity of integral and limits), nonnegative (by
monotonicity of integral) and satisfies $\Lambda(1) = 1$ (since each $\hat{\mu}_k$
is a probability measure).  

To show that $\Lambda$ defines a probability measure, we bring the
tightness hypothesis to the table.  Pick $\epsilon > 0$ and by
tightness take a compact set $K \subset S$ such that $\sup_{\mu \in M}
\mu(K) > 1 - \epsilon$.  For any $f \in U_b^{\hat{d}}(S ; \reals)$ we
have
\begin{align*}
\abs{\Lambda(f)} 
&= \lim_{k \to \infty} \abs{\int f \, d\hat{\mu}_k}
= \lim_{k \to \infty} \abs{\int f \characteristic{K} \, d\hat{\mu}_k +
\int f \characteristic{S \setminus K} \, d\hat{\mu}_k} \leq \sup_{x
\in K} \abs{f(x)} + \epsilon \norm{f}_\infty
\end{align*}
so we may apply Lemma \ref{StoneDaniellProbability} to conclude there
exists a probability measure $\mu$ such that for all $f \in U_b^{\hat{d}}(S ; \reals)$ we
have $\Lambda(f) =  \lim_{k \to \infty} \int f \, d\hat{\mu}_k =
\int f \, d\mu$.  Since $U_b^{\hat{d}}(S ; \reals)$ contains all
bounded Lipschitz functions by the Portmanteau Theorem
\ref{PortmanteauTheorem} we conclude $\hat{\mu}_k$ converges weakly to
$\mu$.

Now assume that $S$ is complete and separable and let $M$ be a weakly
relatively compact set of measures.  Let $x_1, x_2, \dotsc$ be a countable dense
subset of $S$.  For every integer $n > 0$ we have $S =
\cup_{k=1}^\infty B(x_k, 1/n)$.  Thus $\cap_{N=1}^\infty \cap_{k=1}^N
B(x_k, 1/n)^c = \emptyset$ so by continuity of measure (Lemma
\ref{ContinuityOfMeasure}) for any fixed probability measure $\mu$ we
can find an $N_{n, \mu} > 0$ such that $\mu(\cap_{k=1}^{N_{n, \mu}}
B(x_k, 1/n)^c ) < \epsilon/2^n$.  We claim that, because $M$ is
compact, we can find an $N_n$ for
which this is true uniformly over the measures in $M$.

\begin{clm}\label{ProhorovClaim2}For every $n > 0$ there exists $N_n > 0$ such that $\mu(\cap_{k=1}^{N_{n}}
B(x_k, 1/n)^c ) < \epsilon/2^n$ for all $\mu \in M$.
 \end{clm}

We argue by contraction by reducing the case where $M$ is a singleton
set (where we have already shown the claim holds).  If Claim \ref{ProhorovClaim2} is not
true then there exists $n$ such that for every integer $N>0$ we have
some $\mu_N \in M$ such that $\mu_N(\cap_{k=1}^{N} B(x_k, 1/n)^c )
\geq \epsilon/2^n$.  By sequential compactness of $M$ we know that
there is a weakly convergent subsequence $\mu_{N_j}$ such that
$\mu_{N_j} \toweak \mu$ for some probability measure $\mu$.  For every
$N > 0$ we have $\cap_{k=1}^{N} B(x_k, 1/n)^c$ is closed and therefore
by the Portmanteau Theorem \ref {PortmanteauTheorem}
\begin{align*}
\epsilon/2^n &\leq \limsup_{j \to \infty} \mu_{N_j}(\cap_{k=1}^{N_j}
B(x_k, 1/n)^c) \\
&\leq \limsup_{j \to \infty} \mu_{N_j}(\cap_{k=1}^{N} B(x_k, 1/n)^c)
\\
&\leq \mu(\cap_{k=1}^{N} B(x_k, 1/n)^c)
\end{align*}
where in the second inequality we have used the fact that the limit
only depends on the tail of the sequence of sets $\cap_{k=1}^{N_j}
B(x_k, 1/n)^c$ and by a union bound for sufficiently large $N_j$ we
have $\mu_{N_j}(\cap_{k=1}^{N_j} B(x_k, 1/n)^c)  \leq
\mu_{N_j}(\cap_{k=1}^{N} B(x_k, 1/n)^c)$.  To finish we get a
contradiction by taking the 
limit and using continuity of measure
\begin{align*}
0 < \epsilon/2^n \leq \lim_{N \to \infty} \mu(\cap_{k=1}^{N} B(x_k,
1/n)^c) = 0
\end{align*}

With Claim \ref{ProhorovClaim2} proven we mimic the proof of Ulam's Theorem.  Let 
\begin{align*}
K &=
\cap_{m=1}^\infty \cup_{j=1}^{N_m} \overline{B}(x_j,\frac{1}{m})
\end{align*}
which is easily seen to be closed (hence complete) and by construction
is totally bounded thus is compact (Theorem
\ref{CompactnessInMetricSpaces})
and furthermore for all $\mu \in M$,
\begin{align*}
\mu(K^c) &\leq \mu((\cap_{m=1}^\infty
\cup_{j=1}^{N_m} B(x_j,\frac{1}{m}))^c) \\
&=\mu(\cup_{m=1}^\infty 
\cap_{j=1}^{N_m} B(x_j,\frac{1}{m})^c) \\
&=\sum_{m=1}^\infty \mu(
\cap_{j=1}^{N_m} B(x_j,\frac{1}{m})^c) \\
&\leq \sum_{m=1}^\infty \frac{\epsilon}{2^m} = \epsilon
\end{align*}

\end{proof}

\begin{lem}For $f,g \in C([0,\infty) ; \reals)$ define
\begin{align*}
\rho(f,g) &= \sum_{n=1}^\infty \frac{1}{2^n} \sup_{0 \leq t \leq n}
(\abs{f(t) - g(t)} \wedge 1)
\end{align*}
then $\rho$ is a metric on $C([0,\infty) ; \reals)$ and $C([0,\infty);
\reals)$ is complete and separable with respect to this metric.
\end{lem}
\begin{proof}
It is clear that $\rho(f,f) = 0$ and furthermore if $\rho(f,g) = 0$
then $f = g$ on every interval $[0,n]$ and therefore $f = g$.
Symmetry and the triangle inequality of $\rho$ is immediate from the
corresponding properties of the absolute value (TODO: OK the triangle
inequality may need a bit more of an argument).

We claim that the set of polynomials with rational coefficients is
dense in $C([0,\infty); \reals)$.  Pick $f \in C([0,\infty); \reals)$
and let $\epsilon > 0$ be given.  Now take $m > 0$ sufficiently large
so that $1/2^m < \epsilon / 2$ and by the Stone Weierstrass Theorem \ref{StoneWeierstrassApproximation} we
pick a polynomial with rational coefficients $p$ such that $\sup_{0
  \leq t \leq m} \abs{f(t) - p(t)} < \epsilon/2$ then we have
\begin{align*}
\rho(f,p) &\leq \sum_{n=1}^m\frac{1}{2^n} \sup_{0 \leq t \leq n}
\abs{f(t) - p(t)} + \sum_{n=m+1}^\infty \frac{1}{2^n} \\
&\leq \sup_{0 \leq t \leq m}
\abs{f(t) - p(t)} \sum_{n=1}^m\frac{1}{2^n} + \epsilon/2 <\epsilon
\end{align*}

Completeness follows from arguing over intervals $[0,n]$.  Suppose
$f_n$ is a Cauchy sequence in $C([0,\infty); \reals)$.  Given
$\epsilon > 0$ and $n > 0$ we can find $N > 0$ such that $\rho(f_m,
f_N) < \epsilon/2^n$ for all $m \geq N$.  Thus $\sup_{0 \leq t \leq n}
\abs{f_m(t) - f_N(t)} < \epsilon$ for all $m \geq N$ so we see that
$f_n$ is uniformly  Cauchy on every interval $[0,n]$.  By completeness
of $C([0,n];\reals)$ we know that the pointwise limit of $f_n$ exists
on every $[0,n]$ and is a continuous function.  Therefore we have a
limit $f$ defined on $[0,\infty)$ and since continuity is a local
property $f \in C([0,\infty); \reals)$.  It remains to show that $f_n$
converges to $f$ in the metric $\rho$.  This follows arguing as we
have above.  Let $\epsilon > 0$ be given and choose $n > 0$ such that
$\frac{1}{2^n} < \epsilon/2$ and choose $N > 0$ such that $\sup_{0 \leq t \leq n}
\abs{f_m(t) - f_N(t)} < \epsilon/2$ and then observe
\begin{align*}
\rho(f_m, f_N) &\leq \sum_{k=1}^n \frac{1}{2^k} \sup_{0 \leq t \leq k}
\abs{f_m(t) - f_N(t)} + \sum_{k=n+1}^\infty \frac{1}{2^k}  < \epsilon
\end{align*}
\end{proof}

The topology defined by $\rho$ is often refered to as the topology of
uniform convergence on compact sets by virtue of the following lemma.
\begin{lem}\label{UniformConvergenceOnCompacts}A sequence $f_n$
  converges to $f$ in $C^\infty([0,\infty), \reals)$ if and only if
  $f_n$ converges to $f$ uniformly on every interval $[0,T]$ for $T > 0$.
\end{lem}
\begin{proof}
TODO:  This is elementary.
\end{proof}

\begin{defn}Given a function $f : [0,T] \to \reals$ the \emph{modulus
    of continuity} is the function
\begin{align*}
m(T, f, \delta) &= \sup_{\substack{\abs{s - t} < \delta \\
0 \leq s,t \leq T}} \abs{f(s) - f(t)}
\end{align*}
\end{defn}

\begin{lem}For fixed $T > 0$ and $\delta > 0$, $m(T, f, \delta)$ is a
  continuous function on $C([0,\infty); \reals)$.  For fixed $T > 0$
  and function $f : \reals \to \reals$, $m(T,f,\delta)$ is
  nonincreasing in $\delta$ and 
\begin{align*}
\lim_{\delta \to 0} m(T, f, \delta) = 0
\end{align*}
provided $f \in C([0,\infty); \reals)$.
\end{lem}
\begin{proof}
To see continuity on $C([0,\infty); \reals)$ let $f \in C([0,\infty);
\reals)$, $T > 0$, $\delta > 0$ and $\epsilon > 0$ be given and pick $g$ that $\rho(f,g) <
\epsilon/2^{\ceil{T}+1}$.
From the definition of the metric $\rho$ for any $n > 0$, $\sup_{0 \leq t \leq n} \abs{f(t) - g(t)} \wedge
1 \leq 2^n \epsilon$, so for any $T > 0$, 
\begin{align*}
\sup_{0 \leq t \leq T} \abs{f(t) - g(t)} \wedge
1 &\leq \sup_{0 \leq t \leq \ceil{T}} \abs{f(t) - g(t)} \wedge
1 \leq \epsilon/2
\end{align*}  
Therefore by the triangle inequality,
\begin{align*}
\sup_{\substack{
\abs{s -t} < \delta \\
0 \leq s,t \leq T}} \abs{g(s) - g(t)} \wedge 1 
&\leq 
\sup_{\substack{
\abs{s -t} < \delta \\
0 \leq s,t \leq T}} \left ( \abs{g(s) - f(s)} + \abs{f(s) - f(t)} + \abs{f(t)
- g(t)} \right ) \wedge 1 \\
&\leq \epsilon/2 + 
\sup_{\substack{
\abs{s -t} < \delta \\
0 \leq s,t \leq T}} \abs{f(s) - f(t)} \wedge 1 + \epsilon/2
\end{align*}
and therefore arguing with the roles of $f$ and $g$ reversed shows 
$\abs{m(T, f, \delta) - m(T, g, \delta) } \leq \epsilon$.

The fact that $m(T, f, \delta)$ is decreasing in $\delta$ is clear
because the definition shows that for $\delta_1 \leq \delta_2$ we
have 
\begin{align*}
\lbrace \abs{f(t) - f(s) } \mid 0 \leq s,t \leq T \text{ and }
  \abs{s-t} < \delta_1 \rbrace 
&\subset 
\lbrace \abs{f(t) - f(s) } \mid 0 \leq s,t \leq T \text{ and }
  \abs{s-t} < \delta_2 \rbrace
\end{align*} and therefore $m(T, f, \delta_2) \leq
  m(T, f, \delta_1)$.

Lastly if we suppose $f \in C([0,\infty); \reals)$ then $f$ is
uniformly continuous on $[0,T]$ for every $T > 0$ (Theorem
\ref{UniformContinuityOnCompactSets}).  Thus given an $\epsilon > 0$
there exists $\delta>0$ such that 
\begin{align*}
\sup_{\substack{
\abs{s -t} < \delta \\
0 \leq s,t \leq T}} \abs{f(s) - f(t)} < \epsilon
\end{align*}
which shows $\lim_{\delta \to 0} m(T, f, \delta) = 0$.
\end{proof}
The following Theorem is a version of the Arzela-Ascoli Theorem of
real analysis.
\begin{thm}[Arzela-Ascoli Theorem]\label{ArzelaAscoliTheorem}A set $A
  \subset C([0,\infty); \reals)$ is relatively compact if and only if 
\begin{itemize}
\item[(i)]$\sup_{f \in A} \abs{f(0)} < \infty$
\item[(ii)]$\lim_{\delta \to 0} \sup_{f \in A} m(T, f, \delta) = 0$
  for all $T > 0$.
\end{itemize}
\end{thm}
\begin{proof}
To see the necessity of condition (i), observe that $\overline{A}$ is
compact and by completeness of $C([0,\infty); \reals)$ we know that
$\overline{A}$ comprises continuous functions.  Therefore we know that
$A \subset \overline{A} \subset \cup_{n=1}^\infty \lbrace f \in
C([0,\infty) \mid
\abs{f(0)} < n\rbrace$.  Since each $\lbrace f \in
C([0,\infty) \mid
\abs{f(0)} < n\rbrace$ is easily seen to be an open set, by
compactness of $\overline{A}$ we have a finite subcover which implies
there exists an $N$ such that $A \subset \overline{A} \subset \lbrace f \in
C([0,\infty) \mid
\abs{f(0)} < N\rbrace$.

To see the necessity of condition (ii), fix $\epsilon > 0$, $T > 0$
and define for each $\delta > 0$ the set 
\begin{align*}
F_\delta &= \lbrace f \in \overline{A} \mid m(T, f, \delta) \geq
\epsilon \rbrace
\end{align*}
By continuity of $m(T, f, \delta)$ we know that $F_\delta$ is closed.
Since $F_\delta \subset \overline{A}$ with $\overline{A}$ compact we
conclude that $F_\delta$ is compact.  Furthermore since for fixed $f
\in \overline{A}$ continuity (more specifically uniform continuity on compact
sets) implies $\lim_{\delta \to 0} m(T,f,\delta) = 0$, we know that
$\cap_{\delta > 0} F_\delta = \emptyset$.  By nestedness and
compactness of the
$F_\delta$ we know that there is some specific $\delta>0$ for which $F_\delta =
\emptyset$ (Lemma \ref{IntersectionOfNestedCompactSets}) and (ii) is established.

To see the sufficiency of conditions (i) and (ii), we first construct
the limiting subsequence on a the set of rationals $\rationals_+
\subset [0,\infty)$.  To do this, we first claim that for any $T \in
\rationals_+$, (in fact any $T \in [0,\infty)$, the set $\lbrace
\abs{f(x)} \mid f \in A \rbrace$ is bounded.  The claim follows for
$T>0$ by using (ii) to select a $\delta > 0$ such that $\sup_{f \in A} m(T, f,
\delta) < 1$.  Picking the integer $m \geq 0$ such that $m \delta < T \leq
(m+1)\delta$ and considering  the grid $0, \delta, 2\delta, \dotsc,
m\delta, T$ we can write the telescoping sum
\begin{align*}
f(T) - f(0) = f(T) - f(m\delta) + \sum_{k=1}^m f(k \delta) - f((k-1)\delta)
\end{align*}
and use the triangle inequality to conclude that $\abs{f(T)}
\leq \abs{f(0)} + m+1$ for every $f \in A$.  Coupled with (i) this shows that
$\sup_{f \in A} \abs{f(T)} < \infty$.

We now enumerate the rationals $\rationals_+$ and use
compactness in $\reals$ and a diagonal
subsequence argument to pick a sequence $f_n$ with $f \in A$ such that
$f_n(T)$ converges for every $T \in \rationals_+$.  Define $f :
\rationals_+ \to \reals$ by $f(T) = \lim_{n \to \infty} f_n(T)$.

Having selected a convergent subsequence $f_n$ and defined $f$ on
$\rationals_+$ we proceed to see that $f$ is uniformly continuous.
This follows by using (ii) to see that for every $f_n$, $T > 0$ and 
$\epsilon > 0$ there is $\delta > 0$ such that $\abs{f_n(s) - f_n(t)} <
\epsilon$ when $0 \leq s,t \leq T$ and $\abs{s - t} <\delta$.  From
this we have for every $n>0$, and $s,t \in \rationals$, $0 \leq s,t
\leq T$ and $\abs{s-t} < \delta$
\begin{align*}
\abs{f(s) -f(t)} &\leq \abs{f(s) -f_n(s)} + \abs{f_n(s) -f_n(t)} +
\abs{f_n(t) - f(t)} \\
&\leq \abs{f(s) -f_n(s)} + \epsilon +
\abs{f_n(t) - f(t)}
\end{align*}
Taking the limit as $n \to \infty$ using pointwise convergence of
$f_n$ to $f$ shows uniform continuity on every
$[0,T] \cap \rationals$ hence on $\rationals_+$.  Since $f$ is uniformly continuous on
$\rationals_+$ it follows that $f$ has a continuous extension to $f :
[0,\infty) \to \reals$.  Moreover we have shown that $\abs{f(s) -f(t)}
< \epsilon$ when $\abs{s -t} < \delta$.

It remains to prove that $f_n \to f$ in $C([0,\infty); \reals)$.  It
suffices (Lemma \ref{UniformConvergenceOnCompacts}) to show that $f_n
\to f$ uniformly on every interval $[0,T]$.  Let $T > 0$ be given.
Pick $\epsilon > 0$ and let
$\delta > 0$ be such that $m(T,f_n,\delta) < \epsilon$ (hence $m(T,f,\delta)
< \epsilon$ by the above comment).   Pick $N > 0$ such that
$\abs{f_n(k\delta) - f(k\delta)} < \epsilon/3$ for all $k=0,1, \dotsc,
\ceil{T/\delta}$ and $n \geq N$.  Then for every $0 \leq t \leq T$ and
$n \geq N$ let $k\geq 0$ be such that $k\delta \leq t < (k+1)\delta$
\begin{align*}
\abs{f_n(t) - f(t)} &\leq \abs{f_n(t) - f_n(k\delta)}
+\abs{f_n(k\delta) - f(k\delta)} +\abs{f(k\delta) - f(t)} < \epsilon
\end{align*}
and we are done.
\end{proof}

Provided with a characterization of compact sets in
$C^\infty([0,\infty); \reals)$ we can now state the probabilistic
analogue that characterizes tightness.
\begin{lem}\label{TightnessOfContinuousFunctions}A sequence of Borel probability measures $\mu_n$ on $C^\infty([0,\infty);
  \reals)$ is tight if and only if 
\begin{itemize}
\item[(i)]$\lim_{\lambda \to \infty} \sup_{n \geq 1} \sprobability{\abs{f(0)}
  \geq \lambda}{\mu_n} = 0$.
\item[(ii)] $\lim_{\delta \to 0} \sup_{n \geq 1} \sprobability{m(T, f,
  \delta) \geq \lambda}{\mu_n} = 0$ for all $\lambda > 0$ and $T > 0$.
\end{itemize}
\end{lem}
\begin{proof}
Let $\mu_n$ be a tight sequence.  Let $\epsilon > 0$ be given and pick
$K \subset C^\infty([0,\infty); \reals)$ compact with $\mu_n(K) >
1-\epsilon$ for all $n$.  Then by Theorem \ref{ArzelaAscoliTheorem} we know that $\sup_{f \in K}
\abs{f(0)} < \infty$ and therefore $\sprobability{\abs{f(0)}\geq
\lambda}{\mu_n} \leq \mu_n(K^c) < \epsilon$ for any $\lambda > \sup_{f
\in K} \abs{f(0)}$.  Thus (i) is shown.  Similarly applying Theorem \ref{ArzelaAscoliTheorem} we know that for
every $T > 0$ and $\lambda>0$
there exists $\delta>0$ such that $\sup_{f \in K} m(T, f, \delta) <
\lambda$.  Therefore $\lbrace f \mid m(T,f,\delta) \geq \lambda \rbrace
\subset K^c$ and by a union bound, for every $n>0$ we have $\sprobability{m(T,f,\delta) \geq
  \lambda}{\mu_n} \leq \sprobability{K^c}{\mu_n} < \epsilon$.
Therefore we have shown (ii).

Now assume that (i) and (ii) hold and suppose that $\epsilon > 0$ is
given.  By (i) there exists $\lambda > 0$ such that $\sup_{n \geq 1}
\sprobability{\abs{f(0)} \geq \lambda}{\mu_n} < \epsilon/2$.  By (ii)
for every integer $T > 0$ and $k > 0$, there exists a $\delta_{T,k}$
such that $\sup_{n \geq 1} \sprobability{m(T, f, \delta_{T,k}) \geq
  1/k}{\mu_n} < \epsilon/2^{T+k+1}$.  If we define 
\begin{align*}
A_T &= \lbrace f \mid 
m(T,f,\delta_{T,k}) < 1/k \text{ for all } k \geq 1\rbrace
\end{align*}
so that $A^c_T \subset \cup_{k=1}^\infty \lbrace f \mid m(T, f, \delta_{T,k}) \geq
  1/k \rbrace$ then by a union bound
\begin{align*}
\sup_{n \geq 1} \mu_n(A_T) &= \sup_{n \geq 1} (1 - \mu_n(A^c_T))\\
&\geq \sup_{n \geq 1} \left(1 - \sum_{k=1}^\infty \sprobability{m(T, f, \delta_{T,k}) \geq
  1/k}{\mu_n}\right ) \\
&\geq 1 - \epsilon/2^{T+1}
\end{align*}
If we define $K = \lbrace f \mid \abs{f(0)} < \lambda \rbrace \cap
\cap_{T=1}^\infty A_T$ then another union bound shows $\sup_{n \geq 1}
\mu_n(K) > 1 - \epsilon$ and by construction the set $K$ satisfies the
conditions of Theorem \ref{ArzelaAscoliTheorem} so is proven compact.  
\end{proof}

To prove that the rescaled and linearly interpolated random walk converges we need
prove tightness.  To prove tightness we need to show equicontinuity.
The following Lemma begins the process by demonstrating equicontinuity
at $0$.  Keep in mind the picture of the scaling of the random walk at
level $n$
which places the value of $S_j$ at the point $j/n$ scaled by the
factor $1/\sigma\sqrt{n}$.  With this geometry in mind note that what
we are proving is a bound for each of the sequence of rescaled random
walks on the interval $[0,\delta]$.

TODO: Replace $\epsilon$ by $\lambda$ in the following Lemma?
\begin{lem}\label{RandomWalkEquicontinuityAt0} Let $\xi_n$ be i.i.d. with mean $0$ and finite variance
  $\sigma^2$ and define $S_n = \sum_{k=1}^n \xi_k$.  Then for all
  $\epsilon > 0$ 
\begin{align*}
\lim_{\delta \to 0} \limsup_{n \to \infty} \frac{1}{\delta}
\probability{\max_{1 \leq j \leq \floor{n\delta}+1} \frac{\abs{S_j}}{\sigma\sqrt{n}} \geq
  \epsilon} = 0
\end{align*}
\end{lem}
\begin{proof}
The idea of the proof is to leverage the Central Limit Theorem and
Gaussian tail bounds to control behavior at the right endpoint of the
interval under consideration.  Then independence of increments and
finite variance can be used to control the behavior over the entire
interval.

The sequence of random variables $\frac{1}{\sigma
  \sqrt{\floor{n\delta}+1}}S_{\floor{n\delta}+1}$ is a subsequence of
$\frac{1}{\sigma\sqrt{n}}S_n$ and therefore converges in distribution
to $N(0,1)$ by the Central Limit Theorem.  Furthermore, $\lim_{n \to
  \infty} \frac{\sqrt{\floor{n\delta}+1}}{\sqrt{n\delta}} = 1$ so by
Slutsky's Lemma we also have $\frac{1}{\sigma\sqrt{n\delta}}
S_{\floor{n\delta}+1} \todist Z$ where $Z$ is an $N(0,1)$ Gaussian
random variable.  By the Portmanteau Theorem (Theorem
\ref{PortmanteauTheorem}) and
a Markov bound (Lemma \ref{MarkovInequality}) we have
\begin{align*}
\limsup_{n \to \infty}
\probability{\abs{\frac{1}{\sigma\sqrt{n\delta}}S_{\floor{n\delta}+1}}
  \geq \lambda} &\leq \probability{\abs{Z}  \geq \lambda} \leq \frac{\expectation{\abs{Z}^3}}{\lambda^3}
\end{align*}

We want to leverage this bound to create a maximal inequality that controls the entire interval of
values of the rescaled random walk the approach being to leverage the
fact that either the final point is in the tail (in which
case the Central Limit Theorem bound just proven applies) or the final
point is outside the tail and some interior point is in the tail
providing us with an amount of variation whose probability can be
controlled by use of a second moment bound.  With $\epsilon > 0$ fixed as in
the hypothesis of the Lemma, define the random variable $\tau
= \min \lbrace j \geq 1 \mid \abs{\frac{S_j}{\sigma\sqrt{n}} } >
\epsilon\rbrace$ (this is a stopping time though we make no use of the
concept here).  Pick $\delta > 0$ satisfying $0 < \delta <
\epsilon^2/2$.

\begin{align*}
&\probability{\max_{1 \leq j \leq \floor{n\delta} + 1} \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon
  } \\
&=\probability{\max_{1 \leq j \leq \floor{n\delta} + 1}
  \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon; 
\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}  \geq \epsilon -
\sqrt{2\delta}} \\
&+ \probability{\max_{1 \leq j \leq \floor{n\delta} + 1}
  \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon; 
\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}} < 
  \epsilon - \sqrt{2\delta}} \\
&\leq \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\sum_{j=1}^{\floor{n\delta}}
\probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}} <
  \epsilon - \sqrt{2\delta}; \tau = j} \\
&= \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\sum_{j=1}^{\floor{n\delta}}
\probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}} -
    \frac{S_j}{\sigma \sqrt{n}}} > \sqrt{2\delta}; \tau = j} \\
&\leq \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\frac{1}{2\delta}\sum_{j=1}^{\floor{n\delta}}
\expectation{\left(
    \frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}} - \frac{S_j}{\sigma
      \sqrt{n}} \right)^2 \characteristic{\tau = j}} \\
&= \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\frac{1}{2\delta}\sum_{j=1}^{\floor{n\delta}}
\expectation{\left( \sum_{i=j+1}^{\floor{n\delta}+1}
    \frac{\xi_i}{\sigma\sqrt{n}} \right)^2 } \probability{\tau = j} \\
&= \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\frac{\floor{n\delta}}{2 n \delta}\sum_{j=1}^{\floor{n\delta}}
\probability{\tau = j} \\
&= \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\frac{1}{2}\probability{\max_{1 \leq j \leq \floor{n\delta} + 1} \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon}
\end{align*}
Therefore we have shown that 
\begin{align*}
\probability{\max_{1 \leq j \leq \floor{n\delta} + 1} \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon}
&\leq 2 \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} 
\end{align*}
and we can use our tail bound derived from the Central Limit Theorem
(with $\lambda = \frac{\epsilon - \sqrt{2\delta}}{\sqrt{\delta}}$)
to see that 
\begin{align*}
\lim_{\delta \to 0} \limsup_{n \to \infty} \frac{1}{\delta} \probability{\max_{1 \leq j \leq \floor{n\delta} + 1} \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon}
&\leq \lim_{\delta \to 0 } \frac{2}{\delta} \expectation{\abs{Z}^3}
\left(\frac {\sqrt{\delta}}{\epsilon - \sqrt{2\delta}}\right)^3 = 0
\end{align*}
\end{proof}

The next step is to extend the estimate that provides equicontinuity
at $0$ to prove equicontinuity of the random walk on all finite intervals.  
\begin{lem}\label{RandomWalkEquicontinuity}
Let $\xi_n$ be i.i.d. with mean $0$ and finite variance
  $\sigma^2$ and define $S_n = \sum_{k=1}^n \xi_k$.  Then for all
  $\epsilon > 0$ and $T > 0$ 
\begin{align*}
\lim_{\delta \to 0} \limsup_{n \to \infty} 
\probability{\max_{
\substack{1 \leq j \leq \floor{n\delta}+1 \\
0 \leq k \leq \floor{nT}+1}} \frac{\abs{S_{j+k}
    - S_k}}{\sigma\sqrt{n}} \geq
  \epsilon} = 0
\end{align*}
\end{lem}
\begin{proof}
Pick $0 \leq \delta \leq T$ and let $m \geq 2$ be the integer such that
$T/m < \delta \leq T/(m-1)$.  Since
\begin{align*}
\lim_{n \to \infty} \frac{\floor{nT}+1}{\floor{n\delta}+1} &=
\frac{T}{\delta} < m
\end{align*}
we know that for sufficiently large $n$ we have $\floor{nT}+1  <
(\floor{n\delta}+1)m$.  For any such $n$, suppose $\frac{\abs{S_{j+k}
    - S_k}}{\sigma\sqrt{n}} > \epsilon$ for some $k$ with $0 \leq k
\leq \floor{nT}+1$ and some $j$ with $0 \leq j \leq
\floor{n\delta}+1$.  Now let $p$ be the integer such that $0 \leq p
\leq m -1$ and 
\begin{align*}
(\floor{n\delta}+1)p \leq k < (\floor{n\delta}+1)(p+1)
\end{align*}
Since $0 \leq j \leq \floor{n\delta}+1$ either 
\begin{align*}
(\floor{n\delta}+1)p \leq k+j < (\floor{n\delta}+1)(p+1)
\end{align*}
or
\begin{align*}
(\floor{n\delta}+1)(p+1) \leq k+j < (\floor{n\delta}+1)(p+2)
\end{align*}
In the first case by the triangle inequality we have
\begin{align*}
\abs{S_{j+k} - S_k} \leq \abs{S_{k} - S_{(\floor{n\delta}+1)p}}+ \abs{S_{j+k} - S_{(\floor{n\delta}+1)p}}
\end{align*}
and therefore we know that either $\frac{\abs{S_{k}
    - S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq \epsilon/2 > \epsilon/3$ or $\frac{\abs{S_{k+j}
    - S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq \epsilon/2 > \epsilon/3$.  In
the second case by the triangle inequality we have
\begin{align*}
\abs{S_{j+k} - S_k} \leq \abs{S_{k} - S_{(\floor{n\delta}+1)p}}+ \abs{S_{(\floor{n\delta}+1)(p+1)} - S_{(\floor{n\delta}+1)p}}+ \abs{S_{j+k} - S_{(\floor{n\delta}+1)(p+1)}}
\end{align*}
and therefore we know that either  
$\frac{\abs{S_{k} - S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq
\epsilon/3$,  
$\frac{\abs{S_{(\floor{n\delta}+1)(p+1)} -
    S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq \epsilon/3$ or 
$\frac{\abs{S_{k+j} - S_{(\floor{n\delta}+1)(p+1)}}}{\sigma\sqrt{n}}
\geq \epsilon/3$.  Therefore we have the inclusion of events
\begin{align*}
\left \lbrace\max_{
\substack{1 \leq j \leq \floor{n\delta}+1 \\
0 \leq k \leq \floor{nT}+1}} \frac{\abs{S_{j+k}
    - S_k}}{\sigma\sqrt{n}} \geq
  \epsilon\right \rbrace 
&\subset 
\bigcup_{p=0}^{m} \left \lbrace \max_{1 \leq j \leq \floor{n\delta}+1}
\frac{\abs{S_{j + (\floor{n\delta}+1)p} - S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq
\epsilon/3 \right \rbrace
\end{align*}
By the i.i.d. nature of $\xi_n$ and the fact that $S_0 = 0$ we know that 
\begin{align*}
\probability{\max_{1 \leq j \leq \floor{n\delta}+1}
\frac{\abs{S_{j + (\floor{n\delta}+1)p} - S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq
\epsilon/3 } &= \probability{\max_{1 \leq j \leq \floor{n\delta}+1}
\frac{\abs{S_{j}}}{\sigma\sqrt{n}} \geq \epsilon/3 }
\end{align*}
and therefore 
\begin{align*}
\probability{\max_{
\substack{1 \leq j \leq \floor{n\delta}+1 \\
0 \leq k \leq \floor{nT}+1}} \frac{\abs{S_{j+k}
    - S_k}}{\sigma\sqrt{n}}} \leq (m+1) \probability{\max_{1 \leq j \leq \floor{n\delta}+1}
\frac{\abs{S_{j}}}{\sigma\sqrt{n}} \geq \epsilon/3 }
\end{align*}
Since $\lim_{\delta \to 0} (m+1)\delta < \lim_{\delta \to 0} (T/\delta
+ 2) \delta = T < \infty$ we can apply Lemma \ref{RandomWalkEquicontinuityAt0} to get the result.
\end{proof}

By Prohorov's Theorem \ref{Prohorov} we know that a tight sequence of probability
measures on a separable metric space has a convergent subsequence.  What is often required is some
way of proving that a particular measure is indeed the limit of that
subsequence.  Recalling Lemma \ref{ProcessLawsAndFDDs} we know that
finite dimensional distributions characterize the laws of stochastic
processes which leads one to the following general procedure for
proving convergence of a sequence of processes.

TODO: Kallenberg (Chapter 16) has general results here for $C(T ; S)$ with $T$ a
$lcscH$-space and $S$ metric.  Of course there are also results for
spaces of discontinuous functions for use in proving convergence of
empirical distribution functions.  Kallenberg also has results for
point process/spaces of measures.

We are taking the point of view of Brownian motion and the linearly
interpolated random walk as being a random element in $C([0,\infty) ;
\reals)$.  On the other hand we have thus far treated a stochastic
process as a random element in a subset of a path space $(S^T,
\mathcal{S}^{\otimes T})$ \emph{equipped with the product
  $\sigma$-algebra}.  It is tempting to gloss over this
point, however to tie in the general definition of stochastic
processes with the random elements of $C([0,\infty); \reals)$ we are
dealing with it is
important to understand the relationship between the Borel
$\sigma$-algebra on $C([0,\infty); \reals)$ and the product
$\sigma$-algebra $\mathcal{B}(\reals)^{\otimes [0,\infty)}$ used in the definition of
processes.
\begin{lem}\label{BorelGeneratedByProjections}For every $t \in [0,\infty)$ let $\pi_t : C([0,\infty); \reals) \to
  \reals$ be the evaluation map $\pi_t(f) = f(t)$.  The Borel $\sigma$-algebra on $C([0,\infty); \reals)$ is
  equal to $\sigma(\lbrace \pi_t \mid t \in [0,\infty) \rbrace)$ and
  therefore $\mathcal{B}(C([0,\infty); \reals)) = C([0,\infty);
  \reals) \cap \mathcal{B}(\reals)^{\otimes [0,\infty)}$.
\end{lem}
\begin{proof}Since each $\pi_t$ is a continuous function, it is Borel
  measurable and therefore the Borel $\sigma$-algebra contains
  $\sigma(\lbrace \pi_t \mid t \in [0,\infty) \rbrace)$.

TODO: The following proof looks incorrect; I am using the supremum on $[0,\infty)$ which is not the metric on $C([0,\infty); \reals)$.  What follows is valid on $C([0,T];\reals)$ but not on $C([0,\infty); \reals)$.

On the other hand, we know that $C([0,\infty) ; \reals)$ is separable
so we may pick a countable dense set $f_1, f_2, \dotsc$.
If we let $U \subset C([0,\infty) ; \reals)$ be open then for every
$f_j \in U$ there exists $r_j > 0$ such that $B(f_j, r_j) \subset U$
and $U$ is the union of such $B(f_j, r_j)$ (indeed, any $y \in U$ not in the
union of balls can't be the limit of the $f_j$ that are in $U$; on the
other hand it can't be the limit of the $f_j$ that are in $U^c$ since
the latter set is closed; thus the existence of such a $y$ would
contradict the density of $f_1, f_2, \dotsc$).  To show $U \in
\sigma(\lbrace \pi_t \mid t \in [0,\infty) \rbrace)$
it suffices to show that $B(f, r) \in \sigma(\lbrace \pi_t \mid t \in
[0,\infty) \rbrace)$ for every $f \in C([0,\infty) ; \reals)$ and $r > 0$.

Let $B(f, r)$ be given and note that by continuity of the
elements of $C([0,\infty) ; \reals)$ the closed ball
\begin{align*}
\overline{B(f, r)} &= \lbrace g \mid \sup_{x \in [0,\infty)} \abs{f(x)
  - g(x)} \leq r \rbrace \\
&= \lbrace g \mid \sup_{\substack{x \in
    [0,\infty) \\ x \in \rationals}} \abs{f(x)
  - g(x)} \leq r \rbrace \\
&= \cap_{\substack{x \in
    [0,\infty) \\ x \in \rationals}} \pi_x^{-1} ([f(x)-r,f(x)+r]) 
\end{align*}
which shows that $\overline{B(f, r)} \in \sigma(\lbrace \pi_t \mid t \in
[0,\infty) \rbrace)$ and $B(f, r) = \cap_{n=1}^\infty \overline{B(f, r+1/n)}$
which shows that $B(f, r) \in \sigma(\lbrace \pi_t \mid t \in
[0,\infty) \rbrace)$.
\end{proof}

\begin{thm}\label{ConvergenceInDistributionOfContinuousAsTightnessAndFDDs}Let $X_n$ be a tight sequence of continuous processes such
  that for all $d > 0$ and $0 \leq t_1 < \dotsb < t_d < \infty$ the
  sequence $(X_{n, t_1}, \dotsc , X_{n, t_d})$ converges in
  distribution, then the laws $X_n$ converge to a Borel probability
  distribution $\mu$ on $C([0,\infty); \reals)$ for which the
  canonical process $W_t(\omega) = \omega(t)$ satisfies
\begin{align*}
(X_{n, t_1}, \dotsc , X_{n, t_d}) \todist (W_{t_1}, \dotsc, W_{t_d})
\end{align*}
\end{thm}
\begin{proof}
By tightness and Prohorov's Theorem \ref{Prohorov} we know that $X_n$
has a weakly convergent subsequence.  Our first claim is that any two weakly convergent subsequences of
$X_n$ have the same limiting distribution.  Let $\check{X}_n$ and $\hat{X}_n$ be two
such subsequences and suppose that $\pushforward{\check{X}_n}{P} \to
\check{\mu}$ and $\pushforward{\hat{X}_n}{P} \to\hat{\mu}$
respectively.  Fix $0 \leq t_1 < \dotsb < t_d < \infty$ and note that
by the Continuous Mapping Theorem \ref{ContinuousMappingTheorem} we
know that $\pushforward{(\check{X}_{n,t_1}, \dotsc,
  \check{X}_{n,t_d})}{P} \todist \pushforward{(\pi_{t_1}, \dotsc,
  \pi_{t_d})}{\check{\mu}}$ and $\pushforward{(\hat{X}_{n,t_1}, \dotsc,
  \hat{X}_{n,t_d})}{P} \todist \pushforward{(\pi_{t_1}, \dotsc,
  \pi_{t_d})}{\hat{\mu}}$.  By hypothesis we conclude that $\pushforward{(\pi_{t_1}, \dotsc,
  \pi_{t_d})}{\check{\mu}} = \pushforward{(\pi_{t_1}, \dotsc,
  \pi_{t_d})}{\hat{\mu}}$ and therefore by
Lemma \ref{BorelGeneratedByProjections} we can apply Lemma
\ref{ProcessLawsAndFDDs} to conclude $\check{\mu}=\hat{\mu}$ which we
now refer to as $\mu$.

Now suppose that the distributions of $X_n$ do not converge weakly to
$\mu$.  Then there exists a bounded continuous $f$ such that either
$\lim_{n \to \infty} \expectation{f(X_n)}$ does not exist or exists
and is different from $\int f \, d\mu$.  In either case by the
boundedness of $f$ we know that 
\begin{align*}
-\infty &< -\norm{f}_\infty \leq \liminf_{n \to \infty}
\expectation{f(X_n)} \leq \limsup_{n \to \infty} \expectation{f(X_n)}
\leq \norm{f}_\infty  < \infty
\end{align*}
and we can extract
a subsequence $\check{X}_n$ such that $\lim_{n \to \infty}
\expectation{f(\check{X}_n)}$ exists and $\lim_{n \to \infty}
\expectation{f(\check{X}_n)} \neq \int f \, d\mu$.  This is a
contradiction since by tightness we know that $\check{X}_n$ has a weakly
convergent subsequence and we have already just shown that the limiting
distribution is $\mu$.
\end{proof}

The power of this Theorem is that it is often not too difficult to
prove weak convergence of finite dimensional distributions because we
have the power of a rich theory available (e.g. the Central Limit
Theorem, Slutsky's Theorem, characteristic functions).

\begin{lem}\label{ConvergenceOfRandomWalkFDD}Let $\xi_n$ be i.i.d. with mean $0$ and finite variance
  $\sigma^2$, define $S_n = \sum_{k=1}^n \xi_k$, $S_n^*(t) =
  S_{\floor{t}} + (t - \floor{t})\xi_{\floor{t}+1}$ and
  $X_n(t) = \frac{1}{\sigma \sqrt{n}} S_n^*(nt)$ where the latter are
  interpreted as random elements of the Borel measurable space
  $C([0,\infty);\reals)$.  For every $d > 0$ and real numbers $0 \leq t_1 < \cdots < t_d
  < \infty$ we have 
\begin{align*}
(X_n(t_1), \dotsc, X_n(t_d)) \todist (B_{t_1}, \dotsc, B_{t_d})
\end{align*}
where $B_t$ is a standard Brownian motion.
\end{lem}
\begin{proof}
Let $0 \leq t_1 < \dotsb < t_n < \infty$ be given.  The basic point is
that the result follows by the Central Limit Theorem; however due to
the linear interpolation there is a bit of extra work to do.

First note that by definition 
\begin{align*}
\abs{X_n(t) - \frac{1}{\sigma \sqrt{n}} S_{\floor{nt}} }
&\leq  \frac{1}{\sigma \sqrt{n}}\abs{\xi_{\floor{nt}+1}}
\end{align*}
so by a Chebyshev bound (Lemma \ref{ChebInequality}) we have
\begin{align*}
\lim_{n \to \infty} \probability{\abs{X_n(t) - \frac{1}{\sigma
      \sqrt{n}} S_{\floor{nt}} } > \epsilon}
&\leq  \lim_{n \to \infty} \frac{1}{n\epsilon^2} = 0
\end{align*}
thus $X_n(t)  \toprob \frac{1}{\sigma\sqrt{n}} S_{\floor{nt}}$ and by
Lemma \ref{ConvergenceInProbabilityInProductSpaces}
we have $(X_n(t_1), \dotsc, X_n(t_d)) \toprob (\frac{1}{\sigma\sqrt{n}}
S_{\floor{nt_1}}, \dotsc, \frac{1}{\sigma\sqrt{n}}
S_{\floor{nt_d}})$.  Our result will follow by Slutsky's Theorem
\ref{Slutsky} if we can show that
\begin{align*}
(\frac{1}{\sigma\sqrt{n}}
S_{\floor{nt_1}}, \dotsc, \frac{1}{\sigma\sqrt{n}}S_{\floor{nt_d}}) \todist (B_{t_1}, \dotsc, B_{t_d})
\end{align*}
Application of the Continuous Mapping Theorem
\ref{ContinuousMappingTheorem} lets us reduce further to showing that 
\begin{align*}
(\frac{1}{\sigma\sqrt{n}}(S_{\floor{nt_1}} -
S_{\floor{nt_0}}), \dotsc, \frac{1}{\sigma\sqrt{n}}
(S_{\floor{nt_d}}- S_{\floor{nt_{d-1}}}))\todist (B_{t_1} - B_{t_0}, \dotsc, B_{t_d} - B_{t_{d-1}})
\end{align*}
where for uniformity of notation we have defined $t_0 = 0$.  Since the
$\xi_n$ are independent this implies that the $S_{\floor{nt_j}}-
S_{\floor{nt_{j-1}}}$ are independent for $j=1, \dotsc, d$ and by
definition of independent increments property of Brownian motion we
know that $B_{t_j} - B_{t_{j-1}}$ are independent, thus by Lemma \ref{IndependenceProductMeasures} it suffices to
show that $\frac{1}{\sigma\sqrt{n}} (S_{\floor{nt_j}}-S_{\floor{nt_{j-1}}}) \todist N(0, t_j -
t_{j-1})$.  We shall prove this fact for an arbitrary $0 \leq s < t < \infty$.

By the definition of $S_n$ we write $\frac{1}{\sigma\sqrt{n}}
(S_{\floor{nt}}-S_{\floor{ns}}) =
\frac{1}{\sigma\sqrt{n}}\sum_{i=\floor{ns}+1}^{\floor{nt}}
\xi_i$.  For every $\epsilon > 0$ we have by another Chebyshev bound 
\begin{align*}
&\lim_{n \to \infty} \probability{\abs{\frac{1}{\sigma\sqrt{n}}
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i
- \frac{\sqrt{t-s}}{\sigma\sqrt{\floor{nt}-\floor{ns}}}
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i } > \epsilon} \\
&\leq \lim_{n \to \infty}\frac{1}{\epsilon^2}\variance{\left(\frac{1}{\sigma\sqrt{n}}
-\frac{\sqrt{t-s}}{\sigma\sqrt{\floor{nt}-\floor{ns}}} \right)
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i} \\
&= \lim_{n \to \infty}\frac{1}{\epsilon^2}\left(\frac{1}{\sigma\sqrt{n}}
-\frac{\sqrt{t-s}}{\sigma\sqrt{\floor{nt}-\floor{ns}}} \right)^2
(\floor{nt}-\floor{ns}) \sigma^2\\
&= \lim_{n \to \infty} \frac{1}{\epsilon^2} \left(
  \frac{\sqrt{\floor{nt}-\floor{ns}}}{\sqrt{n}} -
  \sqrt{t-s}\right)^2 = 0
\end{align*}
Therefore we have $\frac{1}{\sigma\sqrt{n}}
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i \toprob \frac{\sqrt{t-s}}{\sigma\sqrt{\floor{nt}-\floor{ns}}}
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i $ and one last appeal to
Slutsky's Theorem \ref{Slutsky} implies that it suffices to show
\begin{align*}
\frac{\sqrt{t-s}}{\sigma\sqrt{\floor{nt}-\floor{ns}}}
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i \todist N(0, t-s)
\end{align*}
which is just the Central Limit Theorem (and to be precise the
Continuous Mapping Theorem \ref{ContinuousMappingTheorem} to account for the multiplication by $\sqrt{t-s}$).
\end{proof}

The last step we make is in extending the equicontinuity of the random
walk to equicontinuity of the linearly interpolated random walk which
are honest elements of $C([0, \infty); \reals)$.  This equicontinuity
will prove tightness and weak convergence of the linearly interpolated
random walk.  One of the elements of proving the equicontinuity of the
linearly interpolated random walk is a general fact about the modulus
of continuity of a class of piecewise linear functions which we prove
as a separate lemma.

\begin{lem}\label{ModulusOfContinuityOfPL}Let $f(t)$ be a continuous function that is linear on every
  interval $[j,j+1]$ for $j=0, 1, \dotsc$.  For every integer $M
  > 0$ and $N > 0$, we have
\begin{align*}
\sup_{\substack{\abs{s -t} \leq M \\ 0 \leq s,t
    \leq N} } \abs{f(s) - f(t)}
&\leq
\sup_{\substack{1 \leq j \leq M \\ 0 \leq k
    \leq N}} \abs{f(k+j) - f(k)}
\end{align*}
\end{lem}
\begin{proof}
Pick $0 \leq s<t \leq M$.  If there exists $j < N$ such that $j \leq s
< t \leq j+1$ then it is clear from linearity  that $\abs{f(s) - f(t)}
\leq \abs{f(j) - f(j+1)}$ so it suffices to consider the case in which 
$j \leq s < j +1 < \dotsb < j+k < t \leq j+k+1$ for some $j \geq 0$ and $k
> 0$.  If we let $f(t)$ has slope $a_j$ on the interval $[j,j+1]$
then we can write $f(t) - f(s) = a_{j}(j+1 -s) + \dotsb + a_{j+k}(t -
j -k)$.  Note that
if $f(t) - f(s)$ has a different sign  than $a_j$ then $\abs{f(t) -
  f(s)} \leq \abs{f(t) -  f(j+1)}$ and similarly with $a_{j+k}$ so if
suffices to assume that $a_j$ and $a_{j+k}$ have the same sign as
$f(t)-f(s)$.  Now if $\abs{a_j} \leq \abs{a_{j+k}}$ then we slide the
pair $(s,t)$ to the right until either $s$ or $t$ hits an integer.
More formally if $j+1 - s \leq j+k+1 -t$ then we get
$\abs{f(t) -
  f(s)} \leq \abs{f(t + j+1-s) - f(j+1)}$ and if $j+k+1 -t \leq j+1 -
  s$
we get the bound $\abs{f(t) -
  f(s)} \leq \abs{f(j+k+1) - f(s + j+k+1 -t)}$.  If we $\abs{a_j} \geq
\abs{a_{j+k}}$ we slide to the left in an analogous way.  The point is
that we are reduced to the case in which either $s=j-1$ or $t=j+k+1$.

Once we know that either $s=j-1$ or $t=j+k+1$ , because $M$ is integer
we know that in fact $k \leq M$ and therefore we get a final bound
$\abs{f(t) - f(s)} \leq \abs{f(j+k+1) - f(j-1)}$ which proves the
result.

TODO: This proof is grotesque.  Try to do better!
\end{proof}

We are finally ready to put all of the pieces together to prove
Donsker's Theorem on the convergence of random walks to Brownian
motion.  Note that we have not used the existence of Brownian motion
anywhere in the proof so this Theorem is among other things an
existence proof for Brownian motion.
\begin{thm}[Donsker's Invariance Principle for Random Walks]\label{Donsker2}Let $\xi_n$ be i.i.d. with mean $0$ and finite variance
  $\sigma^2$, define $S_n = \sum_{k=1}^n \xi_k$, $S_n^*(t) =
  S_{\floor{t}} + (t - \floor{t})\xi_{\floor{t}+1}$ and
  $X_n(t) = \frac{1}{\sigma \sqrt{n}} S_n^*(nt)$ where the latter are
  interpreted as random elements of the Borel measurable space
  $C([0,\infty);\reals)$.  
Then the law of $X_n$  converges weakly to a probability measure under
which the coordinate mapping $(f,t) \to f(t)$ is a standard Brownian motion.
\end{thm}
\begin{proof}
Lemma \ref{ConvergenceOfRandomWalkFDD} shows that finite dimensional
distributions of the linearly interpolated and rescaled random walk
converge to the finite dimensional distributions of Brownian motion.
Therefore by Theorem
\ref{ConvergenceInDistributionOfContinuousAsTightnessAndFDDs} it
remains to show that $X_n$ is a tight sequence of processes.
By Lemma \ref{TightnessOfContinuousFunctions} we must show for all $X_n(t)$,
\begin{itemize}
\item[(i)]$\lim_{\lambda \to \infty} \sup_{n \geq 1} \probability{\abs{X_n(0)}
  \geq \lambda}= 0$.
\item[(ii)] $\lim_{\delta \to 0} \sup_{n \geq 1} \probability{m(T, X_n,
  \delta) \geq \lambda} = 0$ for all $\lambda > 0$ and $T > 0$.
\end{itemize}
Since $X_n(0) = 0$ the condition (i) holds trivially.  As for
condition (ii) we first argue that it suffices to show $\lim_{\delta \to 0} \limsup_{n \geq 1} \probability{m(T, X_n,
  \delta) \geq \lambda}= 0$.  This follows from the fact that for
fixed $n > 0$,
$\lim_{\delta \to 0} \probability{m(T, X_n,  \delta) \geq \lambda} =
0$ (continuity of $X_n$) and $\probability{m(T, X_n,  \delta) \geq \lambda}$ is a decreasing
function of $\delta$.  Indeed, if we let $\epsilon > 0$ be given pick
$\Delta > 0$ such that $\limsup_{n \geq 1} \probability{m(T, X_n,
  \delta) \geq \lambda} < \epsilon$ for all $\delta \leq \Delta$.  Then pick $N > 0$ is such that $\sup_{n \geq N} \probability{m(T, X_n,
  \Delta) \geq \lambda} < \epsilon$ and note that because $\probability{m(T, X_n,
  \delta) \geq \lambda}$ is decreasing in fact we have $\sup_{n \geq N} \probability{m(T, X_n,
  \delta) \geq \lambda} < \epsilon$ for all $\delta \leq \Delta$.
Since $\lim_{\delta \to 0} \probability{m(T, X_n,
  \delta) \geq \lambda} = 0 $ for every $n>0$ we can find
$\hat{\Delta} < \Delta$ such that $\probability{m(T, X_n,
  \delta) \geq \lambda}  < \epsilon$ for all $n=1, \dotsc, N-1$ and
$\delta \leq \hat{\Delta}$ and
thus $\sup_{n \geq 1} \probability{m(T, X_n,  \Delta) \geq \lambda} < \epsilon$ for all $\delta < \hat{\Delta}$.

With this reduction in hand, we can estimate
\begin{align*}
\probability{m(T, X_n,  \delta) \geq \lambda} &=
\probability{\sup_{\substack{\abs{s -t} \leq \delta \\ 0 \leq s,t
    \leq T}} \abs{X_n(s) - X_n(t)} \geq \lambda} \\
&\leq
\probability{\sup_{\substack{\abs{s -t} \leq \floor{n \delta} + 1 \\ 0 \leq s,t
    \leq \floor{T \delta}+1}} \abs{S^*_n(s) - S^*_n(t)} \geq \sigma
\sqrt{n} \lambda} \\
&\leq 
\probability{\sup_{\substack{1 \leq j \leq \floor{n \delta} + 1 \\ 0 \leq k
    \leq \floor{T \delta}+1}} \abs{S_n(k+j) - S_n(k)} \geq \sigma
\sqrt{n} \lambda} 
\end{align*}
where the last inequality follows Lemma \ref
{ModulusOfContinuityOfPL}.  Now we can apply Lemma
\ref{RandomWalkEquicontinuity} to conclude $\lim_{\delta \to
  0}\limsup_{n \to \infty} \probability{m(T, X_n,  \delta) \geq
  \lambda}$ and tightness is shown.
\end{proof}

\section{Banach Spaces}

We start with some of the standard examples of Banach spaces.

\begin{defn}Let $(\Omega, \mathcal{A})$ be a measurable space, let $B(\Omega)$ be space of bounded measurable functions $f : \Omega \to \reals$ and define $\norm{f}_\infty = \sup_{\omega \in \Omega} \abs{f(\omega)}$.
\end{defn}

\begin{prop}The space $B(\Omega)$ is a Banach space.
\end{prop}
\begin{proof}
It is elementary that $B(\Omega)$ is a vector space : if $f$ is bounded and measurable with $\norm{f}_\infty$ then clearly $af$ is bounded measurable with bound $\abs{a}\norm{f}_\infty$ for all $a \in \reals$ and if $f$ and $g$ are bounded and measurable with bounds $M$ and $N$ respectively then $f+g$ is bounded and measurable with bound $M+N$.

To see that $\norm{\cdot}_\infty$ is in fact a norm, first note that it is immediate from the definition that $\norm{f}_\infty \geq 0$ and $\norm{f}_\infty$ if and only if $f = 0$.  Let $f \in B(\Omega)$ and let $a \in \reals$ with $a \neq 0$.  Then for every $\epsilon>0$ we may find $\omega \in \Omega$ such that $\norm{f}_\infty - \epsilon/\abs{a} \leq \abs{f(\omega)}$.  It follows that $\abs{a} \norm{f}_\infty - \epsilon \leq \abs{a f (\omega)} \leq \norm{af}_\infty$.  Since $\epsilon >0$  was arbitrary we may take $\epsilon \downarrow 0$ to conclude $\abs{a} \norm{f}_\infty \leq \norm{af}_\infty$.  In a similar way we may find an $\omega \in \Omega$ such that 
\begin{align*}
\norm{af}_\infty - \epsilon &\leq \abs{af(\omega)} = \abs{a}\abs{f(\omega)} \leq \abs{a} \norm{f}_\infty
\end{align*}
and since $\epsilon > 0$ was arbitrary we conclude the opposite inequality $\norm{af}_\infty \leq \abs{a} \norm{f}_\infty$.  To see the triangle inequality let $f, g \in B(\Omega)$ be given.  For every $\omega \in \Omega$ we have $\abs{(f+g)(\omega)} \leq \abs{f(\omega)}+\abs{g(\omega)} \leq \norm{f}_\infty + \norm{g}_\infty$.  Now take the supremum over all $\omega$ to conclude $\norm{f+g}_\infty \leq \norm{f}_\infty + \norm{g}_\infty$.

Lastly we have to show that $B(\Omega)$ is complete.  Let $f_n$ be a Cauchy sequence in $B(\Omega)$.  Thus for every $\epsilon > 0$ there exists and $n \in \naturals$ such that for all $m \geq n$ we have $\norm{f_n - f_m} < \epsilon$.  In particular, for every $\omega \in \Omega$ we have $\abs{f_n(\omega) - f_m(\omega)} \leq \norm{f_n - f_m} < \epsilon$ so that $f_n(\omega)$ is a Cauchy sequence in $\reals$ for every $\omega \in \Omega$.  By completeness of $\reals$ we know that $f_n$ converges pointwise and by Lemma \ref{LimitsOfMeasurable} we know that there is a measurable function $f : \Omega \to \reals$ such that $f_n \to f$.  

Now note that for any $\epsilon > 0$ we can pick $N \in \naturals$ such that for $m,n \geq N$ we have $\norm{f_n-f_m}_\infty < \epsilon$ so by using the triangle inequality and taking limits over $m \in \naturals$ we get for every $\omega \in \Omega$ and $n \geq N$,
\begin{align*}
\abs{f (\omega) -f_n(\omega)}_\infty &\leq \lim_{m \to \infty} \abs{f_m(\omega) -f_n(\omega)} + \lim_{m \to \infty} \abs{f(\omega) -f_m(\omega)} 
\leq \lim_{m \to \infty} \norm{f_m -f_n}_\infty \leq \epsilon
\end{align*}
Now using the fact with the triangle inequality we get $\abs{f(\omega)} \leq \abs{f(\omega) - f_n(\omega)} + \abs{f_n(\omega)} \leq \epsilon + \norm{f_n}_\infty$ which shows $f$ is bounded.  Taking the supremum over $\omega$ shows that $\norm{f -f_n} \leq \epsilon$ for all $n \geq N$ which shows that $f_n$ converges to $f$ in $B(\Omega)$.
\end{proof}

\begin{defn}Let $(\Omega, \mathcal{A}, \mu)$ be a measure space and let $f : \Omega \to \reals$ be a measurable function.  We say that $a \in \reals$ is an \emph{essential upper bound of $f$} if $\mu(f^{-1}(a, \infty)) = 0$ (i.e. $f(x) \leq a \mu$-almost everywhere).  The \emph{essential supremum of $f$} is defined as the infimum of the set of essential upper bounds of $f$: 
\begin{align*}
\esssup f &= \inf \lbrace a \in \reals \mid \mu(f^{-1}(a,\infty)) = 0 \rbrace
\end{align*}
A measurable function $f$ is said to be \emph{essentially bounded} if $\esssup \abs{f} < \infty$.
\end{defn}

The set of essentially bounded functions on a measure space can be made into a Banach space under the essential supremum norm.  As is standard we have to pass to equivalence classes to do this so we first note when a function has an essential supremum of $0$.
\begin{prop}Let $(\Omega, \mathcal{A}, \mu)$ be a measure space and let $f : \Omega \to \reals$ be a measurable function then $\esssup \abs{f} = 0$ if and only if $f = 0$ $\mu$-almost everywhere.
\end{prop}
\begin{proof}
Clearly if $f$ is almost everywhere $0$ then $\abs{f}^{-1}(0, \infty) = \lbrace f \neq 0 \rbrace$ hence by monotonicity of measure, $\mu(\abs{f}^{-1}(0,\infty)) = 0$.  Thus $\esssup \abs{f} \leq 0$.  On the other hand for every $a < 0$ we know that $\abs{f}^{-1}(a,\infty) \supset \lbrace f = 0 \rbrace$ and therefore $\mu(\abs{f}^{-1}(a,\infty)) = \mu(\Omega) > 0$ and therefore $\esssup \abs{f} \geq 0$.  

On the other hand if $f$ is not almost everywhere zero then by Fatou's Lemma $0 < \mu(\abs{f}^{-1}(0,\infty)) \leq \liminf_{n \to \infty} \mu(\abs{f}^{-1}(1/n, \infty))$ which implies that $\mu(\abs{f}^{-1}(1/n, \infty)) > 0$ for some $n \in \naturals$ and therefore $\esssup \abs{f} \geq 1/n > 0$.
\end{proof}

\begin{defn}Let $(\Omega, \mathcal{A}, \mu)$ be a measure space then $L^\infty(\Omega, \reals)$ is space of equivalence classes of essentially bounded measurable functions under the equivalence relation of $\mu$-almost everywhere equality.
\end{defn}

\begin{thm}$L^\infty(\Omega, \reals)$ with essential supremum as a norm is a Banach space.
\end{thm}
\begin{proof}
We have already seen that $\norm{f}_\infty = 0$ implies $f = 0$ in $L^\infty$.  Let $a \in \reals$ with $a \neq 0$ and $f \in L^\infty$ then we note that $b$ is an essential upper bound for $\abs{f}$ if and only if $\abs{a} b$ is an essential upper bound for $\abs{af}$.  This follows from the set identity that
\begin{align*}
\abs{f}^{-1}(b,\infty) &= \lbrace \omega \mid b < \abs{f(\omega)} \rbrace =  \lbrace \omega \mid \abs{a} b < \abs{af(\omega)} \rbrace = \abs{af}^{-1}(\abs{a}b, \infty)
\end{align*}
Thus we have 
\begin{align*}
\norm{af}_\infty = \inf \lbrace \abs{a} b \mid \mu(\abs{f}^{-1}(b,\infty)) = 0 \rbrace = \abs{a}\norm{f}_\infty
\end{align*}
If we are given $f, g \in L^\infty$ then if $a$ is an essential upper bound for $f$ and $b$ is an essential upper bound for $g$ then by the triangle inequality in $\reals$,  $\abs{f+g}^{-1}(a+b, \infty) \subset \abs{f}^{-1}(a,\infty) \cup \abs{g}^{-1}(b,\infty)$ and by subadditivity of $\mu$ we know that $\mu(\abs{f+g}^{-1}(a+b, \infty)) = 0$.  Thus $a+b$ is an essential upper bound of $\abs{f+g}$.  For any $\epsilon > 0$ we can find essential upper bounds $a$ and $b$ such that $a \leq \norm{f}_\infty + \epsilon/2$ and $b \leq \norm{g}_\infty + \epsilon/2$ and therefore $\norm{f+g}_\infty \leq a + b \leq \norm{f}_\infty + \norm{g}_\infty + \epsilon$.  As $\epsilon > 0$ was arbitrary the triangle inequality follows.

To see completeness, suppose that $f_n$ is a Cauchy sequence.  We choose representatives of the equivalence class that are everywhere bounded on $\Omega$; thus there exist constants $M_n$ such that $\sup \abs{f} \leq M_n$.  For each $\epsilon > 0$ there exists an $N \in \naturals$ such that $\esssup \abs{f_n - f_m} < \epsilon$ for all $n,m \geq N$.  For each such pair $n,m$ if follows that $\abs{f_n - f_m} < \epsilon$ except on a set of $\mu$ null set.  Taking the union of such null sets over all rational $\epsilon > 0$ and all pairs $n,m$ we can take a countable union of null sets and find a single null set $\mathcal{N}$ such that for every $\epsilon \in \rationals_+$ there exists an $N \in \naturals$ such that $\abs{f_n(\omega) - f_m(\omega)} < \epsilon$ for all $n,m \geq N$ and $\omega \notin \mathcal{N}$.   Without changing equivalence classes of or the uniform bounds on the $f_n$ we may redefine $f_n$ to be zero on $\mathcal{N}$ and then it follows that for each $\epsilon \in \rationals_+$ there exists an $N \in \integers$ such that $\abs{f_n(\omega) - f_m(\omega)} < \epsilon$ for all $n,m \geq N$ and all $\omega \in \Omega$.  Thus $f_n$ is pointwise Cauchy and by completeness of $\reals$ we may define $f$ as the pointwise limit of $f_n$ and we know that $f$ is a measurable function.  In fact the convergence is easily seen to be uniform.  For every $\epsilon \in \rationals_+$ there exists an $n \in \naturals$ such that $\abs{f_n(\omega) - f_m(\omega)} \leq \epsilon$ for all $m \geq n$ and $\omega \in \Omega$ and therefore using the triangle inequality and pointwise convergence to $f$ we have
\begin{align*}
\abs{f_n(\omega) - f(\omega)} &\leq \lim_{m \to \infty} (\abs{f_n(\omega) - f_m(\omega)} + \abs{f_m(\omega) - f(\omega)} ) \leq \epsilon
\end{align*} 
for all $\omega \in \Omega$.  In particular by chosing $\epsilon = 1$, by the triangle inequality we have $\abs{f(\omega)} \leq \abs{f_n(\omega)} + \abs{f_n(\omega) - f(\omega)} \leq M_n + 1$ for all $\omega \in \Omega$ so $f \in L^\infty$.  
\end{proof}

\begin{defn}Let $X$ be a topological space the $C_c(X)$ is the set of
  all continuous function $f : X \to \reals$ with compact support
  (i.e. $supp(f) = \overline{\{x \in X \mid f(x) \neq 0 \}}$ is
  compact).
\end{defn}

\begin{defn}Let $X$ be a topological space the $C_0(X)$ is the set of
  all continuous function $f : X \to \reals$ which vanish at infinity
  in the sense that for every $\lambda >0$ the set $\lbrace x \in X \mid
  \abs{f(x)} \geq \lambda \rbrace$ is  compact.
\end{defn}

\begin{prop}\label{BanachSpaceOfFunctionsVanishingAtInfinity}If $X$ is a topological space, then  for each $f \in C_0(X)$
  define $\norm{f} = \sup_{x \in X} \abs{f(x)}$ then $\norm{f}$ is a
  norm on $C_0(X)$ and $C_0(X)$ is a Banach space.  Furthermore
  $C_c(X)$ is dense in $C_0(X)$.
\end{prop}
\begin{proof}
TODO:
\end{proof}

Note that if $X$ is compact then every continuous function vanishes at infinity so it follows that the space of 
all continuous functions on $X$ is a Banach space under the uniform norm.  In the case that $X$ is locally compact
and Hausdorff but not necessarily compact we can embed $X$ in its one point compactification $\tilde{X}$.  It is useful to understand
the relationship between $C_0(X)$ and $C(\tilde{X})$; in fact the following justifies the use of the phrase ``vanishing at infinity''.
\begin{prop}\label{IsometricEmbeddingVanishingAtInfinityIntoCompact}Let $X$ be a locally compact Hausdorff space and let $\tilde{X}$ be the one point compactification of $X$.  Every $f \in C_0(X)$ extends
uniquely to an element $\tilde{f} \in C(\tilde{X})$ by defining $\tilde{f}(\Delta) = 0$.  This defines an isometry of $C_0(X)$ with the subspace $\lbrace g \in C(\tilde{X}) \mid g(\Delta) = 0 \rbrace$.  
\end{prop}
\begin{proof}
We first show that defining $\tilde{f}(\Delta) =0$ makes $\tilde{f} : \tilde{X} \to \reals$ continuous.  Suppose $U \subset \reals$ be open.  If $0 \notin U$ then
$\tilde{f}^{-1}(U) = f^{-1}(U) \subset U$ which is open $X$ by continuity of $f$; it is also open  in $\tilde{X}$ by the definition of the topology on $\tilde{X}$.  If $0 \in U$ then
there exists $\epsilon > 0$ such that $(-\epsilon, \epsilon) \subset U$ and it follows from the fact that $f$ vanishes at infinity that $K = \lbrace \abs{f(x)} \geq \epsilon \rbrace$ is
compact.  Since we can write $\tilde{f}^{-1}(U) = f^{-1}(U) \cup (X \setminus K) \cup \lbrace \Delta \rbrace$ which is open in $\tilde{X}$ we see that $\tilde{f}$ is continuous.

Since $\tilde{X}$ is Hausdorff it follows that the value of $\tilde{f}$ at $\Delta$ is determined by the restriction to $X$ and thus the uniqueness of the continuous extension $\tilde{f}$ is determined.  It is elementary that the assignment $f \mapsto \tilde{f}$ is linear and it is trivial that $\sup_{v \in X} \abs{f(v)} \leq \sup_{v \in \tilde{X}} \abs{\tilde{f}(v)}$.  On the other hand for arbitrary $\epsilon > 0$ we have 
\begin{align*}
\sup_{v \in \tilde{X}} \abs{\tilde{f}(v)} &\leq \sup_{\substack{v \in X \\ \abs{f(v)} \geq \epsilon}} \abs{f(v)} + \epsilon \leq \sup_{v \in X} \abs{f(v)} + \epsilon
\end{align*}
Since $\epsilon>0$ was arbitrary we let $\epsilon \to 0$ and see that we have an isometry; in particular the mapping is injective.

It remains to characterize the range.  For that suppose that $g \in C(\tilde{X})$ is such that $g(\Delta) = 0$, it suffices to show that restriction of $g$ to $X$ vanishes at infinity.
For every $\epsilon > 0$ by continuity of $g$ there exists an open neighborhood $V$ of $\Delta$ such that $g(V) \subset (-\epsilon, \epsilon)$.  By definition of the topology on $\tilde{X}$ we can write $V = U \cap (X \setminus K) \cup \lbrace \Delta \rbrace$ with $U \subset X$ open and $K \subset X$ compact.  In particular it follows that $\abs{g(v)} < \epsilon$ for all $v \in X \setminus K$ which is to say that $\lbrace v \in X \mid \abs{g(v)} \geq \epsilon \rbrace \subset K$.  As a closed subset of a compact set in a Hausdorff space it follows that $\lbrace v \in X \mid \abs{g(v)} \geq \epsilon \rbrace$ is compact and thus $g \mid_X$ vanishes at infinity.
\end{proof}

As an example of the utility of this result consider the following
\begin{cor}\label{VanishingAtInfinityLocallyCompactAttainsNormInfSup}Let $X$ be locally compact Hausdorff and suppose that $f \in C_0(X)$ then it follows that there exists $x_0 \in X$ such that $\abs{f(x_0)} = \norm{f}$.  
\end{cor}
\begin{proof}
If $f=0$ the result is trivial so assume that $f$ is non-zero.  Apply Proposition \ref{IsometricEmbeddingVanishingAtInfinityIntoCompact} to extend $f$ by zero to $\tilde{f}$.  We know that $\abs{\tilde{f}(x)}$ is a continuous positive function on $\tilde{X}$ such that $\abs{\tilde{f}(\Delta)}=0$.  Since $\tilde{X}$ is compact we know that it attains its 
supremum (TODO: Where do we show this???) so that there exists $x_0 \in \tilde{X}$ such that $\abs{\tilde{f}(x_0)} = \norm{\tilde{f}} = \norm{f}  > 0$; it follows that $x_0 \in X$.

To see that $f$ attains its supremum, first assume that $f(x) \geq 0$ for all $x \in S$, then $\sup_{x \in X} f(x) = \norm{f}$.  Now we apply the result for norm attainment to $f$.  For general $f$ we simply consider $f - \inf_{x \in S} f(x)$.  The result for infimums follow from that for supremums since there exists $x_0$ such that $-f(x_0) = \sup_{x \in X} -f(x) = - \inf_{x \in X} f(x)$.
\end{proof}

We will also have reason to use the following simple fact.
\begin{cor}\label{VanishingAtInfinityLocallyCompactSupInfContinuity}Let $X$ be locally compact Hausdorff then $f \mapsto \inf_{x \in X} f(x)$ and $f \mapsto \sup_{x \in X} f(x)$ are 
continuous.
\end{cor}
\begin{proof}
Since $\inf_{x \in X} f(x) = - \sup_{x \in X} (-f(x))$ it suffices to handle the case of $\sup_{x \in X} f(x)$.  Let $f \in C_0(X)$ and $\epsilon > 0$ be given.  By Corollary \ref{VanishingAtInfinityLocallyCompactAttainsNormInfSup} we see that there exists $x_0 \in X$ such that $f(x_0) = \sup_{x \in X} f(x)$ therefore if $\norm{g - f} < \epsilon$
we have 
\begin{align*}
\sup_{x \in X} g(x) &\geq g(x_0) \geq f(x_0) - \epsilon = \sup_{x \in X} f(x) - \epsilon
\end{align*}
and if we pick $x_1$ such that $g(x_1) = \sup_{x \in X} g(x)$ then
\begin{align*}
\sup_{x \in X} g(x) &= g(x_1) \leq f(x_1) + \epsilon \leq \sup_{x \in X} f(x)  + \epsilon
\end{align*}

TODO: Prefer the following argument using sequences?  In a metric space (or any first countable topological space) convergence is determined by sequences right?
Using Corollary \ref{VanishingAtInfinityLocallyCompactAttainsNormInfSup}  for each $n \in \naturals$ we select
$x_n \in S$ such that $f_n(x_n) = \inf_{x \in S} f_n(x)$ and select $x_0 \in S$ such that $f(x_0) = \inf_{x \in S} f(x)$.  From the definition of $x_n$ and $\lim_{n \to \infty} f_n = f$ we see that $\lim_{n \to \infty} f_n(x_n) \leq \lim_{n \to \infty} f_n(x_0) = f(x_0)$.  On the other hand for every $\epsilon > 0$
there exists $N$ such that $\sup_{x \in S} \abs{f_n(x) - f(x)} < \epsilon$ for $n \geq N$ and therefore $f(x_0) - \epsilon \leq f(x_n) - \epsilon \leq f_n(x_n)$ for $n \geq N$.  This implies $f(x_0) - \epsilon \leq \lim_{n \to \infty} f_n(x_n)$ and since $\epsilon >0$ was arbitrary we get $f(x_0) \leq \lim_{n \to \infty} f_n(x_n)$.
\end{proof}

\begin{cor}\label{VanishingAtInfinityLocallyCompactSeparable}Let $X$
  be locally compact separable Hausdorff then $C_0(X)$ is separable.
\end{cor}
\begin{proof}
We know that $C(\tilde{X})$ is separable (Lemma
\ref{SeparabilityOfBoundedUniformlyContinuous}) so we may take a
countable dense set $f_n$.  We claim that $f_n - f_n(\Delta)$ is dense
in $C_0(X)$.  To see this, note that for any $f \in C_0(X)$ we know
that $f_n \to f$ in $C(\tilde{X})$ along some subsequence $N$.  On the
other hand, $f \mapsto f - f(\Delta)$ is a continuous map from
$C(\tilde{X})$ to $C_0(X)$ and therefore $f_n - f_n(\Delta) \to f$
along $N$.
\end{proof}

Question: is there a compact non-Hausdorff space that is not locally compact?

\begin{thm}[Principle of Uniform Boundedness]\label{PrincipleOfUniformBoundedness}Let $X$ be a Banach space, $Y$ be a normed vector space and
  $A_\alpha : X \to Y$ be a family of bounded linear maps.  If for all
  $v \in X$ we have $\sup_\alpha \norm{A_\alpha v} <\infty$ then
  $\sup_\alpha \norm{A_\alpha} < \infty$.
\end{thm}
\begin{proof}
For each $n \in \naturals$ let 
\begin{align*}
V_n &= \lbrace x \in X \mid \sup_\alpha
\norm{A_\alpha x} > n \rbrace
= \cup_\alpha \lbrace x \in X \mid \norm{A_\alpha x} > n \rbrace
\end{align*}
 and note that $V_n$ is open.
Furthermore by our hypothesis we know that $\cap_{n=1}^\infty V_n =
\emptyset$.  If follows from the Baire Category Theorem
\ref{BaireCategoryTheorem} that some $V_n$ is not dense.  Thus there
exists an $n \in \naturals$, $x \in X$ and $r > 0$ such that
$\overline{B}(x,r) \cap V_n = \emptyset$; that is to say for all $y
\in X$ such that $\norm{x-y} \leq r$ we have $\sup_\alpha
\norm{A_\alpha y} \leq n$.  
Now let $u \in X$ with
$\norm{u} \leq 1$ and observe that by linearity for all $\alpha$ 
\begin{align*} 
\norm{A_\alpha u} &=
r^{-1} \norm{A_\alpha ru}  \leq r^{-1}( \norm{A_\alpha (x + ru) } +
                    \norm{A x}) \leq 2 r^{-1} n
\end{align*}
Therefore $\sup_\alpha \norm{A_\alpha} = \sup_\alpha \sup_{\norm{u}
  \leq 1} \norm{A_\alpha u}  \leq 2 r^{-1} n < \infty$.
\end{proof}

\section{Riesz Representation}

We saw in the Daniell-Stone Theorem \ref{DaniellStoneTheorem} that one
may recapture a part of integration theory by considering certain
linear functionals on a space of functions.  There is a analogue to
that result that applies in the case of measure on topological
spaces and allows one to bring the machinery of functional analysis to
bear on problems of measure theory.  

The Riesz representation theorem is actually a class of different
theorems with different hypotheses made about the measures involved
and the topology on the
underlying space.  Here we concentrate the reasonable general case of
Hausdorff locally compact spaces.  Other presentations may treat the
slightly simpler cases in which either second countability,
compactness or
$\sigma$-compactness are added as hypotheses on the topological space.  More general
presentations may drop the the assumption of local compactness and treat
arbitrary Hausdorff spaces.  

\begin{defn}A topological space $X$ is said to be \emph{locally
    compact} if every point in $X$ has a compact neighborhood
  (i.e. for every $x \in X$ there exists an open set $U$ and a compact
  set $K$ such that $x \in U \subset K$).
\end{defn}

\begin{lem}\label{LocallyCompactEquivalences}Let $X$ be a Hausdorff topological space then the following
  are equivalent
\begin{itemize}
\item[(i)]$X$ is locally compact
\item[(ii)]Every point in $X$ has an open neighborhood with compact closure
\item[(iii)]$X$ has a base of relatively compact neighborhoods
\end{itemize}
\end{lem}
\begin{proof}
(i) implies (ii):  If $X$ is Hausdorff then a closed subset of a compact set is compact
and therefore if $X$ is locally compact and $x \in X$ we take $U$ open
and $K$ compact such that $x \in U \subset K$ and then it follows that
$\overline{U}$ is compact hence (ii) follows.  

The fact that (ii) implies (i) is immediate.

(ii) implies (iii): For each $x \in X$ pick a relatively compact
neighborhood $U_x$, let $\mathcal{B}_x = \{ U \subset U_x \mid U
\in \mathcal{T} \}$ and let $\mathcal{B} = \cup_{x \in X}
\mathcal{B}_x$.  It is clear that $\mathcal{B}$ is a base for the
topology $\mathcal{T}$.  Moreover for each $U
\in \mathcal{B}$ there exists $x \in X$ such that $U \subset U_x$ with
$\overline{U}_x$ compact and then since $X$ is Hausdorff we know that
$\overline{U}$ is compact.

(iii) implies (ii) is immediate.
\end{proof}

A locally compact space can be embedded in a compact space by adding a single point.
\begin{defn}\label{OnePointCompactificationDefinition}Let $X$ be a locally compact Hausdorff space with topology $\tau$.  The \emph{one point compactification} of $X$ is the set
$X \cup \lbrace \Delta \rbrace$ where $\Delta \notin X$ and topology given by $\tau$ and sets of the form $X \setminus K \cup \lbrace \Delta \rbrace$
where $K \subset X$ is compact.  The point $\Delta$ is called the \emph{point at infinity}.
\end{defn}
The one point compactification is indeed compact.
\begin{thm}\label{OnePointCompactification}The one point compactification is a compact space.
\end{thm}
\begin{proof}
First we show that $\tilde{\tau} = \tau \cup \cup_{\substack{K \in \tau \\ K \text{ is compact}}} X \setminus K \cup \lbrace \Delta \rbrace$ is a topology.  It is clear that
$\emptyset \in \tilde{\tau}$ since $\emptyset \in \tau$ and moreover since $\emptyset$ is compact we see that $X \cup \lbrace \Delta \rbrace = (X \setminus \emptyset) \cup \lbrace \Delta\rbrace \in \tilde{\tau}$.  
Let $U_1, \dotsc, U_n$ be open and and $K_1, \dotsc, K_m$ be compact then if $n > 0$
\begin{align*}
&U_1 \cap \dotsb \cap U_n \cap (X \setminus K_1 \cup \lbrace \Delta \rbrace) \cap \dotsb \cap (X \setminus K_m \cup \lbrace \Delta \rbrace) \\
&=U_1 \cap \dotsb \cap U_n \cap (X \setminus K_1 \cap \dotsb \cap X \setminus K_m  \in \tau \subset \tilde{\tau}
\end{align*}
and if $n = 0$ then 
\begin{align*}
(X \setminus K_1 \cup \lbrace \Delta \rbrace) \cap \dotsb \cap (X \setminus K_m \cup \lbrace \Delta \rbrace)
&= (X \setminus K_1 \cap \dotsb \cap X \setminus K_m) \cup \lbrace \Delta \rbrace \in \tilde{\tau}
\end{align*}

If $A$ and $B$ are arbitrary index sets and we have $U_\alpha$ for $\alpha \in A$ open in $X$ and $K_\beta$ for $\beta \in B$ compact in $X$ then
if we define $U = \cup_{\alpha \in A} U_\alpha$ and $K= \cap_{\beta \in B} K_\beta$ note that $U \in \tau$ and $K$ is a closed subset of a compact set hence compact.  Furthermore, $U^c \cap K$ is compact for the same reason.  We compute using De Morgan's Law
\begin{align*}
\cup_{\alpha \in A} U_\alpha \cup \cup_{\beta \in B} (X \setminus K_\beta) \cup \lbrace \Delta \rbrace &=
\left[ U \cup \cup_{\beta \in B} (X \setminus K_\beta) \right ] \cup \lbrace \Delta \rbrace \\
&=X \setminus (U^c \cap K) \cup \lbrace \Delta \rbrace \in \tilde{\tau}
\end{align*}

Lastly note that $X \cup \lbrace \Delta \rbrace$ is compact.  If $U_\alpha$ and $(X \setminus K_\beta) \cup \lbrace \Delta \rbrace$ cover $X \cup \lbrace \Delta \rbrace$.  As before $\cap_{\beta \in B} K_\beta$ is compact and it follows that the $U_\alpha$ for $\alpha \in A$ form a cover.  Thus we may find a finite subcover $U_{\alpha_1}, \dotsc, U_{\alpha_n}$
and it follows that $U_{\alpha_1}, \dotsc, U_{\alpha_n}, (X \setminus \cap_{\beta \in B} K_\beta) \cup \lbrace \Delta \brace$ is a finite subcover of $X \cup \lbrace \Delta \rbrace$.
\end{proof}

\begin{prop}\label{CompleteRegularityLCH}A locally compact Hausdorff space $X$ is completely regular
  (i.e. for every $x \in X$ and closed set $F \subset X$ such that $x
  \notin F$ there is are disjoint open sets $U$ and $V$ such that $x
  \in U$ and $F \subset V$).
\end{prop}
\begin{proof}
Let $F$ be a closed set and pick $x \in X \setminus F$.  By Lemma
\ref{LocallyCompactEquivalences} and the openness of $X \setminus F$
we can find a relatively compact neighborhood $U_0$ of $x$ such that $x
\in U_0 \subset X \setminus F$.  The set $\overline{U_0} \cap F$ is a closed subset of a compact
set hence is compact.  For each $y \in \overline{U_0} \cap F$ by the
Hausdorff property we may find open neighborhoods $x \in U_y$ and $y
\in V_y$ such that $U_y \cap V_y = \emptyset$.  By compactness of
$\overline{U} \cap F$ we get a finite subcover $V_{y_1}, \dotsc,
V_{y_n}$ of $\overline{U_0} \cap F$.  Now define $U = U_0 \cap U_{y_1}
\cap \dotsb U_{y_n}$.  This is an open neighborhood of $x$ and
moreover $\overline{U} \cap F = \emptyset$.  Define $V = X \setminus \overline{U}$.
\end{proof}

TODO: Show that a locally compact separable Hausdorff space is
metrizable (in fact Polish).

\begin{defn}Let $X$ be a topological space, then a subset $A \subset
  X$ is said to be \emph{bounded} if there exists a compact set $K$
  such that 
  $A \subset K \subset X$.  A subset $A \subset
  X$ is said to be \emph{$\sigma$-bounded} if there exists a sequence
  of compact sets $K_1, K_2, \dotsc$ such that
$A \subset  \cup_{i=1}^\infty K_i \subset X$.
\end{defn}

\begin{prop}\label{SigmaBoundedEquivalence}A set $A$ is $\sigma$-bounded Borel set if and only if
  there exist disjoint bounded Borel sets $A_1, A_2, \dotsc$ such that $A =
  \cup_{i=1}^\infty A_i$.
\end{prop}
\begin{proof}
Suppose $A$ is a $\sigma$-bounded Borel set and let $K_1, K_2, \dotsc$
be compact sets such that $A \subset \cup_{i=1}^\infty K_i$.  Define
$A_1= A \cap K_1$ and for $n>1$ let $A_n = A \cap K_n \setminus
\cup_{j=1}^{n-1} A_j$  Trivially each $A_n$ is bounded (it is contained in
$K_n$), $A = \cup_{i=1}^\infty A_i$ (by construction $A_n \subset A$
and for any $x \in A$ we can find $n$ such that $x \in K_n$; it
follows that $x \in A_n$).  Moreover by construction it is clear that
the $A_n$ are Borel.  On the other hand, if $A =
\cup_{i=1}^\infty A_i$ with $A_i$ bounded and Borel and disjoint, then take $K_i$ compact
such that $A_i \subset K_i$ and it follows that $A \subset
\cup_{i=1}^\infty K_i$.  $A$ is clearly Borel as it is a countable
union of Borel sets.
\end{proof}

\begin{lem}\label{BoundedNeighborhoodsOfCompactSets}Let $K$ be a
  compact set in a locally compact Hausdorff topological
  space $X$, then there exists a bounded open set $U$ such that $K
  \subset U$.  Moreover if $V$ is a open set such that $K \subset V$
  then there is a bounded open set $U$ such that $K \subset U \subset
  \overline{U} \subset V$.
\end{lem}
\begin{proof}
By taking $V = X$ we see the second assertion implies the first so it
suffices to prove the second assertion.  By complete regularity of $X$
(Proposition \ref{CompleteRegularityLCH}) and local compactness of $X$
for each $x \in K$ we may
find a relatively compact open neighborhood $x \in U_x$ such that
$\overline{U}_x \cap V^c = \emptyset$.  By compactness of $K$ we may
take a finite subcover $U_{x_1}, \dotsc, U_{x_n}$.  Then
$U = U_{x_1} \cup \dotsb \cup U_{x_n}$ is an open set with $K \subset
U$ and
$\overline{U} = \overline{U}_{x_1} \cup \dotsb \cup \overline{U}_{x_n}$ is
a finite union of
compact sets and is therefore compact.  Lastly $\overline{U} \cap
V^c = (\overline{U}_{x_1} \cap V^c) \cup \dotsb \cup
(\overline{U}_{x_n} \cap V^c) = \emptyset$ and therefore $K \subset U
\subset \overline{U} \subset V$.
\end{proof}

For our purposes the reason for bringing up $\sigma$-bounded sets is
the fact that the properties of inner and outer regularity are
essentially equivalent on them.

\begin{lem}\label{InnerOuterRegularityEquivalence}Let $X$ be a locally compact Hausdorff topological space and let $\mu$ be a measure
  that is finite on compact sets.  Then $\mu$ is inner regular on
  $\sigma$-bounded Borel sets if and only if $\mu$ is outer regular on
  $\sigma$-bounded Borel sets.
\end{lem}
\begin{proof}
Suppose that $\mu$ is inner regular on $\sigma$-bounded sets.  Let $A$
be a bounded Borel set and suppose $\epsilon > 0$ is given.   First,
note that $\overline{A}$ is compact so may apply
Lemma \ref{BoundedNeighborhoodsOfCompactSets} to find a bounded
open set $U$ such that $\overline{A} \subset U$.  Therefore
$\overline{U} \setminus A$ is a bounded Borel set so by inner
regularity we may find a
compact set $K \subset \overline{U} \setminus A$ such that 
\begin{align*}
\mu(\overline{U} \setminus A) - \epsilon &< \mu(K) \leq \mu(\overline{U} \setminus A)
\end{align*}
Let $V = U \cap K^c$.  Then $V$ is an open set and $A \subset V$.
Moreover,
\begin{align*}
\mu(V) &= \mu(U) - \mu(K) \leq \mu(\overline{U}) - \mu(\overline{U}
\setminus A) + \epsilon = \mu(A) + \epsilon
\end{align*}
Since $\epsilon > 0$ was arbitrary we see that $\mu$ is outer regular
on bounded Borel sets.  Now we need to extend to outer regularity on
$\sigma$-bounded sets.  Let $A$ be a $\sigma$-bounded Borel set and
let $\epsilon > 0$ be given.  Apply Lemma
\ref{SigmaBoundedEquivalence}
to find disjoint bounded Borel sets $A_i$ such that $A = \cup_{i=1}^\infty
A_i$.  By the just proven outer regularity on bounded Borel sets we
may find open sets $U_i$ such that $\mu(U_i) \leq \mu(A_i) +
\epsilon/2^i$.  Then clearly $A \subset U$, $U$ is open and 
\begin{align*}
\mu(U) &\leq \sum_{i=1}^\infty \mu(U_i) \leq \epsilon + \sum_{i=1}^\infty
\mu(A_i)  = \epsilon + \mu(A)
\end{align*}
Again, as $\epsilon > 0$ is arbitrary we see that $\mu$ is outer
regular on $\sigma$-bounded Borel sets.

Now we assume that $\mu$ is outer regular on $\sigma$-bounded Borel
sets.  As before we start with the bounded case.  Let $A$ be a bounded
Borel set and suppose that $\epsilon > 0$ is given.  Let $L$ be a
compact set such that $A \subset L$.  Since $L \setminus A$ is also a
bounded Borel set, we may apply outer regularity to find an open set
$U$ such that $L \setminus A \subset U$ and 
\begin{align*}
\mu(U) - \epsilon < \mu(L \setminus A) \leq \mu(U)
\end{align*}
Define $K = L \setminus U = L \cap U^c$. As $K$ is a closed subset of the compact set $L$
it is compact.  Also
\begin{align*}
\mu(K) &= \mu(L) - \mu(L \cap U) \geq \mu(L) - \mu(U) = \mu(A) + \mu(L
\setminus A) - \mu(U) > \mu(A) - \epsilon
\end{align*}
As $\epsilon >0$ was arbitrary we see that $\mu$ is inner regular on
bounded Borel sets.  

Lastly we extend inner regularity to
$\sigma$-bounded Borel sets.  Let $A$ be $\sigma$-bounded Borel and
write $A = \cup_{i=1}^\infty A_i$ with the $A_i$ disjoint and each
$A_i$ bounded Borel (Lemma \ref{SigmaBoundedEquivalence}).  Let
$\epsilon > 0$ be given.  As each
$A_i$ is bounded and $\mu$ is finite on compact sets it follows that
$\mu(A_i) < \infty$ for all $i \in \naturals$.  Now by the just proven
inner regularity on bounded Borel sets we find $L_i \subset A_i$ with
$L_i$ compact and
\begin{align*}
\mu(A_i) - \epsilon/2^i < \mu(L_i) \leq \mu(A_i)
\end{align*}
The disjointness of the $A_i$ implies that the $L_i$ are disjoint as well.
Let $K_n = L_1 \cup \dotsb \cup L_n$ and note that
\begin{align*}
\mu(K_n) &= \sum_{i=1}^n \mu(L_i) > \sum_{i=1}^n \left( \mu(A_i) -
\epsilon/2^i \right ) > \sum_{i=1}^n \mu(A_i) - \epsilon
\end{align*}
Now take the limit as $n \to \infty$ to conclude that 
\begin{align*}
\sup \lbrace \mu(K) \mid K \subset A \text{ and $K$ is compact}
\rbrace &\geq \sup_n \mu(K_n) \geq \mu(A) - \epsilon
\end{align*}
and as $\epsilon > 0$ was arbitrary inner regularity of $\mu$ on
$\sigma$-bounded Borel sets is proven.
\end{proof}

The difficult part of the Riesz-Markov Theorem is the construction of
a Radon measure that corresponds to a positive functional.  The
tradition is to break that construction into two pieces: first the
construction of a set function on a smaller class of sets than the
full $\sigma$-algebra  and secondly the extension of that set function
to a full blown Radon measure. In many developments the set function
is defined on the compact subsets of the  locally compact
Hausdorff space $X$ and are called \emph{contents}.  Following
Arveson, we choose a set function is one
that is defined on just the open subsets of $X$.

The description of the desireable properties of the set function and
the process of extending the set function to a Radon measure is dealt with in the following Lemma.
\begin{lem}\label{ExtensionToRadonMeasure}Let $X$ be a locally compact Hausdorff space and let $m$ be
  a function from the open set of $X$ to $[0,\infty]$ satisfying:
\begin{itemize}
\item[(i)]$m(U) < \infty$ if $\overline{U}$ is compact
\item[(ii)]if $U \subset V$ then $m(U) \leq m(V)$
\item[(iii)]$m(\cup_{i=1}^\infty U_i) \leq \sum_{i=1}^\infty m(U_i)$
  for all open sets $U_1, U_2, \dotsc$.
\item[(iv)]if $U \cap V = \emptyset$ then $m(U \cup V) = m(U) + m(V)$
\item[(v)]$m(U) = \sup \lbrace m(V) \mid V \text{ is open, } \overline{V} \subset U \text{ and }
  \overline{V} \text{ is compact} \rbrace$
\end{itemize}
then there is a unique Radon measure $\mu$ such that $\mu(U) = m(U)$
for all open sets $U$.  Moreover every Radon measure satisfies
properties (i) through (v) when restricted to the open subsets of $X$.
\end{lem}
\begin{proof}
First we show that a Radon measure satisfies properties (i) through
(v) on the open sets of $X$.  In fact, properties (ii), (iii) and (iv)
follow for all measures and (i) follows from the fact that $\mu$ is
finite on compact subsets and monotonicity of measure.  Property (v)
requires a bit more justification.  If we let $U$ is an open set and
$\epsilon > 0$ is given then by inner regularity of $\mu$ we may find
a compact set $K$ such that $K \subset U$ and $\mu(U) \geq \mu(K) > \mu(U) -
\epsilon$.  By Lemma \ref{BoundedNeighborhoodsOfCompactSets} we may
find a relatively compact open set $V$ such that $K \subset V \subset
\overline{V} \subset U$.  Then by monotonicity we have $\mu(U) \geq \mu(V) > \mu(U) -
\epsilon$ and since $\epsilon$ was arbitrary (v) follows.

Next we prove uniqueness of the extension of $m$ to a Radon measure
$\mu$.  Since a Radon measure is inner regular on all Borel sets Lemma
\ref{InnerOuterRegularityEquivalence} implies that any extension $\mu$
is outer regular on all $\sigma$-bounded Borel sets.  Since the values
of $\mu$ are determined on all open sets this implies that the values
of $\mu$ are determined on all $\sigma$-bounded Borel sets; in
particular the values of $\mu$ are determined on all compact
sets. Clearly a Radon measure is determined uniquely by its values on
compact sets.

Now we turn to proving existence of the extension $\mu$.  The proof
goes in a few steps.  First we define an outer measure from $m$ and
observe that Borel sets are measurable with respect to it; though the
Caratheordory restriction of the outer
measure is outer regular it is not necessarily inner regular.  The
second step is to 
modify the Caratheodory restriction to make it inner regular.  

We begin by defining the outer measure in a standard way.  Let $A$ be
an arbitrary subset of $X$ and define
\begin{align*}
\mu^*(A) &= \inf \lbrace m(U) \mid A \subset U \text{ and $U$
  is open} \rbrace
\end{align*}
Note that $\mu^*(U) = m(U)$ for all open sets.

\begin{clm}$\mu^*$ is an outer measure
\end{clm}

Note that because the emptyset is relatively compact we know from (i)
that $m(\emptyset) < \infty$ and thus from (iv) we see that
$m(\emptyset) = 2m(\emptyset)$.  Thus $m(\emptyset) = 0$ and it
follows that $\mu^*(\emptyset) = 0$.  If $A \subset B$ then it is
trival that 
\begin{align*}
\lbrace m(U) \mid A \subset U \text{ and $U$
  is open } \rbrace &\subset \lbrace m(U) \mid B \subset U \text{ and $U$
  is open } \rbrace
\end{align*}
which implies $\mu^*(A) \leq \mu^*(B)$.  If we let $A_1, A_2, \dotsc$
be given and define $A = \cup_{i=1}^\infty A_i$.  If any
$\mu^*(A_i) = \infty$ it follows that $\mu(A) \leq \sum_{i=1}^\infty
\mu^*(A_i) = \infty$.  If on the other hand every $\mu^*(A_i) <
\infty$ then let $\epsilon > 0$ be given and find an open set $U_i
\subset A_i$ such that $m(U_i) \leq \mu^*(A_i) + \epsilon / 2^i$.
Clearly $\cup_{i=1}^\infty U_i$ is an open subset of $A$ and it
follows from (iii) and the definition of $\mu^*$ that
\begin{align*}
\mu^*(A) &\leq m(\cup_{i=1}^\infty U_i) \leq \sum_{i=1}^\infty m(U_i)
\leq \sum_{i=1}^\infty m(A_i) + \epsilon
\end{align*}
Since $\epsilon > 0$ is arbitrary we see that $\mu^*$ is countably
subadditive and is therefore proven to be an outer measure.

\begin{clm}Borel sets are $\mu^*$-measurable.
\end{clm}

The $\mu^*$-measurable sets form a $\sigma$-algebra by Lemma
\ref{CaratheodoryRestriction} and therefore it suffices to show that
open sets are $\mu^*$-measurable.  Let $U$ be open subset and $A$ be an
arbitrary subset of $X$, by subadditivity of $\mu^*$ we only have to
show the inequality 
\begin{align*}
\mu^*(A) &\geq \mu^*(A \cap U) + \mu^*(A \cap U^c)
\end{align*}
Obviously we may assume that $\mu^*(A) < \infty$ since otherwise the
inequality is trivially satisfied.  
We first assume that $A$ is an open set.  Since $\mu^*$ and $m$ agree
on open sets we have to show
\begin{align*}
m(A) &\geq m(A \cap U) + \mu^*(A \cap U^c)
\end{align*}
Let $\epsilon > 0$ be given and use property (v) so we can find an relatively compact open set $V$ such
that $\overline{V} \subset A \cap U$ and $m(V) \geq m(A \cap U) -
\epsilon$.  Then $A \cap \overline{V}^c$ is an open set containing $A
\cap U^c$ disjoint from $V$ and it follows from (ii), (iv)  and the
definition of $\mu^*$ that
\begin{align*}
m(A) &\geq m(V \cup A \cap \overline{V}^c) = m(V) + m(A \cap
\overline{V}^c) \geq m(A \cap U) - \epsilon + \mu^*(A \cap U^c) 
\end{align*}
As $\epsilon > 0$ was arbitrary we are done with the case of open
sets $A$.  Now suppose that $A$ is an arbitrary set with $\mu^*(A) <
\infty$ and let $\epsilon
> 0$ be given.  We find an open set $V$ such that $A \subset V$ and
$m(V) \leq \mu^*(A) + \epsilon$.  From what we have just proven of
open sets and the monotonicity of $\mu^*$
\begin{align*}
\mu^*(A) + \epsilon &\geq \mu^*(V) \geq \mu^*(V \cap U) + \mu^*(V \cap
U^c) \geq \mu^*(A \cap U) + \mu^*(A \cap U^c) 
\end{align*}
The claim follows by observing that $\epsilon >0$ was arbitrary.

Now by Caratheodory Restriction (Lemma \ref{CaratheodoryRestriction}) we may restrict $\mu^*$ to a Borel measure
$\overline{\mu}$ that is outer regular by definition and that
satisfies $\overline{\mu}(U) = m(U)$ for all open sets $U$.  Moreover
$\overline{\mu}(K) < \infty$ for all compact sets since by Lemma
\ref{BoundedNeighborhoodsOfCompactSets} we may find a relatively
compact open neighborhood $U$ such that $K \subset U$; monotonicity
and (i) tell us that 
\begin{align*}
\overline{\mu}(K) \leq \overline{\mu}(U) = m(U) <
\infty
\end{align*}  
Since $\overline{\mu}$ is outer regular on all Borel sets \emph{a fortiori}
it is outer regular on all $\sigma$-bounded Borel sets.  By  Lemma
\ref{InnerOuterRegularityEquivalence} it follows that
$\overline{\mu}$ is inner regular on all $\sigma$-bounded Borel sets.
Note that if we assume that $X$ is $\sigma$-compact (i.e. all Borel
sets are $\sigma$-bounded) then we already know that $\overline{\mu}$
is a Radon measure.  In the general case it is not necessarily true
and we must make a further modification to $\overline{\mu}$ to make it
inner regular.

For an arbitrary Borel set $A$ we define
\begin{align*}
\mu(A) &= \sup \lbrace \overline{\mu}(B) \mid B \subset A \text{ and
  $B$ is a $\sigma$-bounded Borel set } \rbrace
\end{align*}
Clearly, $\mu(\emptyset) = \overline{\mu}(\emptyset) = 0$.
It is also immediate from the definition that $\mu(A) = \overline{\mu}(A)$
for all $\sigma$-bounded Borel sets $A$ and therefore that $\mu(U) =
m(U)$ for all $\sigma$-bounded open sets $U$.  In fact more is true.

\begin{clm}$\mu(U) = m(U)$ for all open sets $U$.
\end{clm}

Let $V$ be a relatively compact open set with $\overline{V} \subset U$.  We have
\begin{align*}
m(U) &= \overline{\mu}(U) \geq \mu(U) \geq \mu(V) = m(V)
\end{align*}
Now we take the supremum over all such $V$ and by property (v) 
\begin{align*}
m(U) &\geq \mu(U) \sup \lbrace m(V) \mid V \text{ is relatively
  compact and $\overline{V} \subset U$} \rbrace = m(U)
\end{align*}
and therefore $\mu(U) = m(U)$.

\begin{clm}$\mu$ is a measure.
\end{clm}

To see that $\mu$ is a measure it remains to show countable
additivity.  Let $A_1, A_2, \dotsc$ be disjoint Borel sets.  First we
show countable subadditivity.  Let $B$ be a $\sigma$-bounded Borel
subset of $\cup_{i=1}^\infty A_i$ and define $B_i = B \cap A_i$.
Clearly the $B_i$ are disjoint $\sigma$-bounded Borel measures, thus
using the countable additivity of $\overline{\mu}$ we get
\begin{align*}
\overline{\mu{B}} &= \sum_{i=1}^\infty \overline{\mu}(B_i) \leq \sum_{i=1}^\infty \mu(A_i)
\end{align*}
Taking the supremum over all such $B$ subadditivity follows.

We need to show the opposite inequality.  Suppose that some $\mu(A_j) = \infty$ for some $j$.  Then we may find
a sequence of $\sigma$-bounded Borel sets $B_n$ such that
$\overline{\mu}(B_n) \geq n$.  Since $B_n \subset \cup_{i=1}^\infty
A_i$ we also see that $\mu(\cup_{i=1}^\infty A_i) = \infty$.  Thus we
may now assume that $\mu(A_i) < \infty$ for all $i$.  Let $\epsilon >
0$ be given and for each $i$ find a $\sigma$-bounded Borel set $B_i$
such that $B_i \subset A_i$ and $\overline{\mu}(B_i) \geq \mu(A_i) -
\epsilon/2^i$.  For each $n$ define $C_n = \cup_{j=1}^n B_j$ and note
that $C_n$ is a $\sigma$-bounded Borel set such that $C_n \subset
\cup_{i=1}^\infty A_i$.  Also, for every $n$, 
\begin{align*}
\mu(\cup_{i=1}^\infty A_i) \geq \mu(C_n) = \overline{\mu}(C_n) =
\sum_{j=1}^n \overline{\mu}(B_j) \geq \sum_{j=1}^n \mu(A_j)  -
\epsilon/2^j \geq \sum_{j=1}^n \mu(A_j)  -\epsilon
\end{align*}
Now take the limit as $n \to \infty$ and using the fact that $\epsilon
> 0$ was arbitrary, we get $\sum_{j=1}^\infty \mu(A_j) \leq \mu(\cup_{j=1}^\infty A_j)$.

\begin{clm}$\mu$ is a Radon measure.
\end{clm}

The fact that $\mu(K) < \infty$ for all compact sets follows from the
fact that $\mu$ and $\overline{\mu}$ agree on $\sigma$-bounded sets
and the fact that $\overline{\mu}(K) < \infty$.  To see inner
regularity, let $A$ be a Borel set and let $\epsilon > 0$ be given.
By the definition of $\mu$ we find a $\sigma$-bounded Borel set $B
\subset A$ such that $\overline{\mu}(B) \geq \mu(A) - \epsilon/2$.  Then by
the fact that $\overline{\mu}$ is inner regular on $\sigma$-bounded
sets we find a compact set $K$ such that $\overline{\mu}(K) \geq
\overline{\mu}(B) - \epsilon/2$.  Combining the two inequalities and
using the fact that $\mu$ and $\overline{\mu}$ agree on compact sets
we get $\mu(K) \geq \mu(A) - \epsilon$.  Since $\epsilon > 0$ was
arbitrary we are done.
\end{proof}

Given a Radon measure on a locally compact Hausdorff space, all
compactly supported continuous functions are integrable: $\int \abs{f}
\, d\mu \leq \norm{f}_\infty \mu(supp(f)) < \infty$.  Thus such a
measure yields a linear functional on $C_c(X)$.  Such functionals
possess another simple property in addition to linearity.
\begin{defn}A linear functional $\Lambda$ on $C_c(X)$ is said to be
  \emph{positive} if $f \geq 0$ implies $\Lambda(f) \geq 0$.
\end{defn}

The reader should be careful that we have not been using any type of
topology on $C_c(X)$ at this point and in particular we are not claiming
that the function defined by a Radon measure is continuous with respect to 
any underlying topology.  In fact as we will see later, $C_c(X)$ is a normed vector space 
(not complete) under the sup norm but it is only for finite Radon measures that the
corresponding functional is continuous.

It is clear that the linear functional defined by integration with
respect to a Radon measure is positive.  The Riesz-Markov Theorem
tells us that the positive linear functionals are precisely those
generated by integration with respect to a Radon measure.  To prove
the result we will need to figure out how to define a measure from a
positive linear functional.  As a warm up let's first answer that
question in the case of integration with respect to a Radon measure.

\begin{lem}\label{RieszMarkovUniquenessOnOpen}Let $X$ be a locally compact Hausdorff space and let $\mu$
  be a Radon measure on $X$, then for every open set $U$ we have
\begin{align*}
\mu(U) &= \sup \lbrace \int f \, d\mu \mid 0 \leq f \leq 1, f \in
C_c(X), supp(f) \subset U \rbrace
\end{align*}
\end{lem}
\begin{proof}
For the inequality $\geq$, suppose that $f \in C_c(X)$ satisfies $0
\leq f \leq 1$ and $supp(f) \subset U$, then observe the hypotheses
imply that $f \leq \characteristic{U}$ so that
\begin{align*}
\int f(x) \, d\mu(x) &\leq \int \characteristic{U}(x) \, d\mu(x) = \mu(U)
\end{align*}
and the inequality follows by taking the supremum over all such $f$.

For the inequality $\leq$ we leverage the inner regularity of $\mu$.
Let $K \subset U$ be a compact set.  By Lemma
\ref{BoundedNeighborhoodsOfCompactSets} we find an relatively compact
open set $V$ with $K \subset V \subset \overline{V} \subset U$.  Since
$\overline{V}$ is a compact Hausdorff space, it is normal and we may
apply the Tietze Extension Theorem \ref{TietzeExtensionTheorem} to
find a continuous function $g : \overline{V} \to [0,1]$ such that $g
\equiv 1$ on $K$.  Applying Urysohn's Lemma \ref{UrysohnsLemma} we
construct a continuous function $h : X \to [0,1]$ such that $h = 1$ on
$K$ and $h=0$ on $V^c$.  We define
\begin{align*}
f(x) &= \begin{cases}
h(x) g(x) & \text{if $x \in \overline{V}$} \\
0 & \text{if $x \notin \overline{V}$}
\end{cases}
\end{align*}
By the corresponding properties of $g$ and $h$, it is clear that $0 \leq f \leq 1$ and that $f=1$ on $K$.  We claim that $f$ is continuous on all of $X$.  Since $h$ restricts to a continuous function
on $\overline{V}$ it is clear that the restriction of $f$ to
$\overline{V}$ is continuous.  Let $O \subset \reals$ be an open
set.  If $0 \notin O$ then it follows that $f^{-1}(O) \subset V$ and
is therefore open by the continuity of $f$ restricted to $V$.  If on
the other hand, $0 \in O$ then $f^{-1}(O) \cap \overline{V}$ is open
in $\overline{V}$ hence is of the form $Z \cap \overline{V}$ for some
open subset $Z \subset X$.  Because $0 \in O$ if follows that $Z
\subset f^{-1}(O)$ and therefore by the definition of $f$ we we may
write $f^{-1}(O) = Z \cup \overline{V}^c$ which is an open set.

TODO: This is a locally compact Hausdorff version of Tietze, factor it
out into a separate result.

With the extension $f$ in hand we see that 
\begin{align*}
\mu(K) &\leq \int f(x) \, d\mu(x) \leq \sup \lbrace \int f \, d\mu \mid 0 \leq f \leq 1, f \in
C_c(X), supp(f) \subset U \rbrace
\end{align*}
Now taking the supremum over all compact subsets $K \subset U$ and
using the inner regularity of $\mu$ the result follows.
\end{proof}


Before we state an prove the Riesz-Markov theorem we need the
existence of finite partitions of unity on compact sets in an LCH: a standard
bit of general topology.

\begin{lem}\label{PartitionOfUnity}Let $X$ be a locally compact Hausdorff space, $K$ be a
  compact subset of $X$ and $\lbrace U_\alpha \rbrace$ an open
  covering of $K$.  There exists a finite subset $\alpha_1, \dotsc,
  \alpha_n$ and continuous functions with compact support $f_{\alpha_1}, \dotsc,
  f_{\alpha_n}$ such that $supp(f_{\alpha_j}) \subset U_{\alpha_j}$
  and $f_{\alpha_1} + \dotsb + f_{\alpha_n} = 1$ on $K$.
\end{lem}
\begin{proof}
Pick an $x \in K$, pick an $U_{\alpha_x}$ such that $x \in U_{\alpha_x}$ and
using complete regularity of $X$, construct a continuous function
$g_x$ from $X$ to $[0,1]$ such that $g_x(x) = 1$ and
$g_x \equiv 0$ on $U_{\alpha_x}^c$.  Thus $g_x^{-1}(0,1] \subset
U_{\alpha_x}$ and the $g_x^{-1}(0,1]$ form an open cover of $K$.  By
compactness of $K$ we extract a finite subcover $g_{x_1}^{-1}(0,1],
\dotsc, g_{x_n}^{-1}(0,1]$.  If we clean up notation by denoting $U_{\alpha_{x_j}} =
U_{\alpha_j}$ and $g_{\alpha_j} = g_{\alpha_{x_j}}$, it follows that
$U_{\alpha_1}, \dotsc, U_{\alpha_n}$ is an open cover of $K$ and $g =
\sum_{j=1}^n g_{\alpha_j}$ is strictly positive on $K$.  Moreover by
compactness of $K$ we know that $g$ has a minimum value $C > 0$ on
$K$.  Define $h = g \vee C$ so that $h$ is continuous, $h = g$ on $K$ and $h
\geq C > 0$ everywhere on $X$.  By continuity and strict positivity of
$h$ we can define $f_{\alpha_j} = g_{\alpha_j}/h$ so that
$f_{\alpha_j}$ is continuous and moreover $f_{\alpha_1} + \dotsb +
f_{\alpha_n} = g/h = 1$ on $K$.
\end{proof}

\begin{thm}[Riesz-Markov Theorem]\label{RieszMarkov}Let $X$ be a
  locally compact Hausdorff space and let $\Lambda : C_c(X) \to
  \reals$ be a positive linear functional then there exists a unique
  Radon measure $\mu$ such that $\Lambda(f) = \int f \, d\mu$ for all
  $f \in C_c(X)$.
\end{thm}
\begin{proof}
The uniqueness part of the result is straightforward.  By Lemma
\ref{RieszMarkovUniquenessOnOpen} we know that the values of $\mu$ on open sets
are determined by $\Lambda$.  By Lemma
\ref{InnerOuterRegularityEquivalence} we conclude that the values of
$\mu$ on $\sigma$-bounded Borel sets are determined by $\Lambda$, in
particular the values on compact sets are determined by $\Lambda$.
The inner regularity of $\mu$ implies that the values on all Borel
sets are determined by $\Lambda$.

For existence we follow the lead of Lemma
\ref{RieszMarkovUniquenessOnOpen} and define the set function on the
open sets of $X$
\begin{align*}
m(U) &=  \sup \lbrace \Lambda( f ) \mid 0 \leq f \leq 1, f \in C_c(X), supp(f) \subset U \rbrace
\end{align*}
We proceed by showing that $m(U)$ satisfies properties (i) through (v)
from Lemma \ref{ExtensionToRadonMeasure} and that if $\mu$ is the
Radon measure constructed by that result that we indeed have
$\Lambda(f) = \int f \, d\mu$.

\begin{clm}$m$ satisfies (i)
\end{clm}

Let $U$ be a relatively compact open set.  By Lemma
\ref{BoundedNeighborhoodsOfCompactSets} we can find another relatively
compact open set $V$ such that $\overline{U} \subset V$.  By the
Tietze Extension Theorem argument of Lemma \ref{ExtensionToRadonMeasure} we can find a continuous function $g: X \to
[0,1]$ such that $g = 1$ on $\overline{U}$ and $g = 0$ on $V^c$.
Since $g \in C_c(X)$ we have $\Lambda(g) < \infty$.  Now suppose that
$f \in C_c(X)$ satisfies $0 \leq f \leq 1$ and  $f \in C_c(X), supp(f)
\subset U$.  It follows that $0 \leq f \leq g$ and linearity and
positivity of $\Lambda$ we know that $\Lambda(f) \leq \Lambda(g)$.
Taking the supremum over all such $f$ we get
\begin{align*}
m(U) &= \sup \lbrace \Lambda( f ) \mid 0 \leq f \leq 1, f \in C_c(X),
supp(f) \subset U \rbrace \leq \Lambda(g) < \infty
\end{align*}

\begin{clm}$m$ satsifies (ii)
\end{clm}

This is immediate since $U \subset V$ implies 
\begin{align*}
\lbrace \Lambda( f ) \mid 0 \leq f \leq 1, f \in C_c(X),
supp(f) \subset U \rbrace \subset \lbrace \Lambda( f ) \mid 0 \leq f \leq 1, f \in C_c(X),
supp(f) \subset V \rbrace
\end{align*}

\begin{clm}$m$ satisfies (iii)
\end{clm}

Let $U_1, U_2, \dotsc$ be open sets and let $f \in C_c(X)$ satisfy $0
\leq f \leq 1$ and $supp(f) \subset \cup_{n=1}^\infty U_n$.  By
compactness of $supp(f)$ and Lemma
\ref{PartitionOfUnity} we may find an $N$ and continuous functions
$g_i$ for $i=1, \dotsc, N$ such that $0 \leq g_i \leq 1$, $supp(g_i) \subset U_i$ and
$\sum_{i=1}^N g_i = 1$ on $supp(f)$ and therefore $f = f \cdot
\sum_{i=1}^N g_i$.  It also follows that $0 \leq f g_i
\leq 1$ and $supp(f g_i) \subset U_i$ for $i=1, \dotsc, N$ and thus
\begin{align*}
\Lambda(f) &= \sum_{i=1}^N \Lambda( f g_i) \leq \sum_{i=1}^N m(U_i)
\leq \sum_{i=1}^\infty m(U_i)
\end{align*}
Now we take the supremum over all such $f$ to conclude that
$m(\cup_{i=1}^\infty U_i) \leq \sum_{i=1}^\infty m(U_i)$.

\begin{clm}$m$ satisfies (iv)
\end{clm}

Suppose $U$ and $V$ are disjoint open sets.  We only need to show that
$m(U \cup V) \geq m(U) + m(V)$ since the opposite inequality follows
from (iii).  Let $f, g \in C_c(X)$ such that $0 \leq f,g \leq 1$,
$supp(f) \subset U$ and $supp(g) \subset V$.  By disjointness of $U$
and $V$ it follows that $f+g \in C_c(X)$, $0 \leq f+g \leq 1$ and
$supp(f+g) \subset supp(f) \cup supp(g) \subset U \cup V$.  Therefore
\begin{align*}
\Lambda(f) + \Lambda(g) &= \Lambda(f+g) \leq m(U \cup V)
\end{align*}
Now take the supremum over all $f$ and $g$ to get the result.

\begin{clm}$m$ satisfies (v)
\end{clm}

Let $U$ be an open set and let $f \in C_c(X)$ such that $0 \leq f \leq
1$ and $supp(f) \subset U$.  By compactness of $supp(f)$ and Lemma
\ref{BoundedNeighborhoodsOfCompactSets} we may find a relatively
compact open set $V$ such that 
$supp(f) \subset V \subset \overline{V} \subset U$.  
It follows that 
\begin{align*}
\Lambda(f) &\leq m(V) \leq \sup \lbrace m(V) \mid V \text{ is open, } \overline{V} \subset U \text{ and }
  \overline{V} \text{ is compact} \rbrace
\end{align*}
Now take the supremum over all such $f$.

We may now apply Lemma \ref{ExtensionToRadonMeasure} to construct a
Radon measure $\mu$ such that $\mu(U) = m(U)$.  We need to show that
for every $f \in C_c(X)$ we have $\Lambda(f) = \int f \, d\mu$. By
linearity we know that $\Lambda(0) = \int 0 \, d\mu = 0$ so we may
assume that $f \neq 0$.  We may write $f = f_+
- f_-$ with $f_+ = f \vee 0 \in C_c(X)$ and $f_- = (-f) \vee 0 \in
C_c(X)$.  Since both $\Lambda$ and the integral are linear it suffices
to show the result for $f \geq 0$.  Since $f$ is continuous with
compact support, it follows that $f$ is bounded and since $f \neq 0$
we have $0 < \norm{f}_\infty < \infty$.  Again, by linearity of
$\Lambda$ and integration it suffices to prove the result of
$f/\norm{f}_\infty$ and thus we may assume that $0 \leq f \leq 1$.

We proceed by constructing a generalized upper and lower sum
approximation to the integral of $f$.  Once again apply Lemma
\ref{BoundedNeighborhoodsOfCompactSets} to find a relatively compact
open neighborhood $U_0$ with $supp(f) \subset U_0$.  Let $\epsilon >
0$ be given and choose $n \in \naturals$ such that $\mu(U_0) <
\epsilon n$.  For $j=1, \dotsc, n$ define $U_j = f^{-1} (j/n,
\infty)$.  Because $f$ is continuous and of compact support, each
$U_j$ is a relatively compact open set and it is trivial from the
definitions that we have $\emptyset = U_n \subset U_{n-1} \subset \dotsb \subset
U_0$.  In fact by the continuity of $f$ it is also true that $\overline{U}_j \subset U_{j+1}$.
Define the lower and upper approximations to $f$
\begin{align*}
u(x) &= \begin{cases}
\frac{j}{n} & \text{if $x \in U_j \setminus U_{j+1}$ for some $j=1, \dotsc,  n-1$} \\
0 & \text{if $x \notin U_1$}
\end{cases}
\end{align*}
 and similarly 
\begin{align*}
v(x) &= \begin{cases}
\frac{j}{n} & \text{if $x \in U_{j-1} \setminus U_{j}$ for some
  $j=1,  \dotsc,  n$} \\
0 & \text{if $x \notin U_0$}
\end{cases}
\end{align*}
Note that we have the property that $u \leq f \leq v$ and moreover we
have the useful alternative defintion of $u$ and $v$
\begin{align*}
u(x) &= \frac{1}{n} \sum_{j=1}^n \characteristic{U_j}(x) \\
v(x) &= \frac{1}{n} \sum_{j=1}^{n} \characteristic{U_{j-1}}(x) \\
\end{align*}
which shows that $u,v \in C_c(X)$.

\begin{clm}$\int (v -u) \, d\mu < \epsilon$
\end{clm}

This follows by observing that $v - u = \frac{1}{n}
(\characteristic{U_0} - \characteristic{U_n}) = \frac{1}{n}
\characteristic{U_0}$ since $U_n = \emptyset$.

\begin{clm}$\int u \, d\mu \leq \Lambda(f) \leq \int v \, d\mu + \epsilon$
\end{clm}

To see this claim we decompose $f$ into a representation that is
adapted to the nested sequence $U_n \subset \dotsb \subset U_0$.  For
$j=1, \dotsc, n$ define 
\begin{align*}
\phi_j(x) &= \begin{cases}
1/n & \text{if $x \in U_j$} \\
f(x) - \frac{j-1}{n} & \text{if $x \in U_{j-1} \setminus U_{j}$} \\
0 & \text{if $x \notin U_{j-1}$}
\end{cases} \\
&=[(f(x) - \frac{j-1}{n}) \vee 0] \wedge \frac{1}{n}\\
\end{align*}
where the second representation shows that $\phi_j \in C_c(X)$ and $0 \leq \phi_j \leq \frac{1}{n}$.
Note also
that if we are given $x \in U_{j-1} \setminus U_j$ for some $j=1,
\dotsc, n$ then $\phi_i(x) = 0$ for $j < i \leq n$ and $\phi_i(x) =
\frac{1}{n}$ for $1 \leq i < j$.  Therefore we have 
\begin{align*}
\phi_1(x) + \dotsb + \phi_n(x) &= \phi_1(x) + \dotsb + \phi_j(x) \\
&= \frac{j-1}{n} + f(x) - \frac{j-1}{n} = f(x)
\end{align*}
It is clear that for $x \notin U_0$ we have $f(x) = 0$ and
$\phi_j(x) = 0$ for all $j=1, \dotsc, n$ so we have
 $f = \phi_1 + \dotsb + \phi_n$ on all of $X$.

We now need to bound $\Lambda(\phi_j)$ in terms of $\mu(U_k) = m(U_k)$ for
suitable $k=1, \dotsc, n$.  First we get a lower bound on
$\Lambda(\phi_j)$.  Let $1 \leq j \leq n$ be given.  Suppose that we have a $g \in C_c(X)$ with
$0 \leq g \leq 1$ and $supp(g) \subset U_j$.  Then by positivity of
$\phi_j$ and the fact that $\phi_j(x) = \frac{1}{n}$ on $U_j$ we see
that $g \leq \characteristic{U_j} \leq n \phi_j$ and therefore
$\Lambda(g) \leq n \Lambda(\phi_j)$.    Taking the supremum over all
such $g$ we see that $\frac{1}{n} \mu(U_j) \leq \Lambda(\phi_j)$.  Taking the
sum over all $j=1, \dotsc, n$ and using linearity of $\Lambda$ we get
\begin{align*}
\int u \, d\mu &= \frac{1}{n} \sum_{j=1}^n \mu(U_j) \leq \sum_{j=1}^n
\Lambda(\phi_j) = \Lambda(f)
\end{align*}

Now we get an upper bound on $\Lambda(\phi_j)$.   For $j=2, \dotsc, n$ we that
$n phi_j \in C_c(X)$, $0 \leq n \phi_j \leq 1$ and $supp(n \phi_j)
\subset \overline{U_{j-1}} \subset U_{j-2}$.  From the definition of
$\mu(U_{j-2}) =m(U_{j-2})$ it follows that $\Lambda(\phi_j) \leq
\frac{1}{n} \mu(U_{j-2})$.  As for $\phi_1$, we have
$n \phi_1 \in C_c(X)$ and $0 \leq n\phi_1 \leq 1$ by exactly the same
argument as for $j \geq 2$.  We also have $supp(n \phi_1) \subset
supp(f) \subset U_0$ so that $\Lambda(\phi_1) \leq \frac{1}{n}
\mu(U_0)$.  If we define $U_{-1} = U_0$ the we get
have $\Lambda(\phi_j) \leq \frac{1}{n} \Lambda(U_{j-2})$ for $j=1,
\dotsc, n$.  Again we sum and use linearity of $\Lambda$,
\begin{align*}
\Lambda(f) &= \sum_{j=1}^n \Lambda(\phi_j) \leq \frac{1}{n}
\sum_{j=1}^n \mu(U_{j-2}) = \frac{1}{n} \mu(U_{-1}) +  \frac{1}{n}
\sum_{j=1}^{n-1} \mu(U_{j-1}) \\
&\leq \frac{1}{n} \mu(U_{0}) +  \frac{1}{n}
\sum_{j=1}^{n} \mu(U_{j-1}) \leq \epsilon + \int v \, d\mu
\end{align*}

It remains to stitch together the previous claims to
show that $\Lambda(f) = \int f \, d\mu$.  Integrating the inequality
$u \leq f \leq v$ we get $\int u \, d\mu \leq f \leq \int v \, d\mu$.
Now using this fact and previous two claims we get
\begin{align*}
\Lambda(f) - \int f \, d\mu &\leq \Lambda(f) - \int v \, d\mu \leq
\int u \, d\mu  - \int u \, d\mu + \epsilon \leq 2 \epsilon
\end{align*}
and
\begin{align*}
\Lambda(f) - \int f \, d\mu &\geq \int u \, d\mu - \int f \, d\mu \geq
\int u \, d\mu - \int v \, d\mu \geq -\epsilon
\end{align*}
from which we conclude that $\abs{\Lambda(f) - \int f \, d\mu} \leq
2\epsilon$.  Since $\epsilon > 0$ was arbitrary we are done.
\end{proof}

\begin{defn}Let $X$ be a locally compact Hausdorff space then a
  bounded signed
  measure $\mu$ is said to be a \emph{finite signed Radon measure} if its
  total variation $\mu_+ + \mu_-$ is a Radon measure on $X$.
\end{defn}

\begin{thm}\label{BanachSpaceRadonMeasuresLCH}Let $X$ be a locally compact Hausdorff space then the space
  of finite signed Radon measures is a Banach space with norm given by
  $\norm{\mu} = \mu_+(X) + \mu_-(X)$.
\end{thm}
\begin{proof}
TODO:
\end{proof}

TODO: The Riesz Representation Theorem itself; namely the dual space of $C_0(X)$ for a locally compact Hausdorff $X$ is 
isomorphic to the Banach space of finite signed Radon measures with
the total variation norm.
\begin{thm}\label{RieszRepresentationLCH}Let $X$ be a locally compact
  Hausdorff space then the dual space of $C_0(X)$ is isomorphic to the
  space of finite signed Radon measures under the total variation norm.
\end{thm}
\begin{proof}
TODO:
\end{proof}

\begin{defn}Let $\mu$ be a measure on the Borel $\sigma$-algebra of a
Hausdorff topological space $S$.  
\begin{itemize}
\item[(i)] A Borel set $B$ is \emph{inner regular} if for
 $\mu(B) = \sup_{K \subset B} \mu(K)$ where $K$
  is compact. $\mu$ is inner regular if every Borel set is inner regular.
\item[(ii)]A Borel set $B$ is \emph{outer regular} if $\mu(B) = \inf_{U \supset B} \mu(U)$ where $U$
  is open.  A measure $\mu$ is outer regular if every Borel set
  $B$ is outer regular.
\item[(iii)] $\mu$ is \emph{locally finite} if every $x \in S$ has an
  open neighborhood $x \in U$ such that $\mu(U) < \infty$.
\item[(iv)] $\mu$ is a \emph{Radon measure} it is inner regular and
  locally finite.
\item[(v)] $\mu$ is a \emph{Borel measure} when?????  In some cases
  I've seen it required that $\mu(B) < \infty$ for all Borel sets $B$
  (reference?) and in other cases just that the Borel sets are measurable.
\item[(vi)]A Borel set  $B$ is \emph{closed regular} if $\mu(B) = \inf_{F \subset B} \mu(F)$ where $F$
  is closed (e.g. Dudley pg. 224).  A measure $\mu$ is closed regular
  if every Borel set $B$ is closed regular.
\item[(vii)] If $\mu$ is finite, then we say \emph{tight} if and only if
  X is inner regular (e.g. Dudley pg. 224).
\end{itemize}
\end{defn}

\begin{prop}Let $\mu$ be a measure on the Borel $\sigma$-algebra of a
  locally compact Hausdorff space $S$.  Then $\mu$ is locally finite
  if and only if $\mu(K) < \infty$ for all compact sets $K \subset S$.
\end{prop}
\begin{proof}
If $\mu(K) < \infty$ for all compact sets $K$ we let $x \in S$ and
pick a relatively compact neighborhood $U$ of $x$.  Then $\mu(U) \leq
\mu(\overline{U}) < \infty$ which shows $\mu$ is locally finite.  On
the other hand, suppose $\mu$ is locally finite and let $K$ be a
compact set.  For each $x \in K$ we take an open neighborhood $U_x$
such that $\mu(U_x) < \infty$ and then extract a finite subcover $U_{x_1},
\dotsc,U_{x_n}$.  By subadditivity, we have $\mu(K) \leq \mu(U_{x_1}) + \dotsb
+ \mu(U_{x_n}) < \infty$.
\end{proof}

\begin{defn}Let $\mu$ be a Borel measure on a Hausdorff topological space. A set measurable set $A$ is called \emph{regular} if 
\begin{itemize}
\item[(i)]$\mu(A) = \inf_{U \supset A} \mu(A)$ where $U$ are open
\item[(ii)]$\mu(A) = \sup_{F \subset A} \mu(A)$ where $F$ are closed 
\end{itemize}
TODO: Alternative def assumes that $F$ are compact (see inner
regularity above).  If every measurable set is regular then $\mu$ is
said to be regular.  Note that if we assume the definition of
regularity uses compact inner approximations then regular measures are
inner and outer regular (although inner and outer regularity refer to
only Borel sets; is that a meaningful distinction?)  I think this use
of closed inner regularity is a bit non-standard should probably get
rid of it.
\end{defn}


TODO: Regularity of outer measures and the relationship to regularity
of measures as defined above (see Evans and Gariepy).  Note that
regularity of outer measure implies that if we take an outer measure $\mu$
and the measure on the $\mu$-measurable sets and then take the induced
outer measure we get $\mu$ back if and only $\mu$ is a regular outer
measure.  Evans and Gariepy show that Radon outer measures on
$\reals^n$ are inner
regular as measures on the $\mu$-measurable sets (I think we prove this more
generally above in the context of LCH spaces; note that every set in
$\reals^n$ is $\sigma$-bounded).  Note that inner
regular is part of the most common definition of Radon measure so
their result can be taken as showing a weaker definition of Radon
measure holds on $\reals^n$ (but also they phrase everything in terms
of outer measures...).

TODO: How much this stuff on regularity can be extended to outer
measures????  I want to understand the overlap with the results in
Evans and Gariepy.

\begin{lem}\label{InnerRegularSetsSigmaAlgebra}Let $X$ be a Hausdorff topological space, $\mathcal{A}$
  a $\sigma$-algebra on $X$ and $\mu$ a finite tight measure.  Then
\begin{align*}
\mathcal{R} &= \lbrace A \in \mathcal{A} \mid A \text { and } A^c
\text{ are $\mu$-inner regular} \rbrace
\end{align*}
is a $\sigma$-algebra.  The same is true if the condition is replaced
by sets that are $\mu$-closed inner regular (without the requirement
that $\mu$ is tight).
\end{lem}
\begin{proof}
By definition, $\mathcal{R}$ is closed under complement.  By
assumption that $\mu$ is tight we have $X \in \mathcal{R}$ so all that
needs to be shown is closure under countable union.

Assume $A_1, A_2, \dots \in \mathcal{R}$ and let $\epsilon>0$ be
given.  By finiteness of $\mu$, $\mu(\cup_{n=1}^\infty A_n) < \infty$ and
continuity of measure (Lemma \ref{ContinuityOfMeasure}) there exists $M>0$ such that $\mu(\cup_{n=1}^M
A_n) > \mu(\cup_{n=1}^\infty A_n) - \epsilon$.
 By assumption that $A_n \in \mathcal{R}$ and finiteness of $\mu$, for each
$A_n$ there exists a compact $K_n$ such that $\mu(A_n \setminus K_n) <
\frac{\epsilon}{2^n}$ and there exists compact $L_n$ such that $\mu(A_n^c \setminus L_n) <
\frac{\epsilon}{2^n}$. Let
\begin{align*}
K &= \cup_{n=1}^M K_n \\
L &= \cap_{n=1}^\infty L_n
\end{align*}
and note that both $K$ and $L$ are compact (in the latter case,
because X is Hausdorff we know that each $L$ is closed hence the
intersection is a closed subset of a compact set hence compact).
Furthermore we can compute
\begin{align*}
\mu(\cup_{n=1}^\infty A_n \setminus K) &= \mu(\cup_{n=1}^\infty A_n
\setminus \cup_{n=1}^M K_n)  \\
&= \mu(\cup_{n=1}^M A_n
\setminus \cup_{n=1}^M K_n)  + \mu(\cup_{n=1}^\infty A_n \setminus \cup_{n=1}^M A_n
\setminus \cup_{n=1}^M K_n)\\
&\leq \mu(\cup_{n=1}^M A_n \setminus K_n)  + \mu(\cup_{n=1}^\infty A_n
\setminus \cup_{n=1}^M A_n)\\
&\leq \sum_{n=1}^M(A_n \setminus K_n)  + \epsilon \\
&\leq 3 \epsilon
\end{align*}
and
\begin{align*}
\mu((\cup_{n=1}^\infty A_n)^c \setminus L) &=\mu(\cap_{n=1}^\infty
A_n^c \setminus \cap_{n=1}^\infty L_n) \\
 &=\mu(\cap_{n=1}^\infty
A_n^c \cap \cup_{n=1}^\infty L_n^c) \\
 &=\mu(\cup_{n=1}^\infty \cap_{m=1}^\infty
A_m^c \cap L_n^c) \\
 &\leq \mu(\cup_{n=1}^\infty 
A_n^c \cap L_n^c) \\
 &\leq \sum_{n=1}^\infty (
A_n^c \setminus L_n) \\
&\leq 2 \epsilon
\end{align*}

TODO: The closed inner regular case...
\end{proof}

TODO:  In metric space, tightness is equivalent to inner regularity.
Then Ulam's Theorem that finite measures on separable metric spaces
are automatically inner regular.  Also finite measures on arbitrary
metric spaces are closed inner regular as well as outer regular.

\begin{lem}\label{FiniteMeasuresOnMetricSpacesAreClosedInnerRegular}
Let $(S,d)$ be a metric space and $\mu$ be a Borel measure on $(S,
\mathcal{B}(S))$, then $\mu$ is closed inner regular.  If in addition
$\mu$ is a finite measure then it is outer regular.
\end{lem}
\begin{proof}
Let $U$ be an open set in $S$.  Then $U^c$ is closed and the
function $f(x) = d(x, U^c)$ is continuous.  If we define 
\begin{align*}
F_n &= f^{-1}([1/n, \infty))
\end{align*}
then each $F_n$ is closed, $F_1 \subset F_2 \subset \cdots$ and
$\cup_{n=1}^\infty F_n = U$.  By continuity of measure (Lemma
\ref{ContinuityOfMeasure}) we know that $\lim_{n \to \infty} \mu(F_n)
= \mu(U)$.  So this shows that every open set is inner closed
regular.  Furthermore it is trivial to note that $U^c$ is inner closed
regular because it is closed.  

By Lemma \ref{InnerRegularSetsSigmaAlgebra} we know know that 
\begin{align*}
\mathcal{B}(S) &\subset \mathcal{R} = \lbrace A \subset S \mid A
\text{ and } A^c
\text{ are inner closed regular}  \rbrace
\end{align*}

Outer regularity follows from taking complements and using the
finiteness of $\mu$.
\end{proof}

If we add the criterion that the metric space is separable, then we
can upgrade the closed inner regularity to inner regularity.
\begin{lem}\label{SeparableInnerRegularTight}Let $(S,d)$ be a
  separable metric space and $\mu$ be a finite Borel measure on $(S,
\mathcal{B}(S))$, then $\mu$ is inner regular if and only if it is tight.
\end{lem}
\begin{proof}
Clearly inner regularity implies tightness (which is just inner
regularity of the set $S$), so it suffices to show
that tightness implies inner regularity.

Suppose that $\mu$ is a tight measure.  By Lemma
\ref{InnerRegularSetsSigmaAlgebra} it suffices to show that both open
and closed sets are inner regular.

Pick $\epsilon >0$ and select $K \subset S$ a compact set such that $\mu(S \setminus K) < \frac{\epsilon}{2}$.
By Lemma \ref{FiniteMeasuresOnMetricSpacesAreClosedInnerRegular} we
know that for any Borel set $B$ there exists a closed set $F \subset
B$ such that $\mu(B \setminus F) < \frac{\epsilon}{2}$.  Note that $F
\cap K$ is compact.   We have
\begin{align*}
\mu(B \setminus (F \cap K)) &\leq \mu(B \cap F^c) + \mu(B \cap K^c) \leq \mu(B \cap F^c) + \mu(S \cap K^c) < \epsilon
\end{align*}
\end{proof}
\begin{thm}[Ulam's Theorem]\label{UlamsTheorem}Let $(S,d)$ be a
  complete separable metric space and $\mu$ be a finite Borel measure on $(S,
\mathcal{B}(S))$, then $\mu$ is inner regular.
\end{thm}
\begin{proof}
By Lemma \ref{SeparableInnerRegularTight} it suffices to show that $\mu$ is tight.  Pick
$\epsilon > 0$ and we construct a compact set $K \subset S$ such that
$\mu(S \setminus K) < \epsilon$.  Let
$\overline{B}(x,r)$ denote the closed ball of radius $r$ around $x \in
S$.  Pick
a countable dense subset $x_1, x_2, \dotsc \in S$.  For each $m \in
\naturals$, by density of $\lbrace x_n \rbrace$, we know $\cap_{n=1}^\infty \left ( S
\setminus \cup_{j=1}^n \overline{B}(x_j, \frac{1}{m}) \right ) =
\emptyset$, thus by
continuity of measure (Lemma \ref{ContinuityOfMeasure}) there exists
$N_m > 0$ such that $\mu(S
\setminus \cup_{j=1}^n \overline{B}(x_j, \frac{1}{m}) < \frac{\epsilon}{2^m}$ for
all $n \geq N_m$.
If we define
\begin{align*}
K &= \cap_{m=1}^\infty \cup_{j=1}^{N_m} \overline{B}(x_j, \frac{1}{m})
\end{align*}
we claim that $K$ is compact.  Note that $K$ is easily seen to be
closed as it is an intersection of a finite union of closed balls.
Since $S$ is complete this implies that $K$ is also complete.  Also it
is easy to see that $K$ is totally bounded since by construction we
have demonstrated a cover by a finite number of balls of radius
$\frac{1}{m}$ for each $m \in \naturals$.  So by Theorem
\ref{CompactnessInMetricSpaces} we know $K$ is compact.

To finish the result we claim $\mu(S \setminus K) < \epsilon$:
\begin{align*}
\mu(S \setminus K) 
&= \mu(S \cap \left(\cap_{m=1}^\infty
  \cup_{j=1}^{N_m} \overline{B}(x_j, \frac{1}{m})\right)^c) \\
&= \mu(S \cap \cup_{m=1}^\infty \left(
  \cup_{j=1}^{N_m} \overline{B}(x_j, \frac{1}{m})\right)^c) \\
&= \mu(\cup_{m=1}^\infty S \setminus 
  \cup_{j=1}^{N_m} \overline{B}(x_j, \frac{1}{m})) \\
&\leq \sum_{m=1}^\infty \mu( S \setminus 
  \cup_{j=1}^{N_m} \overline{B}(x_j, \frac{1}{m})) \\
&< \epsilon
\end{align*}
\end{proof}

\begin{thm}Let $\mu$ be a finite Borel measure on a metric space $S$,
  then $\mu$ is closed regular.  If $\mu$ is tight then $\mu$ is regular.
\end{thm}
TODO: Specialize the definition of Radon measure in the presence of
more assumptions on $X$ (in particular local compactness,
$\sigma$-compactness, second countability).

TODO: Are Radon measures automatically outer regular?  Finite ones are
I believe.

Tao proves Riesz representation under assumption of local compactness, Hausdorff
and $\sigma$-compactness.

Kallenberg proves Riesz representation under assumption of LCH and
second countability (this is less general than the Tao
result as lcscH implies LCH $\sigma$-compact)
and targets Radon measures.  Our results taken from Arveson are more
general as the remove the second countability assumption and the $\sigma$-compactness
assumptions.

Evans and Gariepy prove Riesz representation only on $\reals^n$ using
Radon outer measures.  This is probably subsumed by our results taken
from Arveson but I need to understand whether the use of outer
measures adds anything to the picture in general.

Arveson has some well known lecture notes that prove Riesz on general
LCH spaces
and emphasizes Radon measures (it also explores how Baire measures figure in the
picture).  I have chosen to follow these notes.  Note that Arveson
mentions that the lcscH approach of Kallenberg avoids the distinction
between Baire and Borel sets (sounds like the $\sigma$-algebras agree
in this case).

Fremlin probably has some very general account of Reisz representation
(of course).

Dudley proves Riesz representation of compact Hausdorff spaces and
phrases things in terms of Baire measures.  Dudley does not really
discuss Radon measures.  Arveson discusses the relationship between
the use of Baire and Radon measures.

\section{Covering Theorems in $\reals^n$}

Since our purposes have been to understand probability theory we have
hitherto avoided making assumptions that we are dealing with
$\reals^n$.  While this decision has benefits, it has drawbacks as
well.  Among those drawbacks are that we lose sight of some history and also some very
beautiful and deep understanding of the measure theory of the reals.
TODO: Vitali and Besicovich.

\section{Hausdorff Measure}

\subsection{Introduction}

In this section we discuss the construction of a family of outer
measures on $\reals^n$ called \emph{Hausdorff measures}.  Note the
construction can be generalized to metric spaces.  The following is
motivation why a tool like Hausdorff measure may be useful.  Suppose
very specifically that we are
in $\reals^3$, then the Lebesgue product measure essentially
corresponds to a notion of volume.  What about the surface area of a
$2$-dimensional object or the length of a $1$-dimensional object?  As
you may have learned in advanced calculus these ideas can indeed be
describe in great generality by the notion of differential forms.
However, the formalism of forms usually has some notion of smoothness
associated with it (hence the adjective differential); a natural question to ask is whether one can fine
a purely measure theoretic approach to the problem.  Hausdorff measures
provide one answer to this question.   The broad form of the theory
is perhaps a bit more general than one might expect; for any space
there is a Hausdorff outer measure for every real number $s$.  The
case of integers
$s=1$ corresponds to arclength, $s=2$ surface area, $s=3$ volume and so
on.  Measures with $s$ non-integral are
\emph{fractal}.  On $\reals^n$, the Hausdorff measure with $s=n$ is equal to
Lebesgue measure and any Hausdorff measure with $s > n$ is trivial
(gives $0$ measure to all sets).  We'll prove all of this and more in
what follows.

\subsection{Construction of Hausdorff Measure}

The following technical Lemma is useful (we'll use it when
discussing Hausdorff outer measures).  If the reader is in a hurry,
no harm will come from skipping over this result and returning to it
when the need arises.  Note that if the user is only interested in
probability theory this result may never come up.
\begin{lem}[Caratheodory Criterion]\label{CaratheodoryCriterion}Let $(S,d)$ be a metric space with an outer measure $\mu^*$.
  Then $\mu^*$ is a Borel outer measure (i.e. all Borel sets are
  $\mu^*$-measurable) if and only if $\mu^*(A \cup B) = \mu^*(A) +
  \mu^*(B)$ for all $A,B$ such that $d(A, B) > 0$.
\end{lem}
\begin{proof}
We begin with the only if direction.  Let $A$ be a closed set in $S$
and let $B \subset S$.  To show $A$ is $\mu^*$-measurable it suffices
to show $\mu^*(B) \geq \mu^*(A \cap B) + \mu^*(A^c \cap B)$.  Since
the inequality is trivially satisfied when $\mu^*(B) = \infty$ we
assume that $\mu^*(B) < \infty$.  For
every $n \in \naturals$, let $A_n =
\lbrace x \in S \mid d(x, A) \leq \frac{1}{n} \rbrace$.  By definition
of $A_n$, we have $d(A,
A_n^c) > \frac{1}{n} > 0$ and therefore $d(A \cap B, A_n^c \cap B)
> \frac{1}{n} > 0$.  Now by our assumption, we can conclude $\mu^*((A
\cap B) \cup (A_n^c \cap B)) = \mu^*(A \cap B) +
\mu^*(A_n^c \cap B)$.

We claim that $\lim_{n \to \infty} \mu^*(A_n^c \cap B) = \mu^*(A^c
\cap B)$.  Note that if we prove the claim the Lemma is proven because then we have
\begin{align*}
\mu^*(B) &\geq \mu^*((A
\cap B) \cup (A_n^c \cap B)) & & \text{by monotonicity}\\
&= \mu^*(A \cap B) +
\mu^*(A_n^c \cap B)
\end{align*}
and taking limits we have 
\begin{align*}
\mu^*(B) \geq \lim_{n\to \infty} \mu^*(A \cap B) +
\mu^*(A_n^c \cap B) &= \mu^*(A \cap B) +
\mu^*(A^c \cap B)
\end{align*}
To prove the claim we observe that monotonicity of outer measure
implies that $\lim_{n \to \infty} \mu^*(A_n^c \cap B) \leq \mu^*(A^c
\cap B)$ so we just need to
work on the opposite inequality.  To see it first define the rings
around $A$
\begin{align*}
R_n &= \lbrace x \mid \frac{1}{n+1} < d(x, A) \leq \frac{1}{n} \rbrace
\end{align*}
and note that because $A$ is closed, for each $n$,
\begin{align*}
A^c &= \lbrace x \in S \mid d(x, A) > 0 \rbrace \\
&=\lbrace x \in S \mid d(x, A) > n \rbrace \cup \bigcup_{m=n}^\infty \lbrace
x \in S \mid \frac{1}{m+1} < d(x, A) \leq \frac{1}{m} \rbrace \\
&=A_n^c \cup \bigcup_{m=n}^\infty R_m
\end{align*}
It follows that
$A^c \cap B = A_n^c \cap B \cup \cup_{m=n}^\infty
R_m \cap B$ and therefore by subadditivity of outer measure 
\begin{align*}
\mu^*(A^c \cap B) \leq \mu^*(A_n^c \cap B) + \sum_{m=n}^\infty
\mu^*(R_m \cap B)
\end{align*}
The claim will follow if we can show $\lim_{n \to \infty} \sum_{m=n}^\infty
\mu^*(R_m \cap B)=0$ which in turn will follow if we can show that $\sum_{m=1}^\infty
\mu^*(R_m \cap B)$ converges.  By construction, $d(R_{2m}, R_{2n})
> 0$ and therefore $d(R_{2m} \cap B, R_{2n} \cap B)
> 0$ for any $m \neq n$.  So if we consider only the even terms of the
series we can use our hypothesis to show that for any $n$
\begin{align*}
\sum_{m=1}^n \mu^*(R_{2m} \cap B) &= \mu^*(\cup_{m=1}^n
R_{2m} \cap B) \leq \mu^*(B) < \infty
\end{align*}
and by taking limits $\sum_{m=1}^\infty \mu^*(R_{2m} \cap B) \leq \mu^*(B)$
The same argument applies to the odd indexed terms and we get
\begin{align*}
\sum_{m=1}^\infty \mu^*(R_{m} \cap B) &\leq 2\mu^*(B) < \infty
\end{align*}
The claim and the Lemma follow.
\end{proof}


TODO:  Here I am taking the path of Evans and Gariepy and normalizing
Hausdorff measure so that $\mathcal{H}^n = \lambda_n$.  I am not sure
if this winds up being inconvenient when one considers Hausdorff
measure in arbitrary metric spaces (nor do I know whether we'll bother
considering Hausdorff measures in metric spaces).

\begin{lem}Let $\lambda_n$ be Lebesgue measure on $\reals^n$, then
  $\lambda_n(B(0, 1)) = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)}$.
\end{lem}
\begin{proof}
TODO
\end{proof}

\begin{defn}Let $(S,d)$ be a metric space and $A \subset S$, the
  \emph{diameter} of $A$ is 
\begin{align*}
\diam(A) &= \sup \lbrace d(x,y) \mid x,y \in A \rbrace
\end{align*}
\end{defn}

\begin{defn}Let $(S,d)$ be a metric space, $0 \leq s < \infty$ and $0
  < \delta$.  Then for $A \subset S$,
\begin{align*}
\mathcal{H}^s_\delta(A) &= \inf \lbrace \sum_{n=1}^\infty \alpha(s)
\left ( \frac{\diam(C_n)}{2}\right )^s \mid A \subset
\cup_{n=1}^\infty C_n \text{ where } \diam(C_n) \leq \delta \text{ for
  all } n\rbrace
\end{align*}
where 
\begin{align*}
\alpha(s) &= \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)}
\end{align*}
For $A$ and $s$ as above define
\begin{align*}
\mathcal{H}^s(A) &= \lim_{\delta \to 0} \mathcal{H}_\delta^s(A) = \sup_{\delta>0} \mathcal{H}_\delta^s(A)
\end{align*}
\end{defn}

\section{Integration in Banach Spaces}

Our prior development of measure and integration theory made use of the
special properties of the reals in various places and as a
result the theory does not hold for functions with values in arbitrary
vector spaces.  As we shall soon see it is useful to be able to
integrate functions with vector space values (in particular Banach
spaces) so we need an integration theory.  As it turns out there are a
couple of directions that one can go.  In the simplest case that will
suffice for many of our needs, we simply develop the theory of Riemann
integrals.  The primary loss of generality is
that the domains of functions in the Riemann integral case must be 
functions of a real variable.  For our purposes we shall only be
requiring the Riemann theory for a single real variable so that shall
suit us fine.  For problems in which the domain in an arbitrary
measurable space we need a Lebesgue-like theory that was developed by
Bochner.  The reader may want to be made aware that in addition to these integrals there is also an
integral due to Gelfand and Pettis that we shall not discuss.

\subsection{Riemann Integrals}
As mentioned we shall only bother to develop the Riemann integral for
a single real variable.
\begin{defn}Let $a \leq b$ be real numbers then a \emph{partition} of
  the interval $[a,b]$ is a finite sequence of real numbers $a=a_0
  \leq a_1 \leq \dotsb \leq a_n = b$.  Let $X$ be a Banach space then
  a map $f : [a,b] \to X$ is said to be a \emph{step map with repsect
    to P} if there
  exists a partition $P=\lbrace a_j \rbrace_{j=0}^n$ and elements $w_1,
  \dotsc, w_n \in X$ such that $f(t) = w_j$ for $a_{j-1} < t < a_j$.
  A \emph{step map} is any map $f$ such that for which there exists a partition
  $P$ for which $f$ is a step map with respect to $P$.
  The \emph{integral} of a step map with repsect to $P$ is
\begin{align*}
I_P(f) &= 
\sum_{j=1}^n (a_j - a_{j-1}) w_j
\end{align*}
\end{defn}
Note that a step map has it values constrained on the open intervals
$(a_{j-1},a_j)$ but not at the points $a_j$.

With all of these elementary definitions in hand we come to our first
task which is to show that the integral of a step map is well defined.
\begin{prop}Let $X$ be a Banach space and let $f : [a,b] \to X$ be a
  step map with respect to partitions $P$ and $Q$ then it follows that
  $I_P(f) = I_Q(f)$.
\end{prop}
\begin{proof}
Given a partition $P$ of the form $a=a_0 \leq \dotsb \leq a_n=b$ let
$c \in [a,b]$ and let the refinement $P_c$ represent the partition
obtained by adding $c$ to the set of $a_j$.  It is clear that $f$ is
still a step map with respect to $P_c$ and that $I_P(f)
= I_{P_c}(f)$.  A partition $R$ is said to be a refinement of $P$ if
it is a subset of $P$; by induction we see that $I_P(f) = I_R(f)$
whenever $R$ is a refinement of $P$.  Now given arbitrary partitions
$P$ and $Q$ as in the hypotheses we simply find a common refinement
(e.g. take the union of $P$ and $Q$) and the result follows.
\end{proof}

Now we extend the integral by a limiting procedure.  To do this we use 
somewhat abstract language of Banach space theory.  First let us set
up the Banach space in which we operate.

\begin{prop}Let $X$ be a normed vector space, let $S$ be an arbitrary
  set and let $\mathfrak{B}(S, X)$
  represent the set of bounded functions $f : S \to X$.  Let
  $\abs{x}$ denote the norm on $X$.  If we
  define $\norm{f} = \sup_{s \in S} \abs{f(s)}$ then $\norm{f}$
  makes $\mathfrak{B}(S, X)$ into a normed vector space.
\end{prop}
\begin{proof}
We first observe that $\mathfrak{B}(S, X)$ is a vector space.  
This follows from the fact that if $f$ is bounded by $C$ then for all
$a \in \reals$ we have $af$ is bounded by $\abs{a} C$ if both $f$ and
$g$ are bounded by $C_1$ and $C_2$ respectively then using the
triangle inequality in $X$ we see that $f+g$ is bounded by $C_1 + C_2$.

Next we prove that we have defined a norm.  The fact that $\norm{f}
\geq 0$ and $\norm{0} =0$ follow immediately from the definition and
the fact that $\abs{\cdot}$ is a norm on $X$.   If $\norm{f}
= 0$ the it follows that $\abs{f(s)} = 0$ for all $s \in S$
and therefore $f=0$.  Let $c \in \reals$ then since $\abs{cf(s)} =
\abs{c} \abs{f(s)}$ it follows that $\norm{cf} \leq \abs{c}\norm{f}$.
On the other hand, let $\epsilon > 0$ be given then we may find an $s
\in S$
such that $\norm{f} - \epsilon < \abs{f(s)}$.  It follows that 
\begin{align*}
\abs{c} \norm{f} - \abs{c} \epsilon < \abs{c} \abs{f(s)} = \abs{cf(s)}
\end{align*}
Now $\epsilon$ was chosen arbitrarily so we may let $\epsilon \to 0$
and we get the inequality $\abs{c} \norm{f} \leq
\abs{cf(s)}$.  Now take the supremum over $s \in S$ to get opposite
inequality $\abs{c} \norm{f} \leq \norm{cf}$ and it follows that
$\abs{c} \norm{f} = \norm{cf}$.  The triangle inequality follows in a
similar way.  Given an $f$ and $g$ we see using the triangle
inequality in $X$ that for all $s \in S$ we
have $\abs{f(s) + g(s)} \leq \abs{f(s)} + \abs{g(s)} \leq \norm{f}
+\norm{g}$; taking the supremum over $s \in S$ we get $\norm{f+g} \leq
\norm{f} + \norm{g}$.
\end{proof}

Now we have the following extension result
\begin{lem}Let $X$ be a normed vector space and let $Y$ be a Banach
  space.  Suppose that $V \subset X$ is a subspace and $A : V \to Y$
  is a bounded linear map, then $A$ has a unique extension
  $\overline{A} : \overline{V} \to Y$ from the closure of $V$ into
  $Y$.  Moreover if $C$ is a bound on $A$ the $C$ is also a bound on $\overline{A}$.
\end{lem}
\begin{proof}
TODO:
\end{proof}

As is usual to compute with integrals it is imperative to connect the
integration with differentiation.  Since we are dealing with the
Riemann integral we must use relatively strong hypotheses however
these results will suffice for our applications and the proof are very
simple.  We start with the Fundamental Theorem of Calculus.

\begin{thm}[Fundamental Theorem of
  Calculus]\label{FundamentalTheoremOfCalculusForBanachSpaceRiemannIntegrals}Let
 $X$ be a Banach space and let $f : [a,b] \to X$ be continuously
 differentiable then 
\begin{align*}
f(b) - f(a) &= \int_a^b Df(t) dt
\end{align*}
\end{thm}
\begin{proof}
First we let $g(t)$ be a regulated function from $[a,b]$ to $X$ and
consider the integral $\int_a^s g(t) \, dt$.  Suppose that $g$ is
continuous at $c \in [a,b]$ and let $\epsilon > 0$ be given.  By
right continuity we may find $\delta > 0$ such that $\abs{g(c+h) -
  g(c)} < \epsilon$ for all $\abs{h} < \delta$.  If we let $G(s) = \int_a^s g(t) \,
dt$ then if $\abs{h} < \delta$ we have
\begin{align*}
\abs{ \frac{G(c + h) - G(c)}{h} - g(c)} &= \abs{\frac{1}{h}
                                          \int_c^{h+c} g(t)\, dt  -
                                          \frac{1}{h} \int_c^{h+c}
                                          g(c) \, dt} \\
&= \abs{\frac{1}{h}  \int_c^{h+c} (g(t) - g(c)) \, dt} \\
&\leq \frac{1}{\abs{h}} \abs{h} \sup_{c \leq s \leq c+h} \abs{g(t) -
  g(c)} 
=\sup_{c \leq s \leq c+h} \abs{g(t) -  g(c)} 
\end{align*}
By continuity
TODO
\end{proof}

\begin{prop}\label{NormRiemannIntegralBanachSpace}Let $X$ be a
  Banach space and let $f : [a,b] \to L(X,Y)$ be regulated then it
  follows that 
\begin{align*}
\norm{\int_a^b f(t) \, dt} &\leq \int_a^b \norm{f(t)} \, dt
\end{align*}
\end{prop}
\begin{proof}
TODO
\end{proof}

\begin{prop}\label{RiemannIntegralOfContinuousMaps}Let $X$ and $Y$ be
  Banach spaces and let $f : [a,b] \to L(X,Y)$ be regulated then it
  follows that for every $x \in X$ we have
\begin{align*}
\int_a^b f(t) x \, dt &= \int_a^b f(t) \, dt \cdot x
\end{align*}
\end{prop}
\begin{proof}
TODO
\end{proof}

\begin{defn}Let $X$ and $Y$ be Banach spaces then an \emph{unbounded
    operator} is $A$ a linear
  map of a subspace of $X$ into $Y$.  We let $\domain{A}$ be the
  domain of $A$.  We say that $A$ is a \emph{closed
  operator} if its graph is a closed linear subspace of $X \times Y$;
  that is to say if $v_n \to v$ in $X$ and $A v_n \to w$  in $Y$ then
  $v \in \domain{A}$ and $y = A v_n$.
\end{defn}

The domain of a linear operator is a crucial part of its definition
and there is no small amount of pain in having to be careful about
handling to be careful when dealing with these domains.  In
particular, if one is given two linear operators $A : X \to Y$ and $B
: X \to Y$ then we
can define $A+B : X \to Y$ where $\domain{A+B} = \domain{A} \cap
\domain{B}$.  
Given two linear operators $A : X \to Y$ and $B
: Y \to Z$ then we
can define $B \circ A: X \to Z$ where $\domain{B \circ A} = \lbrace v
\in \domain{A} \mid Av \in \domain{B} \rbrace$.

\begin{prop}\label{ClosedOperatorOfRiemannIntegral}Let $X$ and $Y$ be
  Banach spaces, let $A$ be a closed linear operator from $X$ to $Y$ and
  let $f : [a,b] \to X$ be continuous such that
\begin{itemize}
\item[(i)] $f(t) \in \domain{A}$ for all $a \leq t \leq b$
\item[(ii)] $Af : [a,b] \to Y$ is continuous
\end{itemize}
then it follows that $\int_a^b f(t) \, dt \in \domain{A}$ and
\begin{align*}
A \int_a^b f(t) \, dt &= \int_a^b A f(t) \, dt
\end{align*}
\end{prop}
\begin{proof}

TODO
\end{proof}

\subsection{Bochner Integrals}

TODO: We develop Bochner integrals on $\sigma$-finite measure spaces;
does it exist without that assumption?  What if one uses nets for
convergence of simple functions instead of sequences?

TODO: Preliminaries on norming subspaces.

TODO:
The Bochner integral is the analogue of a vector valued integral using
general measure theory.  Though it can be developed in a bit more
generality, we define it here for Banach space valued functions.  
The first thing to do is to observe that the defintion of simple functions
extends trivially to this context.
\begin{defn}Given a set $(\Omega, \mathcal{A})$, a Banach space $X$
  sets $A_1, \dotsc, A_n \subset \Omega$ and $v_1, \dotsc, v_n \in X$ a
  linear combination $v_1
  \characteristic{A_1} + \cdots + v_n \characteristic{A_n}$ is called
  a \emph{simple function}.
\end{defn}

\begin{lem}\label{SimpleVectorValuedFunctions}A function $f : \Omega \to X$ is simple if and only it
  takes a finite number of values.  A simple function is measurable if
  an only if $f^{-1}(v_j)$ is measurable for each of its distinct
  values $v_j \in X$.
\end{lem}
\begin{proof}
The proof of Lemma \ref{SimpleFunctions} applies here with essentially
no changes.
\end{proof}

From this point on we will tacitly assume that all simple functions
are measurable. Integrals of simple functions can be defined in the obvious way.

The natural way to proceed would be to observe that $X$ can be given
the Borel $\sigma$-algebra and thus we can talk of measurable
functions.  One might then try to show that all measurable functions
can be approximated by simple functions and the integral can be
extended by use of such approximations.  In fact that is a little too much to hope for in a
non-separable Banach space (not all measurable functions can be thus
approximated) 
and we have to restrict ourselves to the class of functions that can be approximated
by simple functions.  We'll have to spend a bit of time understanding
this class of functions. 

\begin{defn}Let $(\Omega,\mathcal{A})$ be a measurable space and let
  $X$ be a Banach space then a function $f : \Omega \to X$ is said to
  be \emph{strongly measurable} if and only if there exist simple
  functions $f_n : \Omega \to X$ such that $f_n(\omega)$ converges to 
$f(\omega)$ for  all $\omega \in \Omega$.
\end{defn}

Intuitively strongly measurable functions must have some kind of
countability restriction since they are a limit of a countable number
of finite valued functions.  This is indeed true and is at the heart
of matter why can't approximate a general Borel measurable function
with simple functions.  To make this precise we need a couple of
definitions before stating a theorem that provides a descriptive
characterization of strongly measurable functions.

\begin{defn}Let $\Omega$ be a set, a function $f : \Omega \to X$ is
  \emph{separably valued} if there exists a closed separable subspace $V
  \subset X$ such that $f(\Omega) \subset V$.  
\end{defn}

\begin{defn}Let $(\Omega, \mathcal{A})$ be a measurable space, a function $f : \Omega \to X$ is
  \emph{weakly measurable} if for every $\lambda \in X^*$ the function
  $\lambda \circ f : \Omega \to \reals$ is measurable. 
\end{defn}

\begin{thm}Let $(\Omega, \mathcal{A})$ be a measurable space and $X$
  be a Banach space then $f$ is strongly measurable if and only if $f$
  is separably valued and weakly measurable.  In fact it suffices to
  show that $f$ is separably valued and $\lambda \circ f$ is
  measurable for $\lambda$ in a norming subspace of $E^*$.
\end{thm}
\begin{proof}
TODO
\end{proof}

It is useful to note that one cannot approximate any more functions by
using strongly measurable functions rather than just simple functions.
\begin{cor}A pointwise limit of strongly measurable functions is strongly measurable.
\end{cor}
\begin{proof}
TODO
\end{proof}

We also have the following useful consequence that shows that a
strongly measurable function is a separably valued Borel measurable
function.  In particular, all measurable functions with values in a
separable Banach space are strongly measurable.

Now we introduce a $\sigma$-finte measure and consider the measure
space $(\Omega, \mathcal{A}, \mu)$.  
\begin{defn}Given a set $(\Omega, \mathcal{A})$, a Banach space $X$,
  sets $A_1, \dotsc, A_n \in \mathcal{A}$ with $\mu(A_j) < \infty$ for
  $j=1, \dotsc, n$ and $v_1, \dotsc, v_n \in X$ a
  linear combination $v_1
  \characteristic{A_1} + \cdots + v_n \characteristic{A_n}$ is called
  a \emph{$\mu$-simple function}.
\end{defn}

\section{Differentiation in Banach Spaces}

TODO:
\begin{itemize}
\item Absolute convergence of a infinite series in a Banach space
\item Define space of linear maps with operator norm
\item Show that Frechet deriviative is equal to Jacobian matrix on
  finite dimensional spaces
\end{itemize}

\begin{prop}Let $X$ be a Banach space then if $\sum_{j=0}^\infty a_j$
  converges absolutely then $\sum_{j=0}^\infty a_j$ converges in $X$.
\end{prop}
\begin{proof}
By completeness of $X$ it suffices to show that $S_n = \sum_{j=0}^n
a_j$ is a Cauchy sequence.  Let $\epsilon > 0$ be given and pick $n >
0$ such that $\sum_{j=n}^\infty \norm{a_j} < \epsilon$.  Then for all
$m \geq n$ we have
\begin{align*}
\norm{S_m - S_n} &\leq \norm{\sum_{j=n}^{m-1} a_j} \\
&\leq \sum_{j=n}^{m-1}\norm{ a_j} \leq \sum_{j=n}^{infty}\norm{ a_j} < \epsilon
\end{align*}
and we are done.
\end{proof}

\begin{prop}Let $X$ be a Banach space.  The set of invertible maps in $L(X)$ is open, moreover
  for any invertible map $A \in L(X)$ and any $\norm{A-B} <
  \norm{A}^{-1}$ we have 
\begin{align*}
B^{-1} &= \sum_{n=0}^\infty A^{-n-1} (A-B)^n 
\end{align*}
In particular, the inversion map is continuously differentiable on its domain.
\end{prop}
\begin{proof}
We first assume that $A= I$ is the identity map.  If we let $\norm{B}
< 1$ then note that 
\begin{align*}
\norm{\sum_{n=0}^m B^n} &\leq \sum_{n=0}^m \norm{ B}^n <
                          \sum_{n=0}^\infty \norm{ B}^n =
                          \frac{1}{1-\norm{B}} < \infty
\end{align*}
which shows that $\sum_{n=0}^\infty B^n$ converges absolutely and
is well defined in $L(X)$.  Moreover we have
\begin{align*}
\norm{(1- B) \sum_{n=0}^\infty B^n }
\end{align*}
TODO: Finish....
\end{proof}

We present some of the basic results on differentiation in Banach
spaces.

\begin{defn}Let $X$ and $Y$ be Banach spaces, let $U \subset X$ be
  open and let $f : U \to Y$ be
  a map.  We say that $f$ is differentiable at $x \in U$ if there
  exists a bounded linear map $L : X \to Y$ such that
\begin{align*}
\lim_{h \to 0} \frac{f(x + h) - f(x) -Lh}{\norm{h}} = 0
\end{align*}
We call the linear map $L$ the \emph{Frechet derivative} of $f$ at $x$
and denote it $Df(x)$.
\end{defn}

As it stand, we have been a little loose in defining \emph{the}
Frechet derivative as we have not ruled out the possibility that
multiple linear maps may satisfy the defining property.   The first
task is to show that in fact the Frechet derivative is uniquely
defined provided it exists.

\begin{prop}Suppose $A$ and $B$ are bounded linear maps satisfying the
  defining property of the Frechet derivative then $A = B$.
\end{prop}
\begin{proof}
Let $\epsilon > 0$ be given and pick $\delta > 0$ so that we have
$\norm{f(x+h) - f(x) -Ah} < \epsilon \norm{h}$ for $\norm{h} < \delta$
and similarly for $B$.  It then follows that
\begin{align*}
\norm{Ah - Bh} &\leq \norm{f(x+h) - f(x) -Ah} + \norm{f(x+h) - f(x)
                 -Bh} < 2 \epsilon \norm{h}
\end{align*}
so by linearity we see that $\norm{A - B} < 2 \epsilon$.  Since
$\epsilon>0$ was arbitrary it follows that $\norm{A -B} = 0$ and
therefore $A = B$.
\end{proof}

There are weaker forms of derivative that one can consider.  For the
most part we shall be concerned with only the Frechet derivative but
it can be helpful to be aware of the alternatives if for no other
reason than to refine one's understanding of the nature of the Frechet derivative.
\begin{defn}Let $X$ and $Y$ be Banach spaces, let $U \subset X$ be
  open and let $f : U \to Y$ be
  a map.  Let $v \in X$, the we say that $f$ has a directional
  derivative at $x$ in the direction of $v$ if the limit 
\begin{align*}
df(x,v) &= \lim_{t \to
    0} \frac{f(x + tv) - f(x)}{t}
\end{align*} 
exists.  We say that $f$ is
  \emph{G\^{a}teaux differentiable at $x$} if it has directional
  derivatives at all $v \in X$.
\end{defn}

We first observe that Frechet differentiability implies G\^{a}teaux differentiability.
\begin{prop}\label{FrechetDifferentiableImpliesGateauxDifferentiable}Let $X$ and $Y$ be Banach spaces, let $U \subset X$ be
  open and let $f : U \to Y$ be
  a Fr\'{e}chet differentiable at $x \in U$.  Then $f$ is G\^{a}teaux
  differentiable at $x$ and the directional derivative at $v$ is equal
  to $Df(x)v$.
\end{prop}
\begin{proof}
Let $\epsilon >0$ be given and pick $\delta>0$ such
that $\norm{f(x+h) - f(x) -Df(x)h} \leq \epsilon \norm{h}$ for all
$\norm{h} < \delta$.  With $v \in X$ fixed and suppose that $\norm{v}
= 1$; we note that for all
$\abs{t} < \delta$ we have $\norm{f(x+tv) - f(x) - t Df(x) v}
\leq \epsilon \abs{t} $ and thus 
\begin{align*}
\norm{\frac{f(x+tv) - f(x) }{t} - Df(x) v} < \epsilon
\end{align*}
so that $df(x,v) = Df(x) v$.  Now it is a simple matter to validate
that $df(x, tv) = t df(x,v) = Df(x) \cdot tv$ for all $t \in \reals$.
\end{proof}
In general G\^{a}teaux derivatives need not be linear (i.e. even
though $df(x,tv) = tdf(x,v)$ it is not necessarily the case that
$df(x,v+w) = df(x,v) + df(x,w)$) and even if
linear need not be bounded.  Somewhat more surprising is that even if
the G\^{a}teaux derivative exists and is bounded and linear the
Fr\'{e}chet derivative may not exist.  What is necessary is that the
limits $\lim_{t \to 0} \frac{f(x+tv) -f(x)}{t}$ converge uniformly for
$v$ in the unit sphere.

We calculate some trivial Frechet derivatives.
\begin{examp}Let $f : X \to Y$ be a constant map $f(x) = y$ for some
  fixed $y \in Y$, then $f$ is differentiable at every point $x \in X$
  and moreover $Df(x) = 0$.
\end{examp}
\begin{examp}Let $A : X \to Y$ be a bounded linear map, then $A$ is differentiable at every point $x \in X$
  and moreover $Df(x) = A$.
\end{examp}

The following example generalizes the product rule of calculus.
\begin{examp}Let $A : X_1  \times \dotsm \times X_n \to Y$ be a bounded
  multilinear map, then $A$ is differentiable at every point $x \in
  X_1 \times \dotsm \times X_n$
  and moreover 
\begin{align*}
Df(x_1, \dotsc, x_n)(h_1, \dotsc, h_n) = A(h_1, x_2, \dotsc, x_n) +
  A(x_1, h_2, x_3, \dotsc, x_n) + \dotsm + A(x_1, x_2, \dotsc, h_n)
\end{align*}
\end{examp}

Another important case is the behavior of deriviative when composing
with a linear map.
\begin{examp}\label{FrechetDerivativeCompositionWithLinearMap}Let $X$, $Y$ and $Z$ be Banach spaces, let $U \subset X$
  be open, let $f : U \to Y$ be differentiable and let $A : Y \to Z$
  be a bounded linear map, then $D (A \circ f)(x) = A \circ Df(x)$.

Note that this would follow from the Chain Rule below (Proposition
\ref{ChainRuleBanachSpaces}) but is worth showing this directly to get
some practice with the definitions.  Let $\epsilon > 0$ be given and
pick $\delta>0$ such that $\norm{f(x+h) - f(x) - Df(x)h} \leq
\frac{\epsilon}{\norm{A}} \norm{h}$ for all $\norm{h} < \delta$.  Note
that
\begin{align*}
\norm{Af(x+h) - Af(x) - A Df(x)h} &\leq \norm{A} \norm{f(x+h) - f(x) -
                                    Df(x)h} \leq \epsilon \norm{h}
\end{align*}
for all $\norm{h} < \delta$ which shows the result.
\end{examp}

\begin{prop}\label{DifferentiabilityImpliesContinuity}Let $X$ and $Y$ be Banach spaces, let $U \subset X$ be
  open and let $f : U \to Y$ be differentiable at $x \in U$ then $f$
  is continuous at $x$.
\end{prop}
\begin{proof}
Let $\epsilon > 0$ be given and define $0 < \delta < \frac{\epsilon}{1 + \norm{Df(x)}}$ small enough so
that $\norm{f(x+h) - f(x) - Df(x)h} \leq \norm{h}$ for all $\norm{h} <
\delta$ then 
\begin{align*}
\norm{f(x + h) - f(x)} &\leq \norm{f(x + h) - f(x) - Df(x)h} +
                         \norm{Df}\norm{h} \\
&\leq \norm{h} + \norm{Df}\norm{h} < \epsilon
\end{align*}
and continuity is proven.
\end{proof}

\begin{prop}[Chain Rule]\label{ChainRuleBanachSpaces}Let $X$, $Y$ and $Z$ be
  Banach spaces, let $U \subset X$ and $f : U \to Y$ be differentiable
  at $x \in U$, let $V \subset Y$ with $f(U) \subset V$, $g : V \to Z$ be
  differentiable at $f(x)$ then $g \circ f : U \to Z$ is
  differentiable at $x$ and moreover
\begin{align*}
D(g \circ f)(x) &= Dg(f(x)) \circ Df(x)
\end{align*}
\end{prop}
\begin{proof}
Let $\epsilon$ be given.  Let $\tilde{\delta} > 0$ be chosen so that
$\norm{g(f(x) + h) - g(f(x)) - Dg(f(x)) h} < \frac{1}{2} \epsilon
\norm{h}$ for all $\norm{h} < \tilde{\delta}$.  By continuity of $f$
at $x$ we can choose $\delta_1>0$ such that $\norm{f(x+h) - f(x)} <
\tilde{\delta}$ for all $\norm{h} < \delta_1$ and by differentiability
we may choose $\delta_2>0$ such that $\norm{f(x+h) - f(x) - Df(x)h} <
\frac{\epsilon}{\epsilon + 2\norm{Dg(f(x))}} \norm{h}$ for all
$\norm{h} < \delta_2$.  Let $\delta = \delta_1 \vee \delta_2$ and then
for $\norm{h} < \delta$ we compute
\begin{align*}
&\norm{g(f(x+h)) - g(f(x)) - Dg(f(x)) Df(x) h} \\
&\leq \norm{g(f(x+h)) -
                                                g(f(x)) - Dg(f(x))
                                                (f(x+h)-f(x)) }  \\
&+\norm{Dg(f(x))(f(x+h)-f(x))
                                                - Dg(f(x)) Df(x) h}\\
&\leq \frac{1}{2} \epsilon\norm{f(x+h)-f(x)}  +\norm{Dg(f(x))}\norm{f(x+h)-f(x) - Df(x) h}\\
&\leq \frac{1}{2} \epsilon \norm{h}  +(\frac{\epsilon}{2} +
  \norm{Dg(f(x))}) \frac{\epsilon}{\epsilon + 2\norm{Dg(f(x))}}
  \norm{h} = \epsilon \norm{h}\\
\end{align*}
and we're done.
\end{proof}

\subsection{Higher Order Derivatives and Taylor's Theorem}

\begin{thm}[Mean Value Theorem]\label{MeanValueTheoremBanachSpaces}Let
  $X$ and $Y$ be  Banach spaces, $U \subset X$ be open and let $f : U
  \to Y$ be continuously differentiable.  Suppose $x \in U$ and $y \in
  X$ such that $x + ty \in U$ for all $0 \leq t \leq 1$ then
\begin{align*}
f(x + y) - f(x) &= \int_0^1 Df(x + ty) y dt = \int_0^1 Df(x + ty) dt
                  \cdot y
\end{align*}
\end{thm}
\begin{proof}
Define $g(t) = f(x + ty)$.  Then by the Chain Rule it follows that
$g(t)$ is continuously differentiable and $Dg(t) = Df(x + ty) y$.
Since $Dg(t)$ is continuous we may apply the Fundamental Theorem of
Calculus (Theorem
\ref{FundamentalTheoremOfCalculusForBanachSpaceRiemannIntegrals}) to
conclude that 
\begin{align*}
f(x+y) - f(x) &= g(1) - g(0) = \int_0^1 Df(x+ty) y dt = \int_0^1
                Df(x+ty) dt \cdot y
\end{align*}
where in the last inequality we have use Proposition \ref{RiemannIntegralOfContinuousMaps}.
\end{proof}

Higher order derivatives are defined by iterating Frechet derivatives.  For
example if we assume that the map $f : U \to Y$ differentiable on all
of $U$ then the second derivative is obtained by
taking the derivative of the map $Df : U \to L(X,Y)$ whereever it
exists.  Thus the second derivative is a map $D^2f : U \to
L(X,L(X,Y))$.  

\begin{examp}Let $A : X \to Y$ be a bounded linear map then $D^2A = 0$.
\end{examp}

Based on the definition via induction we think of $D^nf$ as a map from
$U$ to $L(X, \dotsb ,L(X, Y) \dotsb)$.  The range here actually has a
more convenient representation as the space of multilinear maps $X
\times \dotsm \times X \to Y$.  For example given an element in $f \in
L(X,L(X,Y))$ we may define $\tilde{f}(u,v) = f(u) v$ and note that
\begin{align*}
\tilde{f}(au + bv, w) &= f(au + bv) w = a f(u) w + b f(v) w = a
\tilde{f}(u,w) + b \tilde{f}(v,w)
\end{align*} and 
\begin{align*}
\tilde{f}(u, av + bw) &= f(u)(av+bw) = a f(u) v + b f(u) w = a
                        \tilde{f}(u,v) + b\tilde{f}(u,w)
\end{align*}
so that $\tilde{f}$ is indeed bilinear.  It is easy to see that this
is an isomorphism and that the construction extends to general $n$.

TODO: Do this in the required excruciating detail...

In the sequel, it will be convenient to view higher derivatives as maps
from $U$ to the space of multilinear maps.  It turns out that higher
derivatives are not arbitrary multilinear maps but also have the
property of being symmetric.
\begin{prop}Let $U \subset X$ be open and $f : U \to Y$ be $C^p$ then
  $D^pf(x)$ is multilinear and symmetric for every $x \in U$.
\end{prop}
\begin{proof}
TODO:
We first consider the case $p=2$.  Let $u,v \in X$ and consider
\begin{align*}
D^2f(x) (u,v) = 
\end{align*}
\end{proof}

A more complicated but important example is the computation of the
derivative of the inverse in a Banach algebra.
\begin{prop}\label{FrechetDerivativeOfInverse}The map $\phi(A) = A^{-1}$ on $L(X,X)$ is $C^\infty$ on the open set of
  invertible maps.  In fact we have
\begin{align*}
D^n\phi(A)(h_1, \dotsc, h_n) &= (-1)^n\sum_\sigma A^{-1} h_{\sigma_1}
                               A^{-1} \dotsm h_{\sigma_n} A^{-1}
\end{align*}
where the summation is over all permutations of $\lbrace 1, \dotsc, n \rbrace$.
\end{prop}
\begin{proof}
We first compute the first derivative of $\phi$.  Let $A$ be invertible and
observe that for $\norm{h} < \norm{A^{-1}}^{-1}$ we know that $I +
A^{-1} h$ is invertible and moreover
\begin{align*}
(A + h)^{-1} &= A^{-1} (I + h A^{-1})^{-1} = A^{-1} \sum_{n=0}^\infty
              (-1)^n h^n A^{-n}
\end{align*}
and therefore using the absolute convergence of the series on the
right we get
\begin{align*}
\norm{(A + h)^{-1} - A^{-1} + A^{-1} h A^{-1}} &\leq \sum_{n=2}^\infty
                                                 \norm{h}^n
                                                 \norm{A^{-1}}^n \\
&= \frac{\norm{h}^2\norm{A^{-1}}^2}{1 - \norm{h}\norm{A^{-1}}} < \norm{h}^2\norm{A^{-1}}^2
\end{align*}
which shows us that $D\phi(A)h = -A^{-1} h A^{-1}$ (for $\epsilon > 0$
let $\delta < \epsilon \norm{A^{-1}}^{-2}$).

Now to see that $\phi$ is in fact $C^{\infty}$, we do an induction.
TODO: Finish
\end{proof}

With the defintion of higher derivatives available we are now able to
extend the Mean Value Theorem to Taylor's Theorem in Banach spaces.  
\begin{thm}[Taylor's Theorem]\label{TaylorsTheoremBanachSpaces}Let $X$
  and $Y$ be Banach spaces and let $U \subset X$ be open and of class
  $C^p$.  Suppose that $x \in U$ and $y \in X$ such that $x + ty \in
  U$ for all $0 \leq t \leq 1$ then we have
\begin{align*}
f(x+y) &= f(x) + Df(x) y + \dotsm + \frac{D^{p-1}f(x)
         y^{(p-1)}}{(p-1)!} + \int_0^1 \frac{(1-t)^{p-1}}{(p-1)!}
         D^pf(x + ty) y^{(p)} dt
\end{align*}
where $y^{(k)} = (y, \dotsc, y) \in X^k$.
\end{thm}
\begin{proof}
TODO
\end{proof}

It is worth noting that in the case $Y=\reals$ that Theorem
\ref{TaylorsTheoremBanachSpaces} can be proven using the one
dimensional version Theorem \ref{TaylorsTheorem} and the chain rule
Proposition \ref{ChainRuleBanachSpaces}.  We'll show this in the proof
of the Lagrange form of the remainder term below.

We've presented Taylor's Theorem in Banach spaces with the integral
form of the remainder term.  There are several different versions of
the remainder and estimates derived therefrom that are useful to
note.  The first that we mention is applicable in the important case
in which $Y = \reals$; the Lagrange form of the remainder.
\begin{prop}\label{LagrangeFormRemainderBanachSpaces}There is a number $c \in
  (0,1)$ such that 
\begin{align*}
\int_0^1 \frac{(1-t)^{p-1}}{(p-1)!}
         D^pf(x + ty) y^{(p)} dt &= 
\frac{D^pf(x + cy) y^{(p)}}{p!}
\end{align*}
\end{prop}
\begin{proof}
We derive this from the one dimensional Taylor's Theorem.  Note that
$g(t) = f(x + ty)$ is $C^p$ from $[0,1]$ to $\reals$ and by the chain
rule we have $g^\prime(t) = Df(x + ty) y$.  Now since evaluation $A \to A y$
is a bounded linear map on $L(X,Y)$, an induction argument using either Example \ref
{FrechetDerivativeCompositionWithLinearMap}
or the chain rule shows that $g^{(k)}(t) = D^kf(x+ty) y^{(k)}$.
Now apply Theorem \ref{TaylorsTheorem} to see there is a $0 < c < 1$ such that 
\begin{align*}
f(x+y) &= g(1) = g(0) + g^\prime(0) + \dotsm + \frac{g^{(p-1)} (0)}{(p-1)!} +
         \frac{g^{(p)} (c)}{p!}  \\
&=f(x) + Df(x) y + \dotsm + \frac{D^{p-1}f(x) y^{(p-1)}}{(p-1)!} +
         \frac{D^{p}f(x + cy) y^{(p)}}{p!}
\end{align*}
\end{proof}

TODO: Analytic functional calculus and beyond.

\subsection{Inverse and Implicit Function Theorems}

\begin{thm}\label{InverseFunctionTheoremBanachSpaces}Let $X$ and $Y$ be Banach spaces let $U \subset X$ be an
  open subset of $X$ and suppose that $f : U \to Y$ is continuously
  differentiable and $Df(x)$ is invertible at $x \in U$.  There is an
  open set $V \subset U$ containing $x$ and an open set $W \subset Y$
  containing
  $f(x)$ such that $f : V \to W$ is a bijection and $f^{-1}$ is
  continuously differentialble on $W$.
\end{thm}

The general Banach space proof is rather elegant but feels a bit like
magic.  We'll be a bit redundant a  provide both the general proof as
well as a proof for the finite dimensional case that is more verbose
but is very elementary.

For the finite dimensional proof we use the following simple
consequence of the mean value theorem
that shows a continuously differentiable function is Lipschitz
continuous on a bounded domain.
\begin{lem}\label{IFT:BoundedDerivativeImpliesLipschitz}Let $f : \reals^n \to \reals^n$ be differentiable on an
  open rectangle $R = (a_1,b_1) \times \dotsm \times (a_n, b_n)$ such that 
\begin{align*}
\abs{\frac{\partial f_i}{\partial x_j}(x)} \leq M
\end{align*}
for all $1 \leq i,j \leq n$ and $x \in R$ then it follows that
$\norm{f(y)-f(x)} \leq M \cdot n^2 \cdot \norm{y-x}$ for all $x,y \in R$.
\end{lem}
\begin{proof}
By expanding as a telescoping sum and the one
dimensional mean value theorem we
get for every $i=1, \dotsc, n$ 
\begin{align*}
f_i(y) - f_i(x) &= \sum_{j=1}^n f_i(y_1, \dotsc, y_j, x_{j+1},
                     \dotsc, x_n) - 
f_i(y_1, \dotsc, y_{j-1}, x_{j},
                     \dotsc, x_n)\\
&= \sum_{j=1}^n \frac{\partial f_i}{\partial x_j} (y_1, \dotsc, y_{j-1}, y^*_j, x_{j+1},
                     \dotsc, x_n) (y_j - x_j)\\
\end{align*}
where $a_j < y^*_j  < b_j$ (in fact $x_j \leq y^*_j \leq y_j$ when
$x_j \leq y_j$ and similarly when $y_j < x_j$).  Now by the triangle
inequality and the bound on partials of $f$ we get
\begin{align*}
\norm{f(y) - f(x)} &\leq \sum_{i=1}^n \abs{f_i(y) - f_i(x)} \\
&= \sum_{i=1}^n \sum_{j=1}^n \abs{\frac{\partial f_i}{\partial x_j} (y_1, \dotsc, y_{j-1}, y^*_j, x_{j+1},
                     \dotsc, x_n)} \abs{ y_j - x_j } \\
&\leq \sum_{i=1}^n \sum_{j=1}^n M \norm{y - x} = M \cdot n^2 \cdot
  \norm{y -x}
\end{align*}
\end{proof}

Now we can proceed with the proof of the theorem in the finite
dimensional case.
\begin{proof}
We first make a reduction to the case in which $Df(x)$ is the
identity.  If the result is proven in that case then for general $f$
we can define $Df(x)^{-1} \circ f : X \to X$ where from the Chain Rule
it follows that
$D(Df(x)^{-1} \circ f)(x)$ is the identity.  Applying the inverse
function theorem we see there exists open
sets $V$ and $\tilde{W}$ containing $x$ and $Df(x)^{-1}  f(x)$ respectively such
$Df(x)^{-1} \circ f$ is a bijection from $V$ to $\tilde{W}$ with $(Df(x)^{-1} \circ f)^{-1}$
continuously differentiable.  Now we define $W = Df(x)(\tilde{W})$
which is open by continuity of $Df(x)^{-1}$ and contains $f(x)$.
Since $f^{-1} = Df(x) \circ Df(x)^{-1} \circ f$ it follows by the
Chain Rule that $f^{-1}$ is continuously differentiable on $W$.

\begin{clm}There is an open ball $B(x, \delta) \subset U$ such that $f$ is
injective on the closure of $B(x, \delta)$, $Df(y)$ is invertible for all $y \in B(x,
\delta)$ and 
\begin{align}
\abs{\frac{\partial f_i}{\partial x}(y) - \frac{\partial
    f_i}{\partial x}(x)} 
&< \frac{1}{2n^2} \text{ for all $1 \leq i,j \leq n$ and $y \in
  B(x,\delta)$}
\end{align}\label{IFT:Partials}
\end{clm}

By the the openness of $U$, triangle inequality and the fact that $Df(x)$ is the
identity we know that we can find $\delta > 0$
such that $B(x,\delta) \subset U$ and 
\begin{align*}
\norm{f(x+h) - f(x)} &= \norm{f(x+h) - f(x) - h + h} \geq \norm{h} -
                       \norm{f(x+h) - f(x) - h} \\
&\geq \frac{1}{2}\norm{h}
\end{align*}
so injectivity on $B(x, \delta)$ follows; by continuity of $f$ the
bound and hence the injectivity extends to the closure of $B(x,\delta)$.  Since the invertible linear maps are an open
subset of $L(X,Y)$ and $Df$ is continuous we may also assume that
$\delta > 0$ is chosen so that $Df$ is invertible on $B(x,\delta)$.
Similarly continuity of $Df$ implies the continuity of each partial
derivative $\frac{\partial f_i}{\partial x}$ and therefore
\eqref{IFT:Partials} follows for sufficiently small $\delta > 0$.

The next claim should be thought of as asserting that the inverse of
$f$ is Lipschitz.  As it turns out the estimate is useful in showing
that $f^{-1}$ exists.

\begin{clm}$\norm{y - z}
\leq 2 \norm{f(y) - f(z)}$ for all $y,z \in B(x, \delta)$.
\end{clm}

Define $g(x) = f(x) - x$ on $B(x, \delta)$.  Because $Df(x)$ is the
identity we know that $\frac{\partial f_i}{\partial x}(x) =
\delta_{ij}$ and therefore 
\begin{align*}
\abs{\frac{\partial g_i}{\partial x}(y)} &=
\abs{\frac{\partial f_i}{\partial x}(y) - \frac{\partial f_i}{\partial
                                           x}(x)} 
\leq \frac{1}{2n^2}
\end{align*}
Since $y$ and $z$ are contained in some open rectangle that is a
subset of $B(x,\delta)$ we can apply Lemma \ref{IFT:BoundedDerivativeImpliesLipschitz} and the
triangle inequality to
conclude that 
\begin{align*}
\frac{1}{2} \norm{y - z} &\geq \norm{g(y) - g(z)} \\
&=\norm{f(y) - y - g(z) + z} \\
&\geq \norm{y - z} - \norm{f(y) - g(z)}
\end{align*}
and the claim follows by collecting terms.

The next step in the proof is to validate that the image of $B(x,
\delta)$ under $f$ contains an open set (on which we will then have a
bijection).  Consider the function $f(y) - f(x)$.  It is
continuous and by compactness of the boundary $\partial B(x,\delta)$
and the injectivity of $f$ on the closed ball we know that there
exists an $\epsilon > 0$ such that $g(y) \geq \epsilon$ on $\partial
B(x,\delta)$.  Define $W = B(f(x), \epsilon/2)$ and notice that by the
choice of $\epsilon$, the triangle inequality and the previous claim
we have for all $z \in W$ and $y \in \partial B(x, \delta)$
\begin{align}
\norm{z - f(y)} &\geq \norm{f(x) - f(y)} - \norm {f(x) - z} \geq d -
                  \frac{d}{2} \\
&= \frac{d}{2} > \norm{z - f(x)}
\end{align}\label{IFT:InteriorMinimumEstimate}

This estimate is used to construct an open set in the image of $f$.

\begin{clm}For every $z \in W$ there is a unique $y \in B(x, \delta)$ such
that $f(y) = z$.
\end{clm}

To see existence we let $z \in W$ be given and we define the function
$h(y) = \norm{f(y) - z}^2 = \sum_{j=1}^n (f_j(y) - z_j)^2$.  Differentiability of $h$ follows from the
differentiability of $f$ and the chain rule.  In
particular, $h$ is continuous and therefore by compactness of the
closed ball $\overline{B}(x, \delta)$ it attains its minimum.  By the
estimate \eqref{IFT:InteriorMinimumEstimate} we see that the minimum
must occur in the interior of the ball.  Therefore we know that the
derivative of $h$ must vanish at the minimum so by the Chain Rule we
know that for all $v \in X$
\begin{align*}
0 &=D \norm{f - z}^2 (y) \cdot v = 2 \norm{Df(y)\cdot v}{f(y) - z}
\end{align*}
and by the invertibility of $Df(y)$ it follows that we must have $f(y)
= z$ at the minimum.

The uniqueness of $y$ follows from the injectivity of $f$.

Now we define $V = f^{-1}(W) \cap B(x, \delta)$ and it follows that
$f$ is a bijection from $V$ to $W$. Now that $f^{-1}$ is well defined
we immediately get its continuity.  

\begin{clm}$f^{-1}$ is continuous on $W$.
\end{clm}

The second claim proved the bound $\norm{y - z} \leq 2 \norm{f(y) -
  f(z)}$ on $B(x, \delta)$ which certainly shows that $\norm{f^{-1}(z)
  - f^{-1}(w)} \leq 2 \norm{z -w}$ on $W$ so that $f^{-1}$ is
Lipschitz in particular continuous.

It remains to show that $f^{-1}$ is differentiable.

\begin{clm}$f^{-1}$ is continuously differentiable on $W$.
\end{clm}

In fact we show (as would follow from the Chain Rule) that $Df^{-1}
(z) = \left[ Df(f^{-1}(z) )\right]^{-1}$ for all $z \in W$.  Note that
this is well defined since $Df$ is invertible on all of $V$.  To clean
up the notation a bit let $A = Df(f^{-1}(z) )$.  Let $\epsilon > 0$ be
given.  Using differentiability of $f$ at $f^{-1}(z)$ we choose
$\tilde{\eta} > 0$ such that
\begin{align*}
\norm{f(f^{-1}(z) +
  h) -
  f(f^{-1}(z)) - Ah} &< \frac{\epsilon}{2\norm{A^{-1}}} \norm{h}
                       \text{ for all $\norm{h} < \tilde{\eta}$}
\end{align*}
By continuity of $f^{-1}$ at $z$ we choose $\eta > 0$ such that
$\norm{f^{-1}(z+h) - f^{-1}(z)} < \tilde{\eta}$ for
all $\norm{h} < \eta$.  Pick $h \in Y$ with $\norm{h}<\eta$ and
compute using the Lipschitz continuity of $f^{-1}$
\begin{align*}
&\norm{f^{-1}(z + h) - f^{-1}(z) - A^{-1} h} \\
&=\norm{A^{-1} \left(f(f^{-1}(z+h)) - f(f^{-1}(z)) - A(f^{-1}(z + h) -
                                              f^{-1}(z)) \right)} \\
&\leq \norm{A^{-1}} \norm{f(f^{-1}(z+h)) - f(f^{-1}(z)) - A(f^{-1}(z + h) -
                                              f^{-1}(z))} \\
&\leq \frac{1}{2} \epsilon \norm{f^{-1}(z + h) - f^{-1}(z)} \leq \epsilon \norm{h} 
\end{align*}
which gives us $Df^{-1}(z) = \left[ Df(f^{-1}(z) )\right]^{-1}$.
Continuity of $Df^{-1}$ follows from the continuity of $Df$,
continuity of $f^{-1}$ and continuity of inversion of invertible maps
in $L(X,Y)$.
\end{proof}

The proof of the Inverse Function Theorem in general Banach spaces
rests on a simple result that is of broad applicability.  The result
actually doesn't use the vector space structure and is valid in
general complete metric spaces; it provides a
very general mechanism for solving equations in such spaces.

\begin{prop}[Contraction Mapping
  Principle]\label{ContractionMappingPrinciple}Let $(S,d)$ be a complete
  metric space
  space, let $F \subset S$ be a closed subset and let $g : F \to F$ be
  a mapping such that there exists a constant $0 < K < 1$ such that 
\begin{align*}
d(g(x), g(y)) &\leq K d(x,y) \text{ for all $x,y \in F$}
\end{align*}
then there exists a unique $x_0 \in F$ such that $g(x_0) = x_0$ and
moreover given any $x \in F$ the sequence $\lbrace g^n(x) \rbrace$ is
Cauchy and converges to $x_0$.
\end{prop}
\begin{proof}
First we prove uniqueness.  Suppose there are two points $x$ and $y$
satisfying $g(x) = x$ and $g(y) = y$ then we know that
\begin{align*}
d(x,y) &= d(g(x), g(y)) \leq K d(x,y)
\end{align*}
and since $0 < K < 1$ this shows that $d(x,y) = 0$.

Now we let $x \in F$ be arbitrary and show that $g^n(x)$ is Cauchy.
Suppose that $m > n$ and observe that by a simple induction
\begin{align*}
d(g^n(x), g^m(x)) \leq K^n d(x, g^{m-n}(x))
\end{align*}
In particular, we have that $d(g^n(x), g^{n+1}(x)) \leq K^n d(x,g(x))$
and therefore by the triangle inequality 
\begin{align*}
d(x, g^n(x)) &\leq d(x, g(x)) + \dotsm + d(g^{n-1}(x), g^n(x)) \leq (1
               + \dotsm + K^{n-1}) d(x,g(x)) < \frac{d(x,g(x))}{1-K}
\end{align*}
Putting these two bounds together we see that $d(g^n(x), g^m(x)) \leq  \frac{K^n
  d(x,g(x))}{1-K}$ hence $g^n(x)$ is Cauchy.  

Since $F$ is
a closed subset of a complete metric space, it is 
complete and we know that $g^n(x)$ converges to some $x_0 \in F$; it remains to show that $x_0$
is a fixed point of $g$.  Let $\epsilon > 0$ be given and chose $N >
0$ such that $d(x_0, g^n(x)) < \epsilon$ for all $n \geq N$.  Then we
know that 
\begin{align*}
d(g(x_0), g^n(x)) &\leq K d(x_0, g^{n-1}(x)) \leq K\epsilon < \epsilon
\end{align*}
for all $n \geq N+1$ which shows that $g^n(x)$ converges to $g(x_0)$.  It
follows that $x_0 = g(x_0)$.
\end{proof}

Note that in a Banach space finding fixed points $f(x) = x$ is
equivalent to finding roots $g(x) = 0$ (just find a fixed point of
$g(x) + x$ to find a root of $g$ and find a root of $f(x) - x$ to find
a fixed point of $f$). 

The Inverse Function Theorem has the following equally important
consequence that is known at the Implicit Function Theorem.

\begin{thm}[Implicit Function
  Theorem]\label{ImplicitFunctionTheorem}Let $X$, $Y$ and $Z$ be a Banach
  spaces, let $U \subset X \times Y$ be an open set  and let $f : U
  \to Z$ be $C^p$.  
Suppose $(x_0,y_0) \in U$, that $f(x_0,y_0) = 0$
  and $Df(x_0,y_0)(0,v)$ defines an invertible map from $Y \to Z$, then
  there exists an open set $V \subset X$ such that $x_0 \in V$, an
  open set $W \subset Y$ such that $y_0 \in W$ and a function $g : V \to
  W$ such that $g$ is continuously differentiable, $f(x, g(x)) = 0$
  for all $x \in V$ and $f(x,y) = 0$ if and only if $y = g(x)$ for all
  $(x,y) \in V \times W$.
\end{thm}
\begin{proof}
First define the map $g : U \to X \times Z$ by $g(x,y) = (x,
f(x,y))$.  We claim that $g$ has a local inverse at $(x_0,y_0)$.  To
see this we compute
\begin{align*}
Dg(x_0,y_0) 
\end{align*}
TODO:
\end{proof}

\subsection{Optimization in Banach Spaces}
As one will undoubtedly remember from one's first calculus class the
derivative is an extraordinarily useful tool for finding maxima and
minima of functions of a real variable.  Essentially all of that
theory carries over to the case of general Banach space domains.  One
of the goals in this subsection is to develop that basic theory.  

In a multivariate calculus course the reader almost certainly
encounterd problems of constrained optimization as well: learning the
tool of the Lagrange multiplier for solving such problems. This theory
also carries over to the Banach space setting and we develop it here.

What one has learned up to this point is the theory of equality
constrained optimization.  Though it tends not to be taught in the
introductory
calculus curriculum, in applications it is equally important to be able to
solve optimization with both equality and inequality constraints.
Having the tools we have developed it is no harder to develop such
theory in the general Banach space setting and we do so here.

We will discuss optimization problems in terms of minimization; as a
general rule there is no loss of generality in doing so as
maximization of a function $f$ may be performed by minimizing the
function $-f$.  

First we distinguish the different kinds of minima that we may
characterize.  The primary distinction is the dichotomy between local
and global minimization.  There are subtler distinctions to be made
between different type of local minima.  The definitions make sense
for arbitrary topological spaces.

\begin{defn}Let $X$ be a topological space and let $f : X \to \reals$
  be a function.  We say that $x^* \in X$ is a \emph{global minimizer}
  of $f$ if $f(x^*) \leq f(x)$ for all $x \in X$.  We say that $x^*
  \in X$ is a \emph{local minimizer} if there exists an open set $U
  \subset X$ such that $x^* \in U$ and $f(x^*) \leq f(x)$ for all $x
  \in U$.  We say that $x^*
  \in X$ is a \emph{strict local minimizer} if there exists an open set $U
  \subset X$ such that $x^* \in U$ and $f(x^*) < f(x)$ for all $x
  \in U$ with $x^* \neq x$.  We say that $x^* \in X$ is an
  \emph{isolated local minimizer} if there exists and open set $U
  \subset X$ such that $x^* \in U$ and $x^*$ is the only local
  minimizer in $U$.
\end{defn}

\begin{examp}Let 
\begin{align*}
f(x) &= \begin{cases}
x^4 \cos(1/x) + 2x^4 & \text{if $x \neq 0$} \\
0 & \text{if $x = 0$}
\end{cases}
\end{align*}
then $0$ is a strict local minimizer that is not isolated.  TODO: Show this
\end{examp}

Note that minimizers are not guaranteed to exist since functions may
be unbounded below.  More subtely may have a lower bound but may never
take the value of its greatest lower bound.  We already know a case in
which both of these problems are avoided: namely continuous images of
compact sets are compact in $\reals$ and this guarantees the exists of
a miminum (Theorem \ref{ContinuousImageOfCompact}).  As it turns out
this fact can be generalized a bit by relaxing the property of
continuity.

\begin{defn}Let $X$ be a topological space and let $f : X \to \reals$
  be a function, we say that $f$ is \emph{lower semicontinuous}
  (resp. \emph{upper semicontinuous}) if for every
  $\epsilon > 0$ there exists an open set $U$ containing $x$ such that
  $f(y) \geq f(x) - \epsilon$ (resp. $f(y) \leq f(x) + \epsilon$) for
  every $y \in U$. We say that $f$ is \emph{sequentially lower
    semicontinuous} (resp. \emph{sequentially upper semicontinuous})
  at $x$ if for every sequence $x_n$ such that $\lim_{n \to \infty}
  x_n = x$ we have $f(x) \leq \liminf_{n \to \infty} f(x_n)$ (resp. $f(x) \geq \limsup_{n \to \infty} f(x_n)$).
\end{defn}

It is simple to see that $f$ is lower semicontinuous
(resp. sequentially lower semicontinuous) if and only if $-f$ is upper
semicontinuous (resp. sequentially upper semicontinuous).

A function is lower (resp. upper) semicontinuous at $x$ if its values near $x$ are
either close to $f(x)$ or larger (resp. smaller) than $f(x)$.  In
general sequential semicontinuity is a weaker property than
semicontinuity (since sequences do not characterize convergence in
general topological spaces); however in metric spaces the two concepts
are equivalent.

\begin{prop}Let $X$ be a topological space and let $f$ be a lower
  (resp. upper) semicontinuous at $x$, then $f$ is sequentially
  lower (resp. upper) semicontinuous at $x$.  If $X$ is a metric space and
  $f$ is sequentially lower (resp. upper) semicontinuous at $x$ then $f$ is
  lower (resp. upper) semicontinuous at $x$.
\end{prop}
\begin{proof}If suffices to handle the cases of lower semicontinuity
  since the upper semicontinuity results follow by applying the lower
  semicontinuity case to $-f$.

If $f$ is lower semicontinuous and $x_n \to x$.  Let $\epsilon > 0$ be
given an find an open neighborhood $U$ of $x$ such that $f(y) \geq
f(x) - \epsilon$ for all $y \in U$.  Since $x_n \to x$ we know that
there exists $N > 0$  such that $x_n \in U$ for $n \geq N$ and thus
$\inf_{m \geq n} f(x_m) \geq f(x) - \epsilon$ for every $n \geq N$.
We take the limit at $n \to \infty$ to get $\liminf_{n \to \infty}
f(x_n) \geq f(x) - \epsilon$.  Since $\epsilon > 0$ was arbitrary we
conclude that $f$ is sequentially lower semicontinuous at $x$.

Now let $X$ be a metric space and suppose that $f$ is not
lower semicontinuous at $x$.  Then there exists an $\epsilon > 0$ such
that for every $n \in \naturals$ there exists $x_n$ with $d(x,x_n) <
1/n$ such that $f(x_n) < f(x) - \epsilon$.  Clearly $x_n \to x$ and
moreover $\liminf_{n \to \infty} f(x_n) \leq f(x) - \epsilon < f(x)$
which shows that $f$ is not sequentially lower semicontinuous at $x$.
\end{proof}

Compactness and sequential lower semicontinuity suffice to show that
a function has a global minimizer.
\begin{thm}Let $X$ be a topological space, let $f : X \to [-\infty,
  \infty]$ be a sequentially lower
  semicontinuous function and suppose that there exists $M \in \reals$
  such that $\lbrace x \in X \mid f(x) \leq M \rbrace$ is non-empty
  and compact then
  $f$ has a global minimizer.  Moreover if $f$ is lower
  semicontinuous, the set of global minimizers
  is compact.
\end{thm}
\begin{proof}
We first show the existence of a global minimizer.  Let $\alpha = \inf_{x \in X} f(x)$ and note that we know $\alpha \leq
M < \infty$.  If $\alpha = M$ then in fact $\lbrace x \in X \mid f(x)
\leq M \rbrace  = \lbrace x \in X \mid f(x) = M \rbrace $ and is assumed
non-empty and we are done.  Therefore we assume that $\alpha < M$.  Let $x_n$ be chosen so that
$\lim_{n \to \infty} f(x_n) = \alpha$ (if $\alpha > -\infty$ then
choose $x_n$ such that $f(x_n) < \alpha + 1/n$ otherwise choose $x_n$
so that $f(x_n) \leq -n$).  As $\alpha < M$ we know that there exists
$N>0$ such that $f(x_n) \leq M$ for all $n \geq N$ and therefore by
compactness there exists a convergent subsequence $x_{n_j}$.  Let $x =
\lim_{j \to \infty} x_{n_j}$ and note that by sequential lower semicontinuity
at $x$
\begin{align*}
\alpha \leq f(x) \leq \liminf_{j \to \infty} f(x_{n_j}) = \alpha
\end{align*}
which shows that $f(x) = \alpha$.

Let $G = \lbrace x \in X \mid f(x) = \alpha \rbrace$.  Now we know
that since $\alpha \leq M$  that $G \subset \lbrace x \in X \mid f(x)
\leq M \rbrace$ hence it suffices to show that the set of global
minimizers is closed (Corollary \ref{ClosedSubsetsCompact}).  Let $x$
be in $\overline{G}$ and let $\epsilon > 0$ be given.  Since $f$ is
lower semicontinuous we can find an open set $U$ containing $x$ such
that $f(y) \geq f(x) - \epsilon$.  However we know that $G \cap U \neq
\emptyset$ therefore $\alpha \geq f(x) - \epsilon$.  Since $\epsilon$
is arbitrary we conclude $\alpha \geq f(x)$ and thus $x \in G$.  
\end{proof}


Given that derivatives are determined by the behavior of functions on
arbitrarily small neighborhoods of a point it is clear that they have
little to say about when a point is a global minimizer.  On the other
hand derivatives are rather informative about local minimizers and we
turn our attention to this.

\subsubsection{Unconstrained Optimization}

The first thing to do is to note that there are \emph{necessary}
conditions for a point being a local minimizer that are described by
derivatives.  The first such is the vanishing of the first derivative.

\begin{thm}\label{VanishingFirstDerivativeAtLocalMinimum}Let $X$ be a
  Banach space, let $f : X \to \reals$ be a function and let $x^*$ be
  a local minimum.  If $f$ is $C^1$ on an open neighborhood of $x^*$
  then $Df(x^*) = 0$.
\end{thm}
\begin{proof}
The proof is by contradiction.  Suppose that $Df(x^*) \neq 0$.  Thus
there exists $y \in X$ such that $Df(x^*)y > 0$.  Let $f$ be $C^1$ on
an open neighborhood $U$ of $x^*$.  By continuity of $Df(x)$ on $U$ we
may also find a $\delta > 0$ such that $Df(x) y > 0$ for all $x \in
B(x^*, \delta) \subset U$.  By multiplying $y$ by an
appropriate positive constant we may assume that $\norm{y} < \delta$.  Now we can
apply Taylor's Theorem to conclude that 
\begin{align*}
f(x^* - y) &= f(x^*) - \int_0^1 Df(x^*-ty) y \, dt < f(x^*)
\end{align*}
which shows that $x^*$ is not a local minimizer.

Here is an alternative proof that avoid appealing to Taylor's Theorem.
Find an open ball $\delta > 0$ such that $f(x^*) \leq f(y)$
for all $y$ with $\norm{y - x} < \delta$.  Pick an arbitrary $y \in X$
then for all $0 < t < \delta/\norm{y}$ we have $f(x^* + t y) - f(x^*)
\geq 0$ which implies 
\begin{align*}
Df(x) y &= \lim_{t \to 0} \frac{f(x^* + ty) - f(x^*)}{t} \geq 0
\end{align*}
On the other hand applying the argument to $-y$ and using linearity of
$Df(x)$ show that $Df(x) y = -Df(x) (-y) \leq 0$ and therefore $Df(x)y
= 0$.
\end{proof}

When $f$ has two derivatives then we can say even more.
\begin{thm}\label{PositiveSemidefiniteSecondDerivativeAtLocalMinimum}Let $X$ be a
  Banach space, let $f : X \to \reals$ be a function and let $x^*$ be
  a local minimum.  If $f$ is $C^2$ on an open neighborhood of $x^*$
  then $Df(x^*) = 0$ and $D^2f(x^*)$ is positive semidefinite
  (i.e. $D^2f(x^*) (v,v) \geq 0$ for all $v \in X$).
\end{thm}
\begin{proof}
Again we proceed by contradiction.  Suppose that $D^2f(x^*)(v,v) < 0$.
By continuity of $D^2f$ we may find a $\delta > 0$ such that $D^2f(x)(v,v) < 0$ for
all $x \in B(x^*,\delta) \subset U$.  If necessary multiply $v$ be a
small positive constant to guarantee that $\norm{v} < \delta$.  By Theorem \ref{VanishingFirstDerivativeAtLocalMinimum} we know that
$Df(x^*) = 0$ so Taylor's Theorem says
\begin{align*}
f(x^* + v) &= f(x^*) + \int_0^1 (1 - t) D^2f(x^* + tv) (v,v) \, dt < f(x^*)
\end{align*}
which is a contradiction.
\end{proof}

When $f$ has two derivatives there also exists sufficient conditions
that a point be a local minimizer.
\begin{thm}\label{LocalMinimumAtPositiveDefiniteSecondDerivative}Let $X$ be a
  Banach space, let $f : X \to \reals$ be a function and suppose $f$
  is $C^2$ on an open neighborhood $U$ of $x^*$.
  If $Df(x^*) = 0$ and $D^2f(x^*)$ is positive definite
  (i.e. there exists an $\alpha > 0$ such that $D^2f(x^*) (v,v) >
  \alpha \norm{v}^2$ for all $v \in X$ with $v \neq 0$) then
  $x^*$ is a strict local minimizer of $f$.
\end{thm}
\begin{proof}
Using continuity of $D^2f$ at $x^*$ we may find a $\delta > 0$ such
that $B(x^*,
\delta) \subset U$ and $\norm{D^2f(x^* + y) - D^2f(x^*)}
< \frac{\alpha}{2}$ for all $\norm{y} < \delta$.  Note in particular
that 
\begin{align*}
(D^2f(x^* + y) - D^2f(x^*) )(v,w) &> -\frac{\alpha \norm{v}
  \norm{w}}{2} \text{ for all $\norm{y} < \delta$ and $v,w \in X$}
\end{align*}  
By Taylor's Theorem, 
for all $y$ with $\norm{y} < \delta$
\begin{align*}
f(x^* + y) - f(x) &= 
\int_0^t (1-t) D^2f(x^* + ty)(y,y) \, dt \\
&=\frac{1}{2} D^2f(x^*)(y,y) + \int_0^t (1-t) (D^2f(x^* + ty)
  -D^2f(x^*)) (y,y) \, dt  \\
&\geq \frac{\alpha \norm{y}^2}{2} - \frac{\sup_{0 \leq t \leq 1}
  \norm{D^2f(x^* + ty) - D^2f(x^*)} \norm{y}^2}{2} \\
&\geq  \frac{\alpha
  \norm{y}^2}{4} > 0\\
\end{align*}
which shows that $x^*$ is a local minimizer.
\end{proof}

Note that in finite dimensions the condition of positive definiteness
is equivalent to the apparently weaker condition $D^2f(x^*) (v,v) > 0$
for all $v \neq 0$.

\subsubsection{Constrained Optimization}

We first consider an abstract version of constrained
optimization.  Let $X$ be a Banach space and consider a function $f :
X \to \reals$ then given a closed set $F \subset X$ we can consider
the problem of finding a minimizer of $f$ restricted to $F$.  Note
that the meaning of finding a constrained minimizer is captured by
using our existing definitions of minimizers on the space $F$ with the
relative topology.

In order
to apply derivatives to the problem of characterizing minimizers on
$F$ we need to restrict them to directions that don't leave $F$;
if we have a point $x \in F$ and $f$ is decreasing at $x$ in a direction that immediately takes one out of
$F$ then that alone won't means that $x$ isn't a minimizer when
restricted to $F$.  This leads us to a definition of direction tangent
to a closed set $F$.  Note that if a direction is tangent to a set at
a point then any positive multiple should be tangent (though if the
set has corners then negative multiples may fail to be tangents;
consider the behavior of $\abs{x}$ at the origin).  As a result of
this observation we should be seeking to characterize a cone of
tangent directions.  
\begin{defn}Let $X$ be a Banach space, let $F$ be a closed subset and
  let $x \in F$ then we say that $v \in X$ is a \emph{tangent vector to $F$
  at $x$} if there is a sequence $x_n$ such that $x_n \in F$ and
  $\lim_{n \to \infty} x_n = x$ and a sequence of positive real numbers
  $t_n$ such that $\lim_{n \to \infty} t_n = 0$ that together satisfy
\begin{align*}
\lim_{n \to \infty} \frac{x_n - x}{t_n} = v
\end{align*}
The set $T_F(x)$ of all tangent vectors to $F$ at $x$ is called the
\emph{tangent cone to $F$ at $x$}.
\end{defn}

We call out the fact that the tangent cone is in fact a cone.
\begin{prop}The tangent cone $T_F(x)$ is a cone (i.e. for every
  $\alpha \geq 0$ and $v \in T_F(x)$ we have $\alpha v \in T_F(x)$).
\end{prop}
\begin{proof}
It is trivial to see that $0 \in T_F(x)$ since we can just pick the
$x_n \equiv x$.  Let $v \in T_F(x)$, $\alpha > 0$ and pick sequences $x_n$ and $t_n$ such that $x_n \to x$, $t_n \to 0$ and
$\frac{x_n - x}{t_n} = v$.  Then let $\tilde{t}_n = t_n/\alpha$ and note that
$\tilde{t}_n \to 0$ and $\frac{x_n -x}{\tilde{t}_n} = \alpha v$.
\end{proof}

It is also useful to note the following equivalent form of the definition of a tangent vector.
\begin{prop}Let $X$ be a Banach space, let $F$ be a closed subset and
  let $x \in F$ then = $v \in X$ is a tangent vector to $F$
  at $x$ if and only if
\begin{align*}
\liminf_{t \to 0^+} \frac{d(F, x+tv)}{t} &= 0
\end{align*}
\end{prop}
\begin{proof}
Suppose $\liminf_{t \to 0^+} \frac{d(F, x+tv)}{t} = 0$.  Since $\frac{d(F, x+tv)}{t}$ is non-negative it is equivalent that $\inf_{0 < t < h} \frac{d(F, x+tv)}{t} = 0$ for every $h >0$.  Thus for every $n \in \naturals$ there exists $0 < t_n < 1/n$
and $x_n \in F$ such that 
\begin{align*}
\frac{d(x_n, x+t_n v)}{t_n} &= \norm{\frac{ x_n - x}{t_n}-v} < 1/n
\end{align*}
By construction, $\lim_{n \to \infty} t_n = 0$ and by the triangle inequality, 
\begin{align*}
\lim_{n \to \infty} \norm{x-x_n} &< \lim_{n \to \infty} (\norm{v}t_n + t_n/n) = 0
\end{align*}
thus $v$ is a tangent vector.

If $v$ is a tangent vector to $F$ at $x$ then pick $x_n \in F$ and $t_n$ such that $\lim_{n \to \infty} x_n = x$ and $\lim_{n \to \infty} (x-x_n)/t_n = v$.  Let $\epsilon>0$ be 
given and pick an $N>0$ such that $\norm{x - x_n -t_n v}/t_n < \epsilon$ for all $n \geq N$.  Then since $\lim_{n \to \infty} t_n = 0$ it follows that for any $h>0$ there exist
$0 < t_n < h$ such that $\norm{x - x_n -t_n v}/t_n < \epsilon$ which implies 
$\inf_{0 < t < h} \frac{d(F, x+tv)}{t} = 0$ for all $h > 0$.
\end{proof}

The first hint that we have the correct notion of tangent vector is
the following necessary condition for a local minimizer to exist.

A few facts about Landau notation.
\begin{defn}Let $X$, $Y$ and $Z$ be Banach spaces.  Let $x_n$ be a sequence in $X$ and let
  $y_n$ be a sequence in  $Y$ we say that $x_n = o(y_n)$
  as $n \to \infty$ if $\lim_{n \to \infty} \frac{x_n}{\norm{y_n}} =
  0$ (equivalently $\lim_{n \to \infty} \frac{\norm{x_n}}{\norm{y_n}} =
  0$).  We say that $x_n = O(y_n)$ if there exists $M > 0$ and $N \geq
  0$ such that
  $\frac{\norm{x_n}}{\norm{y_n}}  \leq M$ for all $n \geq N$.  
Given functions $f : X \to Y$, $g : X \to Z$ and $x_0 \in X$ we say that
  $f(x)$ is $o(g(x))$ as $x \to x_0$ if $\lim_{x \to x_0}
  \frac{f(x)}{\norm{g(x)}} = 0$ (equivalently $\lim_{x \to x_0}
  \frac{\norm{f(x)}}{\norm{g(x)}} = 0$) and we say that $f(x)$ is
  $O(g(x))$ if there exists $M > 0$ and $\delta > 0$ such that
  $\frac{\norm{f(x)}}{\norm{g(x)}} \leq M$ for all $\norm{x - x_0} < \delta$.
\end{defn}
Because the definitions above really only depend on the norms of the
sequences and functions in question, it is often useful to say that a
sequence $x_n \in X$ is $o(\norm{y_n})$ or a function $f(x)$ is
$o(\norm{g(x)})$.  It is also worth pointing out that Landau notation
is confusing for uninitiated in large part because of its abuse of the
equality sign.  

\begin{prop}\label{LandauNotationIdentities}The following are true:
\begin{itemize}
\item[(i)] $o(y_n) + o(y_n) = o(y_n)$.
\item[(ii)] Suppose $z_n = O(y_n)$ then if $x_n = o(z_n)$ it follows
  that $x_n = o(y_n)$.  In shorthand we say that $o(O(y_n)) = o(y_n)$.
\end{itemize}
\end{prop}
\begin{proof}
(i) follows from linearity: if $x_n = o(y_n)$ and $z_n = o(y_n)$ then
it follows that 
\begin{align*}
\lim_{n \to \infty} \frac{x_n + z_n}{\norm{y_n}} &= 
\lim_{n \to \infty} \frac{x_n}{\norm{y_n}}
+ \lim_{n \to \infty} \frac{z_n}{\norm{y_n}}
= 0
\end{align*}
To see (ii), we know that $\lim_{n \to \infty} \frac{\norm{x_n}}{\norm{z_n}} =
0$ and there exist $M,N \geq 0$ such that $\norm{z_n} \leq M
\norm{y_n}$ for all $n \geq N$, therefore 
\begin{align*}
0 &\leq \lim_{n \to \infty} \frac{\norm{x_n}}{\norm{y_n}} \leq M \lim_{n \to \infty}
\frac{\norm{x_n}}{\norm{z_n}} = 0
\end{align*}
\end{proof}


\begin{thm}\label{LocalConstrainedMinimizerFirstDerivative}Let $X$ be a Banach space, $F \subset X$ be closed and let
  $f : U  \to \reals$ be $C^1$ on an open set $U \supset F$.  Then if
  $x^*$ is a local minimizer of $f$ on $F$ we have $Df(x^*) v \leq 0$
  for all $v \in T_F(x^*)$.  
\end{thm}
\begin{proof}
Suppose that we have $x_n \in F$ with $x_n \to x$, $t_n>0$ with $t_n \to 0$
and $(x_n - x)/t_n \to v$.  By Taylor's Theorem and the fact that
$x^*$ is a local minimizer we know that we
can find a neighborhood $x^* \subset V \subset U$ such that
\begin{align*}
f(y)  - f(x^*) = Df(x^*) (y - x) + o(\norm{y - x^*}) \leq 0
\end{align*}
and for $y \in V$.  From the fact that $x_n \to x$ we can find an $N \in \naturals$ such
that $x_n \in V$ for all $n \geq N$.  Since $(x_n - x)/t_n \to v$ we
know that $\norm{x_n -x^*}$ is $O(t_n)$ and therefore $o(\norm{x_n -
  x})$ is $o(t_n)$ (Proposition \label{LandauNotationIdentities}) and since $x_n - x -t_nv$ is $o(t_n)$ we have
\begin{align*}
t_n Df(x^*) v + o(t_n) &= f(x_n) - f(x^*) \leq 0
\end{align*}
which implies that $Df(x^*) v \leq 0$ (divide by $t_n>0$ and let $n
\to \infty$).
\end{proof}

TODO: Define the normal cone(s)...
\begin{defn}Let $X$ be a Hilbert space, $F \subset X$ be closed and let
  $x \in F$ then we say that $v \in X$ is a \emph{regular normal
    vector to $F$ at $x$} if $\langle v, w \rangle \leq 0$ for all $w
  \in T_F(x)$.  The set of all regular normal vectors to $F$ at $x$ is
 called the \emph{regular normal cone} and is denoted
 $\widehat{N}_F(x)$.  We say that $v \in X$ is a \emph{limiting normal vector to $F$
    at $x$} or simply a \emph{normal vector to $F$
    at $x$} if there are sequences $x_n$, $v_n$ with $v_n$ a regular
  normal vector to $F$ at $x_n$ and $\lim_{n \to \infty} x_n = x$ and $\lim_{n \to \infty}
  v_n =v$.
\end{defn}

Facts:

The proximal normal cone is a convex cone but may not be closed
The limiting normal cone is a closed cone but may not be convex.
The limiting normal cone may be defined as the limit of proximal
normal vectors as well as by using regular normal vectors.

\begin{defn}Let $X$ be a Hilbert space, $F \subset X$ be closed and let
  $x \in F$ we say that $F$ is \emph{Clarke regular at $x$} if $F$ is locally closed at $x$ and
$\widehat{N}_F(x) = N_F(x)$.
\end{defn}

\begin{prop}\label{RegularNormalLittleO}$\widehat{N}_F(x)$ is a closed convex cone.  Moreover, $v$ is a regular normal to $F$ at $x$ if and only if
  $\langle v, y-x \rangle \leq o(y -x)$ for $y \in F$ and $x \neq y$.  That is to say
  for every sequence $x_n \in F$ with $x_n \neq x$ and $\lim_{n \to \infty} x_n = x$, we have 
\begin{align*}
\limsup_{n \to \infty} \frac{\langle v, x_n-x \rangle}{\norm{x_n - x}}
&\leq 0
\end{align*}
\end{prop}
\begin{proof}
The fact that $\widehat{N}_F(x)$ is a closed convex cone follows from
the fact that is is an intersection of closed halfspaces.

Suppose that there exists an sequence $x_n \in F$ with $x_n \neq x$,
$x_n \to x$ and 
\begin{align*}
\limsup_{n \to \infty} \frac{\langle v, x_n-x \rangle}{\norm{x_n - x}}
&> 0
\end{align*}
By passing to a subsequence we may assume that the $\limsup$ may be
replaced by a limit.  Let $w_n = \frac{x_n - x}{\norm{x_n - x}}$ and
  by compactness we may pass to a further
subsequence and assume that $w_n$ converges
  to a unit vector $w$ and $\langle v,w \rangle = \lim_{n \to \infty} \langle v,w_n \rangle > 0$. 
On the other hand, $w$ is seen to be a tangent vector to $F$ at $x$ because it is
the limit $\frac{x_n - x}{\norm{x_n - x}}$ and $\norm{x_n -x} \to 0$.

Now suppose that $\langle v, y-x \rangle \leq o(y -x)$ and let $w \in T_F(x)$.  Pick a defining
sequence $x_n \to x$ with $x_n \in F$ and $t_n \downarrow 0$ such that $\lim_{n \to \infty} \frac{x_n - x}{t_n} = w$.
\begin{align*}
\langle v,w \rangle &= \lim_{n \to \infty} \langle v, \frac{x_n - x}{t_n} \rangle \\
&\leq \limsup_{n \to \infty} \langle v, \frac{x_n - x}{\norm{x_n- x}} \rangle \lim_{n \to \infty} \frac{\norm{x_n - x}}{t_n} \\
&= \norm{w} \limsup_{n \to \infty} \langle v, \frac{x_n - x}{\norm{x_n- x}} \rangle \leq 0\\
\end{align*}
\end{proof}

\begin{thm}$v$ is a regular normal to $F$ at $x$ if and only if there exists a function $f$ differentiable at $x$ such that 
$f$ has a local minimum on $F$ at $x$ and $\nabla f (x) = v$.  In fact, we can choose $f$ be differentiable on all of $\reals^n$ 
with a global minimum at $x$.
\end{thm}
\begin{proof}
TODO:  
Let $v \in \widehat{N}_F(x)$ be given.  The first step to building $f$ is to define
\begin{align*}
\theta_0(r) = \sup \lbrace \langle v, y - x \rangle \mid y \in F \text{ and } \norm{y-x} \leq r \rbrace
\end{align*}
It is simple to see that $\theta_0$ is a non-decreasing function of $r$,  $\theta_0(0) = 0$ and
$\theta_0(r) \leq \norm{v} r$ so $\lim_{r \downarrow 0} \theta_0(r) = 0$. 
Moreover from Proposition \ref{RegularNormalLittleO} we get for any $\epsilon > 0$ there exists a $\delta > 0$ such that
$\langle v, y-x \rangle \leq \epsilon \norm{y-x}$ for $y \in F$ and $\norm{y-x} \leq \delta$.    Thus for $0 \leq r \leq \delta$, 
\begin{align*}
\theta_0(r)&= \sup \lbrace \langle v, y - x \rangle \mid y \in F \text{ and } \norm{y-x} \leq r \rbrace \\
&\leq \sup \lbrace \epsilon \norm{y-x} \mid y \in F \text{ and } \norm{y-x} \leq r \rbrace \\
&\leq \epsilon r \\
\end{align*}
and thus $\theta_0(r) = o(r)$.
Now let 
\begin{align*}
h_0(y) &= \langle v, y-x\rangle - \theta_0(\norm{y-x})
\end{align*}
and note since $\theta_0(r) = o(r)$,
\begin{align*}
\lim_{w \to 0} \frac{h_0(x+w) - h_0(x) - \langle v,w \rangle}{\norm{w}} =\lim_{w \to 0} \frac{-\theta_0(\norm{w})}{\norm{w}} =0\\
\end{align*}
which shows $h_0$ is differentiable at $x$ and moreover $\nabla h_0(x) = v$.
\end{proof}

\begin{defn}Let $X$ be a Hilbert space, $F \subset X$ be closed and let
  $x \in F$ then we say that $v \in X$ is a \emph{proximal normal
    vector to $F$ at $x$} if there exists an $M \geq 0$ such that
  $\langle v, y - x \rangle \leq M \norm{y - x}$ for all $y \in F$.
\end{defn}

The following is (supposed to be) the interpretation of a proximal
normal vector: $v$ is a proximal normal vector $F$ at $x$ if there
exists a $y \in X$ such that $x$ is the closest point in $F$ to $y$
and there exists $c > 0$ such that $v = c(y -x)$.  Also an interpretation is that
$v$ is a proximal normal when there exists $y$ and $c \geq 0$ with $v = cy$ and
a closed ball $B(y,r)$ with $B(y,r) \cap F = \lbrace x \rbrace$.

For computational purposes (and in particular for numerical
optimization problems) we are given a constraint set in some concrete
form rather than the abstract formulation we've used.  In practice it
is useful to formulate a constraint set using a combination of
equalities and inequalities.  For the moment we specialize to the case
of finite dimensions.  Let $X$ be a finite dimensional Banach space
(i.e. $\reals^n$) and suppose we are given finite sets $\mathcal{E}$
(the \emph{equality constraints})
and $\mathcal{I}$ (the \emph{inequality constraints}) and for each $i
\in \mathcal{E} \cup \mathcal{I}$ we have a $C^1$ 
function $c_i : X \to \reals$.  Let $f : X \to \reals$ be a $C^1$
function and we consider the constrained minimization problem for $f$
with constraint set 
\begin{align*}
F &= \lbrace x \in X \mid c_i(x) = 0 \text{ for all $i \in
    \mathcal{E}$ and } c_i(x) \geq 0 \text{ for all $i \in
    \mathcal{I}$} \rbrace
\end{align*}
It is clear from the continuity of the $c_i(x)$ that $F$ is closed and
therefore we have the first order necessary condition of Theorem
\ref{LocalConstrainedMinimizerFirstDerivative} for local minimizers of
$f$ restricted to $F$.  What we seek are
conditions in terms of $f$ and the $c_i$ that are implied by the
conditions in Theorem \ref{LocalConstrainedMinimizerFirstDerivative};
for that we need to understand how $T_F(x)$ might be expressed in
terms of $f$ and the $c_i$.  To that end, we first have the following
definitions.
\begin{defn}Given a Banach space $X$, disjoint sets $\mathcal{E}$ and
  $\mathcal{I}$, functions $c_i : X \to \reals$ for each $i \in
  \mathcal{E} \cup \mathcal{I}$ and the set
\begin{align*}
F &= \lbrace x \in X \mid c_i(x) = 0 \text{ for all $i \in
    \mathcal{E}$ and } c_i(x) \geq 0 \text{ for all $i \in
    \mathcal{I}$} \rbrace
\end{align*}
we say that a constraint $c_i$ is \emph{active} at $x \in F$ if either
$i \in \mathcal{E}$ or $i \in \mathcal{I}$ and $c_i(x) = 0$.  For each
$x \in F$ we let the \emph{active constraint set} be 
\begin{align*}
\mathcal{A}(x) &= \lbrace i \in \mathcal{E} \cup \mathcal{I} \mid i
                 \text{ is active at $x$} \rbrace
\end{align*}
Assume that the $c_i$ are continuously differentiable, then the set of \emph{linearized feasible directions at $x$} is defined to
be 
\begin{equation*}
\mathcal{F}(x) =
\left \{ v \in X \mid
\begin{aligned}
Dc_i(x) v = 0 & \text{ for all $i \in \mathcal{E}$} \\
Dc_i(x) v \geq 0 & \text{ for all $i \in \mathcal{A}(x) \cap \mathcal{I}$} \\
\end{aligned}
\right \}
\end{equation*}
\end{defn}
Note that it is trivial to see that $\mathcal{F}(x)$ is a cone.  The
first thing is to note that every tangent vector is a linearized
feasible direction.

\begin{prop}$T_F(x) \subset \mathcal{F}(x)$.
\end{prop}
\begin{proof}
Let $v \in T_F(x)$ and pick a feasible sequence $x_n \to x$ and
sequence of positive numbers $t_n
\to 0$ such that $x - x_n = t_nv + o(t_n)$.  Applying Taylor's Theorem
we can conclude that 
\begin{align*}
c_i(x_n) &= c_i(x) + Dc_i(x) (x_n - x) + o(\norm{x_n-x}) \\
&= c_i(x) + t_n Dc_i(x) v + o(t_n) 
\end{align*}
so if $i \in \mathcal{A}(x)$ we have $c_i(x_n) = t_n Dc_i(x) v +
o(t_n)$. Dividing by $t_n$ and taking the limit as $n \to
\infty$ we get $Dc_i(x) v = \lim_{n \to \infty}
\frac{c_i(x_n)}{t_n}$.  Thus it follows that  $i \in \mathcal{E}$
implies $Df(x) v = 0$ and $i \in \mathcal{A}(x) \cap \mathcal{I}$
implies $Df(x) v \geq 0$.
\end{proof}
It is not true in general that $T_F(x) = \mathcal{F}(x)$ yet the
result that we want to demonstrate requires that this equality holds.
A set of conditions that we place on the $c_i$ that guarantees such an
equality is called a \emph{constraint qualification}; more generally a
constraint qualification may a bit weaker than that and simply imply
that $T_F(x)$ and $\mathcal{F}(x)$ aren't too different.  There are a
variety of choices of constraint qualifications we state a
conceptually straightforward and useful one.
\begin{defn}A set of constraints $c_i$ satisfies the \emph{linearly
    independent constraint qualification (LICQ)} at $x$ if the set of
  derivatives $\lbrace Dc_i(x) \rbrace$ for $i \in \mathcal{A}(x)$ is
  linearly independent in $X^*$.
\end{defn}
The LICQ is a sufficient criterion for the equality of the tangent
cone and the linearized feasible set.

\begin{examp}Consider the set $F \subset \reals^2$ defined by the
  constraints $c_1(x,y) = 1 - x^2 - (y-1)^2 \geq 0$ and $c_2(x,y) =
  -y \geq 0$.

TODO: Show that $T_F(x)$ is a strict subset of $\mathcal{F}(x)$.
\end{examp}

\begin{prop}Let $F$ be defined by a set of constraints $c_i(x)$ which
  satisfy the LICQ at $x$ then $T_F(x) = \mathcal{F}(x)$.
\end{prop}
\begin{proof}
Let $v \in \mathcal{F}(x)$, we need to show that $v$ is a tangent
vector producing a sequence $x_n \in F$ and $t_n > 0$ such that $v = x_n + o(t_k)$. 
By assumption the set of derivatives $Dc_i(x)$ for $i \in
\mathcal{A}(x)$ is linearly independent hence is a basis for the linear 
span $V = \lbrace Dc_i(x) \rbrace_{i \in \mathcal{A}(x)}$.  Let $m$ be the cardinality of $\mathcal{A}(x)$ and $c : X \to \reals^m$ 
be defined by $c(y) = (c_{i_1}(y), \dotsc, c_{i_m}(y))$ where $\lbrace i_1, \dotsc, i_m \rbrace = \mathcal{A}(x)$.
Take the orthogonal complement $W$ of $V$ 
in $X^*$ and pick a basis $w_j$ for $W$ and let $w : X \to \reals^{n-m}$ be defined by $w(y) = (w_1(y), \dotsc, w_{n-m}(y))$.  Now define $R : X \times \reals \to X$ by
\begin{align*}
R(y,t) &= \begin{bmatrix}
c(y) - t Dc(x) v \\
w(y - x - tv)
\end{bmatrix}
\end{align*}
and note that $R(x,0) = 0$.  Moreover 
\begin{align*}
DR(x,0)(u, 0) &= \begin{bmatrix}
Dc(x) u \\
w(u)
\end{bmatrix}
\end{align*}
which is invertible by construction of $w$.  Now we can apply the Implicit Function Theorem \ref{ImplicitFunctionTheorem} to 
conclude that there exists and $\epsilon > 0$ and a function $f : (-\epsilon, \epsilon) \to X$ such that $f(0) = x$, $R(f(t), t) = 0$ 
for all $-\epsilon < t < \epsilon$ and moreover $f(t)$ is the unique solution to the equation $R(x,t) = 0$.  In addition note that
since we have assumed that $v \in \mathcal{F}(x)$ we have from $R(f(t), t) = 0$,
\begin{align*}
c_i(f(t)) &= t Dc_i(x) v = 0 \text{ for $i \in \mathcal{E}$} \\
c_i(f(t)) &= t Dc_i(x) v >= 0 \text{ for $t > 0$ and $i \in \mathcal{A}(x)\cap \mathcal{I}$} \\
\end{align*}
and moreover by continuity we have $c_i(f(t)) > 0$ for $i \in \mathcal{I} \setminus \mathcal{A}(x)$ (here we may need to shrink $\epsilon$ for this to be true.  Thus we have $f(t) \in F$.  



Now pick any sequence $0 < t_n < \epsilon$ with $\lim_{n \to \infty} t_n = 0$ and define $x_n = f(t_n)$; by continuity of $f$ and and the 
fact that $f(0) = x$ we have $x_n \to x$.  If we Taylor expand $R(y,t)$ around $(x,0)$ we get
\begin{align*}
0 &= f(x_n, t_n) = \begin{bmatrix}
Dc(x) (x_n - x - t_n v) \\
w(x_n - x - t_n v)
\end{bmatrix}
+ o(\norm{(x_n,t_n) - (x,0)})
\end{align*}
Since $x_n = f(t_n)$ and $f$ is differentiable it follows that $x_n - x = O(t_n)$ and therefore $o(\norm{(x_n,t_n) - (x,0)}) = o(t_n)$.  Thus by
considering the first component of the vector we have $Dc(x)  (x_n - x - t_n v)  = o(t_n)$ and since 
$Dc(x)$ is invertible we get that $x_n - x -t_n v = o(t_n)$ which shows that $v \in T_F(x)$.
\end{proof}

\subsubsection{Algorithms for Unconstrained Optimization}

We have developed criteria for detecting minimizers (mostly local)
however we have not yet addressed the issue of how we might find one.
There are two basic paradigms to consider: line search and trust
region methods.  We first consider line search.

For motivation we give an interpretation of the Frechet derivative of
a real valued function on a Hilbert space $X$.  Since $Df(x)$ is a
bounded linear functional on $X$, we know by Reisz
representation that there is a unique element of $X$ representing the
functional.

\begin{defn}Let $X$ be a Hilbert space, $U \subset X$ be open and let
  $f : U \to \reals$ be differentiable at $x \in U$.  The
  \emph{gradient of $f$ at $x$} is the unique element $\nabla f(x)$ of
  $X$ such that $\langle \nabla f(x), v \rangle = Df(x) v$ for all $v
  \in X$.
\end{defn}

We now proceed to interpret the vector $-\nabla f(x)$ as the
direction of steepest decrease of the function $f$.  Suppose
that we are at a point $x$ for which $\nabla f(x) \neq 0$.  To see
this, let $v \in X$ be an arbitrary unit vector in $X$ and consider the function of a single
real variable $g_v(t) = f(x + tv)$.  The question we ask is what is the
direction $v$ along which $f$ is decreasing the fastest at $x$.  By the Chain Rule, the definition of the
gradient and Taylor's Theorem we can write
\begin{align*}
f(x + tv) = f(x) + t \langle \nabla f(x), v \rangle + o(t)
\end{align*}
which implies that $g_v^\prime(0) = \langle \nabla f(x), v \rangle$.  So
what we want is to find the unit vector $v$ which minimizes the value
of $g_v^\prime(0)$.  We can write $v = \alpha \nabla f(x)/\norm{\nabla
  f(x)} + w$ where $\langle \nabla f(x), w \rangle = 0$.  Note that on
the one hand $\langle \nabla f(x), v \rangle = \alpha/\norm{\nabla
  f(x)}$ and on the other hand from $\norm{v} = 1$ we see that $-1
\leq \alpha \leq 1$.  Therefore it is clear that the minimum of
$g_v^\prime(0)$ occurs for $\alpha = -1$ which implies that $v = -
\nabla f(x) /\norm {\nabla f(x)}$. It is colloquial to say that the direction of
the gradient is the \emph{direction of steepest descent of $f$}.  Note
that the computation above shows that $g_v^\prime(0) < 0$ precisely
when $\langle \nabla f(x), v \rangle < 0$ which motivates the
following defintion
\begin{defn}Let $X$ be a Hilbert space, $U \subset X$ be open and let
  $f : U \to \reals$ be differentiable at $x \in U$ we say that $v \in
  X$ is a \emph{descent direction for $f$ at $x$} if $\langle \nabla
  f(x) , v\rangle < 0$.
\end{defn}


TODO: Discuss gradient flow in the Hilbert space and observe how the
solutions of the differential equation have limit points equal to the
stationary points of $f$.

Armed with the idea that when we are in possesion of derivatives we can
find directions along which the values of a function decreases, we
seek find an iterative algorithm for minimization.  The obvious idea
is that if at a given point $x_k$ we can find a descent direction
(e.g. the gradient $\nabla f(x_k)$ then we should move in that
direction and thereby expect that the function decreases.  There are
three problems to address about such an algorithm.  The first issue is
that the descent direction is characterized by an infinitessimal
condition and therefore there is no guarantee that a finite step in
that direction will result in a decrease in the function value.  The
second issue is that if our step sizes in the descent direction are
too small asymptotically we may never reach the minimum.  The third
issue is that if we choose a variable descent direction, the descent
direction may get increasing close to being orthogonal to the gradient
in which case function values may not decrease enough to converge
(note this is a non-issue is we choose the steepest descent direction).  We
seek conditions on the choice of step sizes and descent directions
that give us convergence to a stationary point of $f$.



\section{Linear Algebra}

This is a little refresher on linear algebra with more of a focus on matrix factorizations.

\begin{defn}Let $\mathds{F}$ be a field, then a \emph{vector space} over $\mathds{F}$ is an abelian group $(V,+,0)$ together with 
a multiplication operator $\mathds{F} \times V \to V$ such that $a (v + w) = av + aw$.
\end{defn}

\begin{defn}If $W \subset V$ is a vector space then we say that $W$ is a \emph{subspace} of $V$.  We say that a set of elements  $v_1, \dotsc, v_n$ of $V$ is \emph{linearly independent} if and only if $a_1 v_1 + \dotsb + a_n v_n = 0$ implies
$a_1 = \dotsb = a_n = 0$.  If a set is not linearly independent then we say it is  \emph{linearly dependent}.  The \emph{dimension} of a vector space $V$ is the supremum of cardinalities of linearly independent sets in $V$.  Given elements $v_\alpha$ in $V$ the \emph{linear span} is the intersection of all subspaces of $V$ containing the $v_\alpha$.  
\end{defn}

\begin{prop}Let $V$ be a vector space of $\mathds{F}$. 
\begin{itemize}
\item[(i)]Let $W_\alpha$ be a collection of subspaces of $V$, then $W = \cap_\alpha W_\alpha$ is a subspace.  
\item[(ii)] Let $v_\alpha$ be elements in $V$ then the span of $v_\alpha$ is a subspace and moreover the span is precisely the set of finite linear combinations of elements of $v_\alpha$.  
\item[(iii)]The dimension of the span of $\lbrace v_1, \dotsc, v_n \rbrace$ is less than or equal to $n$.  It is equal to $n$ if and only if $v_1, \dotsc, v_n$ are linearly independent.
\item[(iv)]$\dim \lbrace v_1, \dotsc, v_{n+1} \rbrace \leq \dim \lbrace v_1, \dotsc, v_n \rbrace + 1$.
\end{itemize}
\end{prop}
\begin{proof}
The fact that $W$ is a subspace is simple and left to the read.  The fact that a linear span is a subspaces follows from the first assertion as it was defined as an intersection of subspaces.  

To see that the set of finite linear combinations is indeed the span, first note that it is clear all finite linear combinations belong to every subspace containing the $v_\alpha$.  It suffices to show that the set of finite linear combinations is a vector space.  Given $u$ and $w$ which are finite linear combinations with index sets $A \subset \Lambda$ and $B \subset \Lambda$ respectively.  By use of zero coefficients we express both $u$ and $w$ using the index set $A \cup B$.  So to be concrete we write $u = \sum_{i=1}^n u_i v_{\alpha_i}$ and $w = \sum_{i=1}^n w_i v_{\alpha_i}$.  Let $a,b \in \mathds{F}$ and we compute $au + bw = \sum_{i=1}^n (au_i + bw_i) v_{\alpha_i}$.

To see (iii) we use induction.  For $n=1$ it is clear that $\dim\lbrace v_1 \rbrace = 1$ if $v_1 \neq 0$ and $0$ if $v_1 = 0$ and that $v_1 \neq 0$ if and only if $\lbrace v_1 \rbrace$ is linearly independent.  Suppose the result is true for $n-1$ and consider $v_1, \dots, v_n$.  Suppose $w_1, \dotsc, w_m$ is a linearly independent set in the span of the $v_i$.  Write
$w_i = \sum_{j=1}^n a_{ij} v_j$ for $1 \leq i \leq m$.  If $a_{in}=0$ for all $1 \leq i \leq n$

TODO: Finish
\end{proof}


\begin{defn}Let $V$ and $W$ be vector spaces over $\mathds{F}$ then a function $A : V \to W$ is said to be a \emph{linear map} if $A (av + bw) = aAv + bAw$ for all $v,w \in V$ and $a,b \in \mathds{F}$.  In the special case that $W = \mathds{F}$ we may say that $A$ is a \emph{linear functional}.  The set of linear functionals on $V$ is denoted $V^*$ and is called the \emph{dual space} to $V$.  Given any linear map $A : V \to W$ we define the \emph{dual map} $A^* : W^* \to V^*$ by $A^*(\lambda)(v)= \lambda(Av)$.  
\end{defn}
Note that the dual map is well defined since 
\begin{align*}
A^*(\lambda)(av + bw) &= \lambda(A(av+bw)) = \lambda(aAv + b A w) = a \lambda(Av) + b\lambda(Aw) = aA^*(\lambda)(v) + b A^*(\lambda)(w)
\end{align*}
shows that $A^*(\lambda)$ is a linear functional.  The dual space is easily seen to be a vector space and the dual map is easily shown to be a linear map.

\begin{prop}$V^*$ is a vector space over $\mathds{F}$ with addition and scalar multiplication defined pointwise as $(a\lambda + b\mu)(v) = a\lambda v + b \mu(v)$.  With repsect to this vector space structure the dual map $A^*$ is linear.  If $V$ is finite dimensional then $V^*$ is finite dimensional and $\dim V = \dim V^*$.
\end{prop}
\begin{proof}
The proof that $V^*$ is a vector space is elementary and left to the reader.  To see that $A^*$  is a linear map we just compute using the definitions
\begin{align*}
A^*( a \lambda + b \mu)(v) &= (a \lambda + b\mu)(Av) = a \lambda (Av) + b \mu (Av) = (a A^*(\lambda) + b A^*(\mu)) (v) \text{ for all $v \in V$}
\end{align*}

Now if $V$ is finite dimensional we can select a basis $v_1, \dotsc, v_n$.  Define $v_i^* \in V^*$ by $v_i^*(v_j) = \delta_{ij}$ for $1 \leq i,j \leq n$.  We claim that $v^*_i$ is a basis for $V^*$.  Clearly $v^*_i$ spans $V^*$ since if we are given an arbitrary $\lambda$ then by linearity
\begin{align*}
\left( \sum_{i=1}^n \lambda(v_i) v^*_i \right) v &= \sum_{i=1}^n \lambda(v_i) v^*_i (\sum_{j=1}^n a_j v_j) = \sum_{i=1}^n a_i \lambda(v_i) = \lambda(v)
\end{align*}
Moreover the $v^*_i$ are seen to be linearly independent since if $\sum_{i=1}^n a_i v^*_i = 0$ then for each $1 \leq j \leq n$ we have $0 = \left(\sum_{i=1}^n a_i v^*_i \right)v_j = a_j$.
Thus $v^*_i$ is a basis hence $\dim V  = \dim V^* = n$.
\end{proof}

The basis $v^*_i$ constructed from the basis $v_i$ in the above proof is referred to as the \emph{dual basis}.  

\begin{defn}Let $A$ be an $m \times n$ real matrix, then a triple comprising an $m \times m$ orthogonal matrix $U$, an $n \times n$ orthogonal matrix $V$ and a $m \times n$ diagonal matrix $\Sigma$ with $\Sigma_{11} \geq \dotsb \Sigma_{pp}$ where $p = m \wedge n$ such that $A = U \Sigma V^T$ is called a \emph{singular value decomposition}.
\end{defn}

\begin{thm}\label{SingularValueDecomposition}Singular value decompositions exist.
\end{thm}
\begin{proof}
The result is trivially true if $A = 0$ (let $\Sigma=0$, $U$ and $V$ be identity matrices), so assume that $A \neq 0$.  Let $\sigma_1$ be the $L^2$ operator norm of $A$.   By compactness of the unit sphere we can find a unit vector $x_1 \in \reals^n$ such that $0 \neq \sigma_1 = \norm{Ax}$.  Define $y_1 = \sigma^{-1}_1 Ax$ so that $y$ is a unit vector in $\reals^m$.  We can now find an orthonormal basis $\lbrace x_2, \dotsc, x_n \rbrace$ of $x_1^{\perp}$ and $\lbrace y_2, \dotsc, y_m \rbrace$ of $y_1^{\perp}$.  Define the orthogonal matrices $U_1 = [y_1, \dotsc, y_m]$ and $V = [x_1, \dotsc, x_n]$.  We may write
\begin{align*}
U_1^T A V_1 &=
\begin{bmatrix}
\sigma_1 & w^T \\
0 & B
\end{bmatrix}
\end{align*}
where $w \in \reals^{n-1}$ and $B$ is an $(m-1) \times (n-1)$ matrix.
Observe that
\begin{align*}
U_1^T A V_1 \begin{bmatrix} \sigma_1 \\ w\end{bmatrix}
&= \begin{bmatrix} \sigma_1^2 + w^T w \\ 
B \begin{bmatrix} \sigma_1 \\ w\end{bmatrix}
 \end{bmatrix}
\end{align*}
so that $\norm{U_1^T A V_1}^2 \geq (\sigma_1^2 + w^T w )$.  On the other hand, by orthogonality of $U_1$ and $V_1$ we know that
$\norm{U_1^T A V_1} = \norm{A} = \sigma_1$ and therefore we see that $w = 0$.    Now we can use the induction hypothesis to conclude that there exist $U_2$ and $V_2$ such that
$U_2^T B V_2 = \Sigma_2$ is diagonal and note that if we define 
\begin{align*}
U &= 
U_1 \begin{bmatrix}
1 & 0 \\
0 & U_2
\end{bmatrix}, &
V &= V_1 
\begin{bmatrix}
1 & 0 \\
0 & V_2
\end{bmatrix}
\end{align*}
Then 
\begin{align*}
U^T A V 
&=
\begin{bmatrix}
1 & 0 \\
0 & U_2^T
\end{bmatrix} U_1^T A V_1 
\begin{bmatrix}
1 & 0 \\
0 & V_2
\end{bmatrix} 
= 
\begin{bmatrix}
1 & 0 \\
0 & U_2^T
\end{bmatrix}
\begin{bmatrix}
\sigma_1 & 0 \\
0 & B
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
0 & V_2
\end{bmatrix} \\
&= 
\begin{bmatrix}
\sigma_1 & 0 \\
0 & \Sigma_2
\end{bmatrix} 
\end{align*}
\end{proof}

The fact that we may perform matrix factorizations in linear algebra in a Borel measurable way is easy to verify with the following elegant method relying on the ``principle of measurable choice''.
\begin{thm}\label{PrincipleOfMeasurableChoice}Let $S$ and $T$ be separable complete metric spaces and let $A \subset S \times T$ be closed and $\sigma$-compact.  Then $\pi_1(A)$ is Borel and there exists a Borel measurable function $f : \pi_1(A) \to T$ such that the graph of $f$ is contained in $A$.
\end{thm}
\begin{proof}
We start with the following
\begin{clm}Let $F$ be a closed set in $T$ then $\pi_1(A \cap S\times F)$ is Borel measurable.
\end{clm}
Write $A = \cup_n K_n$ with $K_n$ compact.  Since $F$ be a closed set it follows that $A \cap S\times F$ is closed $A \cap S\times F =  \cup_n K_n \cap S \times F$ where each $K_n \cap S \times F$ is compact.  Since $\pi_1$ is continuous each $\pi_1 (K_n \cap S \times F)$ is compact in $S$ and $\pi_1 (A \cap S\times F) = \cup_n \pi_1(K_n \cap S \times F)$ is therefore Borel.  

Applying the claim with $F = T$ we see that $\pi_1(A)$ is Borel.

Take a countable dense subset $\lbrace y_n \rbrace$ of $T$.  We define $f$ by an iterative approximation scheme.  
For $x \in \pi_1(A)$ define $f_1(x)$ to be the first $y_n$ (in index order) such that $A \cap \lbrace x \rbrace \times \overline{B}(y_n, 1/2) \neq \emptyset$ where $\overline{B}(z,r)$ represents the closed ball of radius $r$ centered at $z$.  Clearly $f_1$ is well defined since there exists an $(x,y) \in A$ and by density of $\lbrace y_n \rbrace$ in $T$ for any $r > 0$ there exists a $y_n$ with $(x,y) \in \lbrace x \rbrace \times \overline{B}(y_n,r)$.

Next observe that $f_1(x)$ is Borel measurable.  To see this, for each $r > 0$ and $n \in \naturals$ we define 
\begin{align*}
C_{n,r} &= \pi_1(A \cap S \times \overline{B}(y_n,r) )\\
&= \lbrace x \in S \mid A \cap \lbrace x \rbrace \times \overline{B}(y_n,r) \neq \emptyset \rbrace
\end{align*}
which is Borel by the claim.  Moreover it follows that $f_1^{-1}(y_n) = C_{1, 1/2}^c \cap \dotsb \cap C_{n-1, 1/2}^c \cap C_{{n}, 1/2}$ is Borel.  Since $f_1$ is countably valued it follows that $f_1$ is Borel measurable.

Now define $f_k(x)$ be the first $y_n$ such that $A \cap \lbrace x \rbrace \times \overline{B}(y_n, 1/2^k) \neq \emptyset$ and $d(f_{k-1}(x), y_n) \leq 1/2^{k-2}$; again density of the $\lbrace y_n \rbrace$ shows that $f_k(x)$ is well defined.  Borel measurability of $f_k$ follows by a simple induction as the Borel measurability of $f_{k-1}$ and Lipschitz continuity of $d$ imply that $D_{n,r} = \lbrace x \mid d(f_{k-1}(x), y_n) \leq r \rbrace$ is Borel measurable for every $n \in \naturals$ and $r > 0$.  Since 
\begin{align*}
&f_k^{-1}(y_n) = \\
&(C_{1,1/2^k} \cap D_{1, 1/2^{k-2}})^c \cap \dotsb \cap (C_{n-1,1/2^k} \cap D_{n-1, 1/2^{k-2}})^c \cap  C_{n,1/2^k} \cap D_{n, 1/2^{k-2}}
\end{align*}
and $f_k$ is countably valued it follows that $f_k$ is Borel measurable.

For a fixed $x$ note that for $j > k$ we have the triangle inequality
\begin{align*}
d(f_k(x), f_j(x)) &\leq \sum_{i=k}^{j-1} d(f_i(x), f_{i+1}(x)) \leq \sum_{i=k}^{j-1} 1/2^{i-1} \leq \sum_{i=k}^\infty 1/2^{i-1} = 1/2^{k-2}
\end{align*}
which shows that $f_k(x)$ is Cauchy.  By completeness of $T$, we can take 
the limit of $f_k$ which is a Borel measurable function (Lemma \ref{LimitsOfMeasurableMetricSpace}).  Since $A$ is closed and $\lim_{k \to \infty} d(f_k(x), A) = 0$ it follows that $(x,f(x)) \in A$ and we are done.

TODO: Do we ever use Polishness of $S$?  Note the reference Azoff ``Borel Measurability in Linear Algebra'' in the Proceedings of the AMS who in turn references Bourbaki.
\end{proof}

We now illustrate how matrix factorizations (and canonical forms) may be shown to be Borel measurable by using the singular value decomposition as an example.  
\begin{cor}\label{BorelMeasurabilitySVD}There exists a Borel measurable function $f (A) = (U,\Sigma,V)$ from  $\reals^{m\times n}$ to $\reals^{m \times m} \times \reals^{m \times n} \times \reals^{n \times n}$ such that $A = U \Sigma V^T$ is a singular value decomposition.
\end{cor}
\begin{proof}
Let 
\begin{align*}
F &= \lbrace (A, U,\Sigma, V) \mid \text{$U, V$ are orthogonal, $\Sigma$ is diagonal and $A = U \Sigma V^T$} \rbrace
\end{align*}
and note that $F$ is closed (because the spaces of orthognal and diagonal matrices are closed and matrix multiplication is continuous).  $F$ is also $\sigma$-compact because the ambient space is (just write $F = \cup_n F \cap \overline{B}(0,n)$).  Since singular value decompositions exist (Theorem \ref{SingularValueDecomposition}) we know that $\pi_1(F) = \reals^{m \times n}$ and therefore the result follows immediately from Theorem \ref{PrincipleOfMeasurableChoice}.
\end{proof}

B\'{e}la Sz Nagy also seems to prove measurability of eigenvalues and eigenvectors using the minimax criterion in ``Harmonic Analysis of Operators on Hilbert Space''.  From this one can bootstrap up to the measurability of the Schur decomposition and then get to the measurability of the SVD by considering Schur decompositions of $A^TA$ and $AA^T$.  There are also some more powerful section theorems that may have some relevance.