\section{More Real Analysis}
Holding area for more advanced topics in real analysis that are
eventually required (and in some cases there may be some topics that I
am just interested in).
\subsection{Topological Spaces}
\begin{lem}\label{OpenAlternative}A set $U \subset X$ is open if and only if for every $x \in
  U$ there is an open set $V \subset U$ such that $x \in V$.
\end{lem}
\begin{proof}
Suppose $U$ is open and $x \in U$, then let $V = U$.

Suppose for every $x \in U$ there exist an open set $V_x$ such that $x
\in V_x \subset U$.  Note that $\cup_x V_x \subset U$ because each
$V_x \subset U$ and on the other hand $\cup_x V_x \supset U$ since
every $x \in U$ satisfies $x \in V_x$.  Thus $U = \cup_x V_x$ which
shows that $U$ is open.
\end{proof}
\begin{defn}A mapping $f : X \to Y$ between topological spaces is said
  to be \emph{continuous} if and only if $f^{-1}(V)$ is open in $X$
  for every $V$ open in $Y$.
\end{defn}
\begin{defn}A mapping $f : X \to Y$ between topological spaces is said
  to be \emph{continuous at x} if and only if for every $V$ open in
  $Y$ such that $f(x) \in V$, there exists an open set $U$ in $X$ with $x \in U$ and $f(U)
  \subset V$.
\end{defn}
\begin{lem}A mapping $f : X \to Y$ between topological spaces is
  continuous if and only if it is continuous at $x$ for every $x \in X$.
\end{lem}
\begin{proof}
Suppose $f$ is continuous and let $x \in X$ and $V$ be open in $Y$
with $f(x) \in V$.  By continuity of $f$, we know that $f^{-1}(V)$ is
open in $X$ and $x \in f^{-1}(V)$.  By Lemma \ref{OpenAlternative} we
can pick an open set $U$ such that $x \in U$ and $U \subset
f^{-1}(V)$.  It follows that $f(U) \subset V$.

Now suppose $f$ is continuous at every $x \in X$ and let $V$ be open
in $Y$.  If $x \in f^{-1}(V)$ then $f$ is continuous at $x$ hence
there exists and open $U$ such that $x \in U$ and $f(U) \subset V$.
It follows that $U \subset f^{-1}(V)$ and by Lemma
\ref{OpenAlternative}  we have shown that $f^{-1}(V)$ is open.
\end{proof}

\begin{defn}A \emph{base} of a topology $\mathcal{T}$ at a point $x
  \in X$ is a collection
  of sets $\mathcal{B}$ such that for every open set $U \in
  \mathcal{T}$ such that $x \in U$ there exists a $B \in \mathcal{B}$ such
  that $x \in B \subset U$.  A base of a topology is a collection of
  sets that is a base at all points $x \in X$.
\end{defn}

\begin{lem}A set $\mathcal{B}$ of sets $B \subset X$ is a base of a
  topology if and only if for every $x \in X$ there exists $B \in
  \mathcal{B}$ such that $x \in B$ and for every $A, B \in
  \mathcal{B}$ and $x \in A \cap B$ there exists $C \in \mathcal{B}$
  such that $x \in C \subset A \cap B$.
\end{lem}
\begin{proof}
Suppose $\mathcal{B}$ satisfies the hypothesized conditions and let
\begin{align*}
\tau &= \lbrace U \subset X \mid \text { for every } x \in U \text{
  there exists } B \in \mathcal{B} \text{ such that } x \in B \subset
U \rbrace
\end{align*}
It is certainly the case that $\mathcal{B}\subset \tau$ and we claim that $\tau$ is a topology.  Certainly $\emptyset \in \tau$.
Let $U_\alpha$ for $\alpha \in \Lambda$ are sets in $\tau$.
Then if $x \in \cup_{\alpha \in \Lambda} U_\alpha$ there exists an
$\alpha \in \Lambda$ such that $x \in U_\alpha$ and by hypothesis we
pick $B$ such that $x \in B \subset U_\alpha \subset  \cup_{\alpha \in
  \Lambda} U_\alpha$.  If $U_1, \dotsc, U_n \in \tau$ and $x \in U_1
\cap \dotsc \cap U_n$ then there exists $B_1, \dotsc, B_n$ such that
$x \in B_j \subset U_j$ for $j = 1, \dotsc, n$ and therefore $x \in
B_1 \cap \dotsc \cap B_n \subset U_1 \cap \dotsc \cap U_n$.  A simple
induction on the hypothesis shows that $B_1 \cap \dotsc \cap B_n \in \mathcal{B}$.
Because $\mathcal{B}$ is cover of $X$ we have $X = \cup_{B \in
  \mathcal{B}} B \in \tau$ and therefore $\tau$ is a topology.  By the
definition of $\tau$ it is immediate that $\mathcal{B}$ is a base of
the topology.
\end{proof}


\begin{defn}
\begin{itemize}
\item[(i)]A topological space is said to be \emph{separable} if and
  only if it has a countable dense subset.
\item[(ii)]A topological space is said to be \emph{first countable} if and
  only if every point has a countable local base.
\item[(ii)]A topological space is said to be \emph{second countable} if and
  only if every the topology has a countable base.
\end{itemize}
\end{defn}
\begin{lem}A metric space is separable if and only if it is second countable.
\end{lem}
\begin{proof}
TODO:
outline of proof is to pick a countable dense subset $\lbrace x_n
\rbrace$ and then pick the open balls $B(x_n; \frac{1}{m})$ for $m \in
\naturals$.  Show this is a base of the topology.
\end{proof}

TODO: The goal of the next set of results is to show that separable
complete metric spaces are Borel.


The following appears in Royden as Theorem 8.11 (with proof delgated
to exercises)
\begin{lem}Let $X$ be a Hausdorff topological space, $Y$ be a
  complete metric space and $Z \subset X$ be a dense subset.  If $f :
  Z \to X$ is a homeomorphism then $Z$ is a countable intersection of
  open sets.
\end{lem}
\begin{proof}
For each $n$ let 
\begin{align*}
O_n &= \lbrace x \in X \mid \text{there exists $U$
  open with $x \in U$ and $\diam(f(U \cap Z)) < \frac{1}{n}$} \rbrace
\end{align*}
Note that $O_n$ is open because for any $x \in O_n$ by definition we
have the open set $U$ that provides the evidence that $x \in O_n$;
$U$ also provides the evidence that proves that every $y \in U$
belongs to $O_n$.  Also
note that $Z \subset O_n$ since for any $n$, by continuity of $f$ at $x \in Z$ and Lemma
\ref{OpenAlternative}  we
can find an open $U \subset X$ such that $x \in U \cap Z$ and $f(U \cap Z) \subset B(f(x),
\frac{1}{2n})$ (sets of the form $U \cap Z$ being precisely the open
sets in $Z$).

Now define $E = \cap_n O_n$.  As noted we know $Z \subset E$ so we
will be done if we can show $E
\subset Z$ as well.  Let $x \in E$; we will construct $z \in Z$
such that $x = z$.  For each $n$ pick $U_n$ such $x \in U_n$ and $\diam(f(U_n \cap Z)) <
\frac{1}{n}$ and let $x_n$ be an arbitrary point in $\cap_{j=1}^n
U_j \cap Z$ (the intersection is non-empty because $Z$ is dense in
$X$).  
For every $n$ and $m \geq n$ we have by construction that $x_n
\in U_n$ and $x_m \in U_n$ hence $d(f(x_n), f(x_m)) < \frac{1}{n}$.
Therefore $f(x_n)$ is Cauchy in
$Y$ and by completeness of $Y$ we know that $f(x_n)$ converges to a
value $y \in Y$ with $d(y, f(x_n)) \leq \frac{1}{n}$.  
Because $f$ is a homeomorphism we know that 
there is a unique $z \in Z$ such that $f(z) = y$; we claim that $x =
z$.  Suppose that $x
\neq z$, then by the Hausdorff property on $X$ we can pick open sets $U$ and
$V$ such that $U \cap V = \emptyset$, $x \in U$ and $z \in V$.  Since
$f$ is a homeomorphism, we know $f(Z \cap V)$ is open and contains
$f(z)$ hence for sufficiently large $n$, $f^{-1}(B(f(z), \frac{1}{n}))
\subset Z \cap V \subset V$.  On
the other hand, by the definition of $x$ we have $U_{2n}$ open such that
$x \in U_{2n}$ and $\diam(f(Z \cap U_{2n})) < \frac{1}{2n}$.  By openness of
$U \cap U_{2n}$ and density of $Z$ we know there is a $w \in U \cap
U_{2n} \cap Z$.  Putting these observations together we have
\begin{align*}
d(f(w), f(z)) &\leq  d(f(w), f(x_{2n})) + d(f(x_{2n}), f(z)) 
< \frac{1}{2n} + \frac{1}{2n} = \frac{1}{n}
\end{align*}
which implies $w \in V$ providing a contradiction of $U \cap V =
\emptyset$ hence we conclude $x = z$.
\end{proof}

\begin{thm}[Tychonoff's Theorem]\label{Tychonoff}Let $I$ be index set
  and let $(X_i,
  \mathcal{T}_i)$ be a topological space for each $i \in I$, the
  cartesian product $\prod_{i \in I} X_i$ with the product topology is
  compact.
\end{thm}
\begin{proof}
TODO:
\end{proof}

\begin{defn}Given a topological space $(X, \mathcal{T})$ the Baire
  $\sigma$-algebra is smallest $\sigma$-algebra for which all bounded
  continuous functions are measurable.  Equivalently 
\begin{align*}
Ba(X,\mathcal{T}) &= \sigma(\lbrace f^{-1}(U) \mid U \subset \reals
\text{ is open; } f \in C_b(X,\reals)\rbrace)
\end{align*}
\end{defn}
\begin{lem}For every topological space $(X, \mathcal{T})$, $Ba(X)
  \subset \mathcal{B}(X)$.  For a metric space $(S,d)$, $Ba(S) = \mathcal{B}(S)$.
\end{lem}
\begin{proof}
To see the inclusion $Ba(X)
  \subset \mathcal{B}(X)$, note that by continuity of $f \in
  C_b(X;\reals)$, every set $f^{-1}(U)$ is open.

Now suppose $(S,d)$ is a metric space.  To show $\mathcal{B}(S)
\subset Ba(S)$, it suffices if we show every closed set $F \subset S$
can be written as $f^{-1}(G)$ where $G \subset \reals$ is closed and
$f \in C_b(S; \reals)$.  By the triangle inequality (see e.g. Lemma
\ref{DistanceToSetLipschitz}) we know
that $g(x) = d(x, F)$ is continuous (in fact Lipschitz) and by Lemma
\ref{MaxMinOfLipschitz} we know that $f(x) = d(x, F) \wedge 1$ is also
Lipschitz and therefore $f(x) \in C_b(S; \reals)$.  Because $F$ is
closed we also know that $F = f^{-1}(\lbrace 0 \rbrace)$ and we are done.
\end{proof}

TODO: How much this stuff on regularity can be extended to outer
measures????  I want to understand the overlap with the results in
Evans and Gariepy.

\begin{lem}\label{InnerRegularSetsSigmaAlgebra}Let $X$ be a Hausdorff topological space, $\mathcal{A}$
  a $\sigma$-algebra on $X$ and $\mu$ a finite tight measure.  Then
\begin{align*}
\mathcal{R} &= \lbrace A \in \mathcal{A} \mid A \text { and } A^c
\text{ are $\mu$-inner regular} \rbrace
\end{align*}
is a $\sigma$-algebra.  The same is true if the condition is replaced
by sets that are $\mu$-closed inner regular (without the requirement
that $\mu$ is tight).
\end{lem}
\begin{proof}
By definition, $\mathcal{R}$ is closed under complement.  By
assumption that $\mu$ is tight we have $X \in \mathcal{R}$ so all that
needs to be shown is closure under countable union.

Assume $A_1, A_2, \dots \in \mathcal{R}$ and let $\epsilon>0$ be
given.  By finiteness of $\mu$, $\mu(\cup_{n=1}^\infty A_n) < \infty$ and
continuity of measure (Lemma \ref{ContinuityOfMeasure}) there exists $M>0$ such that $\mu(\cup_{n=1}^M
A_n) > \mu(\cup_{n=1}^\infty A_n) - \epsilon$.
 By assumption that $A_n \in \mathcal{R}$ and finiteness of $\mu$, for each
$A_n$ there exists a compact $K_n$ such that $\mu(A_n \setminus K_n) <
\frac{\epsilon}{2^n}$ and there exists compact $L_n$ such that $\mu(A_n^c \setminus L_n) <
\frac{\epsilon}{2^n}$. Let
\begin{align*}
K &= \cup_{n=1}^M K_n \\
L &= \cap_{n=1}^\infty L_n
\end{align*}
and note that both $K$ and $L$ are compact (in the latter case,
because X is Hausdorff we know that each $L$ is closed hence the
intersection is a closed subset of a compact set hence compact).
Furthermore we can compute
\begin{align*}
\mu(\cup_{n=1}^\infty A_n \setminus K) &= \mu(\cup_{n=1}^\infty A_n
\setminus \cup_{n=1}^M K_n)  \\
&= \mu(\cup_{n=1}^M A_n
\setminus \cup_{n=1}^M K_n)  + \mu(\cup_{n=1}^\infty A_n \setminus \cup_{n=1}^M A_n
\setminus \cup_{n=1}^M K_n)\\
&\leq \mu(\cup_{n=1}^M A_n \setminus K_n)  + \mu(\cup_{n=1}^\infty A_n
\setminus \cup_{n=1}^M A_n)\\
&\leq \sum_{n=1}^M(A_n \setminus K_n)  + \epsilon \\
&\leq 3 \epsilon
\end{align*}
and
\begin{align*}
\mu((\cup_{n=1}^\infty A_n)^c \setminus L) &=\mu(\cap_{n=1}^\infty
A_n^c \setminus \cap_{n=1}^\infty L_n) \\
 &=\mu(\cap_{n=1}^\infty
A_n^c \cap \cup_{n=1}^\infty L_n^c) \\
 &=\mu(\cup_{n=1}^\infty \cap_{m=1}^\infty
A_m^c \cap L_n^c) \\
 &\leq \mu(\cup_{n=1}^\infty 
A_n^c \cap L_n^c) \\
 &\leq \sum_{n=1}^\infty (
A_n^c \setminus L_n) \\
&\leq 2 \epsilon
\end{align*}

TODO: The closed inner regular case...
\end{proof}

TODO:  In metric space, tightness is equivalent to inner regularity.
Then Ulam's Theorem that finite measures on separable metric spaces
are automatically inner regular.  Also finite measures on arbitrary
metric spaces are closed inner regular as well as outer regular.

\begin{lem}\label{FiniteMeasuresOnMetricSpacesAreClosedInnerRegular}
Let $(S,d)$ be a metric space and $\mu$ be a Borel measure on $(S,
\mathcal{B}(S))$, the $\mu$ is closed inner regular.  If in addition
$\mu$ is a finite measure then it is outer regular.
\end{lem}
\begin{proof}
Let $U$ be an open set in $S$.  Then $U^c$ is closed and the
function $f(x) = d(x, U^c)$ is continuous.  If we define 
\begin{align*}
F_n &= f^{-1}([1/n, \infty))
\end{align*}
then each $F_n$ is closed, $F_1 \subset F_2 \subset \cdots$ and
$\cup_{n=1}^\infty F_n = U$.  By continuity of measure (Lemma
\ref{ContinuityOfMeasure}) we know that $\lim_{n \to \infty} \mu(F_n)
= \mu(U)$.  So this shows that every open set is inner closed
regular.  Furthermore it is trivial to note that $U^c$ is inner closed
regular because it is closed.  

By Lemma \ref{InnerRegularSetsSigmaAlgebra} we know know that 
\begin{align*}
\mathcal{B}(S) &\subset \mathcal{R} = \lbrace A \subset S \mid A
\text{ and } A^c
\text{ are inner closed regular}  \rbrace
\end{align*}

Outer regularity follows from taking complements and using the
finiteness of $\mu$.
\end{proof}

If we add the criterion that the metric space is separable, then we
can upgrade the regularity to inner regularity.
\begin{lem}\label{SeparableInnerRegularTight}Let $(S,d)$ be a
  separable metric space and $\mu$ be a finite Borel measure on $(S,
\mathcal{B}(S))$, then $\mu$ is inner regular if and only if it is tight.
\end{lem}
\begin{proof}
Clearly inner regularity implies tightness (which is just inner
regularity of the set $S$), so it suffices to show
that tightness implies inner regularity.

Suppose that $\mu$ is a tight measure.  By Lemma
\ref{InnerRegularSetsSigmaAlgebra} it suffices to show that both open
and closed sets are inner regular.

Pick $\epsilon >0$ and select $K \subset S$ a compact set such that $\mu(S \setminus K) < \frac{\epsilon}{2}$.
By Lemma \ref{FiniteMeasuresOnMetricSpacesAreClosedInnerRegular} we
know that for any Borel set $B$ there exists a closed set $F \subset
B$ such that $\mu(B \setminus F) < \frac{\epsilon}{2}$.  Note that $F
\cap K$ is compact.   We have
\begin{align*}
\mu(B \setminus (F \cap K)) &\leq \mu(B \cap F^c) + \mu(B \cap K^c) \leq \mu(B \cap F^c) + \mu(S \cap K^c) < \epsilon
\end{align*}
\end{proof}
\begin{thm}[Ulam's Theorem]\label{UlamsTheorem}Let $(S,d)$ be a
  complete separable metric space and $\mu$ be a finite Borel measure on $(S,
\mathcal{B}(S))$, then $\mu$ is inner regular.
\end{thm}
\begin{proof}
By Lemma \ref{SeparableInnerRegularTight} it suffices to show that $\mu$ is tight.  Pick
$\epsilon > 0$ and we construct a compact set $K \subset S$ such that
$\mu(S \setminus K) < \epsilon$.  Let
$\overline{B}(x,r)$ denote the closed ball of radius $r$ around $x \in
S$.  Pick
a countable dense subset $x_1, x_2, \dotsc \in S$.  For each $m \in
\naturals$, by density of $\lbrace x_n \rbrace$, we know $\cap_{n=1}^\infty \left ( S
\setminus \cup_{j=1}^n \overline{B}(x_j, \frac{1}{m}) \right ) =
\emptyset$, thus by
continuity of measure (Lemma \ref{ContinuityOfMeasure}) there exists
$N_m > 0$ such that $\mu(S
\setminus \cup_{j=1}^n \overline{B}(x_j, \frac{1}{m}) < \frac{\epsilon}{2^m}$ for
all $n \geq N_m$.
If we define
\begin{align*}
K &= \cap_{m=1}^\infty \cup_{j=1}^{N_m} \overline{B}(x_j, \frac{1}{m})
\end{align*}
we claim that $K$ is compact.  Note that $K$ is easily seen to be
closed as it is an intersection of a finite union of closed balls.
Since $S$ is complete this implies that $K$ is also complete.  Also it
is easy to see that $K$ is totally bounded since by construction we
have demonstrated a cover by a finite number of balls of radius
$\frac{1}{m}$ for each $m \in \naturals$.  So by Theorem
\ref{CompactnessInMetricSpaces} we know $K$ is compact.

To finish the result we claim $\mu(S \setminus K) < \epsilon$:
\begin{align*}
\mu(S \setminus K) 
&= \mu(S \cap \left(\cap_{m=1}^\infty
  \cup_{j=1}^{N_m} \overline{B}(x_j, \frac{1}{m})\right)^c) \\
&= \mu(S \cap \cup_{m=1}^\infty \left(
  \cup_{j=1}^{N_m} \overline{B}(x_j, \frac{1}{m})\right)^c) \\
&= \mu(\cup_{m=1}^\infty S \setminus 
  \cup_{j=1}^{N_m} \overline{B}(x_j, \frac{1}{m})) \\
&\leq \sum_{m=1}^\infty \mu( S \setminus 
  \cup_{j=1}^{N_m} \overline{B}(x_j, \frac{1}{m})) \\
&< \epsilon
\end{align*}
\end{proof}

\begin{lem}\label{SeparabilityOfBoundedUniformlyContinuous}Let $(S,d)$ be a separable metric space, then $X$ is
  homeomorphic to a subset of $[0,1]^{\integers_+}$ and furthermore
\begin{itemize}
\item[(i)]$S$ has a metric making it totally bounded
\item[(ii)]If $S$ is compact then $C(S; \reals)$ with the uniform
  topology is separable.
\item[(iii)]If $\hat{d}$ is a totally bounded metric on $S$ then
  $U_b(S)$ is separable
\end{itemize}
\end{lem}
\begin{proof}
Let $\rho$ be the product metric $\rho(x,y) = \sum_{n=1}^\infty
\frac{\abs{x_n-y_n}}{2^n}$ on the space $[0,1]^{\integers_+}$. 
 Pick a countable dense subset $x_1, x_2, \dotsc$ of $S$ and define 
$f : S \in [0,1]^{\integers_+}$ by 
\begin{align*}
f(x) &= \left ( \frac{d(x_1, x)}{1 + d(x_1, x)}, \frac{d(x_2, x)}{1 +
    d(x_2, x)}, \dotsc \right )
\end{align*}
 
Claim 1: $f(x)$ is continuous.

By definition of the product topology $f(x)$ is continuous if and only
if each coordinate is.  For any given fixed $x_j$, we know that
$d(x_j, x)$ is continuous (in fact Lipschitz by Lemma
\ref{DistanceToSetLipschitz}) and thus the result follows from the
continuity of $x/(1+x)$ on $\reals_+$.

Claim 2: $f(x)$ is injective.

For any $z \neq y$ we find $\epsilon > 0$ such that $B(z ; \epsilon)
\cap B( y ; \epsilon) = \emptyset$ and then using density of $x_1,
x_2, \dotsc$ to pick an $x_n$ such that $d(z,x_n) < \epsilon$ and
$d(y, x_n) \geq \epsilon$ showing $f(z) \neq f(y)$.  

Claim 3: The inverse of $f(x)$ is continuous.

Fix an $x \in S$ and let $\epsilon >0$ be given.  Pick $x_n$ such that
$d(x_n, x) < \epsilon/2$.  If we let $g(x) : [0,1) \to \reals_+$ be
defined by $g(x) = x/(1-x)$ then $g(x)$ is the inverse of $x/(1+x)$ 
and by continuity of $g(x)$ at the point $\frac{d(x_n, x)}{1+d(x_n,x)}$ we know that there exists a $\delta > 0$
such that $\abs{\frac{d(x_n, x)}{1+d(x_n,x)} - 
\frac{d(x_n,  y)}{1+d(x_n,y)}}< \delta$ implies $\abs{d(x_n,x) - d(x_n,y)} <
  \epsilon/2$.
Then if $f(y) \in B(f(x), \frac{\delta}{2^n})$ we have
\begin{align*}
\abs{\frac{d(x_n, x)}{1+d(x_n,x)} - 
\frac{d(x_n,  y)}{1+d(x_n,y)}} &\leq 2^n \rho(f(x), f(y)) < \delta
\end{align*}
$d(x,y) \leq d(x_n,x) + \abs{d(x_n,x) - d(x_n,y)} < \epsilon$.

Now to see (i) we simply pull back the metric $\rho$ via the embedding
$f(x)$ and use the facts that $\rho$ generates the product topology,
$[0,1]^{\integers_+}$ is compact in product topology
(by Tychonoff's Theorem \ref{Tychonoff}; alternatively one can avoid
the use of Tychonoff's Theorem for it is easy to
see with a diagonal subsequence argument that a countable product of
sequentially compact metric spaces is sequentially compact) hence totally bounded (Theorem
\ref{CompactnessInMetricSpaces}).

Here is the argument that $\rho$ generates the product topology; TODO:
put this in a separate lemma.  To see that the topology generated by
$\rho$ is finer than the product topology, suppose $U$ is open in the
topology generated by $\rho$.  Pick $x \in U$ and select $N > 0$ such
that $B(x,\epsilon) \subset U$.  Then pick $N > 0$ such that $2^{-N-1} <
\epsilon$ and consider $B=B(x_1, \epsilon/2)
\times \dotsb \times B(x_{2^N}, \epsilon/2) \times S \times \dotsb$
which is open in the product topology.  If $y \in B$ then 
\begin{align*}
\rho(x,y) &=
\sum_{n=1}^\infty \frac{\abs{x_n-y_n}}{2^n} = \sum_{n=1}^{2^N}
\frac{\abs{x_n-y_n}}{2^n} + \sum_{n=2^N+1}^\infty \frac{\abs{x_n-y_n}}{2^n} \leq
\frac{\epsilon}{2} \sum_{n=1}^{2^N} \frac{1}{2^n} +
\sum_{n=2^N+1}^\infty \frac{1}{2^n} < \epsilon
\end{align*}
To see that the product topology is finer than the metric topology,
suppose $n >0$ is an integer, $U\subset[0,1]$ is open and consider $\pi_n^{-1}(U)$.  Let $x
\in \pi_n^{-1}(U)$ and find an $\epsilon > 0$ such that $B(x_n,
\epsilon) \subset U$.  Note that if $y \in B(x, \frac{\epsilon}{2^n})$
then $\abs{x_n - y_n} < 2^n \rho(x,y) \leq \epsilon$ and therefore
$B(x, \frac{\epsilon}{2^n}) \subset \pi_n^{-1}(B(x_n, \epsilon))
\subset U$.

To see (ii), if $S$ is compact then $f(S) \subset [0,1]^{\integers_+}$
is compact (Lemma \ref{ContinuousImageOfCompact}).  Observe that 
\begin{align*}
A &= \lbrace \Pi_{i=1}^np_i(x_i) \mid n \in \naturals \text{ and } p_i
\in \rationals[x] \rbrace
\end{align*}
is a subalgebra of $C([0,1]^{\integers_+} ; \reals)$ and $A$ separates points (
given $x \neq y \in [0,1]$, pick
$n$ such that $x_n \neq y_n$ and pick the function $g(x) = x_n$).  By
the Stone-Weierstrass Theorem \ref{StoneWeierstrassApproximation} we know that $A$ is dense in $C([0,1]^{\integers_+};
\reals)$; now pullback $A$ under $f(x)$ to a countable dense subset of
$C(S;\reals)$. 

To see (iii), suppose $\hat{\rho}$ is a totally bounded metric on
$S$.  Let $\hat{S}$ be the completion of $S$ with respect to
this metric.  

Claim 4: $\hat{\rho}$ extends to a totally bounded
metric on $\hat{S}$.  

Let $\epsilon>0$ be given and cover $S$ by ball
$B(x_i, \epsilon/2)$; we show that $B(x_i, \epsilon)$ covers
$\hat{S}$.  Biven $y \in \hat{S}$ we can find $x \in
S$ such that $\hat{\rho}(x,y) < \epsilon/2$.  Since $x \in S$ there
exists an $x_i$ such that $x \in B(x_i, \epsilon/2)$ and therefore
$\hat{\rho}(x_i,y) \leq \hat{\rho}(x,x_i) + \hat{\rho}(x_i,y) <
\epsilon$.

Because $(\hat{S},\hat{\rho})$ is complete and totally bounded we know
it is compact (Theorem \ref{CompactnessInMetricSpaces}) and we have
just shown that $C(\hat{S} ; \reals)$ has a countable dense subset.

Claim 5: $f\vert_S : C(\hat{S} ; \reals) \to U_b^{\hat{\rho}}(S ;
\reals)$ is a well defined, continuous and surjective.

Being well defined in this context means that restriction to $S$
results in a bounded uniformly continuous function.  This follows
from the fact that any continuous function of a compact set is bounded
and uniformly continuous (Theorem \ref{ContinuousImageOfCompact} and
Theorem \ref{UniformContinuityOnCompactSets} respectively) and these
properties are preserved upon restriction.  To see surjectivity, let
$g : S \to \reals$ be bounded and uniformly continuous.  TODO: Make
this a separate Lemma.  Let $x \in \hat{S}$, pick a sequence $x_n$
in $S$ such that $\lim_{n \to \infty} x_n = x$ and observe that by
uniform continuity of $f(x)$, for
any $\epsilon > 0$ there exists a $\delta > 0$ such that $\hat{d}(x,y)
< \delta$ implies $\abs{f(x) - f(y)} < \epsilon$.  If we pick $N > 0$
such that $\hat{d}(x_n,y) < \delta/2$ for $n \geq N$ then
$\hat{d}(x_n, x_m) < \delta$ for all $n,m \geq N$ and thus $\hat{d}(f(x_n), f(x_m)) < \epsilon$ for all
$n,m \geq N$.  This shows that the sequence $f(x_n)$ is Cauchy and by
completeness of $\reals$ we can take the limit; we define $f(x) =
\lim_{n\to infty} f(x_n)$.  We claim that this definition is
independent of the sequence chosen.  Indeed, let $y_n$ be another
sequence from $S$ such that $\lim_{n \to \infty} y_n = x$.  Pick an
$\epsilon > 0$ and by uniform continuity of $f(x)$ let $\delta$ be
chosen such that $\abs{f(x)-f(y)} < \epsilon/2$ whenever $\hat{d}(x,y) < \delta$.
There exists $N_1 > 0$ such that $\rho(y_n, x_n) < \delta$ for every
$n > N_1$ and there exists $N_2 > 0$ such that $\abs{f(x_n) - f(x)} <
\epsilon/2$ for all $n \geq N_2$.  Then we have for all $n \geq N_1 \vee
N_2$ by the triangle inequality $\abs{f(y_n) - f(x)} < \epsilon$.
Note that this also shows that the extension $f(x)$ to $\hat{S}$ is
continuous at $x \in \hat{S}$; since is was continuous at all points
of $S$ we know the extension is continuous.  

Now the continuous image of a dense set under a surjective map is also
dense.  This is easily seen by picking a point $f(x)$ in the image;
picking a sequence $x_n$ such that $x_n \to x$ and then considering
the image $f(x_n) \to f(x)$.  Thus the result is proven.
\end{proof}

\begin{lem}[Dini's Theorem]\label{DinisTheorem}Let $K$ be a compact
  topological space and let $f_n: K \to \reals$ be a sequence  of continuous
  functions such that $f_n \downarrow 0$ pointwise on $K$, then $f_n
  \to 0$ uniformly.
\end{lem}
\begin{proof}
Given $\epsilon > 0$ define $U_n = f_n^{-1}((-\infty,\epsilon))$.
Then each
$U_n$ is open, $U_1 \subset U_2 \subset \dotsb$ (since the $f_n$ are
decreasing) and the $U_n$ form an open cover of $K$.  We can extract a
finite subcover which since the $U_n$ are nested implies that $K =
U_N$ for some $N > 0$.  This is exactly the statement that $\sup_{x
  \in K} \abs{f_n(x)} < \epsilon$ for all $n \geq N$ hence the result proven.
\end{proof}

\begin{lem}\label{StoneDaniellProbability}Let $(S,d)$ be a separable metric space and let $\Lambda :
  U_b^d(S; \reals) \to \reals$ be a linear map such that 
\begin{itemize}
\item[(i)] $\Lambda$ is non-negative (i.e. if
  $f \geq 0$ then $\Lambda(f) \geq 0$) 
\item[(ii)] $\Lambda(1) = 1$
\item[(iii)] for all $\epsilon > 0$ there exists a compact set $K
  \subset S$ such that for all $f \in U_b^d(S; \reals)$,
\begin{align*}
\abs{\Lambda(f)} &\leq \sup_{x \in K} \abs{f(x)} + \epsilon \norm{f}_u
\end{align*}
\end{itemize}
then there exists a Borel probability measure $\mu$ on $S$ such that
$\Lambda(f) = \int f \, d\mu$.  Whenever such a probability measure
exists it is unique.
\end{lem}
\begin{proof}
We construct $\mu$ by use of the Daniell-Stone Theorem
\ref{DaniellStoneTheorem}.  It is clear that $U_b^d(S; \reals)$ is
closed under max and min and contains the constant functions so
$U_b^d(S; \reals)$ is a Stone Lattice.  It remains to show that
$\Lambda$ obeys the ``montone convergence'' property: if $f_n
\downarrow 0$ pointwise then $\Lambda(f_n) \downarrow 0$.  This
property is a corollary of Dini's Theorem \ref{DinisTheorem} since by that result,
if $f_n$ are continuous and $f_n \downarrow 0$ pointwise on a compact
set then the converge uniformly to $0$ on the compact set.  In
particular, pick an $\epsilon > 0$ and let $K \subset S$ be compact
as in the hypothesis.  By Dini's Theorem there exists $N > 0$ such
that $\sup_{x \in K} f_n(x) < \epsilon$ for all $n \geq N$.  Therefore
for all $N > 0$,
\begin{align*}
\abs{\Lambda(f_n)} &\leq \sup_{x \in K} \abs{f_n(x)} + \epsilon
\norm{f_n}_\infty \\
&\leq \epsilon(1 + \norm{f_1}_\infty)
\end{align*}
thus $\lim_{n \to \infty} \Lambda(f_n) = 0$ and we can apply Theorem \ref{DaniellStoneTheorem}. 

Uniqueness follows because a probability measure is determined by its
integrals over $U_b^d(S;\reals)$ (in fact over the subset of bounded
Lipschitz functions).  This follows because for any closed $F \subset
S$ we can define $f_n(x) = n d(x, F) \wedge 1$ so that $f_n
\downarrow \characteristic{F}$ and apply Montone Convergence (see the
proof of the Portmanteau Theorem \ref{PortmanteauTheorem} for complete
details on this argument).
\end{proof}

\begin{thm}[Prohorov's Theorem]\label{Prohorov}Let $(S,d)$ be a
  separable metric space, then a tight set of probability measures on
  $S$ is weakly relatively compact.  If $S$ is also complete then a
  weakly relatively compact set is tight.
\end{thm}
\begin{proof}
By the Portmanteau Theorem \ref{PortmanteauTheorem} we know that a
set of measures is tight if and only if its weak closure is tight
(sets only gain mass in a weak limit).  Thus it suffices to assume
that we have a closed tight set $M$ of measures.  Put a totally
bounded metric $\hat{d}$ on $S$ so that $U_b^{\hat{d}}(S;\reals)$ is separable
(Lemma \ref{SeparabilityOfBoundedUniformlyContinuous}); let $f_1, f_2,
\dotsc$ be a countable uniformly dense subset.  

Pick a sequence $\mu_n$ from $M$; we must show that it has a weakly
convergent subsequence.  For every fixed $f_m$
we know that $\abs{\int f_m \, d\mu_n} \leq \norm{f_m}_u < \infty$ so
there is a subsequence $N \subset \naturals$ such that $\int f_m \,
d\mu_n$ converges along $N$.  Since is true for every $m>0$ by a
diagonalization argument we know there is a subsequence $\hat{\mu}_{k}$ such that
$\lim_{k \to \infty} \int f_m \, d\hat{\mu}_{k}$ exists for every
$m>0$.  Define $\Lambda(f_m) =   \lim_{k \to \infty} \int f_m \,
d\hat{\mu}_{k}$ for every such $f_m$.  Our next goal is to extend
$\Lambda$ to all of $U_b^\rho(S;\reals)$.  Since $\Lambda$ is
uniformly continuous on a dense subset we know that a continuous
extension is defined; however we need a little bit more information.

Claim 1: $\lim_{k \to \infty} \int f \, d\hat{\mu}_{k}$ exists for every
$f \in U_b^\rho(S;\reals)$; moreover $\lim_{k \to \infty} \int f_m \,
d\hat{\mu}_{k} = lim_{m \to \infty} \Lambda(\hat{f}_m)$ where
$\hat{f}_m$ is any subsequence of $f_m$ that converges uniformly to $f$.

Pick a subsequence of the $f_m$ that converges to $f$.  Let that
subsequence be donoted $\hat{f}_m$ so that $\lim_{m \to \infty} \norm{\hat{f}_m -
  f}_\infty = 0$.  For every $m > 0$ we have
\begin{align*}
\int \hat{f}_m \, d \hat{\mu}_k -\norm{\hat{f}_m - f}_\infty &\leq 
\int f \, d\hat{\mu}_k \leq \int \hat{f}_m \, d \hat{\mu}_k + \norm{\hat{f}_m - f}_\infty
\end{align*}
and therefore taking limits in $k$ and using the definition of
$\Lambda$ at the points $f_m$,
\begin{align*}
\Lambda(\hat{f}_m) -\norm{\hat{f}_m - f}_\infty 
&\leq \liminf_{k \to \infty} \int f \, d\hat{\mu}_k 
\leq \limsup_{k \to \infty} \int f \, d\hat{\mu}_k 
\leq \Lambda(\hat{f}_m) + \norm{\hat{f}_m - f}_\infty
\end{align*}
Now letting $m$ go to infinity we get $\lim_{m \to \infty}
\Lambda(\hat{f}_m) = \lim_{k \to \infty} \int f \, d\hat{\mu}_k$.

As a result of the claim, we now define $\Lambda(f) = \lim_{k \to
  \infty} \int f \, d\hat{\mu}_{k}$ for every $f$  and it is clearly
linear (by linearity of integral and limits), nonnegative (by
monotonicity of integral) and satisfies $\Lambda(1) = 1$ (by direct
computation).  

To show that $\Lambda$ defines a probability measure, we bring the
tightness hypothesis to the table.  Pick $\epsilon > 0$ and by
tightness take a compact set $K \subset S$ such that $\sup_{\mu \in M}
\mu(K) > 1 - \epsilon$.  For any $f \in U_b^{\hat{d}}(S ; \reals)$ we
have
\begin{align*}
\abs{\Lambda(f)} 
&= \lim_{k \to \infty} \abs{\int f \, d\hat{\mu}_k}
= \lim_{k \to \infty} \abs{\int f \characteristic{K} \, d\hat{\mu}_k +
\int f \characteristic{S \setminus K} \, d\hat{\mu}_k} \leq \sup_{x
\in K} \abs{f(x)} + \epsilon \norm{f}_\infty
\end{align*}
so we may apply Lemma \ref{StoneDaniellProbability} to conclude there
exists a probability measure $\mu$ such that for all $f \in U_b^{\hat{d}}(S ; \reals)$ we
have $\Lambda(f) =  \lim_{k \to \infty} \abs{\int f \, d\hat{\mu}_k} =
\int f \, d\mu$.  Since $U_b^{\hat{d}}(S ; \reals)$ contains all
bounded Lipschitz functions by the Portmanteau Theorem
\ref{PortmanteauTheorem} we conclude $\mu_n$ converges weakly to
$\mu$.

Now assume that $S$ is complete and separable and let $M$ be a weakly
relatively compact set of measures.  Let $x_1, x_2, \dotsc$ be a countable dense
subset of $S$.  For every integer $n > 0$ we have $S =
\cup_{k=1}^\infty B(x_k, 1/n)$.  Thus $\cap_{N=1}^\infty \cap_{k=1}^N
B(x_k, 1/n)^c = \emptyset$ so by continuity of measure (Lemma
\ref{ContinuityOfMeasure}) for any fixed probability measure $\mu$ we
can find an $N_{n, \mu} > 0$ such that $\mu(\cap_{k=1}^{N_{n, \mu}}
B(x_k, 1/n)^c ) < \epsilon/2^n$.  We claim that, because $M$ is
compact, we can find an $N_n$ for
which this is true uniformly over the measures in $M$.

Claim 2: For every $n > 0$ there exists $N_n > 0$ such that $\mu(\cap_{k=1}^{N_{n}}
B(x_k, 1/n)^c ) < \epsilon/2^n$ for all $\mu \in M$.
 
We argue by contraction by reducing the case where $M$ is a singleton
set (where we have already shown the claim holds).  If Claim 2 is not
true then there exists $n$ such that for every integer $N>0$ we have
some $\mu_N \in M$ such that $\mu_N(\cap_{k=1}^{N} B(x_k, 1/n)^c )
\geq \epsilon/2^n$.  By sequential compactness of $M$ we know that
there is a weakly convergent subsequence $\mu_{N_j}$ such that
$\mu_{N_j} \toweak \mu$ for some probability measure $\mu$.  For every
$N > 0$ we have $\cap_{k=1}^{N} B(x_k, 1/n)^c$ is closed and therefore
by the Portmanteau Theorem \ref {PortmanteauTheorem}
\begin{align*}
\epsilon/2^n &\leq \limsup_{j \to \infty} \mu_{N_j}(\cap_{k=1}^{N_j}
B(x_k, 1/n)^c) \\
&\leq \limsup_{j \to \infty} \mu_{N_j}(\cap_{k=1}^{N} B(x_k, 1/n)^c)
\\
&\leq \mu(\cap_{k=1}^{N} B(x_k, 1/n)^c)
\end{align*}
where in the second inequality we have used the fact that the limit
only depends on the tail of the sequence of sets $\cap_{k=1}^{N_j}
B(x_k, 1/n)^c$ and by a union bound for sufficiently large $N_j$ we
have $\mu_{N_j}(\cap_{k=1}^{N_j} B(x_k, 1/n)^c)  \leq
\mu_{N_j}(\cap_{k=1}^{N} B(x_k, 1/n)^c)$.  To finish we get a
contradiction by taking the 
limit and using continuity of measure
\begin{align*}
0 < \epsilon/2^n \leq \lim_{N \to \infty} \mu(\cap_{k=1}^{N} B(x_k,
1/n)^c) = 0
\end{align*}

With Claim 2 proven we mimic the proof of Ulam's Theorem.  Let 
\begin{align*}
K &=
\cap_{m=1}^\infty \cup_{j=1}^{N_m} \overline{B}(x_j,\frac{1}{m})
\end{align*}
which is easily seen to be closed (hence complete) and by construction
is totally bounded thus is compact (Theorem
\ref{CompactnessInMetricSpaces})
and furthermore for all $\mu \in M$,
\begin{align*}
\mu(K^c) &\leq \mu((\cap_{m=1}^\infty
\cup_{j=1}^{N_m} B(x_j,\frac{1}{m}))^c) \\
&=\mu(\cup_{m=1}^\infty 
\cap_{j=1}^{N_m} B(x_j,\frac{1}{m})^c) \\
&=\sum_{m=1}^\infty \mu(
\cap_{j=1}^{N_m} B(x_j,\frac{1}{m})^c) \\
&\leq \sum_{m=1}^\infty \frac{\epsilon}{2^m} = \epsilon
\end{align*}

\end{proof}

\begin{lem}For $f,g \in C([0,\infty) ; \reals)$ define
\begin{align*}
\rho(f,g) &= \sum_{n=1}^\infty \frac{1}{2^n} \sup_{0 \leq t \leq n}
(\abs{f(t) - g(t)} \wedge 1)
\end{align*}
then $\rho$ is a metric on $C([0,\infty) ; \reals)$ and $C([0,\infty);
\reals)$ is complete and separable with respect to this metric.
\end{lem}
\begin{proof}
It is clear that $\rho(f,f) = 0$ and furthermore if $\rho(f,g) = 0$
then $f = g$ on every interval $[0,n]$ and therefore $f = g$.
Symmetry and the triangle inequality of $\rho$ is immediate from the
corresponding properties of the absolute value (TODO: OK the triangle
inequality may need a bit more of an argument).

We claim that the set of polynomials with rational coefficients is
dense in $C([0,\infty); \reals)$.  Pick $f \in C([0,\infty); \reals)$
and let $\epsilon > 0$ be given.  Now take $m > 0$ sufficiently large
so that $1/2^m < \epsilon / 2$ and by the Stone Weierstrass Theorem \ref{StoneWeierstrassApproximation} we
pick a polynomial with rational coefficients $p$ such that $\sup_{0
  \leq t \leq m} \abs{f(t) - p(t)} < \epsilon/2$ then we have
\begin{align*}
\rho(f,p) &\leq \sum_{n=1}^m\frac{1}{2^n} \sup_{0 \leq t \leq n}
\abs{f(t) - p(t)} + \sum_{n=m+1}^\infty \frac{1}{2^n} \\
&\leq \sup_{0 \leq t \leq m}
\abs{f(t) - p(t)} \sum_{n=1}^m\frac{1}{2^n} + \epsilon/2 <\epsilon
\end{align*}

Completeness follows from arguing over intervals $[0,n]$.  Suppose
$f_n$ is a Cauchy sequence in $C([0,\infty); \reals)$.  Given
$\epsilon > 0$ and $n > 0$ we can find $N > 0$ such that $\rho(f_m,
f_N) < \epsilon/2^n$ for all $m \geq N$.  Thus $\sup_{0 \leq t \leq n}
\abs{f_m(t) - f_N(t)} < \epsilon$ for all $m \geq N$ so we see that
$f_n$ is uniformly  Cauchy on every interval $[0,n]$.  By completeness
of $C([0,n];\reals)$ we know that the pointwise limit of $f_n$ exists
on every $[0,n]$ and is a continuous function.  Therefore we have a
limit $f$ defined on $[0,\infty)$ and since continuity is a local
property $f \in C([0,\infty); \reals)$.  It remains to show that $f_n$
converges to $f$ in the metric $\rho$.  This follows arguing as we
have above.  Let $\epsilon > 0$ be given and choose $n > 0$ such that
$\frac{1}{2^n} < \epsilon/2$ and choose $N > 0$ such that $\sup_{0 \leq t \leq n}
\abs{f_m(t) - f_N(t)} < \epsilon/2$ and then observe
\begin{align*}
\rho(f_m, f_N) &\leq \sum_{k=1}^n \frac{1}{2^k} \sup_{0 \leq t \leq k}
\abs{f_m(t) - f_N(t)} + \sum_{k=n+1}^\infty \frac{1}{2^k}  < \epsilon
\end{align*}
\end{proof}

The topology defined by $\rho$ is often refered to as the topology of
uniform convergence on compact sets by virtue of the following lemma.
\begin{lem}\label{UniformConvergenceOnCompacts}A sequence $f_n$
  converges to $f$ in $C^\infty([0,\infty), \reals)$ if and only if
  $f_n$ converges to $f$ uniformly on every interval $[0,T]$ for $T > 0$.
\end{lem}
\begin{proof}
TODO:  This is elementary.
\end{proof}

\begin{defn}Given a function $f : [0,T] \to \reals$ the \emph{modulus
    of continuity} is the function
\begin{align*}
m(T, f, \delta) &= \sup_{\substack{\abs{s - t} < \delta \\
0 \leq s,t \leq T}} \abs{f(s) - f(t)}
\end{align*}
\end{defn}

\begin{lem}For fixed $T > 0$ and $\delta > 0$, $m(T, f, \delta)$ is a
  continuous function on $C([0,\infty); \reals)$.  For fixed $T > 0$
  and function $f : \reals \to \reals$, $m(T,f,\delta)$ is
  nonincreasing in $\delta$ and 
\begin{align*}
\lim_{\delta \to 0} m(T, f, \delta) = 0
\end{align*}
provided $f \in C([0,\infty); \reals)$.
\end{lem}
\begin{proof}
To see continuity on $C([0,\infty); \reals)$ let $f \in C([0,\infty);
\reals)$, $T > 0$, $\delta > 0$ and $\epsilon > 0$ be given and pick $g$ that $\rho(f,g) <
\epsilon/2^{\ceil{T}+1}$.
From the definition of the metric $\rho$ for any $n > 0$, $\sup_{0 \leq t \leq n} \abs{f(t) - g(t)} \wedge
1 \leq 2^n \epsilon$, so for any $T > 0$, 
\begin{align*}
\sup_{0 \leq t \leq T} \abs{f(t) - g(t)} \wedge
1 &\leq \sup_{0 \leq t \leq \ceil{T}} \abs{f(t) - g(t)} \wedge
1 \leq \epsilon/2
\end{align*}  
Therefore by the triangle inequality,
\begin{align*}
\sup_{\substack{
\abs{s -t} < \delta \\
0 \leq s,t \leq T}} \abs{g(s) - g(t)} \wedge 1 
&\leq 
\sup_{\substack{
\abs{s -t} < \delta \\
0 \leq s,t \leq T}} \left ( \abs{g(s) - f(s)} + \abs{f(s) - f(t)} + \abs{f(t)
- g(t)} \right ) \wedge 1 \\
&\leq \epsilon/2 + 
\sup_{\substack{
\abs{s -t} < \delta \\
0 \leq s,t \leq T}} \abs{f(s) - f(t)} \wedge 1 + \epsilon/2
\end{align*}
and therefore arguing with the roles of $f$ and $g$ reversed shows 
$\abs{m(T, f, \delta) - m(T, g, \delta) } \leq \epsilon$.

The fact that $m(T, f, \delta)$ is decreasing in $\delta$ is clear
because the definition shows that for $\delta_1 \leq \delta_2$ we
have 
\begin{align*}
\lbrace \abs{f(t) - f(s) } \mid 0 \leq s,t \leq T \text{ and }
  \abs{s-t} < \delta_1 \rbrace 
&\subset 
\lbrace \abs{f(t) - f(s) } \mid 0 \leq s,t \leq T \text{ and }
  \abs{s-t} < \delta_2 \rbrace
\end{align*} and therefore $m(T, f, \delta_2) \leq
  m(T, f, \delta_1)$.

Lastly if we suppose $f \in C([0,\infty); \reals)$ then $f$ is
uniformly continuous on $[0,T]$ for every $T > 0$ (Theorem
\ref{UniformContinuityOnCompactSets}).  Thus given an $\epsilon > 0$
there exists $\delta>0$ such that 
\begin{align*}
\sup_{\substack{
\abs{s -t} < \delta \\
0 \leq s,t \leq T}} \abs{f(s) - f(t)} < \epsilon
\end{align*}
which shows $\lim_{\delta \to 0} m(T, f, \delta) = 0$.
\end{proof}
The following Theorem is a version of the Arzela-Ascoli Theorem of
real analysis.
\begin{thm}[Arzela-Ascoli Theorem]\label{ArzelaAscoliTheorem}A set $A
  \subset C([0,\infty); \reals)$ is relatively compact if and only if 
\begin{itemize}
\item[(i)]$\sup_{f \in A} \abs{f(0)} < \infty$
\item[(ii)]$\lim_{\delta \to 0} \sup_{f \in A} m(T, f, \delta) = 0$
  for all $T > 0$.
\end{itemize}
\end{thm}
\begin{proof}
To see the necessity of condition (i), observe that $\overline{A}$ is
compact and by completeness of $C([0,\infty); \reals)$ we know that
$\overline{A}$ comprises continuous functions.  Therefore we know that
$A \subset \overline{A} \subset \cup_{n=1}^\infty \lbrace f \in
C([0,\infty) \mid
\abs{f(0)} < n\rbrace$.  Since each $\lbrace f \in
C([0,\infty) \mid
\abs{f(0)} < n\rbrace$ is easily seen to be an open set, by
compactness of $\overline{A}$ we have a finite subcover which implies
there exists an $N$ such that $A \subset \overline{A} \subset \lbrace f \in
C([0,\infty) \mid
\abs{f(0)} < N\rbrace$.

To see the necessity of condition (ii), fix $\epsilon > 0$, $T > 0$
and define for each $\delta > 0$ the set 
\begin{align*}
F_\delta &= \lbrace f \in \overline{A} \mid m(T, f, \delta) \geq
\epsilon \rbrace
\end{align*}
By continuity of $m(T, f, \delta)$ we know that $F_\delta$ is closed.
Since $F_\delta \subset \overline{A}$ with $\overline{A}$ compact we
conclude that $F_\delta$ is compact.  Furthermore since for fixed $f
\in \overline{A}$ continuity (more specifically uniform continuity on compact
sets) implies $\lim_{\delta \to 0} m(T,f,\delta) = 0$, we know that
$\cap_{\delta > 0} F_\delta = \emptyset$.  By nestedness and
compactness of the
$F_\delta$ we know that there is some specific $\delta>0$ for which $F_\delta =
\emptyset$ (Lemma \ref{IntersectionOfNestedCompactSets}) and (ii) is established.

To see the sufficiency of conditions (i) and (ii), we first construct
the limiting subsequence on a the set of rationals $\rationals_+
\subset [0,\infty)$.  To do this, we first claim that for any $T \in
\rationals_+$, (in fact any $T \in [0,\infty)$, the set $\lbrace
\abs{f(x)} \mid f \in A \rbrace$ is bounded.  The claim follows for
$T>0$ by using (ii) to select a $\delta > 0$ such that $\sup_{f \in A} m(T, f,
\delta) < 1$.  Picking the integer $m \geq 0$ such that $m \delta < T \leq
(m+1)\delta$ and considering  the grid $0, \delta, 2\delta, \dotsc,
m\delta, T$ we can write the telescoping sum
\begin{align*}
f(T) - f(0) = f(T) - f(m\delta) + \sum_{k=1}^m f(k \delta) - f((k-1)\delta)
\end{align*}
and use the triangle inequality to conclude that $\abs{f(T)}
\leq \abs{f(0)} + m+1$ for every $f \in A$.  Coupled with (i) this shows that
$\sup_{f \in A} \abs{f(T)} < \infty$.

We now enumerate the rationals $\rationals_+$ and use
compactness in $\reals$ and a diagonal
subsequence argument to pick a sequence $f_n$ with $f \in A$ such that
$f_n(T)$ converges for every $T \in \rationals_+$.  Define $f :
\rationals_+ \to \reals$ by $f(T) = \lim_{n \to \infty} f_n(T)$.

Having selected a convergent subsequence $f_n$ and defined $f$ on
$\rationals_+$ we proceed to see that $f$ is uniformly continuous.
This follows by using (ii) to see that for every $f_n$, $T > 0$ and 
$\epsilon > 0$ there is $\delta > 0$ such that $\abs{f_n(s) - f_n(t)} <
\epsilon$ when $0 \leq s,t \leq T$ and $\abs{s - t} <\delta$.  From
this we have for every $n>0$, and $s,t \in \rationals$, $0 \leq s,t
\leq T$ and $\abs{s-t} < \delta$
\begin{align*}
\abs{f(s) -f(t)} &\leq \abs{f(s) -f_n(s)} + \abs{f_n(s) -f_n(t)} +
\abs{f_n(t) - f(t)} \\
&\leq \abs{f(s) -f_n(s)} + \epsilon +
\abs{f_n(t) - f(t)}
\end{align*}
Taking the limit as $n \to \infty$ using pointwise convergence of
$f_n$ to $f$ shows uniform continuity on every
$[0,T] \cap \rationals$ hence on $\rationals_+$.  Since $f$ is uniformly continuous on
$\rationals_+$ it follows that $f$ has a continuous extension to $f :
[0,\infty) \to \reals$.  Moreover we have shown that $\abs{f(s) -f(t)}
< \epsilon$ when $\abs{s -t} < \delta$.

It remains to prove that $f_n \to f$ in $C([0,\infty); \reals)$.  It
suffices (Lemma \ref{UniformConvergenceOnCompacts}) to show that $f_n
\to f$ uniformly on every interval $[0,T]$.  Let $T > 0$ be given.
Pick $\epsilon > 0$ and let
$\delta > 0$ be such that $m(T,f_n,\delta) < \epsilon$ (hence $m(T,f,\delta)
< \epsilon$ by the above comment).   Pick $N > 0$ such that
$\abs{f_n(k\delta) - f(k\delta)} < \epsilon/3$ for all $k=0,1, \dotsc,
\ceil{T/\delta}$ and $n \geq N$.  Then for every $0 \leq t \leq T$ and
$n \geq N$ let $k\geq 0$ be such that $k\delta \leq t < (k+1)\delta$
\begin{align*}
\abs{f_n(t) - f(t)} &\leq \abs{f_n(t) - f_n(k\delta)}
+\abs{f_n(k\delta) - f(k\delta)} +\abs{f(k\delta) - f(t)} < \epsilon
\end{align*}
and we are done.
\end{proof}

Provided with a characterization of compact sets in
$C^\infty([0,\infty); \reals)$ we can now state the probabilistic
analogue.
\begin{lem}\label{TightnessOfContinuousFunctions}A sequence of Borel probability measures $\mu_n$ on $C^\infty([0,\infty);
  \reals)$ is tight if and only if 
\begin{itemize}
\item[(i)]$\lim_{\lambda \to \infty} \sup_{n \geq 1} \sprobability{\abs{f(0)}
  \geq \lambda}{\mu_n} = 0$.
\item[(ii)] $\lim_{\delta \to 0} \sup_{n \geq 1} \sprobability{m(T, f,
  \delta) \geq \lambda}{\mu_n} = 0$ for all $\lambda > 0$ and $T > 0$.
\end{itemize}
\end{lem}
\begin{proof}
Let $\mu_n$ be a tight sequence.  Let $\epsilon > 0$ be given and pick
$K \subset C^\infty([0,\infty); \reals)$ compact with $\mu_n(K) >
1-\epsilon$ for all $n$.  Then by Theorem \ref{ArzelaAscoliTheorem} we know that $\sup_{f \in K}
\abs{f(0)} < \infty$ and therefore $\sprobability{\abs{f(0)}\geq
\lambda}{\mu_n} \leq \mu_n(K^c) < \epsilon$ for any $\lambda > \sup_{f
\in K} \abs{f(0)}$.  Thus (i) is shown.  Similarly applying Theorem \ref{ArzelaAscoliTheorem} we know that for
every $T > 0$ and $\lambda>0$
there exists $\delta>0$ such that $\sup_{f \in K} m(T, f, \delta) <
\lambda$.  Therefore $\lbrace f \mid m(T,f,\delta) \geq \lambda \rbrace
\subset K^c$ and by a union bound, for every $n>0$ we have $\sprobability{m(T,f,\delta) \geq
  \lambda}{\mu_n} \leq \sprobability{K^c}{\mu_n} < \epsilon$.
Therefore we have shown (ii).

Now assume that (i) and (ii) hold and suppose that $\epsilon > 0$ is
given.  By (i) there exists $\lambda > 0$ such that $\sup_{n \geq 1}
\sprobability{\abs{f(0)} \geq \lambda}{\mu_n} < \epsilon/2$.  By (ii)
for every integer $T > 0$ and $k > 0$, there exists a $\delta_{T,k}$
such that $\sup_{n \geq 1} \sprobability{m(T, f, \delta_{T,k}) \geq
  1/k}{\mu_n} < \epsilon/2^{T+k+1}$.  If we define 
\begin{align*}
A_T &= \lbrace f \mid 
m(T,f,\delta_{T,k}) < 1/k \text{ for all } k \geq 1\rbrace
\end{align*}
so that $A^c_T \subset \cup_{k=1}^\infty \lbrace f \mid m(T, f, \delta_{T,k}) \geq
  1/k \rbrace$ then by a union bound
\begin{align*}
\sup_{n \geq 1} \mu_n(A_T) &= \sup_{n \geq 1} (1 - \mu_n(A^c_T))\\
&\geq \sup_{n \geq 1} \left(1 - \sum_{k=1}^\infty \sprobability{m(T, f, \delta_{T,k}) \geq
  1/k}{\mu_n}\right ) \\
&\geq 1 - \epsilon/2^{T+1}
\end{align*}
If we define $K = \lbrace f \mid \abs{f(0)} < \lambda \rbrace \cap
\cap_{T=1}^\infty A_T$ then another union bound shows $\sup_{n \geq 1}
\mu_n(K) > 1 - \epsilon$ and by construction the set $K$ satisfies the
conditions of Theorem \ref{ArzelaAscoliTheorem} so is proven compact.  
\end{proof}

To prove that the rescaled and linearly interpolated random walk converges we need
prove tightness.  To prove tightness we need to show equicontinuity.
The following Lemma begins the process by demonstrating equicontinuity
at $0$.  Keep in mind the picture of the scaling of the random walk at
level $n$
which places the value of $S_j$ at the point $j/n$ scaled by the
factor $1/\sigma\sqrt{n}$.  With this geometry in mind note that what
we are proving is a bound for each of the sequence of rescaled random
walks on the interval $[0,\delta]$.

TODO: Replace $\epsilon$ by $\lambda$ in the following Lemma?
\begin{lem}\label{RandomWalkEquicontinuityAt0} Let $\xi_n$ be i.i.d. with mean $0$ and finite variance
  $\sigma^2$ and define $S_n = \sum_{k=1}^n \xi_k$.  Then for all
  $\epsilon > 0$ 
\begin{align*}
\lim_{\delta \to 0} \limsup_{n \to \infty} \frac{1}{\delta}
\probability{\max_{1 \leq j \leq \floor{n\delta}+1} \frac{\abs{S_j}}{\sigma\sqrt{n}} \geq
  \epsilon} = 0
\end{align*}
\end{lem}
\begin{proof}
The idea of the proof is to leverage the Central Limit Theorem and
Gaussian tail bounds to control behavior at the right endpoint of the
interval under consideration.  Then independence of increments and
finite variance can be used to control the behavior over the entire
interval.

The sequence of random variables $\frac{1}{\sigma
  \sqrt{\floor{n\delta}+1}}S_{\floor{n\delta}+1}$ is a subsequence of
$\frac{1}{\sigma\sqrt{n}}S_n$ and therefore converges in distribution
to $N(0,1)$ by the Central Limit Theorem.  Furthermore, $\lim_{n \to
  \infty} \frac{\sqrt{\floor{n\delta}+1}}{\sqrt{n\delta}} = 1$ so by
Slutsky's Lemma we also have $\frac{1}{\sigma\sqrt{n\delta}}
S_{\floor{n\delta}+1} \todist Z$ where $Z$ is an $N(0,1)$ Gaussian
random variable.  By the Portmanteau Theorem (Theorem
\ref{PortmanteauTheorem}) and
a Markov bound (Lemma \ref{MarkovInequality}) we have
\begin{align*}
\limsup_{n \to \infty}
\probability{\abs{\frac{1}{\sigma\sqrt{n\delta}}S_{\floor{n\delta}+1}}
  \geq \lambda} &\leq \probability{\abs{Z}  \geq \lambda} \leq \frac{\expectation{\abs{Z}^3}}{\lambda^3}
\end{align*}

We want to leverage this bound to create a maximal inequality that controls the entire interval of
values of the rescaled random walk the approach being to leverage the
fact that either the final point is in the tail (in which
case the Central Limit Theorem bound just proven applies) or the final
point is outside the tail and some interior point is in the tail
providing us with an amount of variation whose probability can be
controlled by use of a second moment bound.  With $\epsilon > 0$ fixed as in
the hypothesis of the Lemma, define the random variable $\tau
= \min \lbrace j \geq 1 \mid \abs{\frac{S_j}{\sigma\sqrt{n}} } >
\epsilon\rbrace$ (this is a stopping time though we make no use of the
concept here).  Pick $\delta > 0$ satisfying $0 < \delta <
\epsilon^2/2$.

\begin{align*}
&\probability{\max_{1 \leq j \leq \floor{n\delta} + 1} \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon
  } \\
&=\probability{\max_{1 \leq j \leq \floor{n\delta} + 1}
  \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon; 
\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}  \geq \epsilon -
\sqrt{2\delta}} \\
&+ \probability{\max_{1 \leq j \leq \floor{n\delta} + 1}
  \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon; 
\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}} < 
  \epsilon - \sqrt{2\delta}} \\
&\leq \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\sum_{j=1}^{\floor{n\delta}}
\probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}} <
  \epsilon - \sqrt{2\delta}; \tau = j} \\
&= \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\sum_{j=1}^{\floor{n\delta}}
\probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}} -
    \frac{S_j}{\sigma \sqrt{n}}} > \sqrt{2\delta}; \tau = j} \\
&\leq \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\frac{1}{2\delta}\sum_{j=1}^{\floor{n\delta}}
\expectation{\left(
    \frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}} - \frac{S_j}{\sigma
      \sqrt{n}} \right)^2 \characteristic{\tau = j}} \\
&= \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\frac{1}{2\delta}\sum_{j=1}^{\floor{n\delta}}
\expectation{\left( \sum_{i=j+1}^{\floor{n\delta}+1}
    \frac{\xi_i}{\sigma\sqrt{n}} \right)^2 } \probability{\tau = j} \\
&= \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\frac{\floor{n\delta}}{2 n \delta}\sum_{j=1}^{\floor{n\delta}}
\probability{\tau = j} \\
&= \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\frac{1}{2}\probability{\max_{1 \leq j \leq \floor{n\delta} + 1} \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon}
\end{align*}
Therefore we have shown that 
\begin{align*}
\probability{\max_{1 \leq j \leq \floor{n\delta} + 1} \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon}
&\leq 2 \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} 
\end{align*}
and we can use our tail bound derived from the Central Limit Theorem
(with $\lambda = \frac{\epsilon - \sqrt{2\delta}}{\sqrt{\delta}}$)
to see that 
\begin{align*}
\lim_{\delta \to 0} \limsup_{n \to \infty} \frac{1}{\delta} \probability{\max_{1 \leq j \leq \floor{n\delta} + 1} \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon}
&\leq \lim_{\delta \to 0 } \frac{2}{\delta} \expectation{\abs{Z}^3}
\left(\frac {\sqrt{\delta}}{\epsilon - \sqrt{2\delta}}\right)^3 = 0
\end{align*}
\end{proof}

The next step is to extend the estimate that provides equicontinuity
at $0$ to prove equicontinuity of the random walk on all finite intervals.  
\begin{lem}\label{RandomWalkEquicontinuity}
Let $\xi_n$ be i.i.d. with mean $0$ and finite variance
  $\sigma^2$ and define $S_n = \sum_{k=1}^n \xi_k$.  Then for all
  $\epsilon > 0$ and $T > 0$ 
\begin{align*}
\lim_{\delta \to 0} \limsup_{n \to \infty} 
\probability{\max_{
\substack{1 \leq j \leq \floor{n\delta}+1 \\
0 \leq k \leq \floor{nT}+1}} \frac{\abs{S_{j+k}
    - S_k}}{\sigma\sqrt{n}} \geq
  \epsilon} = 0
\end{align*}
\end{lem}
\begin{proof}
Pick $0 \leq \delta \leq T$ and let $m \geq 2$ be the integer such that
$T/m < \delta \leq T/(m-1)$.  Since
\begin{align*}
\lim_{n \to \infty} \frac{\floor{nT}+1}{\floor{n\delta}+1} &=
\frac{T}{\delta} < m
\end{align*}
we know that for sufficiently large $n$ we have $\floor{nT}+1  <
(\floor{n\delta}+1)m$.  For any such $n$, suppose $\frac{\abs{S_{j+k}
    - S_k}}{\sigma\sqrt{n}} > \epsilon$ for some $k$ with $0 \leq k
\leq \floor{nT}+1$ and some $j$ with $0 \leq j \leq
\floor{n\delta}+1$.  Now let $p$ be the integer such that $0 \leq p
\leq m -1$ and 
\begin{align*}
(\floor{n\delta}+1)p \leq k < (\floor{n\delta}+1)(p+1)
\end{align*}
Since $0 \leq j \leq \floor{n\delta}+1$ either 
\begin{align*}
(\floor{n\delta}+1)p \leq k+j < (\floor{n\delta}+1)(p+1)
\end{align*}
or
\begin{align*}
(\floor{n\delta}+1)(p+1) \leq k+j < (\floor{n\delta}+1)(p+2)
\end{align*}
In the first case by the triangle inequality we have
\begin{align*}
\abs{S_{j+k} - S_k} \leq \abs{S_{k} - S_{(\floor{n\delta}+1)p}}+ \abs{S_{j+k} - S_{(\floor{n\delta}+1)p}}
\end{align*}
and therefore we know that either $\frac{\abs{S_{k}
    - S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq \epsilon/2 > \epsilon/3$ or $\frac{\abs{S_{k+j}
    - S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq \epsilon/2 > \epsilon/3$.  In
the second case by the triangle inequality we have
\begin{align*}
\abs{S_{j+k} - S_k} \leq \abs{S_{k} - S_{(\floor{n\delta}+1)p}}+ \abs{S_{(\floor{n\delta}+1)(p+1)} - S_{(\floor{n\delta}+1)p}}+ \abs{S_{j+k} - S_{(\floor{n\delta}+1)(p+1)}}
\end{align*}
and therefore we know that either  
$\frac{\abs{S_{k} - S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq
\epsilon/3$,  
$\frac{\abs{S_{(\floor{n\delta}+1)(p+1)} -
    S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq \epsilon/3$ or 
$\frac{\abs{S_{k+j} - S_{(\floor{n\delta}+1)(p+1)}}}{\sigma\sqrt{n}}
\geq \epsilon/3$.  Therefore we have the inclusion of events
\begin{align*}
\left \lbrace\max_{
\substack{1 \leq j \leq \floor{n\delta}+1 \\
0 \leq k \leq \floor{nT}+1}} \frac{\abs{S_{j+k}
    - S_k}}{\sigma\sqrt{n}} \geq
  \epsilon\right \rbrace 
&\subset 
\bigcup_{p=0}^{m} \left \lbrace \max_{1 \leq j \leq \floor{n\delta}+1}
\frac{\abs{S_{j + (\floor{n\delta}+1)p} - S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq
\epsilon/3 \right \rbrace
\end{align*}
By the i.i.d. nature of $\xi_n$ and the fact that $S_0 = 0$ we know that 
\begin{align*}
\probability{\max_{1 \leq j \leq \floor{n\delta}+1}
\frac{\abs{S_{j + (\floor{n\delta}+1)p} - S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq
\epsilon/3 } &= \probability{\max_{1 \leq j \leq \floor{n\delta}+1}
\frac{\abs{S_{j}}}{\sigma\sqrt{n}} \geq \epsilon/3 }
\end{align*}
and therefore 
\begin{align*}
\probability{\max_{
\substack{1 \leq j \leq \floor{n\delta}+1 \\
0 \leq k \leq \floor{nT}+1}} \frac{\abs{S_{j+k}
    - S_k}}{\sigma\sqrt{n}}} \leq (m+1) \probability{\max_{1 \leq j \leq \floor{n\delta}+1}
\frac{\abs{S_{j}}}{\sigma\sqrt{n}} \geq \epsilon/3 }
\end{align*}
Since $\lim_{\delta \to 0} (m+1)\delta < \lim_{\delta \to 0} (T/\delta
+ 2) \delta = T < \infty$ we can apply Lemma \ref{RandomWalkEquicontinuityAt0} to get the result.
\end{proof}

By Prohorov's Theorem \ref{Prohorov} we know that a tight sequence of probability
measures on a separable metric space has a convergent subsequence.  What is often required is some
way of proving that a particular measure is indeed the limit of that
subsequence.  Recalling Lemma \ref{ProcessLawsAndFDDs} we know that
finite dimensional distributions characterize the laws of stochastic
processes which leads one to the following general procedure for
proving convergence of a sequence of processes.

TODO: Kallenberg (Chapter 16) has general results here for $C(T ; S)$ with $T$ a
$lcscH$-space and $S$ metric.  Of course there are also results for
spaces of discontinuous functions for use in proving convergence of
empirical distribution functions.  Kallenberg also has results for
point process/spaces of measures.

We are taking the point of view of Brownian motion and the linearly
interpolated random walk as being a random element in $C([0,\infty) ;
\reals)$.  On the other hand we have thus far treated a stochastic
process as a random element in a subset of a path space $(S^T,
\mathcal{S}^{\otimes T})$ \emph{equipped with the product
  $\sigma$-algebra}.  It is tempting to gloss over this
point, however to tie in the general definition of stochastic
processes with the random elements of $C([0,\infty); \reals)$ we are
dealing with it is
important to understand the relationship between the Borel
$\sigma$-algebra on $C([0,\infty); \reals)$ and the product
$\sigma$-algebra $\mathcal{B}(\reals)^{\otimes [0,\infty)}$ used in the definition of
processes.
\begin{lem}\label{BorelGeneratedByProjections}For every $t \in [0,\infty)$ let $\pi_t : C([0,\infty); \reals) \to
  \reals$ be the evaluation map $\pi_t(f) = f(t)$.  The Borel $\sigma$-algebra on $C([0,\infty); \reals)$ is
  equal to $\sigma(\lbrace \pi_t \mid t \in [0,\infty) \rbrace)$ and
  therefore $\mathcal{B}(C([0,\infty); \reals)) = C([0,\infty);
  \reals) \cap \mathcal{B}(\reals)^{\otimes [0,\infty)}$.
\end{lem}
\begin{proof}Since each $\pi_t$ is a continuous function, it is Borel
  measurable and therefore the Borel $\sigma$-algebra contains
  $\sigma(\lbrace \pi_t \mid t \in [0,\infty) \rbrace)$.

On the other hand, we know that $C([0,\infty) ; \reals)$ is separable
so we may pick a countable dense set $f_1, f_2, \dotsc$.
If we let $U \subset C([0,\infty) ; \reals)$ be open then for every
$f_j \in U$ there exists $r_j > 0$ such that $B(f_j, r_j) \subset U$
and $U$ is the union of such $B(f_j, r_j)$ (indeed, any $y \in U$ not in the
union of balls can't be the limit of the $f_j$ that are in $U$; on the
other hand it can't be the limit of the $f_j$ that are in $U^c$ since
the latter set is closed; thus the existence of such a $y$ would
contradict the density of $f_1, f_2, \dotsc$).  To show $U \in
\sigma(\lbrace \pi_t \mid t \in [0,\infty) \rbrace)$
it suffices to show that $B(f, r) \in \sigma(\lbrace \pi_t \mid t \in
[0,\infty) \rbrace)$ for every $f \in C([0,\infty) ; \reals)$ and $r > 0$.

Let $B(f, r)$ be given and note that by continuity of the
elements of $C([0,\infty) ; \reals)$ the closed ball
\begin{align*}
\overline{B(f, r)} &= \lbrace g \mid \sup_{x \in [0,\infty)} \abs{f(x)
  - g(x)} \leq r \rbrace \\
&= \lbrace g \mid \sup_{\substack{x \in
    [0,\infty) \\ x \in \rationals}} \abs{f(x)
  - g(x)} \leq r \rbrace \\
&= \cap_{\substack{x \in
    [0,\infty) \\ x \in \rationals}} \pi_x^{-1} ([f(x)-r,f(x)+r]) 
\end{align*}
which shows that $\overline{B(f, r)} \in \sigma(\lbrace \pi_t \mid t \in
[0,\infty) \rbrace)$ and $B(f, r) = \cap_{n=1}^\infty \overline{B(f, r+1/n)}$
which shows that $B(f, r) \in \sigma(\lbrace \pi_t \mid t \in
[0,\infty) \rbrace)$.
\end{proof}

\begin{thm}\label{ConvergenceInDistributionOfContinuousAsTightnessAndFDDs}Let $X_n$ be a tight sequence of continuous processes such
  that for all $d > 0$ and $0 \leq t_1 < \dotsb < t_d < \infty$ the
  sequence $(X_{n, t_1}, \dotsc , X_{n, t_d})$ converges in
  distribution, then the laws $X_n$ converge to a Borel probability
  distribution $\mu$ on $C([0,\infty); \reals)$ for which the
  canonical process $W_t(\omega) = \omega(t)$ satisfies
\begin{align*}
(X_{n, t_1}, \dotsc , X_{n, t_d}) \todist (W_{t_1}, \dotsc, W_{t_d})
\end{align*}
\end{thm}
\begin{proof}
By tightness and Prohorov's Theorem \ref{Prohorov} we know that $X_n$
has a weakly convergent subsequence.  Our first claim is that any two weakly convergent subsequences of
$X_n$ have the same limiting distribution.  Let $\check{X}_n$ and $\hat{X}_n$ be two
such subsequences and suppose that $\pushforward{\check{X}_n}{P} \to
\check{\mu}$ and $\pushforward{\hat{X}_n}{P} \to\hat{\mu}$
respectively.  Fix $0 \leq t_1 < \dotsb < t_d < \infty$ and note that
by the Continuous Mapping Theorem \ref{ContinuousMappingTheorem} we
know that $\pushforward{(\check{X}_{n,t_1}, \dotsc,
  \check{X}_{n,t_d})}{P} \todist \pushforward{(\pi_{t_1}, \dotsc,
  \pi_{t_d})}{\check{\mu}}$ and $\pushforward{(\hat{X}_{n,t_1}, \dotsc,
  \hat{X}_{n,t_d})}{P} \todist \pushforward{(\pi_{t_1}, \dotsc,
  \pi_{t_d})}{\hat{\mu}}$.  By hypothesis we conclude that $\pushforward{(\pi_{t_1}, \dotsc,
  \pi_{t_d})}{\check{\mu}} = \pushforward{(\pi_{t_1}, \dotsc,
  \pi_{t_d})}{\hat{\mu}}$ and therefore by
Lemma \ref{BorelGeneratedByProjections} we can apply Lemma
\ref{ProcessLawsAndFDDs} to conclude $\check{\mu}=\hat{\mu}$ which we
now refer to as $\mu$.

Now suppose that the distributions of $X_n$ do not converge weakly to
$\mu$.  Then there exists a bounded continuous $f$ such that either
$\lim_{n \to \infty} \expectation{f(X_n)}$ does not exist or exists
and is different from $\int f \, d\mu$.  In either case by the
boundedness of $f$ we know that 
\begin{align*}
-\infty &< -\norm{f}_\infty \leq \liminf_{n \to \infty}
\expectation{f(X_n)} \leq \limsup_{n \to \infty} \expectation{f(X_n)}
\leq \norm{f}_\infty  < \infty
\end{align*}
and we can extract
a subsequence $\check{X}_n$ such that $\lim_{n \to \infty}
\expectation{f(\check{X}_n)}$ exists and $\lim_{n \to \infty}
\expectation{f(\check{X}_n)} \neq \int f \, d\mu$.  This is a
contradiction since by tightness we know that $\check{X}_n$ has a weakly
convergent subsequence and we have already just shown that the limiting
distribution is $\mu$.
\end{proof}

The power of this Theorem is that it is often not too difficult to
prove weak convergence of finite dimensional distributions because we
have the power of a rich theory available (e.g. the Central Limit
Theorem, Slutsky's Theorem, characteristic functions).

\begin{lem}\label{ConvergenceOfRandomWalkFDD}Let $\xi_n$ be i.i.d. with mean $0$ and finite variance
  $\sigma^2$, define $S_n = \sum_{k=1}^n \xi_k$, $S_n^*(t) =
  S_{\floor{t}} + (t - \floor{t})\xi_{\floor{t}+1}$ and
  $X_n(t) = \frac{1}{\sigma \sqrt{n}} S_n^*(nt)$ where the latter are
  interpreted as random elements of the Borel measurable space
  $C([0,\infty);\reals)$.  For every $d > 0$ and real numbers $0 \leq t_1 < \cdots < t_d
  < \infty$ we have 
\begin{align*}
(X_n(t_1), \dotsc, X_n(t_d)) \todist (B_{t_1}, \dotsc, B_{t_d})
\end{align*}
where $B_t$ is a standard Brownian motion.
\end{lem}
\begin{proof}
Let $0 \leq t_1 < \dotsb < t_n < \infty$ be given.  The basic point is
that the result follows by the Central Limit Theorem; however due to
the linear interpolation there is a bit of extra work to do.

First note that by definition 
\begin{align*}
\abs{X_n(t) - \frac{1}{\sigma \sqrt{n}} S_{\floor{nt}} }
&\leq  \frac{1}{\sigma \sqrt{n}}\abs{\xi_{\floor{nt}+1}}
\end{align*}
so by a Chebyshev bound (Lemma \ref{ChebInequality}) we have
\begin{align*}
\lim_{n \to \infty} \probability{\abs{X_n(t) - \frac{1}{\sigma
      \sqrt{n}} S_{\floor{nt}} } > \epsilon}
&\leq  \lim_{n \to \infty} \frac{1}{n\epsilon^2} = 0
\end{align*}
thus $X_n(t)  \toprob \frac{1}{\sigma\sqrt{n}} S_{\floor{nt}}$ and by
Lemma \ref{ConvergenceInProbabilityInProductSpaces}
we have $(X_n(t_1), \dotsc, X_n(t_d)) \toprob (\frac{1}{\sigma\sqrt{n}}
S_{\floor{nt_1}}, \dotsc, \frac{1}{\sigma\sqrt{n}}
S_{\floor{nt_d}})$.  Our result will follow by Slutsky's Theorem
\ref{Slutsky} if we can show that
\begin{align*}
(\frac{1}{\sigma\sqrt{n}}
S_{\floor{nt_1}}, \dotsc, \frac{1}{\sigma\sqrt{n}}S_{\floor{nt_d}}) \todist (B_{t_1}, \dotsc, B_{t_d})
\end{align*}
Application of the Continuous Mapping Theorem
\ref{ContinuousMappingTheorem} lets us reduce further to showing that 
\begin{align*}
(\frac{1}{\sigma\sqrt{n}}(S_{\floor{nt_1}} -
S_{\floor{nt_0}}), \dotsc, \frac{1}{\sigma\sqrt{n}}
(S_{\floor{nt_d}}- S_{\floor{nt_{d-1}}}))\todist (B_{t_1} - B_{t_0}, \dotsc, B_{t_d} - B_{t_{d-1}})
\end{align*}
where for uniformity of notation we have defined $t_0 = 0$.  Since the
$\xi_n$ are independent this implies that the $S_{\floor{nt_j}}-
S_{\floor{nt_{j-1}}}$ are independent for $j=1, \dotsc, d$ and by
definition of independent increments property of Brownian motion we
know that $B_{t_j} - B_{t_{j-1}}$ are independent, thus by Lemma \ref{IndependenceProductMeasures} it suffices to
show that $\frac{1}{\sigma\sqrt{n}} (S_{\floor{nt_j}}-S_{\floor{nt_{j-1}}}) \todist N(0, t_j -
t_{j-1})$.  We shall prove this fact for an arbitrary $0 \leq s < t < \infty$.

By the definition of $S_n$ we write $\frac{1}{\sigma\sqrt{n}}
(S_{\floor{nt}}-S_{\floor{ns}}) =
\frac{1}{\sigma\sqrt{n}}\sum_{i=\floor{ns}+1}^{\floor{nt}}
\xi_i$.  For every $\epsilon > 0$ we have by another Chebyshev bound 
\begin{align*}
&\lim_{n \to \infty} \probability{\abs{\frac{1}{\sigma\sqrt{n}}
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i
- \frac{\sqrt{t-s}}{\sigma\sqrt{\floor{nt}-\floor{ns}}}
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i } > \epsilon} \\
&\leq \lim_{n \to \infty}\frac{1}{\epsilon^2}\variance{\left(\frac{1}{\sigma\sqrt{n}}
-\frac{\sqrt{t-s}}{\sigma\sqrt{\floor{nt}-\floor{ns}}} \right)
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i} \\
&= \lim_{n \to \infty}\frac{1}{\epsilon^2}\left(\frac{1}{\sigma\sqrt{n}}
-\frac{\sqrt{t-s}}{\sigma\sqrt{\floor{nt}-\floor{ns}}} \right)^2
(\floor{nt}-\floor{ns}) \sigma^2\\
&= \lim_{n \to \infty} \frac{1}{\epsilon^2} \left(
  \frac{\sqrt{\floor{nt}-\floor{ns}}}{\sqrt{n}} -
  \sqrt{t-s}\right)^2 = 0
\end{align*}
Therefore we have $\frac{1}{\sigma\sqrt{n}}
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i \toprob \frac{\sqrt{t-s}}{\sigma\sqrt{\floor{nt}-\floor{ns}}}
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i $ and one last appeal to
Slutsky's Theorem \ref{Slutsky} implies that it suffices to show
\begin{align*}
\frac{\sqrt{t-s}}{\sigma\sqrt{\floor{nt}-\floor{ns}}}
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i \todist N(0, t-s)
\end{align*}
which is just the Central Limit Theorem (and to be precise the
Continuous Mapping Theorem \ref{ContinuousMappingTheorem} to account for the multiplication by $\sqrt{t-s}$).
\end{proof}

The last step we make is in extending the equicontinuity of the random
walk to equicontinuity of the linearly interpolated random walk which
are honest elements of $C([0, \infty); \reals)$.  This equicontinuity
will prove tightness and weak convergence of the linearly interpolated
random walk.  One of the elements of proving the equicontinuity of the
linearly interpolated random walk is a general fact about the modulus
of continuity of a class of piecewise linear functions which we prove
as a separate lemma.

\begin{lem}\label{ModulusOfContinuityOfPL}Let $f(t)$ be a continuous function that is linear on every
  interval $[j,j+1]$ for $j=0, 1, \dotsc$.  For every integer $M
  > 0$ and $N > 0$, we have
\begin{align*}
\sup_{\substack{\abs{s -t} \leq M \\ 0 \leq s,t
    \leq N} } \abs{f(s) - f(t)}
&\leq
\sup_{\substack{1 \leq j \leq M \\ 0 \leq k
    \leq N}} \abs{f(k+j) - f(k)}
\end{align*}
\end{lem}
\begin{proof}
Pick $0 \leq s<t \leq M$.  If there exists $j < N$ such that $j \leq s
< t \leq j+1$ then it is clear from linearity  that $\abs{f(s) - f(t)}
\leq \abs{f(j) - f(j+1)}$ so it suffices to consider the case in which 
$j \leq s < j +1 < \dotsb < j+k < t \leq j+k+1$ for some $j \geq 0$ and $k
> 0$.  If we let $f(t)$ has slope $a_j$ on the interval $[j,j+1]$
then we can write $f(t) - f(s) = a_{j}(j+1 -s) + \dotsb + a_{j+k}(t -
j -k)$.  Note that
if $f(t) - f(s)$ has a different sign  than $a_j$ then $\abs{f(t) -
  f(s)} \leq \abs{f(t) -  f(j+1)}$ and similarly with $a_{j+k}$ so if
suffices to assume that $a_j$ and $a_{j+k}$ have the same sign as
$f(t)-f(s)$.  Now if $\abs{a_j} \leq \abs{a_{j+k}}$ then we slide the
pair $(s,t)$ to the right until either $s$ or $t$ hits an integer.
More formally if $j+1 - s \leq j+k+1 -t$ then we get
$\abs{f(t) -
  f(s)} \leq \abs{f(t + j+1-s) - f(j+1)}$ and if $j+k+1 -t \leq j+1 -
  s$
we get the bound $\abs{f(t) -
  f(s)} \leq \abs{f(j+k+1) - f(s + j+k+1 -t)}$.  If we $\abs{a_j} \geq
\abs{a_{j+k}}$ we slide to the left in an analogous way.  The point is
that we are reduced to the case in which either $s=j-1$ or $t=j+k+1$.

Once we know that either $s=j-1$ or $t=j+k+1$ , because $M$ is integer
we know that in fact $k \leq M$ and therefore we get a final bound
$\abs{f(t) - f(s)} \leq \abs{f(j+k+1) - f(j-1)}$ which proves the
result.

TODO: This proof is grotesque.  Try to do better!
\end{proof}

We are finally ready to put all of the pieces together to prove
Donsker's Theorem on the convergence of random walks to Brownian
motion.  Note that we have not used the existence of Brownian motion
anywhere in the proof so this Theorem is among other things an
existence proof for Brownian motion.
\begin{thm}[Donsker's Invariance Principle for Random Walks]\label{Donsker2}Let $\xi_n$ be i.i.d. with mean $0$ and finite variance
  $\sigma^2$, define $S_n = \sum_{k=1}^n \xi_k$, $S_n^*(t) =
  S_{\floor{t}} + (t - \floor{t})\xi_{\floor{t}+1}$ and
  $X_n(t) = \frac{1}{\sigma \sqrt{n}} S_n^*(nt)$ where the latter are
  interpreted as random elements of the Borel measurable space
  $C([0,\infty);\reals)$.  
Then the law of $X_n$  converges weakly to a probability measure under
which the coordinate mapping $(f,t) \to f(t)$ is a standard Brownian motion.
\end{thm}
\begin{proof}
Lemma \ref{ConvergenceOfRandomWalkFDD} shows that finite dimensional
distributions of the linearly interpolated and rescaled random walk
converge to the finite dimensional distributions of Brownian motion.
Therefore by Theorem
\ref{ConvergenceInDistributionOfContinuousAsTightnessAndFDDs} it
remains to show that $X_n$ is a tight sequence of processes.
By Lemma \ref{TightnessOfContinuousFunctions} we must show for all $X_n(t)$,
\begin{itemize}
\item[(i)]$\lim_{\lambda \to \infty} \sup_{n \geq 1} \probability{\abs{X_n(0)}
  \geq \lambda}= 0$.
\item[(ii)] $\lim_{\delta \to 0} \sup_{n \geq 1} \probability{m(T, X_n,
  \delta) \geq \lambda} = 0$ for all $\lambda > 0$ and $T > 0$.
\end{itemize}
Since $X_n(0) = 0$ the condition (i) holds trivially.  As for
condition (ii) we first argue that it suffices to show $\lim_{\delta \to 0} \limsup_{n \geq 1} \probability{m(T, X_n,
  \delta) \geq \lambda}= 0$.  This follows from the fact that for
fixed $n > 0$,
$\lim_{\delta \to 0} \probability{m(T, X_n,  \delta) \geq \lambda} =
0$ (continuity of $X_n$) and $\probability{m(T, X_n,  \delta) \geq \lambda}$ is a decreasing
function of $\delta$.  Indeed, if we let $\epsilon > 0$ be given pick
$\Delta > 0$ such that $\limsup_{n \geq 1} \probability{m(T, X_n,
  \delta) \geq \lambda} < \epsilon$ for all $\delta \leq \Delta$.  Then pick $N > 0$ is such that $\sup_{n \geq N} \probability{m(T, X_n,
  \Delta) \geq \lambda} < \epsilon$ and note that because $\probability{m(T, X_n,
  \delta) \geq \lambda}$ is decreasing in fact we have $\sup_{n \geq N} \probability{m(T, X_n,
  \delta) \geq \lambda} < \epsilon$ for all $\delta \leq \Delta$.
Since $\lim_{\delta \to 0} \probability{m(T, X_n,
  \delta) \geq \lambda} = 0 $ for every $n>0$ we can find
$\hat{\Delta} < \Delta$ such that $\probability{m(T, X_n,
  \delta) \geq \lambda}  < \epsilon$ for all $n=1, \dotsc, N-1$ and
$\delta \leq \hat{\Delta}$ and
thus $\sup_{n \geq 1} \probability{m(T, X_n,  \Delta) \geq \lambda} < \epsilon$ for all $\delta < \hat{\Delta}$.

With this reduction in hand, we can estimate
\begin{align*}
\probability{m(T, X_n,  \delta) \geq \lambda} &=
\probability{\sup_{\substack{\abs{s -t} \leq \delta \\ 0 \leq s,t
    \leq T}} \abs{X_n(s) - X_n(t)} \geq \lambda} \\
&\leq
\probability{\sup_{\substack{\abs{s -t} \leq \floor{n \delta} + 1 \\ 0 \leq s,t
    \leq \floor{T \delta}+1}} \abs{S^*_n(s) - S^*_n(t)} \geq \sigma
\sqrt{n} \lambda} \\
&\leq 
\probability{\sup_{\substack{1 \leq j \leq \floor{n \delta} + 1 \\ 0 \leq k
    \leq \floor{T \delta}+1}} \abs{S_n(k+j) - S_n(k)} \geq \sigma
\sqrt{n} \lambda} 
\end{align*}
where the last inequality follows Lemma \ref
{ModulusOfContinuityOfPL}.  Now we can apply Lemma
\ref{RandomWalkEquicontinuity} to conclude $\lim_{\delta \to
  0}\limsup_{n \to \infty} \probability{m(T, X_n,  \delta) \geq
  \lambda}$ and tightness is shown.
\end{proof}

\subsection{Riesz Representation}

\begin{defn}Let $\mu$ be a measure on the Borel $\sigma$-algebra of a
  topological space $S$.  
\begin{itemize}
\item[(i)] A Borel set $B$ is \emph{inner regular} if for
 $\mu(B) = \sup_{K \subset B} \mu(K)$ where $K$
  is compact. $\mu$ is inner regular if every Borel set is inner regular.
\item[(ii)]A Borel set $B$ is \emph{outer regular} if $\mu(B) = \inf_{U \supset B} \mu(U)$ where $U$
  is open.  A measure $\mu$ is outer regular if every Borel set
  $B$ is outer regular.
\item[(iii)] $\mu$ is \emph{locally finite} if every $x \in S$ has an
  open neighborhood $x \in U$ such that $\mu(U) < \infty$.
\item[(iv)] $\mu$ is a \emph{Radon measure} it is inner regular and
  locally finite.
\item[(v)] $\mu$ is a \emph{Borel measure} when?????  In some cases
  I've seen it required that $\mu(B) < \infty$ for all Borel sets $B$
  (reference?) and in other cases just that the Borel sets are measurable.
\item[(vi)]A Borel set  $B$ is \emph{closed regular} if $\mu(B) = \inf_{F \subset B} \mu(F)$ where $F$
  is closed (e.g. Dudley pg. 224).  A measure $\mu$ is closed regular
  if every Borel set $B$ is closed regular.
\item[(vii)] If $\mu$ is finite, then we say \emph{tight} if and only if
  X is inner regular (e.g. Dudley pg. 224).
\end{itemize}
\end{defn}

\begin{defn}Let $\mu$ be a Borel measure on a Hausdorff topological space. A set measurable set $A$ is called \emph{regular} if 
\begin{itemize}
\item[(i)]$\mu(A) = \inf_{U \supset A} \mu(A)$ where $U$ are open
\item[(ii)]$\mu(A) = \sup_{F \subset A} \mu(A)$ where $F$ are closed 
\end{itemize}
TODO: Alternative def assumes that $F$ are compact (see inner
regularity above).  If every measurable set is regular then $\mu$ is
said to be regular.  Note that if we assume the definition of
regularity uses compact inner approximations then regular measures are
inner and outer regular (although inner and outer regularity refer to
only Borel sets; is that a meaningful distinction?)
\end{defn}


TODO: Regularity of outer measures and the relationship to regularity
of measures as defined above (see Evans and Gariepy).  Note that
regularity of outer measure implies that if we take an outer measure $\mu$
and the measure on the $\mu$-measurable sets and then take the induced
outer measure we get $\mu$ back if and only $\mu$ is a regular outer
measure.  Evans and Gariepy show that Radon outer measures on
$\reals^n$ are inner
regular as measures on the $\mu$-measurable sets.  Note that inner
regular is part of the most common definition of Radon measure so
their result can be taken as showing a weaker definition of Radon
measure holds on $\reals^n$ (but also they phrase everything in terms
of outer measures...).

\begin{thm}Let $\mu$ be a finite Borel measure on a metric space $S$,
  then $\mu$ is closed regular.  If $\mu$ is tight then $\mu$ is regular.
\end{thm}
TODO: Specialize the definition of Radon measure in the presence of
more assumptions on $X$ (in particular local compactness,
$\sigma$-compactness, second countability).

TODO: Are Radon measures automatically outer regular?

Tao proves Riesz representation under assumption of local compactness
and $\sigma$-compactness.

Kallenberg proves Riesz representation under assumption of local
compactness and second countability (this is more general than the Tao
result as $\sigma$-compactness implies second countability (I think)).

Evans and Gareipy prove Riesz representation only on $\reals^n$.

Dudley proves Riesz representation of compact Hausdorff spaces (in
which cases the dual measures are Baire measures instead of Radon
measures).  Dudley does not really discuss Radon measures.  

\subsection{Covering Theorems in $\reals^n$}

Since our purposes have been to understand probability theory we have
hitherto avoided making assumptions that we are dealing with
$\reals^n$.  While this decision has benefits, it has drawbacks as
well.  Among them we lose sight of some history but also some of the
beautiful and deep understanding of the measure theory of the reals.
TODO: Vitali and Besicovich.

\subsection{Hausdorff Measure}

\subsubsection{Introduction}

In this section we discuss the construction of a family of outer
measures on $\reals^n$ called \emph{Hausdorff measures}.  Note the
construction can be generalized to metric spaces.  The following is
motivation why a tool like Hausdorff measure may be useful.  Suppose
very specifically that we are
in $\reals^3$, then the Lebesgue product measure essentially
corresponds to a notion of volume.  What about the surface area of a
$2$-dimensional object or the length of a $1$-dimensional object?  As
you may have learned in advanced calculus these ideas can indeed be
describe in great generality by the notion of differential forms.
However, the formalism of forms usually has some notion of smoothness
associated with it (hence the adjective differential); a natural question to ask is whether one can fine
a purely measure theoretic approach to the problem.  Hausdorff measures
provide one answer to this question.   The broad form of the theory
is perhaps a bit more general than one might expect; for any space
there is a Hausdorff outer measure for every real number $s$.  The
case of integers
$s=1$ corresponds to arclength, $s=2$ surface area, $s=3$ volume and so
on.  Measures with $s$ non-integral are
\emph{fractal}.  On $\reals^n$, the Hausdorff measure with $s=n$ is equal to
Lebesgue measure and any Hausdorff measure with $s > n$ is trivial
(gives $0$ measure to all sets).  We'll prove all of this and more in
what follows.

\subsubsection{Construction of Hausdorff Measure}

The following technical Lemma is useful (we'll use it when
discussing Hausdorff outer measures).  If the reader is in a hurry,
no harm will come from skipping over this result and returning to it
when the need arises.  Note that if the user is only interested in
probability theory this result may never come up.
\begin{lem}[Caratheodory Criterion]\label{CaratheodoryCriterion}Let $(S,d)$ be a metric space with an outer measure $\mu^*$.
  Then $\mu^*$ is a Borel outer measure (i.e. all Borel sets are
  $\mu^*$-measurable) if and only if $\mu^*(A \cup B) = \mu^*(A) +
  \mu^*(B)$ for all $A,B$ such that $d(A, B) > 0$.
\end{lem}
\begin{proof}
We begin with the only if direction.  Let $A$ be a closed set in $S$
and let $B \subset S$.  To show $A$ is $\mu^*$-measurable it suffices
to show $\mu^*(B) \geq \mu^*(A \cap B) + \mu^*(A^c \cap B)$.  Since
the inequality is trivially satisfied when $\mu^*(B) = \infty$ we
assume that $\mu^*(B) < \infty$.  For
every $n \in \naturals$, let $A_n =
\lbrace x \in S \mid d(x, A) \leq \frac{1}{n} \rbrace$.  By definition
of $A_n$, we have $d(A,
A_n^c) > \frac{1}{n} > 0$ and therefore $d(A \cap B, A_n^c \cap B)
> \frac{1}{n} > 0$.  Now by our assumption, we can conclude $\mu^*((A
\cap B) \cup (A_n^c \cap B)) = \mu^*(A \cap B) +
\mu^*(A_n^c \cap B)$.

We claim that $\lim_{n \to \infty} \mu^*(A_n^c \cap B) = \mu^*(A^c
\cap B)$.  Note that if we prove the claim the Lemma is proven because then we have
\begin{align*}
\mu^*(B) &\geq \mu^*((A
\cap B) \cup (A_n^c \cap B)) & & \text{by monotonicity}\\
&= \mu^*(A \cap B) +
\mu^*(A_n^c \cap B)
\end{align*}
and taking limits we have 
\begin{align*}
\mu^*(B) \geq \lim_{n\to \infty} \mu^*(A \cap B) +
\mu^*(A_n^c \cap B) &= \mu^*(A \cap B) +
\mu^*(A^c \cap B)
\end{align*}
To prove the claim we observe that monotonicity of outer measure
implies that $\lim_{n \to \infty} \mu^*(A_n^c \cap B) \leq \mu^*(A^c
\cap B)$ so we just need to
work on the opposite inequality.  To see it first define the rings
around $A$
\begin{align*}
R_n &= \lbrace x \mid \frac{1}{n+1} < d(x, A) \leq \frac{1}{n} \rbrace
\end{align*}
and note that because $A$ is closed, for each $n$,
\begin{align*}
A^c &= \lbrace x \in S \mid d(x, A) > 0 \rbrace \\
&=\lbrace x \in S \mid d(x, A) > n \rbrace \cup \bigcup_{m=n}^\infty \lbrace
x \in S \mid \frac{1}{m+1} < d(x, A) \leq \frac{1}{m} \rbrace \\
&=A_n^c \cup \bigcup_{m=n}^\infty R_m
\end{align*}
It follows that
$A^c \cap B = A_n^c \cap B \cup \cup_{m=n}^\infty
R_m \cap B$ and therefore by subadditivity of outer measure 
\begin{align*}
\mu^*(A^c \cap B) \leq \mu^*(A_n^c \cap B) + \sum_{m=n}^\infty
\mu^*(R_m \cap B)
\end{align*}
The claim will follow if we can show $\lim_{n \to \infty} \sum_{m=n}^\infty
\mu^*(R_m \cap B)=0$ which in turn will follow if we can show that $\sum_{m=1}^\infty
\mu^*(R_m \cap B)$ converges.  By construction, $d(R_{2m}, R_{2n})
> 0$ and therefore $d(R_{2m} \cap B, R_{2n} \cap B)
> 0$ for any $m \neq n$.  So if we consider only the even terms of the
series we can use our hypothesis to show that for any $n$
\begin{align*}
\sum_{m=1}^n \mu^*(R_{2m} \cap B) &= \mu^*(\cup_{m=1}^n
R_{2m} \cap B) \leq \mu^*(B) < \infty
\end{align*}
and by taking limits $\sum_{m=1}^\infty \mu^*(R_{2m} \cap B) \leq \mu^*(B)$
The same argument applies to the odd indexed terms and we get
\begin{align*}
\sum_{m=1}^\infty \mu^*(R_{m} \cap B) &\leq 2\mu^*(B) < \infty
\end{align*}
The claim and the Lemma follow.
\end{proof}


TODO:  Here I am taking the path of Evans and Gariepy and normalizing
Hausdorff measure so that $\mathcal{H}^n = \lambda_n$.  I am not sure
if this winds up being inconvenient when one considers Hausdorff
measure in arbitrary metric spaces (nor do I know whether we'll bother
considering Hausdorff measures in metric spaces).

\begin{lem}Let $\lambda_n$ be Lebesgue measure on $\reals^n$, then
  $\lambda_n(B(0, 1)) = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)}$.
\end{lem}
\begin{proof}
TODO
\end{proof}

\begin{defn}Let $(S,d)$ be a metric space and $A \subset S$, the
  \emph{diameter} of $A$ is 
\begin{align*}
\diam(A) &= \sup \lbrace d(x,y) \mid x,y \in A \rbrace
\end{align*}
\end{defn}

\begin{defn}Let $(S,d)$ be a metric space, $0 \leq s < \infty$ and $0
  < \delta$.  Then for $A \subset S$,
\begin{align*}
\mathcal{H}^s_\delta(A) &= \inf \lbrace \sum_{n=1}^\infty \alpha(s)
\left ( \frac{\diam(C_n)}{2}\right )^s \mid A \subset
\cup_{n=1}^\infty C_n \text{ where } \diam(C_n) \leq \delta \text{ for
  all } n\rbrace
\end{align*}
where 
\begin{align*}
\alpha(s) &= \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)}
\end{align*}
For $A$ and $s$ as above define
\begin{align*}
\mathcal{H}^s(A) &= \lim_{\delta \to 0} \mathcal{H}_\delta^s(A) = \sup_{\delta>0} \mathcal{H}_\delta^s(A)
\end{align*}
\end{defn}