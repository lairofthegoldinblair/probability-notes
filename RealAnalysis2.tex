\chapter{More Real Analysis}
Holding area for more advanced topics in real analysis that are
eventually required (and in some cases there may be some topics that I
am just interested in).
\section{Metric Spaces}

\section{Topological Spaces}

\begin{defn}Let $A$ be a set in a topological space then the \emph{closure} of $A$ is the set 
\begin{align*}
\overline{A} &= \lbrace x \in X \mid U \cap A \neq \emptyset \text{ for every open $U$ with $x \in U$} \rbrace
\end{align*}
\end{defn}

\begin{prop}\label{ClosureIntersectionOfClosedSets}The closure of a set $A$ is the intersection of all closed sets containing $A$.
\end{prop}
\begin{proof}
Suppose there is a closed set $F$ such that $A \subset F$ and $x \notin F$ the $x \in F^c$, $F^c$ is open and $F^c \cap A = \emptyset$; it follows that $x \notin \overline{A}$.  Suppose that
$x \notin \overline{A}$ then there exists and open neighborhood of $x$ such that $U \cap A = \emptyset$.  Since $U^c$ is closed, $x \notin U^c$ and $A \subset U^c$ it follows that $x$ is not in the 
intersection of all closed sets containing $A$.
\end{proof}
Another way of thinking about the previous result is to say that the closure of $A$ is that it is smallest closed set containing $A$.

\begin{cor}\label{DisjointOpenSetsDisjointClosure}If $A$ and $U$ are disjoint sets with $U$ open then $\overline{A}$ and $U$ are disjoint.
\end{cor}
\begin{proof}
The fact that $A \cap U = \emptyset$ equivalent to $A \subset U^c$.  Since $U^c$ is closed we know that $\overline{A} \subset U^c$ by the Proposition \ref{ClosureIntersectionOfClosedSets} which 
is equivalent to $\overline{A} \cap U = \emptyset$.
\end{proof}

Note that $\overline{U \cap V} \subset \overline{U} \cap \overline{V}$ but they aren't necessarily equal; this follows by considering open intervals $(0,1)$ and $(1,2)$ in $\reals$.

\begin{defn}Let $A \subset X$ then the \emph{interior} of $A$ is the set $\interior(A)$ of
  $x \in A$ such that there exists an open neighborhood $x \in U \subset A$.
\end{defn}

\begin{prop}The interior of $A$ is the largest open set $U \subset A$.
\end{prop}
\begin{proof}
First note that the largest open set in $A$ is the union of all open sets in $A$; this follows since
the latter set is open.  If $x$ is in the interior of $A$ then given the neighborhood $x \in U \subset A$ 
we see that all of $U$ is in the largest open subset of $A$ a fortiori this is true of $x$.  On the other hand given 
any element $x$ of the largest open subset of $A$; that largest open subset is the open neighborhood that shows 
that $x$ is in the interior of $A$.
\end{proof}

\begin{defn}Let $A \subset X$ then the \emph{boundary} of $A$ is the set $\partial A = \overline{A} \setminus \interior(A)$.
\end{defn}

\begin{lem}\label{OpenAlternative}A set $U \subset X$ is open if and only if for every $x \in
  U$ there is an open set $V \subset U$ such that $x \in V$.
\end{lem}
\begin{proof}
Suppose $U$ is open and $x \in U$, then let $V = U$.

Suppose for every $x \in U$ there exist an open set $V_x$ such that $x
\in V_x \subset U$.  Note that $\cup_x V_x \subset U$ because each
$V_x \subset U$ and on the other hand $\cup_x V_x \supset U$ since
every $x \in U$ satisfies $x \in V_x$.  Thus $U = \cup_x V_x$ which
shows that $U$ is open.
\end{proof}
\begin{defn}A mapping $f : X \to Y$ between topological spaces is said
  to be \emph{continuous} if and only if $f^{-1}(V)$ is open in $X$
  for every $V$ open in $Y$.
\end{defn}
\begin{defn}A mapping $f : X \to Y$ between topological spaces is said
  to be \emph{continuous at x} if and only if for every $V$ open in
  $Y$ such that $f(x) \in V$, there exists an open set $U$ in $X$ with $x \in U$ and $f(U)
  \subset V$.
\end{defn}
\begin{lem}A mapping $f : X \to Y$ between topological spaces is
  continuous if and only if it is continuous at $x$ for every $x \in X$.
\end{lem}
\begin{proof}
Suppose $f$ is continuous and let $x \in X$ and $V$ be open in $Y$
with $f(x) \in V$.  By continuity of $f$, we know that $f^{-1}(V)$ is
open in $X$ and $x \in f^{-1}(V)$.  By Lemma \ref{OpenAlternative} we
can pick an open set $U$ such that $x \in U$ and $U \subset
f^{-1}(V)$.  It follows that $f(U) \subset V$.

Now suppose $f$ is continuous at every $x \in X$ and let $V$ be open
in $Y$.  If $x \in f^{-1}(V)$ then $f$ is continuous at $x$ hence
there exists and open $U$ such that $x \in U$ and $f(U) \subset V$.
It follows that $U \subset f^{-1}(V)$ and by Lemma
\ref{OpenAlternative}  we have shown that $f^{-1}(V)$ is open.
\end{proof}

\begin{defn}A \emph{base} of a topology $\mathcal{T}$ at a point $x
  \in X$ is a collection
  of sets $\mathcal{B}$ such that for every open set $U \in
  \mathcal{T}$ such that $x \in U$ there exists a $B \in \mathcal{B}$ such
  that $x \in B \subset U$.  A base of a topology is a collection of
  sets that is a base at all points $x \in X$.
\end{defn}

\begin{lem}\label{CharacterizeBaseOfTopology}A set $\mathcal{B}$ of sets $B \subset X$ is a base of a
  topology if and only if for every $x \in X$ there exists $B \in
  \mathcal{B}$ such that $x \in B$ and for every $A, B \in
  \mathcal{B}$ and $x \in A \cap B$ there exists $C \in \mathcal{B}$
  such that $x \in C \subset A \cap B$.
\end{lem}
\begin{proof}
Suppose $\mathcal{B}$ satisfies the hypothesized conditions and let
\begin{align*}
\tau &= \lbrace U \subset X \mid \text { for every } x \in U \text{
  there exists } B \in \mathcal{B} \text{ such that } x \in B \subset
U \rbrace
\end{align*}
It is certainly the case that $\mathcal{B}\subset \tau$ and we claim that $\tau$ is a topology.  Certainly $\emptyset \in \tau$.
Let $U_\alpha$ for $\alpha \in \Lambda$ are sets in $\tau$.
Then if $x \in \cup_{\alpha \in \Lambda} U_\alpha$ there exists an
$\alpha \in \Lambda$ such that $x \in U_\alpha$ and by hypothesis we
pick $B$ such that $x \in B \subset U_\alpha \subset  \cup_{\alpha \in
  \Lambda} U_\alpha$.  If $U_1, \dotsc, U_n \in \tau$ and $x \in U_1
\cap \dotsc \cap U_n$ then there exists $B_1, \dotsc, B_n$ such that
$x \in B_j \subset U_j$ for $j = 1, \dotsc, n$ and therefore $x \in
B_1 \cap \dotsc \cap B_n \subset U_1 \cap \dotsc \cap U_n$.  A simple
induction on the hypothesis shows that $B_1 \cap \dotsc \cap B_n \in \mathcal{B}$.
Because $\mathcal{B}$ is cover of $X$ we have $X = \cup_{B \in
  \mathcal{B}} B \in \tau$ and therefore $\tau$ is a topology.  By the
definition of $\tau$ it is immediate that $\mathcal{B}$ is a base of
the topology.
\end{proof}


Since every set of subsets of $X$ is contained in the discrete topology the following definition makes sense.
\begin{defn}Let $A_\alpha$ be a set of subsets of $X$ then the topology \emph{generated by $A_\alpha$} is the intersection of topologies containing the $A_\alpha$.  Given a topology $\tau$ any 
set of subsets $A_\alpha$ for which the topology generated by $A_\alpha$ is equal to $\tau$ is called a \emph{generating set} or \emph{subbase} of $\tau$.
\end{defn}

The following result gives the reason for calling a generating set of a topology a subbase.
\begin{prop}\label{BaseFromSubbase}Let $X$ be a set and $A_\alpha$ be set of subsets of $X$.  Then the set of all finite intersections $A_{\alpha_1} \cap \dotsb \cap A_{\alpha_n}$ together with $X$ is a base for the topology generated by $A_\alpha$.  Conversely if the set of all finite intersections together with $X$ is a base for the topology of $X$ then the $A_\alpha$ generate the topology of $X$.
\end{prop}
\begin{proof}
Because we have included $X$ in the proposed base the first condition of Lemma \ref{CharacterizeBaseOfTopology} is immediate.  Similarly because the proposed base is closed under finite intersections the second condition of Lemma \ref{CharacterizeBaseOfTopology} is immediate.

In the opposite direction, it is immediate that the topology generated by the $A_\alpha$ is coarser than the topology on $X$.  Let $U$ be an open set in $X$ and let $x \in U$.  By assumption there exists a finite intersection $x \in A_{\alpha_1} \cap \dotsb \cap A_{\alpha_n} \subset U$ therefore $U$ is also open in the topology generated by the $A_\alpha$.
\end{proof}

It is useful to note that continuity of a map can be checked by using a subbase.
\begin{prop}\label{ContinuityViaSubbase}Let $X$ and $Y$ be topological spaces then $f$ is continuous if and only if $f^{-1}(U)$ is open for $U$ in a subbase of the topology of $Y$.
\end{prop}
\begin{proof}
The only if direction is trivial since any element of a subbase is open.  Suppose that $f^{-1}(U)$ is open for all $U$ in a subbase for the topology of $Y$.  Let $U_1, \dotsc, U_n$ be open sets in the subbase and use Lemma \ref{SetOperationsUnderPullback} to see that $f^{-1}(U_1 \cap \dotsb \cap U_n) = f^{-1}(U_1) \cap \dotsb \cap f^{-1}(U_n)$ is open.  Thus by Proposition \ref{BaseFromSubbase} we know that $f^{-1}(U)$ is open for every $U$ in a base of the topology.  Let $V$ be an
arbitrary open set of $Y$.  Let $x \in f^{-1}(V)$ and pick an element $U$ of the base such that $f(x) \in U$ and $U \subset V$.  By assumption $f^{-1}(U)$ is open in
$X$ and we see that $f^{-1}(U) \subset f^{-1}(V)$ and $x \in f^{-1}(U)$; it follows that $f^{-1}(V)$ is open.
\end{proof}

\begin{prop}\label{UnionOfTopologies}Let $X$ be a set and let $\tau_\alpha$ be topologies on $X$ then $\cap_\alpha \tau_\alpha$ is a topology on $X$.
\end{prop}
\begin{proof}
It is clear that $\emptyset \in \cap_\alpha \tau_\alpha$ and $X \in \cap_\alpha \tau_\alpha$.  Also given a collection of open sets $U_\beta \in \cap_\alpha \tau_\alpha$ we know that
$\cup_\beta U_\beta \in \tau_\alpha$ for every $\alpha$ hence $\cup_\beta U_\beta \in \cap_\alpha \tau_\alpha$.  Similarly we see that $\cap_\alpha \tau_\alpha$ is closed under finite intersections.
\end{proof}

\begin{prop}\label{RelativeTopology}Let $(X, \tau)$ be a topological space and $A \subset X$ then $\tau_A = \lbrace A \cap U \mid U \in tau \rbrace$ defines a topology on $A$.
\end{prop}
\begin{proof}
Since $X \in \tau$ and $A = A \cap X$ it follows that $A \in \tau_A$.  Similarly we see that $\emptyset \in \tau_A$.  If we let $A \cap U_\alpha$ be an arbitrary union in $\tau_A$ then since $\cup_\alpha U_\alpha \in \tau$ and $\cup_\alpha A \cap U_\alpha = A \cap \cup_\alpha U_\alpha$ we see that $\tau_A$ is closed under arbitrary union.  Similarly since 
\begin{align*}
(A \cap U_1) \cap \dotsb \cap (A \cap U_n) &= A \cap (U_1 \cap \dotsb \cap U_n)
\end{align*}
we see that $\tau_A$ is closed under finite intersections.
\end{proof}

\begin{defn}If $(X, \tau)$ be a topological space and $A \subset X$ then $\tau_A = \lbrace A \cap U \mid U \in tau \rbrace$ is called the \emph{relative topology} or the \emph{subspace topology} on $A$.  A set $U \in \tau_A$ is said to be \emph{relatively open}.  
\end{defn}

\begin{defn}
\begin{itemize}
\item[(i)]A topological space is said to be \emph{separable} if and
  only if it has a countable dense subset.
\item[(ii)]A topological space is said to be \emph{first countable} if and
  only if every point has a countable local base.
\item[(ii)]A topological space is said to be \emph{second countable} if and
  only if every the topology has a countable base.
\end{itemize}
\end{defn}

Second countability can be checked on a subbase.
\begin{prop}\label{SecondCountabilityViaSubbase}Let $X$ be a topological space then $X$ is second countable if and only if it has a countable subbase.
\end{prop}
\begin{proof}
The only if direction is trivial since any element of a base of a topology is also a subbase. Suppose that $X$ has a subbase then by Proposition \ref{BaseFromSubbase} it follows that the set of finite intersections is a base; this set is also countable.
\end{proof}

\begin{lem}\label{SeparabilitySecondCountabilityMetricSpaces}A metric space is separable if and only if it is second countable.
\end{lem}
\begin{proof}
TODO:
outline of proof is to pick a countable dense subset $\lbrace x_n
\rbrace$ and then pick the open balls $B(x_n; \frac{1}{m})$ for $m \in
\naturals$.  Show this is a base of the topology.
\end{proof}

TODO:  Fun fact, there is a non-first countable topological space
which is compact but not sequentially compact!  In first countable
spaces sequential compactness is equivalent to 

Separation axioms tells us that we have enough open sets in a topology
to distinguish features of the the underlying set (e.g. distinguishing
points from points or closed sets from closed sets).  Another way of
thinking about the size of a topology is by considering the number of
continuous functions that the topology allows.  The following theorem
shows that in normal topological spaces we have enough continuous
functions to approximate indicator functions of closed sets.

\begin{thm}[Urysohn's Lemma]\label{UrysohnsLemma}Let $X$ be a
  topological space, then following are equivalent
\begin{itemize}
\item[(i)]$X$ is normal
\item[(ii)]Given a closed set $F \subset X$ and an open neighborhood
  $F \subset U$ there is an open set $V$ such that $F \subset V
  \subset \overline{V} \subset U$.
\item[(iii)]Given disjoint closed sets $F$ and $G$ there exists a
  continuous function $f : X \to [0,1]$ such that $f \equiv 1$ on $F$
  and $f \equiv 0$ on $G$.
\item[(iv)]Given a closed set $F$ with an open neighborhood $U$ there
  is a continuous function $f$ such that $\characteristic{F}(x) \leq
  f(x) \leq \characteristic{U}(x)$ for all $x \in X$.
\end{itemize}
\end{thm}
\begin{proof}
(i) $\implies$ (ii): Since $U^c$ is and $F \cap U^c = \emptyset$ we
use normality to find disjoint open sets $V$ and $O$ such that $F \subset V$
and $U^c \subset O$.  Note that $\overline{V} \cap U^c = \emptyset$; if $x \in U^c$ then $O$ is an open neighborhood $x$ such that
$O \cap V$ which implies $x \notin \overline{V}$. Therefore we have $F
\subset V \subset \overline{V} \subset U$.

(ii) $\implies$ (i): Let $F$ and $G$ be closed subsets of $X$, it
follows that $G^c$ is open and $F \subset G^c$.  Find an open set $V$
such that $F \subset V \subset \overline{V} \subset G^c$ and observe
that if we define $U = \overline{V}^c$ then we have $V \cap U =
\emptyset$ and $F \subset V$ and $G \subset U$.

(iii) $\implies$ (iv): Construct continuous $f : X \to [0,1]$ such
that $f$ equals $1$ on $F$ and $f$ equals 0 on $U^c$.  Clearly
$\characteristic{F} \leq f$ and $\characteristic{U^c} \leq 1 -f$.  The
latter is equivalent to $f \leq \characteristic{U}$ since
$\characteristic{U^c} = 1 - \characteristic{U}$.

(iv) $\implies$ (iii):  Note that $F \subset G^c$ and construct $f$
such that $\characteristic{F} \leq f \leq \characteristic{G^c}$.  The
first inequality implies that $f \equiv 1$ on $F$ while the second
implies that $f \equiv 0$ on $(G^c)^c = G$.

(iii) $\implies$ (i):  Given $F$ and $G$ and a continuous function $f
: X \to [0,1]$ such that $F \subset f^{-1}(1)$ and $G \subset
f^{-1}(0)$, simply define $U =  f^{-1}(2/3,1]$ and $V = f^{-1}[0,1/3)$
and note that by continuity of $f$ both $U$ and $V$ are open.  

(ii) $\implies$ (iv):  We construct $f$ as a limit of (discontinuous)
indicator functions.  Suppose that $F$ and $U$ are given as in the
hypothesis in (iv).  Define $F_1 = F$ and $U_0 = U$.  Using (ii) we
find an open neighborhood $V$ such that $F_1 \subset V \subset
\overline{V} \subset U$.  Define $F_{1/2} = \overline{V}$ and $U_{1/2}
= V$ so we may rewrite our inclusions as 
\begin{align*}
F_1 &\subset U_{1/2} \subset F_{1/2} \subset U_{0}
\end{align*}
Now we iterate this construction.  To make it clear and to set the
notation for the iteration we turn the crank one more time we apply
(ii) to the pair $F_1 \subset U_{1/2}$ to construct an open set $U_{3/4}$
and closed set $F_{3/4}$ and to the pair $F_{1/2} \subset U_{0}$ to
construct an open set $U_{1/4}$
and closed set $F_{1/4}$ yielding the inclusions
\begin{align*}
F_1 &\subset U_{3/4} \subset F_{3/4} \subset U_{1/2} \subset F_{1/2} \subset U_{1/4} \subset F_{1/4} \subset U_{0}
\end{align*}
Now we induct over the dyadic rationals $\mathcal{D} = \lbrace a/2^n
\mid a \in \naturals \text{ and } n \in \naturals \rbrace \cap (0,1)$ so that we create a sequence
of open and closed sets $U_q$ and $F_q$ satisfying
\begin{itemize}
\item[(i)] $U_q \subset F_q$ for all $q \in \mathcal{D}$
\item[(i)] $F_r \subset U_q$ for all $r,q \in \mathcal{D}$ with $r > q$.
\end{itemize}
Now let $f(x) = \inf \lbrace q \mid x \in U_q \rbrace$.  
TODO: Show that $f$ works...
\end{proof}

\begin{thm}[Tietze's Extension
  Theorem]\label{TietzeExtensionTheorem}Let $F$ be a closed subset of
  a normal topological space, let $a < b$ be real numbers and let $f :
  F \to [a,b]$ be a continuous function.  There exists a continuous
  function $g : X \to [a,b]$ such that $g\mid_F = f$.  If $f : F \to
  \reals$ is a continuous function then there exists a continuous
  function $g: X \to \reals$ such that $g \mid_F = f$.
\end{thm}
\begin{proof}
We begin with the case of $f$ with bounded range.  We construct $g$
via an iterative procedure.  
TODO:
\end{proof}

\begin{defn}Given a topological space $(X, \mathcal{T})$ the Baire
  $\sigma$-algebra is smallest $\sigma$-algebra for which all bounded
  continuous functions are measurable.  Equivalently 
\begin{align*}
Ba(X,\mathcal{T}) &= \sigma(\lbrace f^{-1}(U) \mid U \subset \reals
\text{ is open; } f \in C_b(X,\reals)\rbrace)
\end{align*}
\end{defn}
\begin{lem}For every topological space $(X, \mathcal{T})$, $Ba(X)
  \subset \mathcal{B}(X)$.  For a metric space $(S,d)$, $Ba(S) = \mathcal{B}(S)$.
\end{lem}
\begin{proof}
To see the inclusion $Ba(X)
  \subset \mathcal{B}(X)$, note that by continuity of $f \in
  C_b(X;\reals)$, every set $f^{-1}(U)$ is open.

Now suppose $(S,d)$ is a metric space.  To show $\mathcal{B}(S)
\subset Ba(S)$, it suffices if we show every closed set $F \subset S$
can be written as $f^{-1}(G)$ where $G \subset \reals$ is closed and
$f \in C_b(S; \reals)$.  By the triangle inequality (see e.g. Lemma
\ref{DistanceToSetLipschitz}) we know
that $g(x) = d(x, F)$ is continuous (in fact Lipschitz) and by Lemma
\ref{MaxMinOfLipschitz} we know that $f(x) = d(x, F) \wedge 1$ is also
Lipschitz and therefore $f(x) \in C_b(S; \reals)$.  Because $F$ is
closed we also know that $F = f^{-1}(\lbrace 0 \rbrace)$ and we are done.
\end{proof}

\subsection{Nets}

Topology is a outgrowth of the search for a way to make sense
of convergence in general spaces.  An interesting question to ponder is the sense in which 
convergence properties characterize a topology.  As it turns out consideration of the convergence of
sequences is not enough define a topology.  The obstruction is that a non-first countable topology can
be so complicated in the neighboorhood of a point that no number of countable sequences is sufficiently discriminating
to pin down the topology.  The situation may be remedied by generalizing the notion of a sequence and studing the
convergence properties of these more general objects which are called \emph{nets}.

\begin{defn}A \emph{directed set} is a set $\mathcal{I}$ with a relation $\prec$ with the properties 
\begin{itemize}
\item[(i)] (Reflexive) $\alpha \prec \alpha$ for all $\alpha \in \mathcal{I}$.
\item[(ii)] (Transitive) For all $\alpha, \beta, \gamma \in \mathcal{I}$ if $\alpha \prec \beta$ and $\beta \prec \gamma$ then $\alpha \prec \gamma$
\item[(iii)] For every $\alpha, \beta \in \mathcal{I}$ there exists $\gamma \in \mathcal{I}$ such that
$\alpha \prec \gamma$ and $\beta \prec \gamma$.
\end{itemize}
A \emph{net} in a topological space $(X, \tau)$ is a directed set $(\mathcal{I}, \prec)$ and a mapping from
$x : \mathcal{I} \to X$.  We generally denote the image of $\alpha \in \mathcal{I}$ under net as $x_\alpha$ and denote the entire net as $\net{x}{\alpha}$.  A net is said to 
\emph{converge} to a point $x \in X$ if and only if for every every open set $U$ containing $x$ there is an $\alpha_0 \in \mathcal{I}$ such
that $x_\alpha \in U$ for all $\alpha \succ \alpha_0$.  If a net converges to $x$ then $x$ is said to be a \emph{limit} of the net.  More generally 
a point $x \in X$ is said to be a \emph{cluster point} of $\net{x}{\alpha}$ if for any open set $U$ with $x \in U$ and $\alpha \in \mathcal{I}$ there exists
$\beta \in \mathcal{I}$ such that $\beta \succ \alpha$ and $x_\beta \in U$. 
\end{defn}

The first thing to note is that the concept of nets is generalization of the concept of sequences.  
\begin{examp}The totally ordered naturals numbers $(\naturals, <)$ is a directed set.  A net with directed set equal to $(\naturals, <)$ is simply a sequence.  It is easy to see that the definitions of
convergence, limits and cluster points of sequences are the same as the corresponding definitions for nets when the directed set is $(\naturals, <)$.  All of these statements extend to directed sets constructed from any infinite subset of $\integers$ that is bounded below.
\end{examp}

Noting that the concept of nets generalizes sequences is somewhat comforting but in no way makes it clear why the definition in question is of use.  The following example clarifies the
situtation and provides the other main source of directed sets used in constructing nets.
\begin{examp} Let $X$ be a topological space and $x \in X$ then the set of all open neighboorhoods of $x$ is a directed set under the relation $U \prec V$ if and only if $V \subset U$.  More generally note that by Lemma \ref{CharacterizeBaseOfTopology} a set $\mathcal{I}$ of open neighborhoods of $x \in X$ is a directed set if and only if $(\mathcal{I}. \supset)$ is a directed set.
\end{examp}

\begin{prop}\label{ClosureAndNets}Given a topological space $X$ and a subset $A \subset X$ then
\begin{itemize}
\item[(i)] $x \in X$ is an accumulation point of $A$ if and only if there exists a net $\net{x}{\alpha}$ with $x_\alpha \in A \setminus \lbrace x \rbrace$ and $x_\alpha \to x$.
\item[(ii)] $x \in \overline{A}$ if and only if there exists a net $\net{x}{\alpha}$ with $x_\alpha \in A$ and $x_\alpha \to x$.
\item[(iii)] $A$ is closed if and only if there is no net in $A$ that converges to a point in $A^c$.
\end{itemize}
\end{prop}
\begin{proof}
TODO:
\end{proof}

It is now very easy to see that convergence of nets characterizes a topology.
\begin{cor}\label{UniquenessOfConvergenceClasses}Let $X$ be a set and $\tau$ and $\sigma$ be topologies on $X$.  Suppose that every net $\net{x}{\alpha}$ that converges in $(X,\tau)$ also converges in $(X,\sigma)$ then it follows that $\sigma \subset \tau$.
\end{cor}
\begin{proof}
We argue by contradiction.  Let $U$ be open in $\sigma$ but not open in $\tau$.  It follows that $U^c$ is not closed in $\tau$ and therefore by Proposition \ref{ClosureAndNets} there exists a 
net $\net{x}{\alpha}$ with $x_\alpha \in U^c$, $x \in U$ and $\net{x}{\alpha}$ converges to $x$ in $\tau$.  By hypothesis $\net{x}{\alpha}$ converges to $x$ in $\sigma$ as well and since $U^c$ is closed in $\sigma$ another application of Proposition \ref{ClosureAndNets} implies $x \in U^c$ which is the desired contradiction.
\end{proof}

\begin{prop}\label{DirectedSetProduct}Let $\mathcal{K}$ be an index  set (not necessarily a directed set) and for each $\alpha \in \mathcal{K}$ let $(\mathcal{I}_\alpha, \prec_\alpha)$ be a directed set.  Let $\mathcal{I} = \Pi_{\alpha \in \mathcal{K}} \mathcal{I}_\alpha$ be the cartesian product and for $x, y \in \mathcal{I}$ define $x \prec y$ if and only if $x_\alpha \prec_\alpha y_\alpha$ for all $\alpha \in \mathcal{K}$, then $(\mathcal{I}, \prec)$.
\end{prop}
\begin{proof}
Reflexivity and transitivity of $\prec$ follow immediately from the corresponding properties of the $\prec_\alpha$.  Now suppose that $x, y \in \mathcal{I}$ and for each $\alpha \in \mathcal{K}$ pick $z_\alpha \in \mathcal{I}_\alpha$ such that $x_\alpha \prec_\alpha z_\alpha$ and $y_\alpha \prec_\alpha z_\alpha$.  Let $z \in \mathcal{I}$ be defined by the $z_\alpha$ then if follows that $x \prec z$ and $y \prec z$.
\end{proof}

As a warmup to see the utility of nets consider the following.
\begin{prop}\label{HausdorffUniqueLimits}A topological space $X$ is Hausdorff if and only if every convergent net has a unique limit.
\end{prop}
\begin{proof}
Suppose that $\net{x}{\alpha}$ is convergent and that $x \neq y$ are both limits of $\net{x}{\alpha}$.  Let $U$  be an open neighborhood of $x$ and $V$  be an open neighborhood of $y$.  There exists $\alpha_x$ such that $x_\alpha \in U$ for $\alpha \succ \alpha_x$ and an $\alpha_y$ such that $x_\alpha \in V$ for $\alpha \succ \alpha_y$.  Now pick a
$\beta$ such that $\beta \succ \alpha_x$ and $\beta \succ \alpha_y$ so that $x_\alpha \in U \cap V$ for $\alpha \succ \beta$.  In particular it follows that $U \cap V \neq \emptyset$ and it follows that $X$ is not Hausdorff.

Now suppose that every convergent net has a unique limit.  For each $x \in X$ recall that the set of open neighborhoods of $x$ is a directed set under set inclusion; call this directed set $\mathcal{I}_x$.  Suppose that $x \neq y$ and that $x$ and $y$ have no disjoint open neighboorhoods.   Construct the product directed set $\mathcal{I}_x \times \mathcal{I}_y$ ( Proposition \ref{DirectedSetProduct})  and define the net $x_{U,V}$ by selecting an arbitrary point in $U \cap V$.  If $U$ is an arbitrary open neighborhood of $x$ and $(V,W) \succ (U,X)$ then $x_{V,W} \in V \cap W \subset U$ hence $x_{U,V} \to x$.  By the same argument $x_{U,V} \to y$ which is a contradiction.
\end{proof}



Now we discuss the relationship between compactness and nets.  The goal is to provide a net-based analogy of sequential compactness that suffices to characterize compactness in general topological spaces; essentially we want to prove a result that says compactness is equivalent to every net having a convergent subnet.   With this goal in mind it is natural that the first step is to define a subnet.
\begin{defn}If $\mathcal{J}$ is a directed set and $y : \mathcal{J} \to X$ is a net then we say that $\net{y}{\beta}$ is a \emph{subnet}
of $\net{x}{\alpha}$ if there exists a map $\iota : \mathcal{J} \to \mathcal{I}$ such that $y = x \circ \iota$ (i.e. $x_{\iota(\beta)} = y_\beta$) and for each $\alpha \in \mathcal{I}$ there
exists $\beta \in \mathcal{J}$ such that $\iota(\gamma) \succ \alpha$ for all $\gamma \succ \beta$. 
\end{defn}

A subtle but important point about the definition of a subnet is that the map $\iota : \mathcal{J} \to \mathcal{I}$ need not be injective.  Another way of saying the same thing is that a subnet of a net with directed set $\mathcal{I}$ is a more general object than a cofinal subset of $\mathcal{I}$ (a cofinal subset $\mathcal{J} \subset \mathcal{I}$ being one with the property that for every $\alpha \in \mathcal{I}$ there exists $\beta \in \mathcal{J}$ with $\beta \succ \alpha$).  It is critical that it be generalized in this way; Kelley has an exercise that provides an example of a compact set and a net which contains no convergent cofinal subset.  The following construction illustrates how we use the more general notion of a subnet and begins the process of being able to construct convergent subnets.

\begin{prop}\label{SubnetConstruction}Let $X$ be a topological space and let $\mathcal{C}$ be a set of subsets of $X$ that is closed under finite intersection.  Suppose $\net{x}{\alpha}$ is a net in $X$ with index set $\mathcal{I}$ such that for every $A \in \mathcal{C}$ and $\alpha \in \mathcal{I}$ there exists $\beta \in \mathcal{I}$ such that $\beta \succ \alpha$ and $x_\beta \in A$.  There exists a subnet $\net{y}{\beta}$ of $\net{x}{\alpha}$ such that for every $A \in \mathcal{U}$ there exists $\gamma \in \mathcal{I}$ such that for every $\beta \succ \gamma$ and $x_\beta \in A$.
\end{prop}
\begin{proof}
Since $\mathcal{C}$ has the finite intersection property $(\mathcal{C}, \supset)$ is a directed set.  
\begin{clm}Let 
\begin{align*}
\mathcal{J} &= \lbrace (\alpha, A) \in \mathcal{I} \times \mathcal{C} \mid x_\alpha \in A \rbrace
\end{align*}
then $\mathcal{J}$ is directed set under the product ordering on $\mathcal{I} \times \mathcal{C}$ (i.e. $(\alpha,A) \prec (\beta, B)$ if and only if $\alpha \prec \beta$ and $A \supset B$).
\end{clm}
Suppose that $(\alpha, A), (\beta, B) \in \mathcal{J}$.  By the finite intersection property of $\mathcal{C}$ we know that $A \cap B \in \mathcal{C}$.  Since $\mathcal{I}$ is a directed set we can find
$\gamma \in \mathcal{I}$ such that $\gamma \succ \alpha$ and $\gamma \succ \beta$.  Since $A \cap B \in \mathcal{C}$ by the hypothesis on $\net{x}{\alpha}$ we can find $\delta \succ \gamma$ such that $x_\delta \in A \cap B$.  It follows that $(\delta, A \cap B) \in \mathcal{J}$, $(\alpha, A) \prec (\delta, A \cap B)$ and $(\beta, B) \prec (\delta, A \cap B)$.

Now let $\iota : \mathcal{J} \to \mathcal{I}$ be defined by $\iota((\alpha, A)) = \alpha$ and $y_{(\alpha,A)} = x_\alpha$.  Given any $\alpha \in \mathcal{I}$ then we can pick an arbitrary $B \in \mathcal{C}$ and find $\beta \succ \alpha$ with $x_\beta \in B$ (i.e. $(\beta, B) \in \mathcal{J}$); it follows from the definition of the product order that for every $(\gamma,C) \succ (\beta,B)$ we have $\gamma \succ \beta$ and therefore by transitivity of $\succ$, $\iota((\gamma,C)) \succ \alpha$.  It follows that $\net{y}{(\alpha,A)}$ is a subnet of $\net{x}{\alpha}$.  

To finish the proof let $A \in \mathcal{C}$.  As noted above there exists $\alpha \in \mathcal{I}$ such that $(\alpha, A) \in \mathcal{J}$ and by definition of $\mathcal{J}$ this equivalent to $y_{(\alpha,A)} = x_\alpha \in A$.  Now for any $(\beta, B) \succ (\alpha, A)$ we know that $x_\beta \in B$ and $B \subset A$ hence also have $y_{(\beta,B)} \in A$.
\end{proof}

We now use subnets to prove an alternative characterization of cluster points of nets.
\begin{prop}\label{ClusterPointsAndSubnets}Let $X$ be a topological space and $\net{x}{\alpha}$ be a net in $X$ then $x \in X$ is a cluster point of $\net{x}{\alpha}$ if and only if there is a 
subnet of $\net{x}{\alpha}$ that converges to $x$.
\end{prop}
\begin{proof}
If $x$ is a cluster point of the net $\net{x}{\alpha}$ then the existence of a subnet follows by applying Proposition \ref{SubnetConstruction} to the set of open neighborhoods of $x$.  On the other hand, suppose that there is a subnet $\net{y}{\beta}$ that converges to $x$.  Let $\mathcal{I}$ be the index set of $\net{x}{\alpha}$ and $\mathcal{J}$ be the index set of $\net{y}{\beta}$ with $\iota : \mathcal{J} \to \mathcal{I}$ defining the subnet.  Let $U$ be an open neighborhood of $x$ and let $\alpha \in \mathcal{I}$. By the convergence of $\net{y}{\beta}$ there exists $\beta \in \mathcal{J}$ such that $y_\delta \in U$ for all $\delta \succ \beta$.  By the subnet property there exists $\gamma \in \mathcal{J}$ such that $\iota(\delta) \succ \alpha$ for all $\delta \succ \gamma$.  Since $\mathcal{J}$ is a directed set there exists $\delta \in \mathcal{J}$ with $\delta \succ \beta$ and $\delta \succ \gamma$ hence $\iota(\delta) \succ \alpha$ and $x_{\iota(\delta)} = y_\delta \in U$.
\end{proof}

Here is yet another interpretation of the defintion of a cluster point.
\begin{lem}\label{ClusterPointsAndClosures}Let $X$ be a topological space and let $\net{x}{\alpha}$ be a net with index set $\mathcal{I}$.  For each $\alpha \in \mathcal{I}$ define
\begin{align*}
A_\alpha &= \lbrace x_\beta \mid \beta \succ \alpha \rbrace
\end{align*}
Then $x$ is a cluster point of $\net{x}{\alpha}$ if and only if $x \in \cap_{\alpha \in \mathcal{I}} \overline{A_\alpha}$.
\end{lem}
\begin{proof}
Let $x$ be a cluster point of $\net{x}{\alpha}$, let $U$ be an open neighborhood of $x$ and $\alpha \in \mathcal{I}$.  By definition of a cluster point there exists $\beta \succ \alpha$ such that $x_\beta \in U$ hence $U \cap A_\alpha \neq \emptyset$.  It follows by definition of closure that $x \in \overline{A_\alpha}$.

Now suppose that $x \in \cap_{\alpha \in \mathcal{I}} \overline{A_\alpha}$ and let $U$ be an open neighborhood of $x$.  Let $\alpha \in \mathcal{I}$ so that by definition of closure $U \cap A_\alpha \neq \emptyset$.  Now by the definition of $A_\alpha$ it follows that for some $\beta \succ \alpha$ we have $x_\beta \in U$ which shows $x$ is a cluster point of $\net{x}{\alpha}$.
\end{proof}

Compare the following to Lemma \ref{IntersectionOfNestedCompactSets} which was formulated for metric spaces.  It simply reformulates the definition of compactness using complementation.
\begin{lem}\label{IntersectionsOfClosedSetsAndCompactness}Let $X$ be a topological space then $X$ is compact if and only if every set $\mathcal{C}$ of closed subsets of $X$ for which
every finite intersection is nonempty also satisfies $\cap{A \in \mathcal{C}} A \neq \emptyset$.
\end{lem}
\begin{proof}
By DeMorgan's Law, $\cap{A \in \mathcal{C}} A = \left(\cap{A \in \mathcal{C}} A^c \right)^c$.  Therefore $\cap{A \in \mathcal{C}} A = \emptyset$ is the $A^c$ for $A \in \mathcal{C}$ are an open cover of $X$.  So, suppose that $X$ is compact and $\cap{A \in \mathcal{C}} A \neq \emptyset$ then it follows there is a finite subcover $A_1^c, \dotsc, A_n^c$ over $X$ hence $A_1 \cap \dotsb \cap A_n = \emptyset$.  On the other hand suppose that every $\mathcal{C}$ with the finite intersection property has non empty intersection.  Then let $U_\alpha$ be a family of open subsets.  Observe that $U_\alpha$ has no finite subcover if and only if $U_\alpha^c$ has the finite intersection property if and only if $U_\alpha$ is not a cover of $X$.  This proves compactness of $X$. 
\end{proof}

\begin{thm}\label{CompactnessAndNets}Let $X$ be a topological space then $X$ is compact if and only if every net in $X$ has a cluster point if and only if every net in $X$ has a convergent subnet.
\end{thm}
\begin{proof}
Suppose that $X$ is compact and let $\net{x}{\alpha}$ be a net with index set $\mathcal{I}$.  For each $\alpha \in \mathcal{I}$ let $A_\alpha = \lbrace x_\beta \mid \beta \succ \alpha \rbrace$.  For any finite set $\alpha_1, \dotsc, \alpha_n \in \mathcal{I}$ because $\mathcal{I}$ is a directed set there exists $\beta$ such that $\beta \succ \alpha_i$ for each $i =1 , \dotsc, n$.  It follows that $A_\beta \in \cap_{i=1}^n A_{\alpha_i}$ and therefore the $A_\alpha$ have the finite intersection property; in particular it follows that the closures $\overline{A_\alpha}$ have the finite intersection property.  It follows from Lemma \ref{IntersectionsOfClosedSetsAndCompactness} that $\cap_{\alpha} \overline{A_\alpha} \neq \emptyset$.  Lemma \ref{ClusterPointsAndClosures} implies that every  $x \in \cap_{\alpha} \overline{A_\alpha}$ is a cluster point.

Now suppose that every net has a cluster point.  Let $\mathcal{C}$ be family of closed subsets of $X$ that have the finite intersection property.  Our goal is to show that $\cap_{A \in \mathcal{C}} A \neq \emptyset$ so it suffices to show the latter property for any family of sets containing $\mathcal{C}$.  In particular we may add all finite intersections of sets in $\mathcal{C}$ (which preserves the finite intersection property) and we therefore assume that $\mathcal{C}$ is closed under finite intersections.  It follows that $(\mathcal{C}, \supset)$ is a directed set.  For every $A \in \mathcal{C}$ pick an arbitrary $x_A \in A$ to construct a net $\net{x}{A}$ with index set $\mathcal{C}$ (TODO: Is $\mathcal{C}$ big enough that we are using the axiom of choice here?   Just curious).  By assumption $\net{x}{A}$ has 
a cluster point $x \in X$.   We show that $x \in \cap{A \in \mathcal{C}} A$.  Let $B \in \mathcal{C}$ and suppose that $x \notin B$.  Then $B^c$ is open there exists an open neighborhood $U$ of $x$ that is disjoint from $B$.  Since $x$ is a cluster point there exists $C \subset B$ with $x_C \in U$.  On the other hand by construction $x_C \in C \subset B$ which is a contradiction.  Thus we know that $\cap_{A \in \mathcal{C}} A \neq \emptyset$ and compactness of $X$ follows from Lemma \ref{IntersectionsOfClosedSetsAndCompactness}.

The last part of the result follows from Proposition \ref{ClusterPointsAndSubnets}.
\end{proof}

 TODO: Show how to characterize classes of convergent nets and then show how to construct a topology out of a class of convergent nets.  Kelley does this but it isn't clear how useful the construction is since it involves the non-trivial ``iterated limit'' property.

\subsection{Metrizable Spaces}

\begin{defn}A topological space $(X,\tau)$ is said to be \emph{metrizable} if there exists a metric $d$ that generates $\tau$.
\end{defn}

\begin{lem}\label{MaxPlusInequality}For $a,b,c \geq 0$ we have $(a+b) \maxop c \leq (a \maxop c) + (b \maxop c)$.
\end{lem}
\begin{proof}
This is proved by breaking down into cases.
First suppose that $a \leq c$ and $b \leq c$ in which case $(a \maxop c) + (b \maxop c) = 2c$ and by non-negativity of $c$ we get $2c \geq c$ and by the assumptions of this cae $2c \geq a+b$.  
Next suppose that $a > c$ and $b \leq c$ in which case $(a \maxop c) + (b \maxop c) = a+c$.  By non-negativity of $a$ we get $a+c \geq c$ and by the assumptions of this cae $a+c \geq a+b$.
The case of $a \leq c$ and $b > c$ follows by the same argument as the previous case.  Lastly assume that $a > c$ and $b > c$ in which case $(a \maxop c) + (b \maxop c) = a+b$.  By non-negativity of $a$ and the assumptions of this case we have $a+b \geq a > c$.
\end{proof}

\begin{prop}The topology on any metrizable space $(X,\tau)$  is generated by a bounded metric.
\end{prop}
\begin{proof}
Suppose $d$ is a metric that generates $\tau$.  Let $d^\prime(x,y) = d(x,y) \maxop 1$; we claim that $d^\prime$ generates the same
topology as $d$.  First note that $d^\prime$ is indeed a metric.  The properties $d^\prime(x,y) = 0$ if and only if $x=y$ follows immediately from
the corresponding property of $d$.  Symmetry follows by noting $x,y \in X$ then $d^\prime(x,y) = d(x,y) \maxop 1 = d(y,x) \maxop 1 = d^\prime(y,x)$.  

Lastly the triangle inequality holds by calculating using Lemma \ref{MaxPlusInequality}
\begin{align*}
d^\prime(x,z) &= d(x,z) \maxop 1 \leq (d(x,y) + d(y,z)) \maxop 1 \leq (d(x,y) \maxop 1) + (d(y,z) \maxop 1) = d^\prime(x,y) + d^\prime(y,z)
\end{align*}

Let $U$ be an open set in the metric space $(X,d)$ and let $x \in U$.  There exists an open ball $B(x,r) \subset U$
and by shrinking $r$ if necessary we can assume $0 < r < 1$.  On the other hand open balls of radius less than $1$ are the same $(X,d)$ and
$(X,d^\prime)$ hence we see that $U$ is open in $(X,d^\prime)$.  The argument that open set in $(X,d^\prime)$ are open in $(X,d)$ is identical.
\end{proof}

\begin{prop}\label{CountableProductOfMetrizableIsMetrizable}A countable product of metrizable spaces $(X_1, \tau_1), (X_2, \tau_2), \dotsc$ is metrizable
\end{prop}
\begin{proof}
TODO:
By the previous Proposition we may assume that for each $n \in \naturals$ the topology $\tau_n$ is generated by a metric $d_n$ that is bounded by $1$.   We will denote a generic element
of the product $X=\Pi_{n=1}^\infty X_n$ as $x$ and will write $x=(x_1, x_2, \dotsc)$ for the coordinates of $x$.  Define
\begin{align*}
d(x,y) &= \sum_{n=1}^\infty d_n(x_n,y_n)/2^n
\end{align*}
where by the assumption that the $d_n$ are bounded by $1$ we see that the sum converges absolutely and is itself bounded by $1$.
We claim that $d$ is a metric and that it generates the product topology on $X$.  

\begin{clm} $d$ is a metric
\end{clm}
It is clear that $d(x,y)$ if and only if $d_n(x_n,y_n)=0$ for all $n \in integers$ if and only if $x_n=y_n$ for all $n \in \naturals$ if and only if $x=y$.  Symmetry of $d$ is equally simple to see
\begin{align*}
d(x,y) &= \sum_{n=1}^\infty d_n(x_n,y_n)/2^n = \sum_{n=1}^\infty d_n(y_n,x_n)/2^n = d(y,x)
\end{align*}
The triangle inequality follows by using the absolute convergence of the sum to justify regrouping terms
\begin{align*}
d(x,z) &= \sum_{n=1}^\infty d_n(x_n,z_n)/2^n \leq \sum_{n=1}^\infty (d_n(x_n,y_n) + d(y_n,z_n)) /2^n = \sum_{n=1}^\infty d_n(x_n,y_n)/2^n  + \sum_{n=1}^\infty d_n(y_n,z_n)/2^n  = d(x,y) + d(y,z)
\end{align*}

Now suppose that $U$ is open in the product topology and $x \in U$.  In order to show that $U$ is open in the metric topology we need to find a ball $B(x,\delta) \subset U$ for some $\delta > 0$.  We may assume that $U$ is an element of a base for the product topology and therefore is of the form $U=U_1 \times \dotsb \times U_n \times \Pi_{j=n+1}^\infty X_j$ where $U_j$ is open in $X_j$ for $j=1, \dotsc, n$.  There exists an $0 < \delta < 1$ such that $B(x_j,\delta) \subset U_j$ for each $j=1, \dotsc, n$.  
Now observe that $B(x,\delta/2^n) \subset U$ since
\begin{align*}
\max_{1 \leq j \leq n} d(x_j, y_j) &\leq 2^n \sum_{j=1}^n d(x_j, y_j)/2^j \leq 2^n d(x,y) < \delta
\end{align*}
This shows that $U$ is open in the metric topology.

By the same argument given above, in order to see that sets open in the metric topology are open in the product topology it suffices to consider open sets in a base of the metric topology.  Thus we may consider $x \in X$ and a ball $B(x, \delta)$ in the metric $d$.  Pick $n \in \naturals$ so that $2^{-n} < \delta/2$ and then let $\epsilon < \delta (\sum_{j=0}^{n-1} 2^{-j})^{-1}$.  Let set 
\begin{align*}
V = \lbrace (y_1, y_2, \dotsc ) \in X \mid d_j(x_j, y_j) < \epsilon \text{ for $j=1, \dotsc, n$} \rbrace
\end{align*}
is an open neighborhood of $x$ in the product topology.  Suppose $y \in V$ then 
\begin{align*}
d(x,y) &= \sum_{j=1}^n d_j(x_j, y_j)/2^j + \sum_{j=n+1}^\infty d_j(x_j, y_j)/2^j \leq \max_{1 \leq j \leq n} d_j(x_j, y_j)  \sum_{j=1}^n 2^{-j} + \sum_{j=n+1}^\infty 2^{-j} \\
&< \epsilon \sum_{j=1}^n 2^{-j} + 2^{-n} \leq \delta/2 + \delta/2 = \delta
\end{align*}
which shows us that $V \subset B(x,\delta)$ and therefore $B(x,\delta)$ is open in the product topology.  
\end{proof}

\begin{prop}\label{CompletelyRegularSeparableIsNormal}A completely regular second countable space $X$ is normal.
\end{prop}
\begin{proof}
Let $U_1, U_2, \dotsc$ be a countable base for $X$.  Let $F$ and $G$ be closed subsets of $X$.  By complete regularity for each $x \in F$ there exist disjoint open sets $U$ and $V$ such that $x \in U$
and $G \subset V$.  Now pick a basis element $U_{n(x)}$ such that $x \in U_{n(x)} \subset U$.  and $U_{n(x)} \cap G = \emptyset$.  Taking the union of the $U_{n(x)}$ over all $x \in F$ we get a countable
open cover $V_1, V_2, \dots$ of $F$ such that $\cup_{n=1}^\infty \overline{V_n} \cap G = \emptyset$.  By the same constrution we get a countable open cover $W_1, W_2, \dotsc$ of $G$ such that $\cup_{n=1}^\infty \overline{W_n} \cap F = \emptyset$.  Now define $\tilde{V}_n = V_n \setminus \cup_{j=1}^n \overline{W_j}$ and $\tilde{W}_n = W_n \setminus \cup_{j=1}^n \overline{V_j}$.  It is clear that 
the $\tilde{V}_n$ and $\tilde{W}_n$ are finite intersections of open sets and are therefore open.  Furthermore $\tilde{V}_n$ is an open cover of $F$; picking an $x \in F$ pick $V_n$ such that $x \in V_n$ and note that since $\cup_{n=1}^\infty \overline{W_n} \cap F = \emptyset$ it follows that $x \in \tilde{V}_n$.  Similarly $\tilde{W}_n$ is an open cover of $G$.  Lastly we claim that $\cup_{n=1}^\infty \tilde{V}_n$ and $\cup_{n=1}^\infty \tilde{W}_n$ are disjoint.  Otherwise there exist $m,n$ such that $\tilde{V}_n \cap \tilde{W}_m \neq \emptyset$.  Assuming $n \geq m$ by definition of $\tilde{V}_n$ we know that $\tilde{V}_n \cap W_m = \emptyset$ which is a contradiction since $\tilde{W}_m \subset W_m$.  The case in which $m \geq n$ follows by the same argument with the roles of $\tilde{V}_n$ and $\tilde{W}_m$ reversed.
\end{proof}

\begin{thm}[Urysohn's Metrization Theorem]\label{UrysohnMetrization}A completely regular second countable space $X$ is metrizable.
\end{thm}
\begin{proof}
The proof proceeds by showing that $X$ is homeomorphic to a subset of of the space $[0,1]^{\naturals}$,   Proposition \ref{CountableProductOfMetrizableIsMetrizable} we know that $[0,1]^{\naturals}$
so the result will follow by restricting the metric on $[0,1]^{\naturals}$ to the image of $X$.

The crux of the construction is the following claim.
\begin{clm}There are continuous functions $f_1, f_2, \dotsc$ from $X$ to $[0,1]$ with the property that for every open set $U \subset X$  and $x \in U$ there is an $n \in \naturals$ such that $f(x) =1$ and $f(y) = 0$ for all $y \in X U^c$.
\end{clm}
Let $V_1, V_2, \dotsc$ be a countable base for $X$.  Consider the pairs $m,n \in \naturals$ such that $\overline{V_m} \subset V_n$.  By Proposition \ref{CompletelyRegularSeparableIsNormal} we know that $X$ is normal and therefore we may apply Urysohn's Lemma  \ref{UrysohnsLemma} to construct a continuous function $g_{m,n} : X \to [0,1]$ such that $g \equiv 1$ on $\overline{V_m}$ and $g \equiv 0$ on $V_n^c$.  We will show that the functions $g_{m,n}$ satisfy the claim.   To see this let $U$ be open and $x \in U$.  By the defining property of bases we pick $V_n \subset U$ with $x \in V_n$.  Now applying complete regularity to the point $x$ and the closed set $V_n^c$ we get disjoint open sets $W_1,W_2$ with $x \in W_1$ and $V_n^c \subset W_2$.  Again applying the defining property of bases we pick $V_m$ such that $V_m \subset W_1$ and $x \in V_m$.  Note that $V_m \cap W_2 = \emptyset$ hence $\overline{V_m} \cap W_2 = \emptyset$ (Corollary \ref{DisjointOpenSetsDisjointClosure}) thus $\overline{V_m} \cap V^c_n = \emptyset$ (i.e. $\overline{V_m} \subset V_n$).  Therefore $g_{m,n}$ has the desired property.

Picking continuous functions $f_1, f_2, \dotsc$ as in the previous claim we define $\phi : X \to [0,1]^\naturals$ by $\phi(x) = (f_1(x), f_2(x), \dotsc)$.  First observe the $\phi$ is injective since given $x \neq y$ we can pick a neighborhood $x \in U$ such that $y \neq U$ and we can pick $f_n$ such that $f_n(x) = 1$ and $f_n(y)$.  Next observe that by the continuity of the $f_n$ and Proposition \ref{ProductTopologyProperties} we know $\phi$ is continuous.  It remains to show that $\phi(U)$ is relatively open for every open set $U \subset X$.  Let $y \in \phi(U)$ and by injectivity of $\phi$ write $y = \phi(x)$ for a unique $x \in U$.  Applying the claim we pick $f_n$ such that $f_n(x) = 1$ and $f_n(z)=0$ for all $z \notin U$.  For every $n \in \naturals$ let $p_n : [0,1]^\naturals  \to [0,1]$ be the projection onto the $n^{th}$ coordinate.  Define
\begin{align*}
V &= \phi(X) \cap \lbrace z \in [0,1]^{\naturals} \mid  p_n(z) > 0 \rbrace
\end{align*}
observe that $V$ is relatively open in $\phi(X)$. To see that $y \in V$ simply calculate
\begin{align*}
p_n(y) &= p_n(\phi(x)) = f_n(x) = 1 > 0
\end{align*}
To see that $V \subset \phi(U)$, write an arbitrary element of $w \in V$ as $w = \phi(z)$ for a unique $z \in X$ then by definitions of $\phi$ and $V$, 
\begin{align*}
f_n(z) = p_n(\phi(z)) = p_n(w) > 0
\end{align*}
Since $f_n(t)=0$ for all $t \notin U$ it follows that $z \in U$.
\end{proof}

\section{Pseudometric Spaces}

\begin{defn}A pseduometric space is a set $S$ together with a function
  $d:SxS \to [0,\infty)$ satisfying
\begin{itemize}
\item[(i)]$d(x,x) \geq 0$ for all $x \in S$
\item[(ii)]For all $x,y \in S$, $d(x,y) = d(y,x)$.
\item[(iii)]For all $x,y,z \in S$, $d(x,z) \leq d(x,y) + d(y,z)$.
\end{itemize}
\end{defn}

Note that by Lemma \ref{MetricSpaceIsPseudometric} a metric space is a
pseudometric space.  Every pseudometric space has a topology generated
by open balls $B(x,r) = \lbrace y \in S \mid d(x,y) < r \rbrace$; by the triangle inequality
and Lemma \ref{CharacterizeBaseOfTopology} it is easy to see that the open balls are in fact
a base for the induced topology.

It is often the case that the best way of dealing with a pseudometric space is by passing to quotient to define a metric space.
\begin{prop}\label{QuotientOfPseudometricSpace}Let $(S,d)$ be a pseudometric space, the the relation $x \equiv y$ if and only $d(x,y) = 0$ defines an equivalence
relation on $S$.  Let $T$ be the quotient space $S/\equiv$ and define $d_{\equiv}([x], [y]) = d(x,y)$ then $d_\equiv$ is a metric.
\end{prop}
\begin{proof}
The fact that $\equiv$ is reflexive, symmetric and transitive follow immediately from properties (i), (ii) and (iii) of the definition of a pseudometric.

If $d(x,x^\prime) = d(y,y^\prime) = 0$ then by the triangle inequality
\begin{align*}
d(x,y) &\leq d(x, x^\prime) + d(x^\prime, y^\prime) + d(y^\prime, y) = d(x^\prime, y^\prime)
\end{align*}
and switching the roles of primed and unprimed points we conclude that $d_\equiv$ is well defined.  Symmetry and the triangle inequality for $d_\equiv$ follow from
the corresponding properties of $d$.  If $d_\equiv([x], [y])$ then it follows that $d(x,y) = 0$ and therefore $[x]=[y]$.
\end{proof}

\section{Borel Spaces}

TODO: The goal of the next set of results is to show that separable
complete metric spaces (actually Polish spaces which are those with
a separable topology which can be metrized by a complete metric) are Borel.

 The following appears in Royden as Theorem 8.11 (with proof delgated
to exercises)
\begin{lem}Let $X$ be a Hausdorff topological space, $Y$ be a
  complete metric space and $Z \subset X$ be a dense subset.  If $f :
  Z \to X$ is a homeomorphism then $Z$ is a countable intersection of
  open sets.
\end{lem}
\begin{proof}
For each $n$ let 
\begin{align*}
O_n &= \lbrace x \in X \mid \text{there exists $U$
  open with $x \in U$ and $\diam(f(U \cap Z)) < \frac{1}{n}$} \rbrace
\end{align*}
Note that $O_n$ is open because for any $x \in O_n$ by definition we
have the open set $U$ that provides the evidence that $x \in O_n$;
$U$ also provides the evidence that proves that every $y \in U$
belongs to $O_n$.  Also
note that $Z \subset O_n$ since for any $n$, by continuity of $f$ at $x \in Z$ and Lemma
\ref{OpenAlternative}  we
can find an open $U \subset X$ such that $x \in U \cap Z$ and $f(U \cap Z) \subset B(f(x),
\frac{1}{2n})$ (sets of the form $U \cap Z$ being precisely the open
sets in $Z$).

Now define $E = \cap_n O_n$.  As noted we know $Z \subset E$ so we
will be done if we can show $E
\subset Z$ as well.  Let $x \in E$; we will construct $z \in Z$
such that $x = z$.  For each $n$ pick $U_n$ such $x \in U_n$ and $\diam(f(U_n \cap Z)) <
\frac{1}{n}$ and let $x_n$ be an arbitrary point in $\cap_{j=1}^n
U_j \cap Z$ (the intersection is non-empty because $Z$ is dense in
$X$).  
For every $n$ and $m \geq n$ we have by construction that $x_n
\in U_n$ and $x_m \in U_n$ hence $d(f(x_n), f(x_m)) < \frac{1}{n}$.
Therefore $f(x_n)$ is Cauchy in
$Y$ and by completeness of $Y$ we know that $f(x_n)$ converges to a
value $y \in Y$ with $d(y, f(x_n)) \leq \frac{1}{n}$.  
Because $f$ is a homeomorphism we know that 
there is a unique $z \in Z$ such that $f(z) = y$; we claim that $x =
z$.  Suppose that $x
\neq z$, then by the Hausdorff property on $X$ we can pick open sets $U$ and
$V$ such that $U \cap V = \emptyset$, $x \in U$ and $z \in V$.  Since
$f$ is a homeomorphism, we know $f(Z \cap V)$ is open and contains
$f(z)$ hence for sufficiently large $n$, $f^{-1}(B(f(z), \frac{1}{n}))
\subset Z \cap V \subset V$.  On
the other hand, by the definition of $x$ we have $U_{2n}$ open such that
$x \in U_{2n}$ and $\diam(f(Z \cap U_{2n})) < \frac{1}{2n}$.  By openness of
$U \cap U_{2n}$ and density of $Z$ we know there is a $w \in U \cap
U_{2n} \cap Z$.  Putting these observations together we have
\begin{align*}
d(f(w), f(z)) &\leq  d(f(w), f(x_{2n})) + d(f(x_{2n}), f(z)) 
< \frac{1}{2n} + \frac{1}{2n} = \frac{1}{n}
\end{align*}
which implies $w \in V$ providing a contradiction of $U \cap V =
\emptyset$ hence we conclude $x = z$.
\end{proof}

\begin{defn}Let $\mathcal{I}$ be index set  and let $(X_\alpha,  \mathcal{T}_\alpha)$ be a topological space for each $\alpha \in \mathcal{I}$ the
\emph{product topology} on the  cartesian product $X = \prod_{\alpha \in \mathcal{I}} X_\alpha$ is the topology generated by sets of the 
form 
\begin{align*}
U &= \lbrace x \in X \mid \pi_{\alpha_i}(x) \in U_{\alpha_i} \text{ for $i=1, \dotsc, n$} \rbrace
\end{align*}
for finite subsets $\lbrace \alpha_1, \dotsc, \alpha_n \rbrace \subset \mathcal{I}$ and open sets $U_{\alpha_j} \in \mathcal{T}_{\alpha_j}$.
\end{defn}

\begin{lem}\label{TopologyGeneratedByFunctionsAndOpenMappings}Let $X$ be a set, $\mathcal{I}$ be index set  and let $(Y_\alpha,  \mathcal{T}_\alpha)$ be a topological space for each $\alpha \in \mathcal{I}$.  
\begin{itemize}
\item[(i)] If we are given functions $f_\alpha : X \to Y_\alpha$ we give $X$ the topology generated by the $f_\alpha$ then each $f_\alpha$ is a continuous function.  
\item [(ii)] Given a net $\net{x}{\beta}$ then $x_\beta \to x$ if and only if $f_\alpha(x_\beta) \to f_\alpha(x)$ for every $\alpha \in \mathcal{I}$.  
\item[(iii)] If for each $\alpha$ be have a subbase $\mathcal{U}_\alpha$ of $Y_\alpha$ then the topology generated by the $f_\alpha$ is also generated by set $f_\alpha^{-1}(U)$ with $U \in \mathcal{U}_\alpha$.
\end{itemize}
\end{lem}
\begin{proof}
Continuity of the $f_\alpha$ is immediate from the definition of the topology generated by the $f_\alpha$.  Therefore if $x_\beta \to x$ it follows that $f_\alpha(x_\beta) \to f_\alpha(x)$ for every $\alpha \in \mathcal{I}$.  Now suppose the latter is true (note that $x$ is not uniquely defined by the values $f_\alpha(x)$).  Let $U$ be an open neighborhood of $x$ and by Proposition \ref{BaseFromSubbase} we may find a finite $\alpha_1, \dotsc, \alpha_n$ and open sets $V_1, \dotsc, V_n$ such that $x \in f_{\alpha_1}^{-1}(V_1) \cap \dotsb \cap f_{\alpha_n}^{-1}(V_n) \subset U$.  Now for each $j=1, \dotsc, n$ by
construction $V_j$ is an open neighborhood of $f_{\alpha_j}(x)$ and therefore we may pick $\gamma_j$ such that $f_{\alpha_j}(x_\beta) \in V_j$ for all $\beta \succ \gamma_j$.  Picking a $\gamma$ with
$\gamma \succ \gamma_j$ for each $j=1, \dotsc, n$ it follows that $x_\beta \in f_{\alpha_1}^{-1}(V_1) \cap \dotsb \cap f_{\alpha_n}^{-1}(V_n)$ for all $\beta \succ \gamma$.

To see (iii), let $U$ be open in the topology generated by the $f_\alpha$ and let $x \in U$.  By Proposition \ref{BaseFromSubbase} there exist $\alpha_1, \dotsc, \alpha_n$ and open sets $V_1, \dotsc, V_n$ such that $x \in f_{\alpha_1}^{-1}(V_1) \cap \dotsb \cap f_{\alpha_n}^{-1}(V_n) \subset U$.  Now for each $j=1, \dotsc, n$ there exist $U_{j,1}, \dotsc, U_{j, m_j}$ such that $f_{\alpha_j}(x) \in U_{j,1} \cap \dotsb \cap U_{j, m_j} \subset V_j$ and therefore $x \in f_{\alpha_j}^{-1}(U_{j, 1}) \cap \dotsb \cap  f_{\alpha_j}^{-1}(U_{j, m_j})$.  Thus 
\begin{align*}
x &\in \cap_{j=1}^n \cap_{i=1}^{m_j}  f_{\alpha_j}^{-1}(U_{j, i}) \subset f_{\alpha_1}^{-1}(V_1) \cap \dotsb \cap f_{\alpha_n}^{-1}(V_n) \subset U
\end{align*} 
and the result follows from Proposition \ref{BaseFromSubbase}.
\end{proof}

\begin{prop}\label{ProductTopologyProperties}Let $\mathcal{I}$ be index set  and let $(Y_\alpha,  \mathcal{T}_\alpha)$ be a topological space for each $\alpha \in \mathcal{I}$ and give $Y = \prod_{\alpha \in \mathcal{I}} Y_\alpha$ the product topology. 
\begin{itemize}
\item[(i)] The topology on $Y$ is generated by the projections $\pi_\alpha : Y \to Y_\alpha$.
\item[(ii)] If $(X, \mathcal{T})$ is a topological space and $f : X \to Y$ is a map. Then $f$ is continuous if and only if $\pi_\alpha \circ f : X \to Y_\alpha$ is
continuous for each $\alpha \in \mathcal{I}$.
\item[(iii)] The projections $\pi_\alpha$ are open mappings
\end{itemize}
\end{prop}
\begin{proof}
To see (i) pick an $\alpha \in \mathcal{I}$ and an open set $U \subset Y_\alpha$; then $\pi_\alpha^{-1}(U)$ is open by the definition of the product topology.  On the other hand note that by definition
the product topology is generated by sets 
\begin{align*}
\lbrace x \in X \mid \pi_{\alpha_i}(x) \in U_{\alpha_i} \text{ for $i=1, \dotsc, n$} \rbrace &= \pi_{\alpha_1}^{-1}(U_{\alpha_1}) \cap \dotsb \cap \pi_{\alpha_n}^{-1}(U_{\alpha_n})
\end{align*}
which shows that the sets $\pi_\alpha^{-1}(U)$ are a subbase of the product topology (Proposition \ref{BaseFromSubbase}).

To see (ii) first suppose $f$ is continuous.  From (i) we know that $\pi_\alpha$ is continuous and it follows that $\pi_\alpha \circ f$ is continuous.  In the other direction suppose that $\pi_\alpha \circ f$ is continuous for all $\alpha \in \mathcal{I}$.  By Proposition \ref{ContinuityViaSubbase} it suffices to show that $f^{-1}(U)$ is open for any $U$ of the form $\pi_{\alpha_1}^{-1}(U_{\alpha_1}) \cap \dotsb \cap \pi_{\alpha_n}^{-1}(U_{\alpha_n})$.  By Lemma \ref{SetOperationsUnderPullback} we know that 
\begin{align*}
f^{-1}(U) &= (f \circ \pi_{\alpha_1})^{-1}(U_{\alpha_1}) \cap \dotsb \cap (f \circ \pi_{\alpha_n})^{-1}(U_{\alpha_n})
\end{align*}

TODO: Show (iii).
\end{proof}

\begin{thm}[Alexander Subbase Theorem]\label{AlexanderSubbaseTheorem}Let $X$ be a topological space and let $\mathcal{B}$ be a subbase of a topology on $X$.  If every open subcover of $X$ by elements of $\mathcal{B}$ has a finite subcover then $X$ is compact.
\end{thm}
\begin{proof}
We argue by contradiction so suppose that $X$ is not compact and every subbase cover has a finite subcover.  Consider the set of covers of $X$ that do not have a finite subcover.  Since $X$ is assumed non-compact this set is non-empty.  Note also that the set of covers is partially ordered via refinement and every chain of covers without a finite subcover has an upper bound given by union of covers; any finite subcover of the union must be a finite subcover of some element of the chain.  By Zorn's Lemma we may select a maximal cover $C$ of $X$ that does not possess a finite subcover; thus given any open set $U \notin C$ we know that $C \cup \lbrace U \rbrace$ has a finite subcover which would have to include $U$.

Now consider the intersection $C \cap \mathcal{B}$.  If $C \cap \mathcal{B}$ were a cover of $X$ then it would have a finite subcover which would be  contradiction.  It follows that there exists 
$x \in X \setminus \cup_{U \in C \cap \mathcal{B}} U$.  Since $C$ is a cover of $X$ there exists $U \in C$ such that $x \in U$ and since $\mathcal{B}$ is a subbase by Proposition \ref{BaseFromSubbase} there exist $V_1, \dotsc, V_n \in \mathcal{B}$ such that $x \in V_1 \cap \dotsb \cap V_n \subset U$.  By choice of $x$, for each $j=1, \dotsc, n$ we know that $V_j \notin C$ and by the maximality of $C$ we conclude that there exists a finite subcover $C_j \cup \lbrace V_j \rbrace$ of $X$ with $C_j \subset C$. Note that any $y \in X$ such that $y$ is not covered by any of $C_1, \dotsc C_n$ must be each of $V_1, \dotsc, V_n$; hence $C_1 \cup \dotsb \cup C_n \cup \lbrace U \rbrace$ is a finite subcover of $C$ which is a contraction.
\end{proof}
TODO: We have used Zorn's Lemma above; apparently we only need the Ultrafilter Lemma.

\begin{thm}[Tychonoff's Theorem]\label{Tychonoff}Let $I$ be index set
  and let $(X_i,  \mathcal{T}_i)$ be a topological space for each $i \in I$, the
  cartesian product $\prod_{i \in I} X_i$ with the product topology is
  compact.
\end{thm}
\begin{proof}
Let $\pi_i : \prod_{i \in I} X_i  \to X_i$ be the $i^{th}$ coordinate projection.  The product topology on $\prod_{i \in I} X_i$ has subbase comprising cylinder sets $\pi_i^{-1}(U)$ for $U$ open in $X_i$.  Suppose there is an open cover $C$ by such cylinder sets that has not finite subcover.  We write $C = \cup_{i \in I} C_i$ where each $C_i$ comprises sets of the form $\pi_i^{-1}(U)$ for $U$ open in $X_i$ (note that each cylinder set not equal to either $\emptyset$ or $X$ corresponds to a unique $i \in I$).  To be a bit more explicit, we define for each $i \in I$ an index set $I_i$ and pick open
sets $U^i_j \subset X_i$ for $j \in I_i$ such that $C_i$ is equal to the sets $\pi_i^{-1}(U^i_j)$ for $j \in I_i$.  Suppose that there were a finite set $U^i_{j_1}, \dotsc, U^i_{j_n}$ that cover $X_i$; it would follow that $\pi_i^{-1}(U^i_{j_1}), \dotsc, \pi_i^{-1}(U^i_{j_n})$ would cover $X$ and would be a contradiction.  Therefore we know that since $X_i$ is compact the $U^i_j$ for $j \in I_i$ can not cover $X_i$ and we may pick $x_i \in X_i \setminus \cup_{j \in I_i} U^i_j$.  

Having found such an $x_i$ for each $i \in I$ (TODO: do we need the full Axiom of Choice here?)  we now define $x \in X$ so that $\pi_i(x) = x_i$ and note that $x$ is not covered by $C$ which is a contradiction.  Thus every cover by subbase elements has a finite subcover and compactness follows from the Alexander Subbase Theorem \ref{AlexanderSubbaseTheorem}.
\end{proof}

\begin{thm}\label{PolishImpliesBorel}A Polish space is Borel.
\end{thm}
\begin{proof}
TODO
\end{proof}

\section{Prohorov's Theorem}

The theory of probability measures on separable metric spaces is simpler in many ways than its general counterpart 
for non-separable metric spaces. The simplicity derives from the fact that a separable metric space is not too far being compact
so in a sense doesn't have too many unbounded continuous functions with respect to which probability measures can misbehave.

One way in which to understand the way in which a separable metric space is close to being compact is to consider the real line.  Through any number of homeomorphisms, the real line is homeomorhpic to the open unit interval $(0,1)$.  In this way, distances on the real line may be rescaled so as to make the real line bounded.  Then by completing the open interval we see that real line is a couple of points away from being compact.

\begin{defn}Let $(S,d)$ be a metric space then we denote by $U^d(S)$ the set of uniformly continuous functions from $S$ to $\reals$ and by $U_b^d(S)$ the set of bounded uniformly continuous functions from $S$ to $\reals$.
\end{defn}
TODO: Show that $U^d_b(S)$ is a Banach space under the supremum norm.

\begin{lem}\label{SeparabilityOfBoundedUniformlyContinuous}Let $(S,d)$ be a separable metric space, then $X$ is
  homeomorphic to a subset of $[0,1]^{\integers_+}$ and furthermore
\begin{itemize}
\item[(i)]$S$ has a metric making it totally bounded
\item[(ii)]If $S$ is compact then $C(S; \reals)$ with the uniform
  topology is separable.
\item[(iii)]If $\hat{d}$ is a totally bounded metric on $S$ then $U^{\hat{d}}(S) = U^{\hat{d}}_b(S)$ and 
  $U^{\hat{d}}_b(S)$ is separable
\end{itemize}
\end{lem}
\begin{proof}
Let $\rho$ be the product metric $\rho(x,y) = \sum_{n=1}^\infty
\frac{\abs{x_n-y_n}}{2^n}$ on the space $[0,1]^{\integers_+}$. 
 Pick a countable dense subset $x_1, x_2, \dotsc$ of $S$ and define 
$f : S \in [0,1]^{\integers_+}$ by 
\begin{align*}
f(x) &= \left ( \frac{d(x_1, x)}{1 + d(x_1, x)}, \frac{d(x_2, x)}{1 +
    d(x_2, x)}, \dotsc \right )
\end{align*}
 
\begin{clm}$f(x)$ is continuous.
\end{clm}

By definition of the product topology $f(x)$ is continuous if and only
if each coordinate is.  For any given fixed $x_j$, we know that
$d(x_j, x)$ is continuous (in fact Lipschitz by Lemma
\ref{DistanceToSetLipschitz}) and thus the result follows from the
continuity of $x/(1+x)$ on $\reals_+$.

\begin{clm}$f(x)$ is injective.
\end{clm}

For any $z \neq y$ we find $\epsilon > 0$ such that $B(z ; \epsilon)
\cap B( y ; \epsilon) = \emptyset$ and then using density of $x_1,
x_2, \dotsc$ to pick an $x_n$ such that $d(z,x_n) < \epsilon$ and
$d(y, x_n) \geq \epsilon$ showing $f(z) \neq f(y)$.  

\begin{clm}The inverse of $f(x)$ is continuous.
\end{clm}

Fix an $x \in S$ and let $\epsilon >0$ be given.  Pick $x_n$ such that
$d(x_n, x) < \epsilon/2$.  If we let $g(x) : [0,1) \to \reals_+$ be
defined by $g(x) = x/(1-x)$ then $g(x)$ is the inverse of $x/(1+x)$ 
and by continuity of $g(x)$ at the point $\frac{d(x_n, x)}{1+d(x_n,x)}$ we know that there exists a $\delta > 0$
such that $\abs{\frac{d(x_n, x)}{1+d(x_n,x)} - 
\frac{d(x_n,  y)}{1+d(x_n,y)}}< \delta$ implies $\abs{d(x_n,x) - d(x_n,y)} <
  \epsilon/2$.
Then if $f(y) \in B(f(x), \frac{\delta}{2^n})$ we have
\begin{align*}
\abs{\frac{d(x_n, x)}{1+d(x_n,x)} - 
\frac{d(x_n,  y)}{1+d(x_n,y)}} &\leq 2^n \rho(f(x), f(y)) < \delta
\end{align*}
$d(x,y) \leq d(x_n,x) + \abs{d(x_n,x) - d(x_n,y)} < \epsilon$.

Now to see (i) we simply pull back the metric $\rho$ via the embedding
$f(x)$ and use the facts that $\rho$ generates the product topology,
$[0,1]^{\integers_+}$ is compact in product topology
(by Tychonoff's Theorem \ref{Tychonoff}; alternatively one can avoid
the use of Tychonoff's Theorem for it is easy to
see with a diagonal subsequence argument that a countable product of
sequentially compact metric spaces is sequentially compact) hence totally bounded (Theorem
\ref{CompactnessInMetricSpaces}).

Here is the argument that $\rho$ generates the product topology; TODO:
put this in a separate lemma.  To see that the topology generated by
$\rho$ is finer than the product topology, suppose $U$ is open in the
topology generated by $\rho$.  Pick $x \in U$ and select $N > 0$ such
that $B(x,\epsilon) \subset U$.  Then pick $N > 0$ such that $2^{-N-1} <
\epsilon$ and consider $B=B(x_1, \epsilon/2)
\times \dotsb \times B(x_{2^N}, \epsilon/2) \times S \times \dotsb$
which is open in the product topology.  If $y \in B$ then 
\begin{align*}
\rho(x,y) &=
\sum_{n=1}^\infty \frac{\abs{x_n-y_n}}{2^n} = \sum_{n=1}^{2^N}
\frac{\abs{x_n-y_n}}{2^n} + \sum_{n=2^N+1}^\infty \frac{\abs{x_n-y_n}}{2^n} \leq
\frac{\epsilon}{2} \sum_{n=1}^{2^N} \frac{1}{2^n} +
\sum_{n=2^N+1}^\infty \frac{1}{2^n} < \epsilon
\end{align*}
To see that the product topology is finer than the metric topology,
suppose $n >0$ is an integer, $U\subset[0,1]$ is open and consider $\pi_n^{-1}(U)$.  Let $x
\in \pi_n^{-1}(U)$ and find an $\epsilon > 0$ such that $B(x_n,
\epsilon) \subset U$.  Note that if $y \in B(x, \frac{\epsilon}{2^n})$
then $\abs{x_n - y_n} < 2^n \rho(x,y) \leq \epsilon$ and therefore
$B(x, \frac{\epsilon}{2^n}) \subset \pi_n^{-1}(B(x_n, \epsilon))
\subset U$.

To see (ii), if $S$ is compact then $f(S) \subset [0,1]^{\integers_+}$
is compact (Lemma \ref{ContinuousImageOfCompact}).  Observe that 
\begin{align*}
A &= \lbrace \Pi_{i=1}^np_i(x_i) \mid n \in \naturals \text{ and } p_i
\in \rationals[x] \rbrace
\end{align*}
is a subalgebra of $C([0,1]^{\integers_+} ; \reals)$ and $A$ separates points (
given $x \neq y \in [0,1]$, pick
$n$ such that $x_n \neq y_n$ and pick the function $g(x) = x_n$).  By
the Stone-Weierstrass Theorem \ref{StoneWeierstrassApproximation} we know that $A$ is dense in $C([0,1]^{\integers_+};
\reals)$; now pullback $A$ under $f(x)$ to a countable dense subset of
$C(S;\reals)$. 

To see (iii), suppose $\hat{\rho}$ is a totally bounded metric on
$S$.  Let $\hat{S}$ be the completion of $S$ with respect to
this metric.  

\begin{clm}$\hat{\rho}$ extends to a totally bounded
metric on $\hat{S}$.  
\end{clm}

Let $\epsilon>0$ be given and cover $S$ by ball
$B(x_i, \epsilon/2)$; we show that $B(x_i, \epsilon)$ covers
$\hat{S}$.  Biven $y \in \hat{S}$ we can find $x \in
S$ such that $\hat{\rho}(x,y) < \epsilon/2$.  Since $x \in S$ there
exists an $x_i$ such that $x \in B(x_i, \epsilon/2)$ and therefore
$\hat{\rho}(x_i,y) \leq \hat{\rho}(x,x_i) + \hat{\rho}(x_i,y) <
\epsilon$.

Because $(\hat{S},\hat{\rho})$ is complete and totally bounded we know
it is compact (Theorem \ref{CompactnessInMetricSpaces}) and we have
just shown that $C(\hat{S} ; \reals)$ has a countable dense subset.

\begin{clm}$f\vert_S : C(\hat{S} ; \reals) \to U_b^{\hat{\rho}}(S ;
\reals)$ is a well defined, continuous and surjective.
\end{clm}

Being well defined in this context means that restriction to $S$
results in a bounded uniformly continuous function.  This follows
from the fact that any continuous function of a compact set is bounded
and uniformly continuous (Theorem \ref{ContinuousImageOfCompact} and
Theorem \ref{UniformContinuityOnCompactSets} respectively) and these
properties are preserved upon restriction.  To see surjectivity, let
$g : S \to \reals$ be uniformly continuous.  We may apply Proposition
\ref{ExtensionOfUniformlyContinuousMapCompleteRange} to see that $g$ has a unique extension
to a continuous function from the closure of $S$ to $\reals$.  Since the closure of $S$ in $\hat{S}$ is
$\hat{S}$ be are done with the claim.  Note that we did not need boundedness of $g$ in order to prove the existence of the extension;
therefore we have shown $U^{\hat{d}}(S) = U^{\hat{d}}_b(S)$.

Now the continuous image of a dense set under a surjective map is also
dense.  This is easily seen by picking a point $f(x)$ in the image;
picking a sequence $x_n$ such that $x_n \to x$ and then considering
the image $f(x_n) \to f(x)$.  Thus the result is proven.
\end{proof}

\begin{lem}[Dini's Theorem]\label{DinisTheorem}Let $K$ be a compact
  topological space and let $f_n: K \to \reals$ be a sequence  of continuous
  functions such that $f_n \downarrow 0$ pointwise on $K$, then $f_n
  \to 0$ uniformly.
\end{lem}
\begin{proof}
Given $\epsilon > 0$ define $U_n = f_n^{-1}((-\infty,\epsilon))$.
Then each
$U_n$ is open, $U_1 \subset U_2 \subset \dotsb$ (since the $f_n$ are
decreasing) and the $U_n$ form an open cover of $K$.  We can extract a
finite subcover which since the $U_n$ are nested implies that $K =
U_N$ for some $N > 0$.  This is exactly the statement that $\sup_{x
  \in K} \abs{f_n(x)} < \epsilon$ for all $n \geq N$ hence the result proven.
\end{proof}

\begin{lem}\label{StoneDaniellProbability}Let $(S,d)$ be a separable metric space and let $\Lambda :
  U_b^d(S; \reals) \to \reals$ be a linear map such that 
\begin{itemize}
\item[(i)] $\Lambda$ is non-negative (i.e. if
  $f \geq 0$ then $\Lambda(f) \geq 0$) 
\item[(ii)] $\Lambda(1) = 1$
\item[(iii)] for all $\epsilon > 0$ there exists a compact set $K
  \subset S$ such that for all $f \in U_b^d(S; \reals)$,
\begin{align*}
\abs{\Lambda(f)} &\leq \sup_{x \in K} \abs{f(x)} + \epsilon \norm{f}_u
\end{align*}
\end{itemize}
then there exists a Borel probability measure $\mu$ on $S$ such that
$\Lambda(f) = \int f \, d\mu$.  Whenever such a probability measure
exists it is unique.
\end{lem}
\begin{proof}
We construct $\mu$ by use of the Daniell-Stone Theorem
\ref{DaniellStoneTheorem}.  It is clear that $U_b^d(S; \reals)$ is
closed under max and min and contains the constant functions so
$U_b^d(S; \reals)$ is a Stone Lattice.  It remains to show that
$\Lambda$ obeys the ``montone convergence'' property: if $f_n
\downarrow 0$ pointwise then $\Lambda(f_n) \downarrow 0$.  This
property is a corollary of Dini's Theorem \ref{DinisTheorem} since by that result,
if $f_n$ are continuous and $f_n \downarrow 0$ pointwise on a compact
set then the converge uniformly to $0$ on the compact set.  In
particular, pick an $\epsilon > 0$ and let $K \subset S$ be compact
as in the hypothesis.  By Dini's Theorem there exists $N > 0$ such
that $\sup_{x \in K} \abs{f_n(x)} < \epsilon$ for all $n \geq N$.  Therefore
for all $n \geq N$,
\begin{align*}
\abs{\Lambda(f_n)} &\leq \sup_{x \in K} \abs{f_n(x)} + \epsilon
\norm{f_n}_\infty \\
&\leq \epsilon(1 + \norm{f_1}_\infty)
\end{align*}
thus $\lim_{n \to \infty} \Lambda(f_n) = 0$ and we can apply Theorem \ref{DaniellStoneTheorem}. 

Uniqueness follows because a probability measure is determined by its
integrals over $U_b^d(S;\reals)$ (in fact over the subset of bounded
Lipschitz functions).  This follows because for any closed $F \subset
S$ we can define $f_n(x) = n d(x, F) \wedge 1$ so that $f_n
\downarrow \characteristic{F}$ and apply Montone Convergence (see the
proof of the Portmanteau Theorem \ref{PortmanteauTheorem} for complete
details on this argument).
\end{proof}

TODO: Apparently tight implies relatively compact does not require that $S$ be separable, find a proof for that (Ethier and Kurtz have one).

\begin{thm}[Prohorov's Theorem]\label{Prohorov}Let $(S,d)$ be a
  separable metric space, then a tight set of probability measures on
  $S$ is weakly relatively compact.  If $S$ is also complete then a
  weakly relatively compact set is tight.
\end{thm}
\begin{proof}
By the Portmanteau Theorem \ref{PortmanteauTheorem} we know that a
set of measures is tight if and only if its weak closure is tight
(compact sets are closed hence can only gain mass in a weak limit).  Thus it suffices to assume
that we have a closed tight set $M$ of measures.  Put a totally
bounded metric $\hat{d}$ on $S$ so that $U_b^{\hat{d}}(S;\reals)$ is separable
(Lemma \ref{SeparabilityOfBoundedUniformlyContinuous}); let $f_1, f_2,
\dotsc$ be a countable uniformly dense subset.  

Pick a sequence $\mu_n$ from $M$; we must show that it has a weakly
convergent subsequence.  For every fixed $f_m$
we know that $\abs{\int f_m \, d\mu_n} \leq \norm{f_m}_u < \infty$ so
there is a subsequence $N \subset \naturals$ such that $\int f_m \,
d\mu_n$ converges along $N$.  Since this is true for every $m>0$, a
diagonalization argument shows there is a subsequence $\hat{\mu}_{k}$ such that
$\lim_{k \to \infty} \int f_m \, d\hat{\mu}_{k}$ exists for every
$m>0$.  Define $\Lambda(f_m) =   \lim_{k \to \infty} \int f_m \,
d\hat{\mu}_{k}$ for every such $f_m$.  Our next goal is to extend
$\Lambda$ to all of $U_b^\rho(S;\reals)$.  Since $\Lambda$ is
uniformly continuous on a dense subset we know that a continuous
extension is defined; however we need a little bit more information.

\begin{clm}$\lim_{k \to \infty} \int f \, d\hat{\mu}_{k}$ exists for every
$f \in U_b^\rho(S;\reals)$; moreover 
\begin{align*}
\lim_{k \to \infty} \int f \,d\hat{\mu}_{k} &= \lim_{m \to \infty} \Lambda(\hat{f}_m)
\end{align*} 
where $\hat{f}_m$ is any subsequence of $f_m$ that converges uniformly to $f$.
\end{clm}

Pick a subsequence of the $f_m$ that converges to $f$.  Let that
subsequence be donoted $\hat{f}_m$ so that $\lim_{m \to \infty} \norm{\hat{f}_m -
  f}_\infty = 0$.  For every $m > 0$ we have
\begin{align*}
\int \hat{f}_m \, d \hat{\mu}_k -\norm{\hat{f}_m - f}_\infty &\leq 
\int f \, d\hat{\mu}_k \leq \int \hat{f}_m \, d \hat{\mu}_k + \norm{\hat{f}_m - f}_\infty
\end{align*}
and therefore taking limits in $k$ and using the definition of
$\Lambda$ at the points $f_m$,
\begin{align*}
\Lambda(\hat{f}_m) -\norm{\hat{f}_m - f}_\infty 
&\leq \liminf_{k \to \infty} \int f \, d\hat{\mu}_k 
\leq \limsup_{k \to \infty} \int f \, d\hat{\mu}_k 
\leq \Lambda(\hat{f}_m) + \norm{\hat{f}_m - f}_\infty
\end{align*}
Now letting $m$ go to infinity we get $\lim_{m \to \infty}
\Lambda(\hat{f}_m) = \lim_{k \to \infty} \int f \, d\hat{\mu}_k$.

As a result of the claim, we now define $\Lambda(f) = \lim_{k \to
  \infty} \int f \, d\hat{\mu}_{k}$ for every $f$  and it is clearly
linear (by linearity of integral and limits), nonnegative (by
monotonicity of integral) and satisfies $\Lambda(1) = 1$ (since each $\hat{\mu}_k$
is a probability measure).  

To show that $\Lambda$ defines a probability measure, we bring the
tightness hypothesis to the table.  Pick $\epsilon > 0$ and by
tightness take a compact set $K \subset S$ such that $\sup_{\mu \in M}
\mu(K) > 1 - \epsilon$.  For any $f \in U_b^{\hat{d}}(S ; \reals)$ we
have
\begin{align*}
\abs{\Lambda(f)} 
&= \lim_{k \to \infty} \abs{\int f \, d\hat{\mu}_k}
= \lim_{k \to \infty} \abs{\int f \characteristic{K} \, d\hat{\mu}_k +
\int f \characteristic{S \setminus K} \, d\hat{\mu}_k} \leq \sup_{x
\in K} \abs{f(x)} + \epsilon \norm{f}_\infty
\end{align*}
so we may apply Lemma \ref{StoneDaniellProbability} to conclude there
exists a probability measure $\mu$ such that for all $f \in U_b^{\hat{d}}(S ; \reals)$ we
have $\Lambda(f) =  \lim_{k \to \infty} \int f \, d\hat{\mu}_k =
\int f \, d\mu$.  Since $U_b^{\hat{d}}(S ; \reals)$ contains all
bounded Lipschitz functions by the Portmanteau Theorem
\ref{PortmanteauTheorem} we conclude $\hat{\mu}_k$ converges weakly to
$\mu$.

Now assume that $S$ is complete and separable and let $M$ be a weakly
relatively compact set of measures.  Let $x_1, x_2, \dotsc$ be a countable dense
subset of $S$.  For every integer $n > 0$ we have $S =
\cup_{k=1}^\infty B(x_k, 1/n)$.  Thus $\cap_{N=1}^\infty \cap_{k=1}^N
B(x_k, 1/n)^c = \emptyset$ so by continuity of measure (Lemma
\ref{ContinuityOfMeasure}) for any fixed probability measure $\mu$ we
can find an $N_{n, \mu} > 0$ such that $\mu(\cap_{k=1}^{N_{n, \mu}}
B(x_k, 1/n)^c ) < \epsilon/2^n$.  We claim that, because $M$ is
compact, we can find an $N_n$ for
which this is true uniformly over the measures in $M$.

\begin{clm}\label{ProhorovClaim2}For every $n > 0$ there exists $N_n > 0$ such that $\mu(\cap_{k=1}^{N_{n}}
B(x_k, 1/n)^c ) < \epsilon/2^n$ for all $\mu \in M$.
 \end{clm}

We argue by contraction by reducing the case where $M$ is a singleton
set (where we have already shown the claim holds).  If Claim \ref{ProhorovClaim2} is not
true then there exists $n$ such that for every integer $N>0$ we have
some $\mu_N \in M$ such that $\mu_N(\cap_{k=1}^{N} B(x_k, 1/n)^c )
\geq \epsilon/2^n$.  By sequential compactness of $M$ we know that
there is a weakly convergent subsequence $\mu_{N_j}$ such that
$\mu_{N_j} \toweak \mu$ for some probability measure $\mu$.  For every
$N > 0$ we have $\cap_{k=1}^{N} B(x_k, 1/n)^c$ is closed and therefore
by the Portmanteau Theorem \ref {PortmanteauTheorem}
\begin{align*}
\epsilon/2^n &\leq \limsup_{j \to \infty} \mu_{N_j}(\cap_{k=1}^{N_j}
B(x_k, 1/n)^c) \\
&\leq \limsup_{j \to \infty} \mu_{N_j}(\cap_{k=1}^{N} B(x_k, 1/n)^c)
\\
&\leq \mu(\cap_{k=1}^{N} B(x_k, 1/n)^c)
\end{align*}
where in the second inequality we have used the fact that the limit
only depends on the tail of the sequence of sets $\cap_{k=1}^{N_j}
B(x_k, 1/n)^c$ and by a union bound for sufficiently large $N_j$ we
have $\mu_{N_j}(\cap_{k=1}^{N_j} B(x_k, 1/n)^c)  \leq
\mu_{N_j}(\cap_{k=1}^{N} B(x_k, 1/n)^c)$.  To finish we get a
contradiction by taking the 
limit and using continuity of measure
\begin{align*}
0 < \epsilon/2^n \leq \lim_{N \to \infty} \mu(\cap_{k=1}^{N} B(x_k,
1/n)^c) = 0
\end{align*}

With Claim \ref{ProhorovClaim2} proven we mimic the proof of Ulam's Theorem.  Let 
\begin{align*}
K &=
\cap_{m=1}^\infty \cup_{j=1}^{N_m} \overline{B}(x_j,\frac{1}{m})
\end{align*}
which is easily seen to be closed (hence complete) and by construction
is totally bounded thus is compact (Theorem
\ref{CompactnessInMetricSpaces})
and furthermore for all $\mu \in M$,
\begin{align*}
\mu(K^c) &\leq \mu((\cap_{m=1}^\infty
\cup_{j=1}^{N_m} B(x_j,\frac{1}{m}))^c) \\
&=\mu(\cup_{m=1}^\infty 
\cap_{j=1}^{N_m} B(x_j,\frac{1}{m})^c) \\
&=\sum_{m=1}^\infty \mu(
\cap_{j=1}^{N_m} B(x_j,\frac{1}{m})^c) \\
&\leq \sum_{m=1}^\infty \frac{\epsilon}{2^m} = \epsilon
\end{align*}

\end{proof}

\begin{lem}For $f,g \in C([0,\infty) ; \reals)$ define
\begin{align*}
\rho(f,g) &= \sum_{n=1}^\infty \frac{1}{2^n} \sup_{0 \leq t \leq n}
(\abs{f(t) - g(t)} \wedge 1)
\end{align*}
then $\rho$ is a metric on $C([0,\infty) ; \reals)$ and $C([0,\infty);
\reals)$ is complete and separable with respect to this metric.
\end{lem}
\begin{proof}
It is clear that $\rho(f,f) = 0$ and furthermore if $\rho(f,g) = 0$
then $f = g$ on every interval $[0,n]$ and therefore $f = g$.
Symmetry and the triangle inequality of $\rho$ is immediate from the
corresponding properties of the absolute value (TODO: OK the triangle
inequality may need a bit more of an argument).

We claim that the set of polynomials with rational coefficients is
dense in $C([0,\infty); \reals)$.  Pick $f \in C([0,\infty); \reals)$
and let $\epsilon > 0$ be given.  Now take $m > 0$ sufficiently large
so that $1/2^m < \epsilon / 2$ and by the Stone Weierstrass Theorem \ref{StoneWeierstrassApproximation} we
pick a polynomial with rational coefficients $p$ such that $\sup_{0
  \leq t \leq m} \abs{f(t) - p(t)} < \epsilon/2$ then we have
\begin{align*}
\rho(f,p) &\leq \sum_{n=1}^m\frac{1}{2^n} \sup_{0 \leq t \leq n}
\abs{f(t) - p(t)} + \sum_{n=m+1}^\infty \frac{1}{2^n} \\
&\leq \sup_{0 \leq t \leq m}
\abs{f(t) - p(t)} \sum_{n=1}^m\frac{1}{2^n} + \epsilon/2 <\epsilon
\end{align*}

Completeness follows from arguing over intervals $[0,n]$.  Suppose
$f_n$ is a Cauchy sequence in $C([0,\infty); \reals)$.  Given
$\epsilon > 0$ and $n > 0$ we can find $N > 0$ such that $\rho(f_m,
f_N) < \epsilon/2^n$ for all $m \geq N$.  Thus $\sup_{0 \leq t \leq n}
\abs{f_m(t) - f_N(t)} < \epsilon$ for all $m \geq N$ so we see that
$f_n$ is uniformly  Cauchy on every interval $[0,n]$.  By completeness
of $C([0,n];\reals)$ we know that the pointwise limit of $f_n$ exists
on every $[0,n]$ and is a continuous function.  Therefore we have a
limit $f$ defined on $[0,\infty)$ and since continuity is a local
property $f \in C([0,\infty); \reals)$.  It remains to show that $f_n$
converges to $f$ in the metric $\rho$.  This follows arguing as we
have above.  Let $\epsilon > 0$ be given and choose $n > 0$ such that
$\frac{1}{2^n} < \epsilon/2$ and choose $N > 0$ such that $\sup_{0 \leq t \leq n}
\abs{f_m(t) - f_N(t)} < \epsilon/2$ and then observe
\begin{align*}
\rho(f_m, f_N) &\leq \sum_{k=1}^n \frac{1}{2^k} \sup_{0 \leq t \leq k}
\abs{f_m(t) - f_N(t)} + \sum_{k=n+1}^\infty \frac{1}{2^k}  < \epsilon
\end{align*}
\end{proof}

The topology defined by $\rho$ is often refered to as the topology of
uniform convergence on compact sets by virtue of the following lemma.
\begin{lem}\label{UniformConvergenceOnCompacts}A sequence $f_n$
  converges to $f$ in $C([0,\infty), \reals)$ if and only if
  $f_n$ converges to $f$ uniformly on every interval $[0,T]$ for $T > 0$.
\end{lem}
\begin{proof}
TODO:  This is elementary.
\end{proof}

\begin{defn}Given a function $f : [0,T] \to \reals$ the \emph{modulus
    of continuity} is the function
\begin{align*}
m(T, f, \delta) &= \sup_{\substack{\abs{s - t} < \delta \\
0 \leq s,t \leq T}} \abs{f(s) - f(t)}
\end{align*}
\end{defn}

\begin{lem}For fixed $T > 0$ and $\delta > 0$, $m(T, f, \delta)$ is a
  continuous function on $C([0,\infty); \reals)$.  For fixed $T > 0$
  and function $f : \reals \to \reals$, $m(T,f,\delta)$ is
  nonincreasing in $\delta$ and 
\begin{align*}
\lim_{\delta \to 0} m(T, f, \delta) = 0
\end{align*}
provided $f \in C([0,\infty); \reals)$.
\end{lem}
\begin{proof}
To see continuity on $C([0,\infty); \reals)$ let $f \in C([0,\infty);
\reals)$, $T > 0$, $\delta > 0$ and $\epsilon > 0$ be given and pick $g$ that $\rho(f,g) <
\epsilon/2^{\ceil{T}+1}$.
From the definition of the metric $\rho$ for any $n > 0$, $\sup_{0 \leq t \leq n} \abs{f(t) - g(t)} \wedge
1 \leq 2^n \epsilon$, so for any $T > 0$, 
\begin{align*}
\sup_{0 \leq t \leq T} \abs{f(t) - g(t)} \wedge
1 &\leq \sup_{0 \leq t \leq \ceil{T}} \abs{f(t) - g(t)} \wedge
1 \leq \epsilon/2
\end{align*}  
Therefore by the triangle inequality,
\begin{align*}
\sup_{\substack{
\abs{s -t} < \delta \\
0 \leq s,t \leq T}} \abs{g(s) - g(t)} \wedge 1 
&\leq 
\sup_{\substack{
\abs{s -t} < \delta \\
0 \leq s,t \leq T}} \left ( \abs{g(s) - f(s)} + \abs{f(s) - f(t)} + \abs{f(t)
- g(t)} \right ) \wedge 1 \\
&\leq \epsilon/2 + 
\sup_{\substack{
\abs{s -t} < \delta \\
0 \leq s,t \leq T}} \abs{f(s) - f(t)} \wedge 1 + \epsilon/2
\end{align*}
and therefore arguing with the roles of $f$ and $g$ reversed shows 
$\abs{m(T, f, \delta) - m(T, g, \delta) } \leq \epsilon$.

The fact that $m(T, f, \delta)$ is decreasing in $\delta$ is clear
because the definition shows that for $\delta_1 \leq \delta_2$ we
have 
\begin{align*}
\lbrace \abs{f(t) - f(s) } \mid 0 \leq s,t \leq T \text{ and }
  \abs{s-t} < \delta_1 \rbrace 
&\subset 
\lbrace \abs{f(t) - f(s) } \mid 0 \leq s,t \leq T \text{ and }
  \abs{s-t} < \delta_2 \rbrace
\end{align*} and therefore $m(T, f, \delta_2) \leq
  m(T, f, \delta_1)$.

Lastly if we suppose $f \in C([0,\infty); \reals)$ then $f$ is
uniformly continuous on $[0,T]$ for every $T > 0$ (Theorem
\ref{UniformContinuityOnCompactSets}).  Thus given an $\epsilon > 0$
there exists $\delta>0$ such that 
\begin{align*}
\sup_{\substack{
\abs{s -t} < \delta \\
0 \leq s,t \leq T}} \abs{f(s) - f(t)} < \epsilon
\end{align*}
which shows $\lim_{\delta \to 0} m(T, f, \delta) = 0$.
\end{proof}
The following Theorem is a version of the Arzela-Ascoli Theorem of
real analysis.
\begin{thm}[Arzela-Ascoli Theorem]\label{ArzelaAscoliTheorem}A set $A
  \subset C([0,\infty); \reals)$ is relatively compact if and only if 
\begin{itemize}
\item[(i)]$\sup_{f \in A} \abs{f(0)} < \infty$
\item[(ii)]$\lim_{\delta \to 0} \sup_{f \in A} m(T, f, \delta) = 0$
  for all $T > 0$.
\end{itemize}
\end{thm}
\begin{proof}
To see the necessity of condition (i), observe that $\overline{A}$ is
compact and by completeness of $C([0,\infty); \reals)$ we know that
$\overline{A}$ comprises continuous functions.  Therefore we know that
$A \subset \overline{A} \subset \cup_{n=1}^\infty \lbrace f \in
C([0,\infty) \mid
\abs{f(0)} < n\rbrace$.  Since each $\lbrace f \in
C([0,\infty) \mid
\abs{f(0)} < n\rbrace$ is easily seen to be an open set, by
compactness of $\overline{A}$ we have a finite subcover which implies
there exists an $N$ such that $A \subset \overline{A} \subset \lbrace f \in
C([0,\infty) \mid
\abs{f(0)} < N\rbrace$.

To see the necessity of condition (ii), fix $\epsilon > 0$, $T > 0$
and define for each $\delta > 0$ the set 
\begin{align*}
F_\delta &= \lbrace f \in \overline{A} \mid m(T, f, \delta) \geq
\epsilon \rbrace
\end{align*}
By continuity of $m(T, f, \delta)$ we know that $F_\delta$ is closed.
Since $F_\delta \subset \overline{A}$ with $\overline{A}$ compact we
conclude that $F_\delta$ is compact.  Furthermore since for fixed $f
\in \overline{A}$ continuity (more specifically uniform continuity on compact
sets) implies $\lim_{\delta \to 0} m(T,f,\delta) = 0$, we know that
$\cap_{\delta > 0} F_\delta = \emptyset$.  By nestedness and
compactness of the
$F_\delta$ we know that there is some specific $\delta>0$ for which $F_\delta =
\emptyset$ (Lemma \ref{IntersectionOfNestedCompactSets}) and (ii) is established.

To see the sufficiency of conditions (i) and (ii), we first construct
the limiting subsequence on a the set of rationals $\rationals_+
\subset [0,\infty)$.  To do this, we first claim that for any $T \in
\rationals_+$, (in fact any $T \in [0,\infty)$), the set $\lbrace
\abs{f(T)} \mid f \in A \rbrace$ is bounded.  The claim follows for
$T>0$ by using (ii) to select a $\delta > 0$ such that $\sup_{f \in A} m(T, f,
\delta) < 1$.  Picking the integer $m \geq 0$ such that $m \delta < T \leq
(m+1)\delta$ and considering  the grid $0, \delta, 2\delta, \dotsc,
m\delta, T$ we can write the telescoping sum
\begin{align*}
f(T) - f(0) = f(T) - f(m\delta) + \sum_{k=1}^m f(k \delta) - f((k-1)\delta)
\end{align*}
and use the triangle inequality to conclude that $\abs{f(T)}
\leq \abs{f(0)} + m+1$ for every $f \in A$.  Coupled with (i) this shows that
$\sup_{f \in A} \abs{f(T)} < \infty$.

We now enumerate the rationals $\rationals_+$ and use
compactness in $\reals$ and a diagonal
subsequence argument to pick a sequence $f_n$ with $f \in A$ such that
$f_n(T)$ converges for every $T \in \rationals_+$.  Define $f :
\rationals_+ \to \reals$ by $f(T) = \lim_{n \to \infty} f_n(T)$.

Having selected a convergent subsequence $f_n$ and defined $f$ on
$\rationals_+$ we proceed to see that $f$ is uniformly continuous on
the
interval $[0,T]$ for every $T \in \rationals_+$.
This follows by using (ii) to see that for every $f_n$, $T > 0$ and 
$\epsilon > 0$ there is $\delta > 0$ such that $\abs{f_n(s) - f_n(t)} <
\epsilon$ when $0 \leq s,t \leq T$ and $\abs{s - t} <\delta$.  From
this we have for every $n>0$, and $s,t \in \rationals$, $0 \leq s,t
\leq T$ and $\abs{s-t} < \delta$
\begin{align*}
\abs{f(s) -f(t)} &\leq \abs{f(s) -f_n(s)} + \abs{f_n(s) -f_n(t)} +
\abs{f_n(t) - f(t)} \\
&\leq \abs{f(s) -f_n(s)} + \epsilon +
\abs{f_n(t) - f(t)}
\end{align*}
Taking the limit as $n \to \infty$ using pointwise convergence of
$f_n$ to $f$ shows uniform continuity on every
$[0,T] \cap \rationals$ hence on $\rationals_+$.  Since $f$ is uniformly continuous on
$\rationals_+$ it follows that $f$ has a continuous extension to $f :
[0,\infty) \to \reals$.  Moreover we have shown that $\abs{f(s) -f(t)}
< \epsilon$ when $\abs{s -t} < \delta$.

It remains to prove that $f_n \to f$ in $C([0,\infty); \reals)$.  It
suffices (Lemma \ref{UniformConvergenceOnCompacts}) to show that $f_n
\to f$ uniformly on every interval $[0,T]$.  Let $T > 0$ be given.
Pick $\epsilon > 0$ and let
$\delta > 0$ be such that $m(T,f_n,\delta) < \epsilon$ (hence $m(T,f,\delta)
< \epsilon$ by the above comment).   Pick $N > 0$ such that
$\abs{f_n(k\delta) - f(k\delta)} < \epsilon/3$ for all $k=0,1, \dotsc,
\ceil{T/\delta}$ and $n \geq N$.  Then for every $0 \leq t \leq T$ and
$n \geq N$ let $k\geq 0$ be such that $k\delta \leq t < (k+1)\delta$
\begin{align*}
\abs{f_n(t) - f(t)} &\leq \abs{f_n(t) - f_n(k\delta)}
+\abs{f_n(k\delta) - f(k\delta)} +\abs{f(k\delta) - f(t)} < \epsilon
\end{align*}
and we are done.
\end{proof}

\section{Donsker's Theorem}

Provided with a characterization of compact sets in
$C([0,\infty); \reals)$ we can now state the probabilistic
analogue that characterizes tightness.
\begin{lem}\label{TightnessOfContinuousFunctions}A sequence of Borel probability measures $\mu_n$ on $C([0,\infty);
  \reals)$ is tight if and only if 
\begin{itemize}
\item[(i)]$\lim_{\lambda \to \infty} \sup_{n \geq 1} \sprobability{\abs{f(0)}
  \geq \lambda}{\mu_n} = 0$.
\item[(ii)] $\lim_{\delta \to 0} \sup_{n \geq 1} \sprobability{m(T, f,
  \delta) \geq \lambda}{\mu_n} = 0$ for all $\lambda > 0$ and $T > 0$.
\end{itemize}
\end{lem}
\begin{proof}
Let $\mu_n$ be a tight sequence.  Let $\epsilon > 0$ be given and pick
$K \subset C([0,\infty); \reals)$ compact with $\mu_n(K) >
1-\epsilon$ for all $n$.  Then by Theorem \ref{ArzelaAscoliTheorem} we know that $\sup_{f \in K}
\abs{f(0)} < \infty$ and therefore $\sprobability{\abs{f(0)}\geq
\lambda}{\mu_n} \leq \mu_n(K^c) < \epsilon$ for any $\lambda > \sup_{f
\in K} \abs{f(0)}$.  Thus (i) is shown.  Similarly applying Theorem \ref{ArzelaAscoliTheorem} we know that for
every $T > 0$ and $\lambda>0$
there exists $\delta>0$ such that $\sup_{f \in K} m(T, f, \delta) <
\lambda$.  Therefore $\lbrace f \mid m(T,f,\delta) \geq \lambda \rbrace
\subset K^c$ and by a union bound, for every $n>0$ we have $\sprobability{m(T,f,\delta) \geq
  \lambda}{\mu_n} \leq \sprobability{K^c}{\mu_n} < \epsilon$.
Therefore we have shown (ii).

Now assume that (i) and (ii) hold and suppose that $\epsilon > 0$ is
given.  By (i) there exists $\lambda > 0$ such that $\sup_{n \geq 1}
\sprobability{\abs{f(0)} \geq \lambda}{\mu_n} < \epsilon/2$.  By (ii)
for every integer $T > 0$ and $k > 0$, there exists a $\delta_{T,k}$
such that $\sup_{n \geq 1} \sprobability{m(T, f, \delta_{T,k}) \geq
  1/k}{\mu_n} < \epsilon/2^{T+k+1}$.  If we define 
\begin{align*}
A_T &= \lbrace f \mid 
m(T,f,\delta_{T,k}) < 1/k \text{ for all } k \geq 1\rbrace
\end{align*}
so that $A^c_T \subset \cup_{k=1}^\infty \lbrace f \mid m(T, f, \delta_{T,k}) \geq
  1/k \rbrace$ then by a union bound
\begin{align*}
\sup_{n \geq 1} \mu_n(A_T) &= \sup_{n \geq 1} (1 - \mu_n(A^c_T))\\
&\geq \sup_{n \geq 1} \left(1 - \sum_{k=1}^\infty \sprobability{m(T, f, \delta_{T,k}) \geq
  1/k}{\mu_n}\right ) \\
&\geq 1 - \epsilon/2^{T+1}
\end{align*}
If we define $K = \lbrace f \mid \abs{f(0)} < \lambda \rbrace \cap
\cap_{T=1}^\infty A_T$ then another union bound shows $\sup_{n \geq 1}
\mu_n(K) > 1 - \epsilon$ and by construction the set $K$ satisfies the
conditions of Theorem \ref{ArzelaAscoliTheorem} so is proven compact.  
\end{proof}

To prove that the rescaled and linearly interpolated random walk converges we need
prove tightness.  To prove tightness we need to show equicontinuity.
The following Lemma begins the process by demonstrating equicontinuity
at $0$.  Keep in mind the picture of the scaling of the random walk at
level $n$
which places the value of $S_j$ at the point $j/n$ scaled by the
factor $1/\sigma\sqrt{n}$.  With this geometry in mind note that what
we are proving is a bound for each of the sequence of rescaled random
walks on the interval $[0,\delta]$.

TODO: Replace $\epsilon$ by $\lambda$ in the following Lemma?
\begin{lem}\label{RandomWalkEquicontinuityAt0} Let $\xi_n$ be i.i.d. with mean $0$ and finite variance
  $\sigma^2$ and define $S_n = \sum_{k=1}^n \xi_k$.  Then for all
  $\epsilon > 0$ 
\begin{align*}
\lim_{\delta \to 0} \limsup_{n \to \infty} \frac{1}{\delta}
\probability{\max_{1 \leq j \leq \floor{n\delta}+1} \frac{\abs{S_j}}{\sigma\sqrt{n}} \geq
  \epsilon} = 0
\end{align*}
\end{lem}
\begin{proof}
The idea of the proof is to leverage the Central Limit Theorem and
Gaussian tail bounds to control behavior at the right endpoint of the
interval under consideration.  Then independence of increments and
finite variance can be used to control the behavior over the entire
interval.

The sequence of random variables $\frac{1}{\sigma
  \sqrt{\floor{n\delta}+1}}S_{\floor{n\delta}+1}$ is a subsequence of
$\frac{1}{\sigma\sqrt{n}}S_n$ and therefore converges in distribution
to $N(0,1)$ by the Central Limit Theorem.  Furthermore, $\lim_{n \to
  \infty} \frac{\sqrt{\floor{n\delta}+1}}{\sqrt{n\delta}} = 1$ so by
Slutsky's Lemma we also have $\frac{1}{\sigma\sqrt{n\delta}}
S_{\floor{n\delta}+1} \todist Z$ where $Z$ is an $N(0,1)$ Gaussian
random variable.  By the Portmanteau Theorem (Theorem
\ref{PortmanteauTheorem}) and
a Markov bound (Lemma \ref{MarkovInequality}) we have
\begin{align*}
\limsup_{n \to \infty}
\probability{\abs{\frac{1}{\sigma\sqrt{n\delta}}S_{\floor{n\delta}+1}}
  \geq \lambda} &\leq \probability{\abs{Z}  \geq \lambda} \leq \frac{\expectation{\abs{Z}^3}}{\lambda^3}
\end{align*}

We want to leverage this bound to create a maximal inequality that controls the entire interval of
values of the rescaled random walk the approach being to leverage the
fact that either the final point is in the tail (in which
case the Central Limit Theorem bound just proven applies) or the final
point is outside the tail and some interior point is in the tail
providing us with an amount of variation whose probability can be
controlled by use of a second moment bound.  With $\epsilon > 0$ fixed as in
the hypothesis of the Lemma, define the random variable $\tau
= \min \lbrace j \geq 1 \mid \abs{\frac{S_j}{\sigma\sqrt{n}} } >
\epsilon\rbrace$ (this is a stopping time though we make no use of the
concept here).  Pick $\delta > 0$ satisfying $0 < \delta <
\epsilon^2/2$.

\begin{align*}
&\probability{\max_{1 \leq j \leq \floor{n\delta} + 1} \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon
  } \\
&=\probability{\max_{1 \leq j \leq \floor{n\delta} + 1}
  \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon; 
\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}  \geq \epsilon -
\sqrt{2\delta}} \\
&+ \probability{\max_{1 \leq j \leq \floor{n\delta} + 1}
  \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon; 
\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}} < 
  \epsilon - \sqrt{2\delta}} \\
&\leq \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\sum_{j=1}^{\floor{n\delta}}
\probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}} <
  \epsilon - \sqrt{2\delta}; \tau = j} \\
&= \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\sum_{j=1}^{\floor{n\delta}}
\probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}} -
    \frac{S_j}{\sigma \sqrt{n}}} > \sqrt{2\delta}; \tau = j} \\
&\leq \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\frac{1}{2\delta}\sum_{j=1}^{\floor{n\delta}}
\expectation{\left(
    \frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}} - \frac{S_j}{\sigma
      \sqrt{n}} \right)^2 \characteristic{\tau = j}} \\
&= \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\frac{1}{2\delta}\sum_{j=1}^{\floor{n\delta}}
\expectation{\left( \sum_{i=j+1}^{\floor{n\delta}+1}
    \frac{\xi_i}{\sigma\sqrt{n}} \right)^2 } \probability{\tau = j} \\
&= \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\frac{\floor{n\delta}}{2 n \delta}\sum_{j=1}^{\floor{n\delta}}
\probability{\tau = j} \\
&= \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} + 
\frac{1}{2}\probability{\max_{1 \leq j \leq \floor{n\delta} + 1} \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon}
\end{align*}
Therefore we have shown that 
\begin{align*}
\probability{\max_{1 \leq j \leq \floor{n\delta} + 1} \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon}
&\leq 2 \probability{\abs{\frac{S_{\floor{n\delta}+1}}{\sigma \sqrt{n}}}
  \geq \epsilon - \sqrt{2\delta}} 
\end{align*}
and we can use our tail bound derived from the Central Limit Theorem
(with $\lambda = \frac{\epsilon - \sqrt{2\delta}}{\sqrt{\delta}}$)
to see that 
\begin{align*}
\lim_{\delta \to 0} \limsup_{n \to \infty} \frac{1}{\delta} \probability{\max_{1 \leq j \leq \floor{n\delta} + 1} \abs{\frac{S_j}{\sigma \sqrt{n}}} \geq \epsilon}
&\leq \lim_{\delta \to 0 } \frac{2}{\delta} \expectation{\abs{Z}^3}
\left(\frac {\sqrt{\delta}}{\epsilon - \sqrt{2\delta}}\right)^3 = 0
\end{align*}
\end{proof}

The next step is to extend the estimate that provides equicontinuity
at $0$ to prove equicontinuity of the random walk on all finite intervals.  
\begin{lem}\label{RandomWalkEquicontinuity}
Let $\xi_n$ be i.i.d. with mean $0$ and finite variance
  $\sigma^2$ and define $S_n = \sum_{k=1}^n \xi_k$.  Then for all
  $\epsilon > 0$ and $T > 0$ 
\begin{align*}
\lim_{\delta \to 0} \limsup_{n \to \infty} 
\probability{\max_{
\substack{1 \leq j \leq \floor{n\delta}+1 \\
0 \leq k \leq \floor{nT}+1}} \frac{\abs{S_{j+k}
    - S_k}}{\sigma\sqrt{n}} \geq
  \epsilon} = 0
\end{align*}
\end{lem}
\begin{proof}
Pick $0 \leq \delta \leq T$ and let $m \geq 2$ be the integer such that
$T/m < \delta \leq T/(m-1)$.  Since
\begin{align*}
\lim_{n \to \infty} \frac{\floor{nT}+1}{\floor{n\delta}+1} &=
\frac{T}{\delta} < m
\end{align*}
we know that for sufficiently large $n$ we have $\floor{nT}+1  <
(\floor{n\delta}+1)m$.  For any such $n$, suppose $\frac{\abs{S_{j+k}
    - S_k}}{\sigma\sqrt{n}} > \epsilon$ for some $k$ with $0 \leq k
\leq \floor{nT}+1$ and some $j$ with $0 \leq j \leq
\floor{n\delta}+1$.  Now let $p$ be the integer such that $0 \leq p
\leq m -1$ and 
\begin{align*}
(\floor{n\delta}+1)p \leq k < (\floor{n\delta}+1)(p+1)
\end{align*}
Since $0 \leq j \leq \floor{n\delta}+1$ either 
\begin{align*}
(\floor{n\delta}+1)p \leq k+j < (\floor{n\delta}+1)(p+1)
\end{align*}
or
\begin{align*}
(\floor{n\delta}+1)(p+1) \leq k+j < (\floor{n\delta}+1)(p+2)
\end{align*}
In the first case by the triangle inequality we have
\begin{align*}
\abs{S_{j+k} - S_k} \leq \abs{S_{k} - S_{(\floor{n\delta}+1)p}}+ \abs{S_{j+k} - S_{(\floor{n\delta}+1)p}}
\end{align*}
and therefore we know that either $\frac{\abs{S_{k}
    - S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq \epsilon/2 > \epsilon/3$ or $\frac{\abs{S_{k+j}
    - S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq \epsilon/2 > \epsilon/3$.  In
the second case by the triangle inequality we have
\begin{align*}
\abs{S_{j+k} - S_k} \leq \abs{S_{k} - S_{(\floor{n\delta}+1)p}}+ \abs{S_{(\floor{n\delta}+1)(p+1)} - S_{(\floor{n\delta}+1)p}}+ \abs{S_{j+k} - S_{(\floor{n\delta}+1)(p+1)}}
\end{align*}
and therefore we know that either  
$\frac{\abs{S_{k} - S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq
\epsilon/3$,  
$\frac{\abs{S_{(\floor{n\delta}+1)(p+1)} -
    S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq \epsilon/3$ or 
$\frac{\abs{S_{k+j} - S_{(\floor{n\delta}+1)(p+1)}}}{\sigma\sqrt{n}}
\geq \epsilon/3$.  Therefore we have the inclusion of events
\begin{align*}
\left \lbrace\max_{
\substack{1 \leq j \leq \floor{n\delta}+1 \\
0 \leq k \leq \floor{nT}+1}} \frac{\abs{S_{j+k}
    - S_k}}{\sigma\sqrt{n}} \geq
  \epsilon\right \rbrace 
&\subset 
\bigcup_{p=0}^{m} \left \lbrace \max_{1 \leq j \leq \floor{n\delta}+1}
\frac{\abs{S_{j + (\floor{n\delta}+1)p} - S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq
\epsilon/3 \right \rbrace
\end{align*}
By the i.i.d. nature of $\xi_n$ and the fact that $S_0 = 0$ we know that 
\begin{align*}
\probability{\max_{1 \leq j \leq \floor{n\delta}+1}
\frac{\abs{S_{j + (\floor{n\delta}+1)p} - S_{(\floor{n\delta}+1)p}}}{\sigma\sqrt{n}} \geq
\epsilon/3 } &= \probability{\max_{1 \leq j \leq \floor{n\delta}+1}
\frac{\abs{S_{j}}}{\sigma\sqrt{n}} \geq \epsilon/3 }
\end{align*}
and therefore 
\begin{align*}
\probability{\max_{
\substack{1 \leq j \leq \floor{n\delta}+1 \\
0 \leq k \leq \floor{nT}+1}} \frac{\abs{S_{j+k}
    - S_k}}{\sigma\sqrt{n}}} \leq (m+1) \probability{\max_{1 \leq j \leq \floor{n\delta}+1}
\frac{\abs{S_{j}}}{\sigma\sqrt{n}} \geq \epsilon/3 }
\end{align*}
Since $\lim_{\delta \to 0} (m+1)\delta < \lim_{\delta \to 0} (T/\delta
+ 2) \delta = T < \infty$ we can apply Lemma \ref{RandomWalkEquicontinuityAt0} to get the result.
\end{proof}

By Prohorov's Theorem \ref{Prohorov} we know that a tight sequence of probability
measures on a separable metric space has a convergent subsequence.  What is often required is some
way of proving that a particular measure is indeed the limit of that
subsequence.  Recalling Lemma \ref{ProcessLawsAndFDDs} we know that
finite dimensional distributions characterize the laws of stochastic
processes which leads one to the following general procedure for
proving convergence of a sequence of processes.

TODO: Kallenberg (Chapter 16) has general results here for $C(T ; S)$ with $T$ a
$lcscH$-space and $S$ an arbitrary metric space.  Of course there are also results for
spaces of discontinuous functions for use in proving convergence of
empirical distribution functions.  Kallenberg also has results for
point process/spaces of measures.

We are taking the point of view of Brownian motion and the linearly
interpolated random walk as being a random element in $C([0,\infty) ;
\reals)$.  On the other hand we have thus far treated a stochastic
process as a random element in a subset of a path space $(S^T,
\mathcal{S}^{\otimes T})$ \emph{equipped with the product
  $\sigma$-algebra}.  It is tempting to gloss over this
point, however to tie in the general definition of stochastic
processes with the random elements of $C([0,\infty); \reals)$ we are
dealing with it is
important to understand the relationship between the Borel
$\sigma$-algebra on $C([0,\infty); \reals)$ and the product
$\sigma$-algebra $\mathcal{B}(\reals)^{\otimes [0,\infty)}$ used in the definition of
processes.
\begin{lem}\label{BorelGeneratedByProjections}For every $t \in [0,\infty)$ let $\pi_t : C([0,\infty); \reals) \to
  \reals$ be the evaluation map $\pi_t(f) = f(t)$.  The Borel $\sigma$-algebra on $C([0,\infty); \reals)$ is
  equal to $\sigma(\lbrace \pi_t \mid t \in [0,\infty) \rbrace)$ and
  therefore $\mathcal{B}(C([0,\infty); \reals)) = C([0,\infty);
  \reals) \cap \mathcal{B}(\reals)^{\otimes [0,\infty)}$.
\end{lem}
\begin{proof}Since each $\pi_t$ is a continuous function, it is Borel
  measurable and therefore the Borel $\sigma$-algebra contains
  $\sigma(\lbrace \pi_t \mid t \in [0,\infty) \rbrace)$.

TODO: The following proof looks incorrect; I am using the supremum on $[0,\infty)$ which is not the metric on $C([0,\infty); \reals)$.  What follows is valid on $C([0,T];\reals)$ but not on $C([0,\infty); \reals)$.

On the other hand, we know that $C([0,\infty) ; \reals)$ is separable
so we may pick a countable dense set $f_1, f_2, \dotsc$.
If we let $U \subset C([0,\infty) ; \reals)$ be open then for every
$f_j \in U$ there exists $r_j > 0$ such that $B(f_j, r_j) \subset U$
and $U$ is the union of such $B(f_j, r_j)$ (indeed, any $y \in U$ not in the
union of balls can't be the limit of the $f_j$ that are in $U$; on the
other hand it can't be the limit of the $f_j$ that are in $U^c$ since
the latter set is closed; thus the existence of such a $y$ would
contradict the density of $f_1, f_2, \dotsc$).  To show $U \in
\sigma(\lbrace \pi_t \mid t \in [0,\infty) \rbrace)$
it suffices to show that $B(f, r) \in \sigma(\lbrace \pi_t \mid t \in
[0,\infty) \rbrace)$ for every $f \in C([0,\infty) ; \reals)$ and $r > 0$.

Let $B(f, r)$ be given and note that by continuity of the
elements of $C([0,\infty) ; \reals)$ the closed ball
\begin{align*}
\overline{B(f, r)} &= \lbrace g \mid \sup_{x \in [0,\infty)} \abs{f(x)
  - g(x)} \leq r \rbrace \\
&= \lbrace g \mid \sup_{\substack{x \in
    [0,\infty) \\ x \in \rationals}} \abs{f(x)
  - g(x)} \leq r \rbrace \\
&= \cap_{\substack{x \in
    [0,\infty) \\ x \in \rationals}} \pi_x^{-1} ([f(x)-r,f(x)+r]) 
\end{align*}
which shows that $\overline{B(f, r)} \in \sigma(\lbrace \pi_t \mid t \in
[0,\infty) \rbrace)$ and $B(f, r) = \cap_{n=1}^\infty \overline{B(f, r+1/n)}$
which shows that $B(f, r) \in \sigma(\lbrace \pi_t \mid t \in
[0,\infty) \rbrace)$.
\end{proof}

\begin{thm}\label{ConvergenceInDistributionOfContinuousAsTightnessAndFDDs}Let $X_n$ be a tight sequence of continuous processes such
  that for all $d > 0$ and $0 \leq t_1 < \dotsb < t_d < \infty$ the
  sequence $(X_{n, t_1}, \dotsc , X_{n, t_d})$ converges in
  distribution, then the laws $X_n$ converge to a Borel probability
  distribution $\mu$ on $C([0,\infty); \reals)$ for which the
  canonical process $W_t(\omega) = \omega(t)$ satisfies
\begin{align*}
(X_{n, t_1}, \dotsc , X_{n, t_d}) \todist (W_{t_1}, \dotsc, W_{t_d})
\end{align*}
\end{thm}
\begin{proof}
By tightness and Prohorov's Theorem \ref{Prohorov} we know that $X_n$
has a weakly convergent subsequence.  Our first claim is that any two weakly convergent subsequences of
$X_n$ have the same limiting distribution.  Let $\check{X}_n$ and $\hat{X}_n$ be two
such subsequences and suppose that $\pushforward{\check{X}_n}{P} \to
\check{\mu}$ and $\pushforward{\hat{X}_n}{P} \to\hat{\mu}$
respectively.  Fix $0 \leq t_1 < \dotsb < t_d < \infty$ and note that
by the Continuous Mapping Theorem \ref{ContinuousMappingTheorem} we
know that $\pushforward{(\check{X}_{n,t_1}, \dotsc,
  \check{X}_{n,t_d})}{P} \todist \pushforward{(\pi_{t_1}, \dotsc,
  \pi_{t_d})}{\check{\mu}}$ and $\pushforward{(\hat{X}_{n,t_1}, \dotsc,
  \hat{X}_{n,t_d})}{P} \todist \pushforward{(\pi_{t_1}, \dotsc,
  \pi_{t_d})}{\hat{\mu}}$.  By hypothesis we conclude that $\pushforward{(\pi_{t_1}, \dotsc,
  \pi_{t_d})}{\check{\mu}} = \pushforward{(\pi_{t_1}, \dotsc,
  \pi_{t_d})}{\hat{\mu}}$ and therefore by
Lemma \ref{BorelGeneratedByProjections} we can apply Lemma
\ref{ProcessLawsAndFDDs} to conclude $\check{\mu}=\hat{\mu}$ which we
now refer to as $\mu$.

Now suppose that the distributions of $X_n$ do not converge weakly to
$\mu$.  Then there exists a bounded continuous $f$ such that either
$\lim_{n \to \infty} \expectation{f(X_n)}$ does not exist or exists
and is different from $\int f \, d\mu$.  In either case by the
boundedness of $f$ we know that 
\begin{align*}
-\infty &< -\norm{f}_\infty \leq \liminf_{n \to \infty}
\expectation{f(X_n)} \leq \limsup_{n \to \infty} \expectation{f(X_n)}
\leq \norm{f}_\infty  < \infty
\end{align*}
and we can extract
a subsequence $\check{X}_n$ such that $\lim_{n \to \infty}
\expectation{f(\check{X}_n)}$ exists and $\lim_{n \to \infty}
\expectation{f(\check{X}_n)} \neq \int f \, d\mu$.  This is a
contradiction since by tightness we know that $\check{X}_n$ has a weakly
convergent subsequence and we have already just shown that the limiting
distribution is $\mu$.
\end{proof}

The power of this Theorem is that it is often not too difficult to
prove weak convergence of finite dimensional distributions because we
have the power of a rich theory available (e.g. the Central Limit
Theorem, Slutsky's Theorem, characteristic functions).

\begin{lem}\label{ConvergenceOfRandomWalkFDD}Let $\xi_n$ be i.i.d. with mean $0$ and finite variance
  $\sigma^2$, define $S_n = \sum_{k=1}^n \xi_k$, $S_n^*(t) =
  S_{\floor{t}} + (t - \floor{t})\xi_{\floor{t}+1}$ and
  $X_n(t) = \frac{1}{\sigma \sqrt{n}} S_n^*(nt)$ where the latter are
  interpreted as random elements of the Borel measurable space
  $C([0,\infty);\reals)$.  For every $d > 0$ and real numbers $0 \leq t_1 < \cdots < t_d
  < \infty$ we have 
\begin{align*}
(X_n(t_1), \dotsc, X_n(t_d)) \todist (B_{t_1}, \dotsc, B_{t_d})
\end{align*}
where $B_t$ is a standard Brownian motion.
\end{lem}
\begin{proof}
Let $0 \leq t_1 < \dotsb < t_n < \infty$ be given.  The basic point is
that the result follows by the Central Limit Theorem; however due to
the linear interpolation there is a bit of extra work to do.

First note that by definition 
\begin{align*}
\abs{X_n(t) - \frac{1}{\sigma \sqrt{n}} S_{\floor{nt}} }
&\leq  \frac{1}{\sigma \sqrt{n}}\abs{\xi_{\floor{nt}+1}}
\end{align*}
so by a Chebyshev bound (Lemma \ref{ChebInequality}) we have
\begin{align*}
\lim_{n \to \infty} \probability{\abs{X_n(t) - \frac{1}{\sigma
      \sqrt{n}} S_{\floor{nt}} } > \epsilon}
&\leq  \lim_{n \to \infty} \frac{1}{n\epsilon^2} = 0
\end{align*}
thus $X_n(t)  \toprob \frac{1}{\sigma\sqrt{n}} S_{\floor{nt}}$ and by
Lemma \ref{ConvergenceInProbabilityInProductSpaces}
we have $(X_n(t_1), \dotsc, X_n(t_d)) \toprob (\frac{1}{\sigma\sqrt{n}}
S_{\floor{nt_1}}, \dotsc, \frac{1}{\sigma\sqrt{n}}
S_{\floor{nt_d}})$.  Our result will follow by Slutsky's Theorem
\ref{Slutsky} if we can show that
\begin{align*}
(\frac{1}{\sigma\sqrt{n}}
S_{\floor{nt_1}}, \dotsc, \frac{1}{\sigma\sqrt{n}}S_{\floor{nt_d}}) \todist (B_{t_1}, \dotsc, B_{t_d})
\end{align*}
Application of the Continuous Mapping Theorem
\ref{ContinuousMappingTheorem} lets us reduce further to showing that 
\begin{align*}
(\frac{1}{\sigma\sqrt{n}}(S_{\floor{nt_1}} -
S_{\floor{nt_0}}), \dotsc, \frac{1}{\sigma\sqrt{n}}
(S_{\floor{nt_d}}- S_{\floor{nt_{d-1}}}))\todist (B_{t_1} - B_{t_0}, \dotsc, B_{t_d} - B_{t_{d-1}})
\end{align*}
where for uniformity of notation we have defined $t_0 = 0$.  Since the
$\xi_n$ are independent this implies that the $S_{\floor{nt_j}}-
S_{\floor{nt_{j-1}}}$ are independent for $j=1, \dotsc, d$ and by
definition of independent increments property of Brownian motion we
know that $B_{t_j} - B_{t_{j-1}}$ are independent, thus by Lemma \ref{IndependenceProductMeasures} it suffices to
show that $\frac{1}{\sigma\sqrt{n}} (S_{\floor{nt_j}}-S_{\floor{nt_{j-1}}}) \todist N(0, t_j -
t_{j-1})$.  We shall prove this fact for an arbitrary $0 \leq s < t < \infty$.

By the definition of $S_n$ we write $\frac{1}{\sigma\sqrt{n}}
(S_{\floor{nt}}-S_{\floor{ns}}) =
\frac{1}{\sigma\sqrt{n}}\sum_{i=\floor{ns}+1}^{\floor{nt}}
\xi_i$.  For every $\epsilon > 0$ we have by another Chebyshev bound 
\begin{align*}
&\lim_{n \to \infty} \probability{\abs{\frac{1}{\sigma\sqrt{n}}
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i
- \frac{\sqrt{t-s}}{\sigma\sqrt{\floor{nt}-\floor{ns}}}
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i } > \epsilon} \\
&\leq \lim_{n \to \infty}\frac{1}{\epsilon^2}\variance{\left(\frac{1}{\sigma\sqrt{n}}
-\frac{\sqrt{t-s}}{\sigma\sqrt{\floor{nt}-\floor{ns}}} \right)
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i} \\
&= \lim_{n \to \infty}\frac{1}{\epsilon^2}\left(\frac{1}{\sigma\sqrt{n}}
-\frac{\sqrt{t-s}}{\sigma\sqrt{\floor{nt}-\floor{ns}}} \right)^2
(\floor{nt}-\floor{ns}) \sigma^2\\
&= \lim_{n \to \infty} \frac{1}{\epsilon^2} \left(
  \frac{\sqrt{\floor{nt}-\floor{ns}}}{\sqrt{n}} -
  \sqrt{t-s}\right)^2 = 0
\end{align*}
Therefore we have $\frac{1}{\sigma\sqrt{n}}
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i \toprob \frac{\sqrt{t-s}}{\sigma\sqrt{\floor{nt}-\floor{ns}}}
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i $ and one last appeal to
Slutsky's Theorem \ref{Slutsky} implies that it suffices to show
\begin{align*}
\frac{\sqrt{t-s}}{\sigma\sqrt{\floor{nt}-\floor{ns}}}
\sum_{i=\floor{ns}+1}^{\floor{nt}}\xi_i \todist N(0, t-s)
\end{align*}
which is just the Central Limit Theorem (and to be precise the
Continuous Mapping Theorem \ref{ContinuousMappingTheorem} to account for the multiplication by $\sqrt{t-s}$).
\end{proof}

The last step we make is in extending the equicontinuity of the random
walk to equicontinuity of the linearly interpolated random walk which
are honest elements of $C([0, \infty); \reals)$.  This equicontinuity
will prove tightness and weak convergence of the linearly interpolated
random walk.  One of the elements of proving the equicontinuity of the
linearly interpolated random walk is a general fact about the modulus
of continuity of a class of piecewise linear functions which we prove
as a separate lemma.

\begin{lem}\label{ModulusOfContinuityOfPL}Let $f(t)$ be a continuous function that is linear on every
  interval $[j,j+1]$ for $j=0, 1, \dotsc$.  For every integer $M
  > 0$ and $N > 0$, we have
\begin{align*}
\sup_{\substack{\abs{s -t} \leq M \\ 0 \leq s,t
    \leq N} } \abs{f(s) - f(t)}
&\leq
\sup_{\substack{1 \leq j \leq M \\ 0 \leq k
    \leq N}} \abs{f(k+j) - f(k)}
\end{align*}
\end{lem}
\begin{proof}
Pick $0 \leq s<t \leq M$.  If there exists $j < N$ such that $j \leq s
< t \leq j+1$ then it is clear from linearity  that $\abs{f(s) - f(t)}
\leq \abs{f(j) - f(j+1)}$ so it suffices to consider the case in which 
$j \leq s < j +1 < \dotsb < j+k < t \leq j+k+1$ for some $j \geq 0$ and $k
> 0$.  If we let $f(t)$ has slope $a_j$ on the interval $[j,j+1]$
then we can write $f(t) - f(s) = a_{j}(j+1 -s) + \dotsb + a_{j+k}(t -
j -k)$.  Note that
if $f(t) - f(s)$ has a different sign  than $a_j$ then $\abs{f(t) -
  f(s)} \leq \abs{f(t) -  f(j+1)}$ and similarly with $a_{j+k}$ so if
suffices to assume that $a_j$ and $a_{j+k}$ have the same sign as
$f(t)-f(s)$.  Now if $\abs{a_j} \leq \abs{a_{j+k}}$ then we slide the
pair $(s,t)$ to the right until either $s$ or $t$ hits an integer.
More formally if $j+1 - s \leq j+k+1 -t$ then we get
$\abs{f(t) -
  f(s)} \leq \abs{f(t + j+1-s) - f(j+1)}$ and if $j+k+1 -t \leq j+1 -
  s$
we get the bound $\abs{f(t) -
  f(s)} \leq \abs{f(j+k+1) - f(s + j+k+1 -t)}$.  If we $\abs{a_j} \geq
\abs{a_{j+k}}$ we slide to the left in an analogous way.  The point is
that we are reduced to the case in which either $s=j-1$ or $t=j+k+1$.

Once we know that either $s=j-1$ or $t=j+k+1$ , because $M$ is integer
we know that in fact $k \leq M$ and therefore we get a final bound
$\abs{f(t) - f(s)} \leq \abs{f(j+k+1) - f(j-1)}$ which proves the
result.

TODO: This proof is grotesque.  Try to do better!
\end{proof}

We are finally ready to put all of the pieces together to prove
Donsker's Theorem on the convergence of random walks to Brownian
motion.  Note that we have not used the existence of Brownian motion
anywhere in the proof so this Theorem is, among other things, an
existence proof for Brownian motion.
\begin{thm}[Donsker's Invariance Principle for Random Walks]\label{Donsker2}Let $\xi_n$ be i.i.d. with mean $0$ and finite variance
  $\sigma^2$, define $S_n = \sum_{k=1}^n \xi_k$, $S_n^*(t) =
  S_{\floor{t}} + (t - \floor{t})\xi_{\floor{t}+1}$ and
  $X_n(t) = \frac{1}{\sigma \sqrt{n}} S_n^*(nt)$ where the latter are
  interpreted as random elements of the Borel measurable space
  $C([0,\infty);\reals)$.  
Then the law of $X_n$  converges weakly to a probability measure under
which the coordinate mapping $(f,t) \to f(t)$ is a standard Brownian motion.
\end{thm}
\begin{proof}
Lemma \ref{ConvergenceOfRandomWalkFDD} shows that finite dimensional
distributions of the linearly interpolated and rescaled random walk
converge to the finite dimensional distributions of Brownian motion.
Therefore by Theorem
\ref{ConvergenceInDistributionOfContinuousAsTightnessAndFDDs} it
remains to show that $X_n$ is a tight sequence of processes.
By Lemma \ref{TightnessOfContinuousFunctions} we must show for all $X_n(t)$,
\begin{itemize}
\item[(i)]$\lim_{\lambda \to \infty} \sup_{n \geq 1} \probability{\abs{X_n(0)}
  \geq \lambda}= 0$.
\item[(ii)] $\lim_{\delta \to 0} \sup_{n \geq 1} \probability{m(T, X_n,
  \delta) \geq \lambda} = 0$ for all $\lambda > 0$ and $T > 0$.
\end{itemize}
Since $X_n(0) = 0$ the condition (i) holds trivially.  As for
condition (ii) we first argue that it suffices to show $\lim_{\delta \to 0} \limsup_{n \geq 1} \probability{m(T, X_n,
  \delta) \geq \lambda}= 0$.  This follows from the fact that for
fixed $n > 0$,
$\lim_{\delta \to 0} \probability{m(T, X_n,  \delta) \geq \lambda} =
0$ (continuity of $X_n$) and $\probability{m(T, X_n,  \delta) \geq \lambda}$ is a decreasing
function of $\delta$.  Indeed, if we let $\epsilon > 0$ be given pick
$\Delta > 0$ such that $\limsup_{n \geq 1} \probability{m(T, X_n,
  \delta) \geq \lambda} < \epsilon$ for all $\delta \leq \Delta$.  Then pick $N > 0$ is such that $\sup_{n \geq N} \probability{m(T, X_n,
  \Delta) \geq \lambda} < \epsilon$ and note that because $\probability{m(T, X_n,
  \delta) \geq \lambda}$ is decreasing in fact we have $\sup_{n \geq N} \probability{m(T, X_n,
  \delta) \geq \lambda} < \epsilon$ for all $\delta \leq \Delta$.
Since $\lim_{\delta \to 0} \probability{m(T, X_n,
  \delta) \geq \lambda} = 0 $ for every $n>0$ we can find
$\hat{\Delta} < \Delta$ such that $\probability{m(T, X_n,
  \delta) \geq \lambda}  < \epsilon$ for all $n=1, \dotsc, N-1$ and
$\delta \leq \hat{\Delta}$ and
thus $\sup_{n \geq 1} \probability{m(T, X_n,  \Delta) \geq \lambda} < \epsilon$ for all $\delta < \hat{\Delta}$.

With this reduction in hand, we can estimate
\begin{align*}
\probability{m(T, X_n,  \delta) \geq \lambda} &=
\probability{\sup_{\substack{\abs{s -t} \leq \delta \\ 0 \leq s,t
    \leq T}} \abs{X_n(s) - X_n(t)} \geq \lambda} \\
&\leq
\probability{\sup_{\substack{\abs{s -t} \leq \floor{n \delta} + 1 \\ 0 \leq s,t
    \leq \floor{T \delta}+1}} \abs{S^*_n(s) - S^*_n(t)} \geq \sigma
\sqrt{n} \lambda} \\
&\leq 
\probability{\sup_{\substack{1 \leq j \leq \floor{n \delta} + 1 \\ 0 \leq k
    \leq \floor{T \delta}+1}} \abs{S_n(k+j) - S_n(k)} \geq \sigma
\sqrt{n} \lambda} 
\end{align*}
where the last inequality follows Lemma \ref
{ModulusOfContinuityOfPL}.  Now we can apply Lemma
\ref{RandomWalkEquicontinuity} to conclude $\lim_{\delta \to
  0}\limsup_{n \to \infty} \probability{m(T, X_n,  \delta) \geq
  \lambda}$ and tightness is shown.
\end{proof}

\section{Banach Spaces}

We start with some of the standard examples of Banach spaces.

\begin{defn}Let $(\Omega, \mathcal{A})$ be a measurable space, let $B(\Omega)$ be space of bounded measurable functions $f : \Omega \to \reals$ and define $\norm{f}_\infty = \sup_{\omega \in \Omega} \abs{f(\omega)}$.
\end{defn}

\begin{prop}The space $B(\Omega)$ is a Banach space.
\end{prop}
\begin{proof}
It is elementary that $B(\Omega)$ is a vector space : if $f$ is bounded and measurable with $\norm{f}_\infty$ then clearly $af$ is bounded measurable with bound $\abs{a}\norm{f}_\infty$ for all $a \in \reals$ and if $f$ and $g$ are bounded and measurable with bounds $M$ and $N$ respectively then $f+g$ is bounded and measurable with bound $M+N$.

To see that $\norm{\cdot}_\infty$ is in fact a norm, first note that it is immediate from the definition that $\norm{f}_\infty \geq 0$ and $\norm{f}_\infty$ if and only if $f = 0$.  Let $f \in B(\Omega)$ and let $a \in \reals$ with $a \neq 0$.  Then for every $\epsilon>0$ we may find $\omega \in \Omega$ such that $\norm{f}_\infty - \epsilon/\abs{a} \leq \abs{f(\omega)}$.  It follows that $\abs{a} \norm{f}_\infty - \epsilon \leq \abs{a f (\omega)} \leq \norm{af}_\infty$.  Since $\epsilon >0$  was arbitrary we may take $\epsilon \downarrow 0$ to conclude $\abs{a} \norm{f}_\infty \leq \norm{af}_\infty$.  In a similar way we may find an $\omega \in \Omega$ such that 
\begin{align*}
\norm{af}_\infty - \epsilon &\leq \abs{af(\omega)} = \abs{a}\abs{f(\omega)} \leq \abs{a} \norm{f}_\infty
\end{align*}
and since $\epsilon > 0$ was arbitrary we conclude the opposite inequality $\norm{af}_\infty \leq \abs{a} \norm{f}_\infty$.  To see the triangle inequality let $f, g \in B(\Omega)$ be given.  For every $\omega \in \Omega$ we have $\abs{(f+g)(\omega)} \leq \abs{f(\omega)}+\abs{g(\omega)} \leq \norm{f}_\infty + \norm{g}_\infty$.  Now take the supremum over all $\omega$ to conclude $\norm{f+g}_\infty \leq \norm{f}_\infty + \norm{g}_\infty$.

Lastly we have to show that $B(\Omega)$ is complete.  Let $f_n$ be a Cauchy sequence in $B(\Omega)$.  Thus for every $\epsilon > 0$ there exists and $n \in \naturals$ such that for all $m \geq n$ we have $\norm{f_n - f_m} < \epsilon$.  In particular, for every $\omega \in \Omega$ we have $\abs{f_n(\omega) - f_m(\omega)} \leq \norm{f_n - f_m} < \epsilon$ so that $f_n(\omega)$ is a Cauchy sequence in $\reals$ for every $\omega \in \Omega$.  By completeness of $\reals$ we know that $f_n$ converges pointwise and by Lemma \ref{LimitsOfMeasurable} we know that there is a measurable function $f : \Omega \to \reals$ such that $f_n \to f$.  

Now note that for any $\epsilon > 0$ we can pick $N \in \naturals$ such that for $m,n \geq N$ we have $\norm{f_n-f_m}_\infty < \epsilon$ so by using the triangle inequality and taking limits over $m \in \naturals$ we get for every $\omega \in \Omega$ and $n \geq N$,
\begin{align*}
\abs{f (\omega) -f_n(\omega)}_\infty &\leq \lim_{m \to \infty} \abs{f_m(\omega) -f_n(\omega)} + \lim_{m \to \infty} \abs{f(\omega) -f_m(\omega)} 
\leq \lim_{m \to \infty} \norm{f_m -f_n}_\infty \leq \epsilon
\end{align*}
Now using the fact with the triangle inequality we get $\abs{f(\omega)} \leq \abs{f(\omega) - f_n(\omega)} + \abs{f_n(\omega)} \leq \epsilon + \norm{f_n}_\infty$ which shows $f$ is bounded.  Taking the supremum over $\omega$ shows that $\norm{f -f_n} \leq \epsilon$ for all $n \geq N$ which shows that $f_n$ converges to $f$ in $B(\Omega)$.
\end{proof}

\begin{defn}Let $(\Omega, \mathcal{A}, \mu)$ be a measure space and let $f : \Omega \to \reals$ be a measurable function.  We say that $a \in \reals$ is an \emph{essential upper bound of $f$} if $\mu(f^{-1}(a, \infty)) = 0$ (i.e. $f(x) \leq a \mu$-almost everywhere).  The \emph{essential supremum of $f$} is defined as the infimum of the set of essential upper bounds of $f$: 
\begin{align*}
\esssup f &= \inf \lbrace a \in \reals \mid \mu(f^{-1}(a,\infty)) = 0 \rbrace
\end{align*}
A measurable function $f$ is said to be \emph{essentially bounded} if $\esssup \abs{f} < \infty$.
\end{defn}

The set of essentially bounded functions on a measure space can be made into a Banach space under the essential supremum norm.  As is standard we have to pass to equivalence classes to do this so we first note when a function has an essential supremum of $0$.
\begin{prop}Let $(\Omega, \mathcal{A}, \mu)$ be a measure space and let $f : \Omega \to \reals$ be a measurable function then $\esssup \abs{f} = 0$ if and only if $f = 0$ $\mu$-almost everywhere.
\end{prop}
\begin{proof}
Clearly if $f$ is almost everywhere $0$ then $\abs{f}^{-1}(0, \infty) = \lbrace f \neq 0 \rbrace$ hence by monotonicity of measure, $\mu(\abs{f}^{-1}(0,\infty)) = 0$.  Thus $\esssup \abs{f} \leq 0$.  On the other hand for every $a < 0$ we know that $\abs{f}^{-1}(a,\infty) \supset \lbrace f = 0 \rbrace$ and therefore $\mu(\abs{f}^{-1}(a,\infty)) = \mu(\Omega) > 0$ and therefore $\esssup \abs{f} \geq 0$.  

On the other hand if $f$ is not almost everywhere zero then by Fatou's Lemma $0 < \mu(\abs{f}^{-1}(0,\infty)) \leq \liminf_{n \to \infty} \mu(\abs{f}^{-1}(1/n, \infty))$ which implies that $\mu(\abs{f}^{-1}(1/n, \infty)) > 0$ for some $n \in \naturals$ and therefore $\esssup \abs{f} \geq 1/n > 0$.
\end{proof}

\begin{defn}Let $(\Omega, \mathcal{A}, \mu)$ be a measure space then $L^\infty(\Omega, \reals)$ is space of equivalence classes of essentially bounded measurable functions under the equivalence relation of $\mu$-almost everywhere equality.
\end{defn}

\begin{thm}$L^\infty(\Omega, \reals)$ with essential supremum as a norm is a Banach space.
\end{thm}
\begin{proof}
We have already seen that $\norm{f}_\infty = 0$ implies $f = 0$ in $L^\infty$.  Let $a \in \reals$ with $a \neq 0$ and $f \in L^\infty$ then we note that $b$ is an essential upper bound for $\abs{f}$ if and only if $\abs{a} b$ is an essential upper bound for $\abs{af}$.  This follows from the set identity that
\begin{align*}
\abs{f}^{-1}(b,\infty) &= \lbrace \omega \mid b < \abs{f(\omega)} \rbrace =  \lbrace \omega \mid \abs{a} b < \abs{af(\omega)} \rbrace = \abs{af}^{-1}(\abs{a}b, \infty)
\end{align*}
Thus we have 
\begin{align*}
\norm{af}_\infty = \inf \lbrace \abs{a} b \mid \mu(\abs{f}^{-1}(b,\infty)) = 0 \rbrace = \abs{a}\norm{f}_\infty
\end{align*}
If we are given $f, g \in L^\infty$ then if $a$ is an essential upper bound for $f$ and $b$ is an essential upper bound for $g$ then by the triangle inequality in $\reals$,  $\abs{f+g}^{-1}(a+b, \infty) \subset \abs{f}^{-1}(a,\infty) \cup \abs{g}^{-1}(b,\infty)$ and by subadditivity of $\mu$ we know that $\mu(\abs{f+g}^{-1}(a+b, \infty)) = 0$.  Thus $a+b$ is an essential upper bound of $\abs{f+g}$.  For any $\epsilon > 0$ we can find essential upper bounds $a$ and $b$ such that $a \leq \norm{f}_\infty + \epsilon/2$ and $b \leq \norm{g}_\infty + \epsilon/2$ and therefore $\norm{f+g}_\infty \leq a + b \leq \norm{f}_\infty + \norm{g}_\infty + \epsilon$.  As $\epsilon > 0$ was arbitrary the triangle inequality follows.

To see completeness, suppose that $f_n$ is a Cauchy sequence.  We choose representatives of the equivalence class that are everywhere bounded on $\Omega$; thus there exist constants $M_n$ such that $\sup \abs{f} \leq M_n$.  For each $\epsilon > 0$ there exists an $N \in \naturals$ such that $\esssup \abs{f_n - f_m} < \epsilon$ for all $n,m \geq N$.  For each such pair $n,m$ if follows that $\abs{f_n - f_m} < \epsilon$ except on a set of $\mu$ null set.  Taking the union of such null sets over all rational $\epsilon > 0$ and all pairs $n,m$ we can take a countable union of null sets and find a single null set $\mathcal{N}$ such that for every $\epsilon \in \rationals_+$ there exists an $N \in \naturals$ such that $\abs{f_n(\omega) - f_m(\omega)} < \epsilon$ for all $n,m \geq N$ and $\omega \notin \mathcal{N}$.   Without changing equivalence classes of or the uniform bounds on the $f_n$ we may redefine $f_n$ to be zero on $\mathcal{N}$ and then it follows that for each $\epsilon \in \rationals_+$ there exists an $N \in \integers$ such that $\abs{f_n(\omega) - f_m(\omega)} < \epsilon$ for all $n,m \geq N$ and all $\omega \in \Omega$.  Thus $f_n$ is pointwise Cauchy and by completeness of $\reals$ we may define $f$ as the pointwise limit of $f_n$ and we know that $f$ is a measurable function.  In fact the convergence is easily seen to be uniform.  For every $\epsilon \in \rationals_+$ there exists an $n \in \naturals$ such that $\abs{f_n(\omega) - f_m(\omega)} \leq \epsilon$ for all $m \geq n$ and $\omega \in \Omega$ and therefore using the triangle inequality and pointwise convergence to $f$ we have
\begin{align*}
\abs{f_n(\omega) - f(\omega)} &\leq \lim_{m \to \infty} (\abs{f_n(\omega) - f_m(\omega)} + \abs{f_m(\omega) - f(\omega)} ) \leq \epsilon
\end{align*} 
for all $\omega \in \Omega$.  In particular by chosing $\epsilon = 1$, by the triangle inequality we have $\abs{f(\omega)} \leq \abs{f_n(\omega)} + \abs{f_n(\omega) - f(\omega)} \leq M_n + 1$ for all $\omega \in \Omega$ so $f \in L^\infty$.  
\end{proof}

\begin{defn}Let $X$ be a topological space the $C_c(X)$ is the set of
  all continuous function $f : X \to \reals$ with compact support
  (i.e. $supp(f) = \overline{\{x \in X \mid f(x) \neq 0 \}}$ is
  compact).
\end{defn}

\begin{defn}Let $X$ be a topological space the $C_0(X)$ is the set of
  all continuous function $f : X \to \reals$ which vanish at infinity
  in the sense that for every $\lambda >0$ the set $\lbrace x \in X \mid
  \abs{f(x)} \geq \lambda \rbrace$ is  compact.
\end{defn}

\begin{prop}\label{BanachSpaceOfFunctionsVanishingAtInfinity}If $X$ is a topological space, then  for each $f \in C_0(X)$
  define $\norm{f} = \sup_{x \in X} \abs{f(x)}$ then $\norm{f}$ is a
  norm on $C_0(X)$ and $C_0(X)$ is a Banach space.  Furthermore
  $C_c(X)$ is dense in $C_0(X)$.
\end{prop}
\begin{proof}
TODO:
\end{proof}

Note that if $X$ is compact then every continuous function vanishes at infinity so it follows that the space of 
all continuous functions on $X$ is a Banach space under the uniform norm.  In the case that $X$ is locally compact
and Hausdorff but not necessarily compact we can embed $X$ in its one point compactification $\tilde{X}$.  It is useful to understand
the relationship between $C_0(X)$ and $C(\tilde{X})$; in fact the following justifies the use of the phrase ``vanishing at infinity''.
\begin{prop}\label{IsometricEmbeddingVanishingAtInfinityIntoCompact}Let $X$ be a locally compact Hausdorff space and let $\tilde{X}$ be the one point compactification of $X$.  Every $f \in C_0(X)$ extends
uniquely to an element $\tilde{f} \in C(\tilde{X})$ by defining $\tilde{f}(\Delta) = 0$.  This defines an isometry of $C_0(X)$ with the subspace $\lbrace g \in C(\tilde{X}) \mid g(\Delta) = 0 \rbrace$.  
\end{prop}
\begin{proof}
We first show that defining $\tilde{f}(\Delta) =0$ makes $\tilde{f} : \tilde{X} \to \reals$ continuous.  Suppose $U \subset \reals$ be open.  If $0 \notin U$ then
$\tilde{f}^{-1}(U) = f^{-1}(U) \subset U$ which is open $X$ by continuity of $f$; it is also open  in $\tilde{X}$ by the definition of the topology on $\tilde{X}$.  If $0 \in U$ then
there exists $\epsilon > 0$ such that $(-\epsilon, \epsilon) \subset U$ and it follows from the fact that $f$ vanishes at infinity that $K = \lbrace \abs{f(x)} \geq \epsilon \rbrace$ is
compact.  Since we can write $\tilde{f}^{-1}(U) = f^{-1}(U) \cup (X \setminus K) \cup \lbrace \Delta \rbrace$ which is open in $\tilde{X}$ we see that $\tilde{f}$ is continuous.

Since $\tilde{X}$ is Hausdorff it follows that the value of $\tilde{f}$ at $\Delta$ is determined by the restriction to $X$ and thus the uniqueness of the continuous extension $\tilde{f}$ is determined.  It is elementary that the assignment $f \mapsto \tilde{f}$ is linear and it is trivial that $\sup_{v \in X} \abs{f(v)} \leq \sup_{v \in \tilde{X}} \abs{\tilde{f}(v)}$.  On the other hand for arbitrary $\epsilon > 0$ we have 
\begin{align*}
\sup_{v \in \tilde{X}} \abs{\tilde{f}(v)} &\leq \sup_{\substack{v \in X \\ \abs{f(v)} \geq \epsilon}} \abs{f(v)} + \epsilon \leq \sup_{v \in X} \abs{f(v)} + \epsilon
\end{align*}
Since $\epsilon>0$ was arbitrary we let $\epsilon \to 0$ and see that we have an isometry; in particular the mapping is injective.

It remains to characterize the range.  For that suppose that $g \in C(\tilde{X})$ is such that $g(\Delta) = 0$, it suffices to show that restriction of $g$ to $X$ vanishes at infinity.
For every $\epsilon > 0$ by continuity of $g$ there exists an open neighborhood $V$ of $\Delta$ such that $g(V) \subset (-\epsilon, \epsilon)$.  By definition of the topology on $\tilde{X}$ we can write $V = U \cap (X \setminus K) \cup \lbrace \Delta \rbrace$ with $U \subset X$ open and $K \subset X$ compact.  In particular it follows that $\abs{g(v)} < \epsilon$ for all $v \in X \setminus K$ which is to say that $\lbrace v \in X \mid \abs{g(v)} \geq \epsilon \rbrace \subset K$.  As a closed subset of a compact set in a Hausdorff space it follows that $\lbrace v \in X \mid \abs{g(v)} \geq \epsilon \rbrace$ is compact and thus $g \mid_X$ vanishes at infinity.
\end{proof}

As an example of the utility of this result consider the following
\begin{cor}\label{VanishingAtInfinityLocallyCompactAttainsNormInfSup}Let $X$ be locally compact Hausdorff and suppose that $f \in C_0(X)$ then it follows that there exists $x_0 \in X$ such that $\abs{f(x_0)} = \norm{f}$.  
\end{cor}
\begin{proof}
If $f=0$ the result is trivial so assume that $f$ is non-zero.  Apply Proposition \ref{IsometricEmbeddingVanishingAtInfinityIntoCompact} to extend $f$ by zero to $\tilde{f}$.  We know that $\abs{\tilde{f}(x)}$ is a continuous positive function on $\tilde{X}$ such that $\abs{\tilde{f}(\Delta)}=0$.  Since $\tilde{X}$ is compact we know that it attains its 
supremum (TODO: Where do we show this???) so that there exists $x_0 \in \tilde{X}$ such that $\abs{\tilde{f}(x_0)} = \norm{\tilde{f}} = \norm{f}  > 0$; it follows that $x_0 \in X$.

To see that $f$ attains its supremum, first assume that $f(x) \geq 0$ for all $x \in S$, then $\sup_{x \in X} f(x) = \norm{f}$.  Now we apply the result for norm attainment to $f$.  For general $f$ we simply consider $f - \inf_{x \in S} f(x)$.  The result for infimums follow from that for supremums since there exists $x_0$ such that $-f(x_0) = \sup_{x \in X} -f(x) = - \inf_{x \in X} f(x)$.
\end{proof}

We will also have reason to use the following simple fact.
\begin{cor}\label{VanishingAtInfinityLocallyCompactSupInfContinuity}Let $X$ be locally compact Hausdorff then $f \mapsto \inf_{x \in X} f(x)$ and $f \mapsto \sup_{x \in X} f(x)$ are 
continuous.
\end{cor}
\begin{proof}
Since $\inf_{x \in X} f(x) = - \sup_{x \in X} (-f(x))$ it suffices to handle the case of $\sup_{x \in X} f(x)$.  Let $f \in C_0(X)$ and $\epsilon > 0$ be given.  By Corollary \ref{VanishingAtInfinityLocallyCompactAttainsNormInfSup} we see that there exists $x_0 \in X$ such that $f(x_0) = \sup_{x \in X} f(x)$ therefore if $\norm{g - f} < \epsilon$
we have 
\begin{align*}
\sup_{x \in X} g(x) &\geq g(x_0) \geq f(x_0) - \epsilon = \sup_{x \in X} f(x) - \epsilon
\end{align*}
and if we pick $x_1$ such that $g(x_1) = \sup_{x \in X} g(x)$ then
\begin{align*}
\sup_{x \in X} g(x) &= g(x_1) \leq f(x_1) + \epsilon \leq \sup_{x \in X} f(x)  + \epsilon
\end{align*}

TODO: Prefer the following argument using sequences?  In a metric space (or any first countable topological space) convergence is determined by sequences right?
Using Corollary \ref{VanishingAtInfinityLocallyCompactAttainsNormInfSup}  for each $n \in \naturals$ we select
$x_n \in S$ such that $f_n(x_n) = \inf_{x \in S} f_n(x)$ and select $x_0 \in S$ such that $f(x_0) = \inf_{x \in S} f(x)$.  From the definition of $x_n$ and $\lim_{n \to \infty} f_n = f$ we see that $\lim_{n \to \infty} f_n(x_n) \leq \lim_{n \to \infty} f_n(x_0) = f(x_0)$.  On the other hand for every $\epsilon > 0$
there exists $N$ such that $\sup_{x \in S} \abs{f_n(x) - f(x)} < \epsilon$ for $n \geq N$ and therefore $f(x_0) - \epsilon \leq f(x_n) - \epsilon \leq f_n(x_n)$ for $n \geq N$.  This implies $f(x_0) - \epsilon \leq \lim_{n \to \infty} f_n(x_n)$ and since $\epsilon >0$ was arbitrary we get $f(x_0) \leq \lim_{n \to \infty} f_n(x_n)$.
\end{proof}

\begin{cor}\label{VanishingAtInfinityLocallyCompactSeparable}Let $X$
  be locally compact separable Hausdorff then $C_0(X)$ is separable.
\end{cor}
\begin{proof}
We know that $C(\tilde{X})$ is separable (Lemma
\ref{SeparabilityOfBoundedUniformlyContinuous}) so we may take a
countable dense set $f_n$.  We claim that $f_n - f_n(\Delta)$ is dense
in $C_0(X)$.  To see this, note that for any $f \in C_0(X)$ we know
that $f_n \to f$ in $C(\tilde{X})$ along some subsequence $N$.  On the
other hand, $f \mapsto f - f(\Delta)$ is a continuous map from
$C(\tilde{X})$ to $C_0(X)$ and therefore $f_n - f_n(\Delta) \to f$
along $N$.
\end{proof}

Question: is there a compact non-Hausdorff space that is not locally compact?

\begin{thm}[Principle of Uniform Boundedness]\label{PrincipleOfUniformBoundedness}Let $X$ be a Banach space, $Y$ be a normed vector space and
  $A_\alpha : X \to Y$ be a family of bounded linear maps.  If for all
  $v \in X$ we have $\sup_\alpha \norm{A_\alpha v} <\infty$ then
  $\sup_\alpha \norm{A_\alpha} < \infty$.
\end{thm}
\begin{proof}
For each $n \in \naturals$ let 
\begin{align*}
V_n &= \lbrace x \in X \mid \sup_\alpha
\norm{A_\alpha x} > n \rbrace
= \cup_\alpha \lbrace x \in X \mid \norm{A_\alpha x} > n \rbrace
\end{align*}
 and note that $V_n$ is open.
Furthermore by our hypothesis we know that $\cap_{n=1}^\infty V_n =
\emptyset$.  If follows from the Baire Category Theorem
\ref{BaireCategoryTheorem} that some $V_n$ is not dense.  Thus there
exists an $n \in \naturals$, $x \in X$ and $r > 0$ such that
$\overline{B}(x,r) \cap V_n = \emptyset$; that is to say for all $y
\in X$ such that $\norm{x-y} \leq r$ we have $\sup_\alpha
\norm{A_\alpha y} \leq n$.  
Now let $u \in X$ with
$\norm{u} \leq 1$ and observe that by linearity for all $\alpha$ 
\begin{align*} 
\norm{A_\alpha u} &=
r^{-1} \norm{A_\alpha ru}  \leq r^{-1}( \norm{A_\alpha (x + ru) } +
                    \norm{A x}) \leq 2 r^{-1} n
\end{align*}
Therefore $\sup_\alpha \norm{A_\alpha} = \sup_\alpha \sup_{\norm{u}
  \leq 1} \norm{A_\alpha u}  \leq 2 r^{-1} n < \infty$.
\end{proof}

\section{Riesz Representation}

We saw in the Daniell-Stone Theorem \ref{DaniellStoneTheorem} that one
may recapture a part of integration theory by considering certain
linear functionals on a space of functions.  There is a analogue to
that result that applies in the case of measure on topological
spaces and allows one to bring the machinery of functional analysis to
bear on problems of measure theory.  

\begin{defn}Let $X$ be a locally compact Hausdorff topological space then a Borel measure $\mu$ is said to be a
\emph{Radon measure} if $\mu(K) < \infty$ for every compact set $K$ and $\mu$ is inner regular i.e. if for every Borel set $A$ we
have 
\begin{align*}
\mu(A) &= \sup \lbrace \mu(K) \mid K \subset A \text{ and $K$ compact} \rbrace
\end{align*}
\end{defn}

N.B. Other definitions of Radon measure are in use.  For example Folland uses the following definition:
\begin{itemize} 
\item[(i)] (locally finite) $\mu(K)< \infty$ for all compact $K$
\item[(ii)] (outer regularity) $\mu(A) = \inf \lbrace \mu(U) \mid A \subset U \text{ and $U$ open} \rbrace$ for every Borel set $A$
\item[(iii)] (inner regularity on open sets) $\mu(U) = \inf \lbrace \mu(K) \mid K \subset U \text{ and $K$ compact} \rbrace$ for every open set $U$
\end{itemize}
The combination of outer regularity and inner regularity on open sets is sometimes referred to as \emph{quasi-regularity}.
For the case of $\sigma$-compact locally compact Hausdorff spaces the definitions coincide but dropping $\sigma$-compactness the definitions are no longer equivalent.  The definition present here is chosen so that the Riesz Representation Theorem below holds.  A Riesz Representation Theorem holds with the alternative definition as well. Note that we will not be concerned with Radon measures on non-locally compact spaces.

To illustrate 
\begin{examp}Let $\reals_d$ be the real line with the discrete topology and let $X = \reals \times \reals_d$.  TODO: Show that $X$ is locally compact.  Show that the compact subsets of $X$ are all of the form $\cup_{j=1}^n K_j \times \lbrace y_j \rbrace$ with $K_j$ compact.  Show that if $A \subset X$ is Borel then $A \cap (\reals \lbrace y \rbrace) = A_y \times \lbrace y \rbrace$ with $A_y$ a Borel subset of $\reals$ with the usual topology.  Define 
\begin{align*}
\mu(A) &= \sum_{y \in \reals} \lambda(A_y)
\end{align*}
Show that $\mu$ is locally finite and inner regular.  Define 
\begin{align*}
\nu(A) &= \begin{cases}
\sum_{y \in \reals} \lambda(A_y) & \text{if at most countably many $A_y \neq \emptyset$} \\
\infty & \text{otherwise}
\end{cases}
\end{align*}
Show that $\nu$ is locally finite and quasi-regular.  Show that $\mu$ and $\nu$ both induce the same continuous linear functional on the compactly supported functions.
\end{examp}

The Riesz representation theorem is actually a class of different
theorems with different hypotheses made about the measures involved
and the topology on the
underlying space.  Here we concentrate the reasonable general case of
Hausdorff locally compact spaces.  Other presentations may treat the
slightly simpler cases in which either second countability,
compactness or
$\sigma$-compactness are added as hypotheses on the topological space.  More general
presentations may drop the the assumption of local compactness and treat
arbitrary Hausdorff spaces.  

\begin{defn}A topological space $X$ is said to be \emph{locally
    compact} if every point in $X$ has a compact neighborhood
  (i.e. for every $x \in X$ there exists an open set $U$ and a compact
  set $K$ such that $x \in U \subset K$).
\end{defn}

\begin{lem}\label{LocallyCompactEquivalences}Let $X$ be a Hausdorff topological space then the following
  are equivalent
\begin{itemize}
\item[(i)]$X$ is locally compact
\item[(ii)]Every point in $X$ has an open neighborhood with compact closure
\item[(iii)]$X$ has a base of relatively compact neighborhoods
\end{itemize}
\end{lem}
\begin{proof}
(i) implies (ii):  If $X$ is Hausdorff then a closed subset of a compact set is compact
and therefore if $X$ is locally compact and $x \in X$ we take $U$ open
and $K$ compact such that $x \in U \subset K$ and then it follows that
$\overline{U}$ is compact hence (ii) follows.  

The fact that (ii) implies (i) is immediate.

(ii) implies (iii): For each $x \in X$ pick a relatively compact
neighborhood $U_x$, let $\mathcal{B}_x = \{ U \subset U_x \mid U
\in \mathcal{T} \}$ and let $\mathcal{B} = \cup_{x \in X}
\mathcal{B}_x$.  It is clear that $\mathcal{B}$ is a base for the
topology $\mathcal{T}$.  Moreover for each $U
\in \mathcal{B}$ there exists $x \in X$ such that $U \subset U_x$ with
$\overline{U}_x$ compact and then since $X$ is Hausdorff we know that
$\overline{U}$ is compact.

(iii) implies (ii) is immediate.
\end{proof}

A locally compact space can be embedded in a compact space by adding a single point.
\begin{defn}\label{OnePointCompactificationDefinition}Let $X$ be a locally compact Hausdorff space with topology $\tau$.  The \emph{one point compactification} of $X$ is the set
$X \cup \lbrace \Delta \rbrace$ where $\Delta \notin X$ and topology given by $\tau$ and sets of the form $X \setminus K \cup \lbrace \Delta \rbrace$
where $K \subset X$ is compact.  The point $\Delta$ is called the \emph{point at infinity}.
\end{defn}
The one point compactification is indeed compact.
\begin{thm}\label{OnePointCompactification}The one point compactification is a compact space.
\end{thm}
\begin{proof}
First we show that $\tilde{\tau} = \tau \cup \cup_{\substack{K \in \tau \\ K \text{ is compact}}} X \setminus K \cup \lbrace \Delta \rbrace$ is a topology.  It is clear that
$\emptyset \in \tilde{\tau}$ since $\emptyset \in \tau$ and moreover since $\emptyset$ is compact we see that $X \cup \lbrace \Delta \rbrace = (X \setminus \emptyset) \cup \lbrace \Delta\rbrace \in \tilde{\tau}$.  
Let $U_1, \dotsc, U_n$ be open and and $K_1, \dotsc, K_m$ be compact then if $n > 0$
\begin{align*}
&U_1 \cap \dotsb \cap U_n \cap (X \setminus K_1 \cup \lbrace \Delta \rbrace) \cap \dotsb \cap (X \setminus K_m \cup \lbrace \Delta \rbrace) \\
&=U_1 \cap \dotsb \cap U_n \cap (X \setminus K_1 \cap \dotsb \cap X \setminus K_m  \in \tau \subset \tilde{\tau}
\end{align*}
and if $n = 0$ then 
\begin{align*}
(X \setminus K_1 \cup \lbrace \Delta \rbrace) \cap \dotsb \cap (X \setminus K_m \cup \lbrace \Delta \rbrace)
&= (X \setminus K_1 \cap \dotsb \cap X \setminus K_m) \cup \lbrace \Delta \rbrace \in \tilde{\tau}
\end{align*}

If $A$ and $B$ are arbitrary index sets and we have $U_\alpha$ for $\alpha \in A$ open in $X$ and $K_\beta$ for $\beta \in B$ compact in $X$ then
if we define $U = \cup_{\alpha \in A} U_\alpha$ and $K= \cap_{\beta \in B} K_\beta$ note that $U \in \tau$ and $K$ is a closed subset of a compact set hence compact.  Furthermore, $U^c \cap K$ is compact for the same reason.  We compute using De Morgan's Law
\begin{align*}
\cup_{\alpha \in A} U_\alpha \cup \cup_{\beta \in B} (X \setminus K_\beta) \cup \lbrace \Delta \rbrace &=
\left[ U \cup \cup_{\beta \in B} (X \setminus K_\beta) \right ] \cup \lbrace \Delta \rbrace \\
&=X \setminus (U^c \cap K) \cup \lbrace \Delta \rbrace \in \tilde{\tau}
\end{align*}

Lastly note that $X \cup \lbrace \Delta \rbrace$ is compact.  If $U_\alpha$ and $(X \setminus K_\beta) \cup \lbrace \Delta \rbrace$ cover $X \cup \lbrace \Delta \rbrace$.  As before $\cap_{\beta \in B} K_\beta$ is compact and it follows that the $U_\alpha$ for $\alpha \in A$ form a cover.  Thus we may find a finite subcover $U_{\alpha_1}, \dotsc, U_{\alpha_n}$
and it follows that $U_{\alpha_1}, \dotsc, U_{\alpha_n}, (X \setminus \cap_{\beta \in B} K_\beta) \cup \lbrace \Delta \brace$ is a finite subcover of $X \cup \lbrace \Delta \rbrace$.
\end{proof}

\begin{prop}\label{CompleteRegularityLCH}A locally compact Hausdorff space $X$ is completely regular
  (i.e. for every $x \in X$ and closed set $F \subset X$ such that $x
  \notin F$ there is are disjoint open sets $U$ and $V$ such that $x
  \in U$ and $F \subset V$).
\end{prop}
\begin{proof}
Let $F$ be a closed set and pick $x \in X \setminus F$.  By Lemma
\ref{LocallyCompactEquivalences} and the openness of $X \setminus F$
we can find a relatively compact neighborhood $U_0$ of $x$ such that $x
\in U_0 \subset X \setminus F$.  The set $\overline{U_0} \cap F$ is a closed subset of a compact
set hence is compact.  For each $y \in \overline{U_0} \cap F$ by the
Hausdorff property we may find open neighborhoods $x \in U_y$ and $y
\in V_y$ such that $U_y \cap V_y = \emptyset$ (hence $\overline{U_y} \cap V_y = \emptyset$ by Corollary \ref{DisjointOpenSetsDisjointClosure}).  By compactness of
$\overline{U} \cap F$ we get a finite subcover $V_{y_1}, \dotsc,
V_{y_n}$ of $\overline{U_0} \cap F$.  Now define $U = U_0 \cap U_{y_1}
\cap \dotsb U_{y_n}$.  This is an open neighborhood of $x$ and
we claim that $\overline{U} \cap F = \emptyset$.  To see this last fact simply note that if $x \in \overline{U} \cap F$ then $x \in \overline{U_0} \cap F$ hence belongs to 
some $V_{y_j}$ for some $j=1, \dotsc, n$.  This contradicts the fact that $x \in \overline{U_{y_j}}$ and $\overline{U_{y_j}} \cap V_{y_j} = \emptyset$.  

Lastly define $V = X \setminus \overline{U}$ and we claim that $F \subset V$.  To see the claim, suppose that if $x \in F$.  If $x \notin \overline{U_0}$ it follows directly that $x \notin \overline{U}$.  If on the other hand $x \in \overline{U_0}$ then $x \in V_{y_j}$ for some $j=1, \dotsc, n$ and it follows that $x \notin \overline{U_{y_j}}$ which shows that $x \notin \overline{U}$.
\end{proof}

\begin{cor}\label{LCSCHIsMetrizable}A locally compact second countable Hausdorff space $X$  is metrizable.  Moreover it is metrizable via a metric with the property 
that every closed ball is compact.
\end{cor}
\begin{proof}
To see that $X$ is metrizable use the the previous proposition and Urysohn's Metrization Theorem \ref{UrysohnMetrization}.  

TODO: Finish the second part.  Perhaps we can use the one-point compactification idea?  Construct $X^+$ and create a metric $d$ on it.  Now define 
\begin{align*}
d^\prime(x,y) = d(x,y) + \abs{\frac{1}{d(x, \infty)} + \frac{1}{d(y,\infty)}}
\end{align*}
Show that $d^\prime$ works.  It could also be that $X$ is complete under $d^\prime$ which would show Polishness.
\end{proof}

TODO: Show that a locally compact separable Hausdorff space is Polish.  This seems to hinge on the fact that an open subset of a Polish space is Polish (Alexandrov's Theorem).  In this particular case you pass to the one point compactification show the latter is Polish and then appeal to Alexandrov's Theorem.  The idea seems to be that there is controlled way of stretching the metric around the puncture to restore completeness.

\begin{defn}Let $X$ be a topological space, then a subset $A \subset
  X$ is said to be \emph{bounded} if there exists a compact set $K$
  such that 
  $A \subset K \subset X$.  A subset $A \subset
  X$ is said to be \emph{$\sigma$-bounded} if there exists a sequence
  of compact sets $K_1, K_2, \dotsc$ such that
$A \subset  \cup_{i=1}^\infty K_i \subset X$.
\end{defn}

\begin{prop}\label{SigmaBoundedEquivalence}A set $A$ is $\sigma$-bounded Borel set if and only if
  there exist disjoint bounded Borel sets $A_1, A_2, \dotsc$ such that $A =
  \cup_{i=1}^\infty A_i$.
\end{prop}
\begin{proof}
Suppose $A$ is a $\sigma$-bounded Borel set and let $K_1, K_2, \dotsc$
be compact sets such that $A \subset \cup_{i=1}^\infty K_i$.  Define
$A_1= A \cap K_1$ and for $n>1$ let $A_n = A \cap K_n \setminus
\cup_{j=1}^{n-1} A_j$  Trivially each $A_n$ is bounded (it is contained in
$K_n$), $A = \cup_{i=1}^\infty A_i$ (by construction $A_n \subset A$
and for any $x \in A$ we can find $n$ such that $x \in K_n$; it
follows that $x \in A_n$).  Moreover by construction it is clear that
the $A_n$ are Borel.  On the other hand, if $A =
\cup_{i=1}^\infty A_i$ with $A_i$ bounded and Borel and disjoint, then take $K_i$ compact
such that $A_i \subset K_i$ and it follows that $A \subset
\cup_{i=1}^\infty K_i$.  $A$ is clearly Borel as it is a countable
union of Borel sets.
\end{proof}

\begin{lem}\label{BoundedNeighborhoodsOfCompactSets}Let $K$ be a
  compact set in a locally compact Hausdorff topological
  space $X$, then there exists a bounded open set $U$ such that $K
  \subset U$.  Moreover if $V$ is a open set such that $K \subset V$
  then there is a bounded open set $U$ such that $K \subset U \subset
  \overline{U} \subset V$.
\end{lem}
\begin{proof}
By taking $V = X$ we see the second assertion implies the first so it
suffices to prove the second assertion.  By complete regularity of $X$
(Proposition \ref{CompleteRegularityLCH}) and local compactness of $X$
for each $x \in K$ we may
find a relatively compact open neighborhood $x \in U_x$ such that
$\overline{U}_x \cap V^c = \emptyset$.  By compactness of $K$ we may
take a finite subcover $U_{x_1}, \dotsc, U_{x_n}$.  Then
$U = U_{x_1} \cup \dotsb \cup U_{x_n}$ is an open set with $K \subset
U$ and
$\overline{U} = \overline{U}_{x_1} \cup \dotsb \cup \overline{U}_{x_n}$ is
a finite union of
compact sets and is therefore compact.  Lastly $\overline{U} \cap
V^c = (\overline{U}_{x_1} \cap V^c) \cup \dotsb \cup
(\overline{U}_{x_n} \cap V^c) = \emptyset$ and therefore $K \subset U
\subset \overline{U} \subset V$.
\end{proof}

For our purposes the reason for bringing up $\sigma$-bounded sets is
the fact that the properties of inner and outer regularity are
essentially equivalent on them.

\begin{lem}\label{InnerOuterRegularityEquivalence}Let $X$ be a locally compact Hausdorff topological space and let $\mu$ be a measure
  that is finite on compact sets.  Then $\mu$ is inner regular on
  $\sigma$-bounded Borel sets if and only if $\mu$ is outer regular on
  $\sigma$-bounded Borel sets.
\end{lem}
\begin{proof}
Suppose that $\mu$ is inner regular on $\sigma$-bounded sets.  Let $A$
be a bounded Borel set and suppose $\epsilon > 0$ is given.   First,
note that $\overline{A}$ is compact so may apply
Lemma \ref{BoundedNeighborhoodsOfCompactSets} to find a bounded
open set $U$ such that $\overline{A} \subset U$.  Therefore
$\overline{U} \setminus A$ is a bounded Borel set so by inner
regularity we may find a
compact set $K \subset \overline{U} \setminus A$ such that 
\begin{align*}
\mu(\overline{U} \setminus A) - \epsilon &< \mu(K) \leq \mu(\overline{U} \setminus A)
\end{align*}
Let $V = U \cap K^c$.  Then $V$ is an open set and $A \subset V$.
Moreover,
\begin{align*}
\mu(V) &= \mu(U) - \mu(K) \leq \mu(\overline{U}) - \mu(\overline{U}
\setminus A) + \epsilon = \mu(A) + \epsilon
\end{align*}
Since $\epsilon > 0$ was arbitrary we see that $\mu$ is outer regular
on bounded Borel sets.  Now we need to extend to outer regularity on
$\sigma$-bounded sets.  Let $A$ be a $\sigma$-bounded Borel set and
let $\epsilon > 0$ be given.  Apply Lemma
\ref{SigmaBoundedEquivalence}
to find disjoint bounded Borel sets $A_i$ such that $A = \cup_{i=1}^\infty
A_i$.  By the just proven outer regularity on bounded Borel sets we
may find open sets $U_i$ such that $\mu(U_i) \leq \mu(A_i) +
\epsilon/2^i$.  Then clearly $A \subset U$, $U$ is open and 
\begin{align*}
\mu(U) &\leq \sum_{i=1}^\infty \mu(U_i) \leq \epsilon + \sum_{i=1}^\infty
\mu(A_i)  = \epsilon + \mu(A)
\end{align*}
Again, as $\epsilon > 0$ is arbitrary we see that $\mu$ is outer
regular on $\sigma$-bounded Borel sets.

Now we assume that $\mu$ is outer regular on $\sigma$-bounded Borel
sets.  As before we start with the bounded case.  Let $A$ be a bounded
Borel set and suppose that $\epsilon > 0$ is given.  Let $L$ be a
compact set such that $A \subset L$.  Since $L \setminus A$ is also a
bounded Borel set, we may apply outer regularity to find an open set
$U$ such that $L \setminus A \subset U$ and 
\begin{align*}
\mu(U) - \epsilon < \mu(L \setminus A) \leq \mu(U)
\end{align*}
Define $K = L \setminus U = L \cap U^c$. As $K$ is a closed subset of the compact set $L$
it is compact.  Also
\begin{align*}
\mu(K) &= \mu(L) - \mu(L \cap U) \geq \mu(L) - \mu(U) = \mu(A) + \mu(L
\setminus A) - \mu(U) > \mu(A) - \epsilon
\end{align*}
As $\epsilon >0$ was arbitrary we see that $\mu$ is inner regular on
bounded Borel sets.  

Lastly we extend inner regularity to
$\sigma$-bounded Borel sets.  Let $A$ be $\sigma$-bounded Borel and
write $A = \cup_{i=1}^\infty A_i$ with the $A_i$ disjoint and each
$A_i$ bounded Borel (Lemma \ref{SigmaBoundedEquivalence}).  Let
$\epsilon > 0$ be given.  As each
$A_i$ is bounded and $\mu$ is finite on compact sets it follows that
$\mu(A_i) < \infty$ for all $i \in \naturals$.  Now by the just proven
inner regularity on bounded Borel sets we find $L_i \subset A_i$ with
$L_i$ compact and
\begin{align*}
\mu(A_i) - \epsilon/2^i < \mu(L_i) \leq \mu(A_i)
\end{align*}
The disjointness of the $A_i$ implies that the $L_i$ are disjoint as well.
Let $K_n = L_1 \cup \dotsb \cup L_n$ and note that
\begin{align*}
\mu(K_n) &= \sum_{i=1}^n \mu(L_i) > \sum_{i=1}^n \left( \mu(A_i) -
\epsilon/2^i \right ) > \sum_{i=1}^n \mu(A_i) - \epsilon
\end{align*}
Now take the limit as $n \to \infty$ to conclude that 
\begin{align*}
\sup \lbrace \mu(K) \mid K \subset A \text{ and $K$ is compact}
\rbrace &\geq \sup_n \mu(K_n) \geq \mu(A) - \epsilon
\end{align*}
and as $\epsilon > 0$ was arbitrary inner regularity of $\mu$ on
$\sigma$-bounded Borel sets is proven.
\end{proof}

The difficult part of the Riesz-Markov Theorem is the construction of
a Radon measure that corresponds to a positive functional.  The
tradition is to break that construction into two pieces: first the
construction of a set function on a smaller class of sets than the
full $\sigma$-algebra  and secondly the extension of that set function
to a full blown Radon measure. In many developments the set function
is defined on the compact subsets of the  locally compact
Hausdorff space $X$ and are called \emph{contents}.  Following
Arveson, we choose a set function is one
that is defined on just the open subsets of $X$.

The description of the desireable properties of the set function and
the process of extending the set function to a Radon measure is dealt with in the following Lemma.
\begin{lem}\label{ExtensionToRadonMeasure}Let $X$ be a locally compact Hausdorff space and let $m$ be
  a function from the open set of $X$ to $[0,\infty]$ satisfying:
\begin{itemize}
\item[(i)]$m(U) < \infty$ if $\overline{U}$ is compact
\item[(ii)]if $U \subset V$ then $m(U) \leq m(V)$
\item[(iii)]$m(\cup_{i=1}^\infty U_i) \leq \sum_{i=1}^\infty m(U_i)$
  for all open sets $U_1, U_2, \dotsc$.
\item[(iv)]if $U \cap V = \emptyset$ then $m(U \cup V) = m(U) + m(V)$
\item[(v)]$m(U) = \sup \lbrace m(V) \mid V \text{ is open, } \overline{V} \subset U \text{ and }
  \overline{V} \text{ is compact} \rbrace$
\end{itemize}
then there is a unique Radon measure $\mu$ such that $\mu(U) = m(U)$
for all open sets $U$.  Moreover every Radon measure satisfies
properties (i) through (v) when restricted to the open subsets of $X$.
\end{lem}
\begin{proof}
First we show that a Radon measure satisfies properties (i) through
(v) on the open sets of $X$.  In fact, properties (ii), (iii) and (iv)
follow for all measures and (i) follows from the fact that $\mu$ is
finite on compact subsets and monotonicity of measure.  Property (v)
requires a bit more justification.  If we let $U$ is an open set and
$\epsilon > 0$ is given then by inner regularity of $\mu$ we may find
a compact set $K$ such that $K \subset U$ and $\mu(U) \geq \mu(K) > \mu(U) -
\epsilon$.  By Lemma \ref{BoundedNeighborhoodsOfCompactSets} we may
find a relatively compact open set $V$ such that $K \subset V \subset
\overline{V} \subset U$.  Then by monotonicity we have $\mu(U) \geq \mu(V) > \mu(U) -
\epsilon$ and since $\epsilon$ was arbitrary (v) follows.

Next we prove uniqueness of the extension of $m$ to a Radon measure
$\mu$.  Since a Radon measure is inner regular on all Borel sets Lemma
\ref{InnerOuterRegularityEquivalence} implies that any extension $\mu$
is outer regular on all $\sigma$-bounded Borel sets.  Since the values
of $\mu$ are determined on all open sets this implies that the values
of $\mu$ are determined on all $\sigma$-bounded Borel sets; in
particular the values of $\mu$ are determined on all compact
sets. Clearly a Radon measure is determined uniquely by its values on
compact sets.

Now we turn to proving existence of the extension $\mu$.  The proof
goes in a few steps.  First we define an outer measure from $m$ and
observe that Borel sets are measurable with respect to it; though the
Caratheordory restriction of the outer
measure is outer regular it is not necessarily inner regular.  The
second step is to 
modify the Caratheodory restriction to make it inner regular.  

We begin by defining the outer measure in a standard way.  Let $A$ be
an arbitrary subset of $X$ and define
\begin{align*}
\mu^*(A) &= \inf \lbrace m(U) \mid A \subset U \text{ and $U$
  is open} \rbrace
\end{align*}
Note that $\mu^*(U) = m(U)$ for all open sets.

\begin{clm}$\mu^*$ is an outer measure
\end{clm}

Note that because the emptyset is relatively compact we know from (i)
that $m(\emptyset) < \infty$ and thus from (iv) we see that
$m(\emptyset) = 2m(\emptyset)$.  Thus $m(\emptyset) = 0$ and it
follows that $\mu^*(\emptyset) = 0$.  If $A \subset B$ then it is
trival that 
\begin{align*}
\lbrace m(U) \mid A \subset U \text{ and $U$
  is open } \rbrace &\subset \lbrace m(U) \mid B \subset U \text{ and $U$
  is open } \rbrace
\end{align*}
which implies $\mu^*(A) \leq \mu^*(B)$.  If we let $A_1, A_2, \dotsc$
be given and define $A = \cup_{i=1}^\infty A_i$.  If any
$\mu^*(A_i) = \infty$ it follows that $\mu(A) \leq \sum_{i=1}^\infty
\mu^*(A_i) = \infty$.  If on the other hand every $\mu^*(A_i) <
\infty$ then let $\epsilon > 0$ be given and find an open set $U_i
\subset A_i$ such that $m(U_i) \leq \mu^*(A_i) + \epsilon / 2^i$.
Clearly $\cup_{i=1}^\infty U_i$ is an open subset of $A$ and it
follows from (iii) and the definition of $\mu^*$ that
\begin{align*}
\mu^*(A) &\leq m(\cup_{i=1}^\infty U_i) \leq \sum_{i=1}^\infty m(U_i)
\leq \sum_{i=1}^\infty m(A_i) + \epsilon
\end{align*}
Since $\epsilon > 0$ is arbitrary we see that $\mu^*$ is countably
subadditive and is therefore proven to be an outer measure.

\begin{clm}Borel sets are $\mu^*$-measurable.
\end{clm}

The $\mu^*$-measurable sets form a $\sigma$-algebra by Lemma
\ref{CaratheodoryRestriction} and therefore it suffices to show that
open sets are $\mu^*$-measurable.  Let $U$ be open subset and $A$ be an
arbitrary subset of $X$, by subadditivity of $\mu^*$ we only have to
show the inequality 
\begin{align*}
\mu^*(A) &\geq \mu^*(A \cap U) + \mu^*(A \cap U^c)
\end{align*}
Obviously we may assume that $\mu^*(A) < \infty$ since otherwise the
inequality is trivially satisfied.  
We first assume that $A$ is an open set.  Since $\mu^*$ and $m$ agree
on open sets we have to show
\begin{align*}
m(A) &\geq m(A \cap U) + \mu^*(A \cap U^c)
\end{align*}
Let $\epsilon > 0$ be given and use property (v) so we can find an relatively compact open set $V$ such
that $\overline{V} \subset A \cap U$ and $m(V) \geq m(A \cap U) -
\epsilon$.  Then $A \cap \overline{V}^c$ is an open set containing $A
\cap U^c$ disjoint from $V$ and it follows from (ii), (iv)  and the
definition of $\mu^*$ that
\begin{align*}
m(A) &\geq m(V \cup A \cap \overline{V}^c) = m(V) + m(A \cap
\overline{V}^c) \geq m(A \cap U) - \epsilon + \mu^*(A \cap U^c) 
\end{align*}
As $\epsilon > 0$ was arbitrary we are done with the case of open
sets $A$.  Now suppose that $A$ is an arbitrary set with $\mu^*(A) <
\infty$ and let $\epsilon
> 0$ be given.  We find an open set $V$ such that $A \subset V$ and
$m(V) \leq \mu^*(A) + \epsilon$.  From what we have just proven of
open sets and the monotonicity of $\mu^*$
\begin{align*}
\mu^*(A) + \epsilon &\geq \mu^*(V) \geq \mu^*(V \cap U) + \mu^*(V \cap
U^c) \geq \mu^*(A \cap U) + \mu^*(A \cap U^c) 
\end{align*}
The claim follows by observing that $\epsilon >0$ was arbitrary.

Now by Caratheodory Restriction (Lemma \ref{CaratheodoryRestriction}) we may restrict $\mu^*$ to a Borel measure
$\overline{\mu}$ that is outer regular by definition and that
satisfies $\overline{\mu}(U) = m(U)$ for all open sets $U$.  Moreover
$\overline{\mu}(K) < \infty$ for all compact sets since by Lemma
\ref{BoundedNeighborhoodsOfCompactSets} we may find a relatively
compact open neighborhood $U$ such that $K \subset U$; monotonicity
and (i) tell us that 
\begin{align*}
\overline{\mu}(K) \leq \overline{\mu}(U) = m(U) <
\infty
\end{align*}  
Since $\overline{\mu}$ is outer regular on all Borel sets \emph{a fortiori}
it is outer regular on all $\sigma$-bounded Borel sets.  By  Lemma
\ref{InnerOuterRegularityEquivalence} it follows that
$\overline{\mu}$ is inner regular on all $\sigma$-bounded Borel sets.
Note that if we assume that $X$ is $\sigma$-compact (i.e. all Borel
sets are $\sigma$-bounded) then we already know that $\overline{\mu}$
is a Radon measure.  In the general case it is not necessarily true
and we must make a further modification to $\overline{\mu}$ to make it
inner regular.

For an arbitrary Borel set $A$ we define
\begin{align*}
\mu(A) &= \sup \lbrace \overline{\mu}(B) \mid B \subset A \text{ and
  $B$ is a $\sigma$-bounded Borel set } \rbrace
\end{align*}
Clearly, $\mu(\emptyset) = \overline{\mu}(\emptyset) = 0$.
It is also immediate from the definition that $\mu(A) = \overline{\mu}(A)$
for all $\sigma$-bounded Borel sets $A$ and therefore that $\mu(U) =
m(U)$ for all $\sigma$-bounded open sets $U$.  In fact more is true.

\begin{clm}$\mu(U) = m(U)$ for all open sets $U$.
\end{clm}

Let $V$ be a relatively compact open set with $\overline{V} \subset U$.  We have
\begin{align*}
m(U) &= \overline{\mu}(U) \geq \mu(U) \geq \mu(V) = m(V)
\end{align*}
Now we take the supremum over all such $V$ and by property (v) 
\begin{align*}
m(U) &\geq \mu(U) \sup \lbrace m(V) \mid V \text{ is relatively
  compact and $\overline{V} \subset U$} \rbrace = m(U)
\end{align*}
and therefore $\mu(U) = m(U)$.

\begin{clm}$\mu$ is a measure.
\end{clm}

To see that $\mu$ is a measure it remains to show countable
additivity.  Let $A_1, A_2, \dotsc$ be disjoint Borel sets.  First we
show countable subadditivity.  Let $B$ be a $\sigma$-bounded Borel
subset of $\cup_{i=1}^\infty A_i$ and define $B_i = B \cap A_i$.
Clearly the $B_i$ are disjoint $\sigma$-bounded Borel measures, thus
using the countable additivity of $\overline{\mu}$ we get
\begin{align*}
\overline{\mu{B}} &= \sum_{i=1}^\infty \overline{\mu}(B_i) \leq \sum_{i=1}^\infty \mu(A_i)
\end{align*}
Taking the supremum over all such $B$ subadditivity follows.

We need to show the opposite inequality.  Suppose that some $\mu(A_j) = \infty$ for some $j$.  Then we may find
a sequence of $\sigma$-bounded Borel sets $B_n$ such that
$\overline{\mu}(B_n) \geq n$.  Since $B_n \subset \cup_{i=1}^\infty
A_i$ we also see that $\mu(\cup_{i=1}^\infty A_i) = \infty$.  Thus we
may now assume that $\mu(A_i) < \infty$ for all $i$.  Let $\epsilon >
0$ be given and for each $i$ find a $\sigma$-bounded Borel set $B_i$
such that $B_i \subset A_i$ and $\overline{\mu}(B_i) \geq \mu(A_i) -
\epsilon/2^i$.  For each $n$ define $C_n = \cup_{j=1}^n B_j$ and note
that $C_n$ is a $\sigma$-bounded Borel set such that $C_n \subset
\cup_{i=1}^\infty A_i$.  Also, for every $n$, 
\begin{align*}
\mu(\cup_{i=1}^\infty A_i) \geq \mu(C_n) = \overline{\mu}(C_n) =
\sum_{j=1}^n \overline{\mu}(B_j) \geq \sum_{j=1}^n \mu(A_j)  -
\epsilon/2^j \geq \sum_{j=1}^n \mu(A_j)  -\epsilon
\end{align*}
Now take the limit as $n \to \infty$ and using the fact that $\epsilon
> 0$ was arbitrary, we get $\sum_{j=1}^\infty \mu(A_j) \leq \mu(\cup_{j=1}^\infty A_j)$.

\begin{clm}$\mu$ is a Radon measure.
\end{clm}

The fact that $\mu(K) < \infty$ for all compact sets follows from the
fact that $\mu$ and $\overline{\mu}$ agree on $\sigma$-bounded sets
and the fact that $\overline{\mu}(K) < \infty$.  To see inner
regularity, let $A$ be a Borel set and let $\epsilon > 0$ be given.
By the definition of $\mu$ we find a $\sigma$-bounded Borel set $B
\subset A$ such that $\overline{\mu}(B) \geq \mu(A) - \epsilon/2$.  Then by
the fact that $\overline{\mu}$ is inner regular on $\sigma$-bounded
sets we find a compact set $K$ such that $\overline{\mu}(K) \geq
\overline{\mu}(B) - \epsilon/2$.  Combining the two inequalities and
using the fact that $\mu$ and $\overline{\mu}$ agree on compact sets
we get $\mu(K) \geq \mu(A) - \epsilon$.  Since $\epsilon > 0$ was
arbitrary we are done.
\end{proof}

Given a Radon measure on a locally compact Hausdorff space, all
compactly supported continuous functions are integrable: $\int \abs{f}
\, d\mu \leq \norm{f}_\infty \mu(supp(f)) < \infty$.  Thus such a
measure yields a linear functional on $C_c(X)$.  Such functionals
possess another simple property in addition to linearity.
\begin{defn}A linear functional $\Lambda$ on $C_c(X)$ is said to be
  \emph{positive} if $f \geq 0$ implies $\Lambda(f) \geq 0$.
\end{defn}

The reader should be careful that we have not been using any type of
topology on $C_c(X)$ at this point and in particular we are not claiming
that the function defined by a Radon measure is continuous with respect to 
any underlying topology.  In fact as we will see later, $C_c(X)$ is a normed vector space 
(not complete) under the sup norm but it is only for finite Radon measures that the
corresponding functional is continuous.

It is clear that the linear functional defined by integration with
respect to a Radon measure is positive.  The Riesz-Markov Theorem
tells us that the positive linear functionals are precisely those
generated by integration with respect to a Radon measure.  To prove
the result we will need to figure out how to define a measure from a
positive linear functional.  As a warm up let's first answer that
question in the case of integration with respect to a Radon measure.

\begin{lem}\label{RieszMarkovUniquenessOnOpen}Let $X$ be a locally compact Hausdorff space and let $\mu$
  be a Radon measure on $X$, then for every open set $U$ we have
\begin{align*}
\mu(U) &= \sup \lbrace \int f \, d\mu \mid 0 \leq f \leq 1, f \in
C_c(X), supp(f) \subset U \rbrace
\end{align*}
\end{lem}
\begin{proof}
For the inequality $\geq$, suppose that $f \in C_c(X)$ satisfies $0
\leq f \leq 1$ and $supp(f) \subset U$, then observe the hypotheses
imply that $f \leq \characteristic{U}$ so that
\begin{align*}
\int f(x) \, d\mu(x) &\leq \int \characteristic{U}(x) \, d\mu(x) = \mu(U)
\end{align*}
and the inequality follows by taking the supremum over all such $f$.

For the inequality $\leq$ we leverage the inner regularity of $\mu$.
Let $K \subset U$ be a compact set.  By Lemma
\ref{BoundedNeighborhoodsOfCompactSets} we find an relatively compact
open set $V$ with $K \subset V \subset \overline{V} \subset U$.  Since
$\overline{V}$ is a compact Hausdorff space, it is normal and we may
apply the Tietze Extension Theorem \ref{TietzeExtensionTheorem} to
find a continuous function $g : \overline{V} \to [0,1]$ such that $g
\equiv 1$ on $K$.  Applying Urysohn's Lemma \ref{UrysohnsLemma} we
construct a continuous function $h : X \to [0,1]$ such that $h = 1$ on
$K$ and $h=0$ on $V^c$.  We define
\begin{align*}
f(x) &= \begin{cases}
h(x) g(x) & \text{if $x \in \overline{V}$} \\
0 & \text{if $x \notin \overline{V}$}
\end{cases}
\end{align*}
By the corresponding properties of $g$ and $h$, it is clear that $0 \leq f \leq 1$ and that $f=1$ on $K$.  We claim that $f$ is continuous on all of $X$.  Since $h$ restricts to a continuous function
on $\overline{V}$ it is clear that the restriction of $f$ to
$\overline{V}$ is continuous.  Let $O \subset \reals$ be an open
set.  If $0 \notin O$ then it follows that $f^{-1}(O) \subset V$ and
is therefore open by the continuity of $f$ restricted to $V$.  If on
the other hand, $0 \in O$ then $f^{-1}(O) \cap \overline{V}$ is open
in $\overline{V}$ hence is of the form $Z \cap \overline{V}$ for some
open subset $Z \subset X$.  Because $0 \in O$ if follows that $Z
\subset f^{-1}(O)$ and therefore by the definition of $f$ we we may
write $f^{-1}(O) = Z \cup \overline{V}^c$ which is an open set.

TODO: This is a locally compact Hausdorff version of Tietze, factor it
out into a separate result.

With the extension $f$ in hand we see that 
\begin{align*}
\mu(K) &\leq \int f(x) \, d\mu(x) \leq \sup \lbrace \int f \, d\mu \mid 0 \leq f \leq 1, f \in
C_c(X), supp(f) \subset U \rbrace
\end{align*}
Now taking the supremum over all compact subsets $K \subset U$ and
using the inner regularity of $\mu$ the result follows.
\end{proof}


Before we state an prove the Riesz-Markov theorem we need the
existence of finite partitions of unity on compact sets in an LCH: a standard
bit of general topology.

\begin{lem}\label{PartitionOfUnity}Let $X$ be a locally compact Hausdorff space, $K$ be a
  compact subset of $X$ and $\lbrace U_\alpha \rbrace$ an open
  covering of $K$.  There exists a finite subset $\alpha_1, \dotsc,
  \alpha_n$ and continuous functions with compact support $f_{\alpha_1}, \dotsc,
  f_{\alpha_n}$ such that $supp(f_{\alpha_j}) \subset U_{\alpha_j}$
  and $f_{\alpha_1} + \dotsb + f_{\alpha_n} = 1$ on $K$.
\end{lem}
\begin{proof}
Pick an $x \in K$, pick an $U_{\alpha_x}$ such that $x \in U_{\alpha_x}$ and
using complete regularity of $X$, construct a continuous function
$g_x$ from $X$ to $[0,1]$ such that $g_x(x) = 1$ and
$g_x \equiv 0$ on $U_{\alpha_x}^c$.  Thus $g_x^{-1}(0,1] \subset
U_{\alpha_x}$ and the $g_x^{-1}(0,1]$ form an open cover of $K$.  By
compactness of $K$ we extract a finite subcover $g_{x_1}^{-1}(0,1],
\dotsc, g_{x_n}^{-1}(0,1]$.  If we clean up notation by denoting $U_{\alpha_{x_j}} =
U_{\alpha_j}$ and $g_{\alpha_j} = g_{\alpha_{x_j}}$, it follows that
$U_{\alpha_1}, \dotsc, U_{\alpha_n}$ is an open cover of $K$ and $g =
\sum_{j=1}^n g_{\alpha_j}$ is strictly positive on $K$.  Moreover by
compactness of $K$ we know that $g$ has a minimum value $C > 0$ on
$K$.  Define $h = g \vee C$ so that $h$ is continuous, $h = g$ on $K$ and $h
\geq C > 0$ everywhere on $X$.  By continuity and strict positivity of
$h$ we can define $f_{\alpha_j} = g_{\alpha_j}/h$ so that
$f_{\alpha_j}$ is continuous and moreover $f_{\alpha_1} + \dotsb +
f_{\alpha_n} = g/h = 1$ on $K$.
\end{proof}

\begin{thm}[Riesz-Markov Theorem]\label{RieszMarkov}Let $X$ be a
  locally compact Hausdorff space and let $\Lambda : C_c(X) \to
  \reals$ be a positive linear functional then there exists a unique
  Radon measure $\mu$ such that $\Lambda(f) = \int f \, d\mu$ for all
  $f \in C_c(X)$.
\end{thm}
\begin{proof}
The uniqueness part of the result is straightforward.  By Lemma
\ref{RieszMarkovUniquenessOnOpen} we know that the values of $\mu$ on open sets
are determined by $\Lambda$.  By Lemma
\ref{InnerOuterRegularityEquivalence} we conclude that the values of
$\mu$ on $\sigma$-bounded Borel sets are determined by $\Lambda$, in
particular the values on compact sets are determined by $\Lambda$.
The inner regularity of $\mu$ implies that the values on all Borel
sets are determined by $\Lambda$.

For existence we follow the lead of Lemma
\ref{RieszMarkovUniquenessOnOpen} and define the set function on the
open sets of $X$
\begin{align*}
m(U) &=  \sup \lbrace \Lambda( f ) \mid 0 \leq f \leq 1, f \in C_c(X), supp(f) \subset U \rbrace
\end{align*}
We proceed by showing that $m(U)$ satisfies properties (i) through (v)
from Lemma \ref{ExtensionToRadonMeasure} and that if $\mu$ is the
Radon measure constructed by that result that we indeed have
$\Lambda(f) = \int f \, d\mu$.

\begin{clm}$m$ satisfies (i)
\end{clm}

Let $U$ be a relatively compact open set.  By Lemma
\ref{BoundedNeighborhoodsOfCompactSets} we can find another relatively
compact open set $V$ such that $\overline{U} \subset V$.  By the
Tietze Extension Theorem argument of Lemma \ref{ExtensionToRadonMeasure} we can find a continuous function $g: X \to
[0,1]$ such that $g = 1$ on $\overline{U}$ and $g = 0$ on $V^c$.
Since $g \in C_c(X)$ we have $\Lambda(g) < \infty$.  Now suppose that
$f \in C_c(X)$ satisfies $0 \leq f \leq 1$ and  $f \in C_c(X), supp(f)
\subset U$.  It follows that $0 \leq f \leq g$ and linearity and
positivity of $\Lambda$ we know that $\Lambda(f) \leq \Lambda(g)$.
Taking the supremum over all such $f$ we get
\begin{align*}
m(U) &= \sup \lbrace \Lambda( f ) \mid 0 \leq f \leq 1, f \in C_c(X),
supp(f) \subset U \rbrace \leq \Lambda(g) < \infty
\end{align*}

\begin{clm}$m$ satsifies (ii)
\end{clm}

This is immediate since $U \subset V$ implies 
\begin{align*}
\lbrace \Lambda( f ) \mid 0 \leq f \leq 1, f \in C_c(X),
supp(f) \subset U \rbrace \subset \lbrace \Lambda( f ) \mid 0 \leq f \leq 1, f \in C_c(X),
supp(f) \subset V \rbrace
\end{align*}

\begin{clm}$m$ satisfies (iii)
\end{clm}

Let $U_1, U_2, \dotsc$ be open sets and let $f \in C_c(X)$ satisfy $0
\leq f \leq 1$ and $supp(f) \subset \cup_{n=1}^\infty U_n$.  By
compactness of $supp(f)$ and Lemma
\ref{PartitionOfUnity} we may find an $N$ and continuous functions
$g_i$ for $i=1, \dotsc, N$ such that $0 \leq g_i \leq 1$, $supp(g_i) \subset U_i$ and
$\sum_{i=1}^N g_i = 1$ on $supp(f)$ and therefore $f = f \cdot
\sum_{i=1}^N g_i$.  It also follows that $0 \leq f g_i
\leq 1$ and $supp(f g_i) \subset U_i$ for $i=1, \dotsc, N$ and thus
\begin{align*}
\Lambda(f) &= \sum_{i=1}^N \Lambda( f g_i) \leq \sum_{i=1}^N m(U_i)
\leq \sum_{i=1}^\infty m(U_i)
\end{align*}
Now we take the supremum over all such $f$ to conclude that
$m(\cup_{i=1}^\infty U_i) \leq \sum_{i=1}^\infty m(U_i)$.

\begin{clm}$m$ satisfies (iv)
\end{clm}

Suppose $U$ and $V$ are disjoint open sets.  We only need to show that
$m(U \cup V) \geq m(U) + m(V)$ since the opposite inequality follows
from (iii).  Let $f, g \in C_c(X)$ such that $0 \leq f,g \leq 1$,
$supp(f) \subset U$ and $supp(g) \subset V$.  By disjointness of $U$
and $V$ it follows that $f+g \in C_c(X)$, $0 \leq f+g \leq 1$ and
$supp(f+g) \subset supp(f) \cup supp(g) \subset U \cup V$.  Therefore
\begin{align*}
\Lambda(f) + \Lambda(g) &= \Lambda(f+g) \leq m(U \cup V)
\end{align*}
Now take the supremum over all $f$ and $g$ to get the result.

\begin{clm}$m$ satisfies (v)
\end{clm}

Let $U$ be an open set and let $f \in C_c(X)$ such that $0 \leq f \leq
1$ and $supp(f) \subset U$.  By compactness of $supp(f)$ and Lemma
\ref{BoundedNeighborhoodsOfCompactSets} we may find a relatively
compact open set $V$ such that 
$supp(f) \subset V \subset \overline{V} \subset U$.  
It follows that 
\begin{align*}
\Lambda(f) &\leq m(V) \leq \sup \lbrace m(V) \mid V \text{ is open, } \overline{V} \subset U \text{ and }
  \overline{V} \text{ is compact} \rbrace
\end{align*}
Now take the supremum over all such $f$.

We may now apply Lemma \ref{ExtensionToRadonMeasure} to construct a
Radon measure $\mu$ such that $\mu(U) = m(U)$.  We need to show that
for every $f \in C_c(X)$ we have $\Lambda(f) = \int f \, d\mu$. By
linearity we know that $\Lambda(0) = \int 0 \, d\mu = 0$ so we may
assume that $f \neq 0$.  We may write $f = f_+
- f_-$ with $f_+ = f \vee 0 \in C_c(X)$ and $f_- = (-f) \vee 0 \in
C_c(X)$.  Since both $\Lambda$ and the integral are linear it suffices
to show the result for $f \geq 0$.  Since $f$ is continuous with
compact support, it follows that $f$ is bounded and since $f \neq 0$
we have $0 < \norm{f}_\infty < \infty$.  Again, by linearity of
$\Lambda$ and integration it suffices to prove the result of
$f/\norm{f}_\infty$ and thus we may assume that $0 \leq f \leq 1$.

We proceed by constructing a generalized upper and lower sum
approximation to the integral of $f$.  Once again apply Lemma
\ref{BoundedNeighborhoodsOfCompactSets} to find a relatively compact
open neighborhood $U_0$ with $supp(f) \subset U_0$.  Let $\epsilon >
0$ be given and choose $n \in \naturals$ such that $\mu(U_0) <
\epsilon n$.  For $j=1, \dotsc, n$ define $U_j = f^{-1} (j/n,
\infty)$.  Because $f$ is continuous and of compact support, each
$U_j$ is a relatively compact open set and it is trivial from the
definitions that we have $\emptyset = U_n \subset U_{n-1} \subset \dotsb \subset
U_0$.  In fact by the continuity of $f$ it is also true that $\overline{U}_j \subset U_{j+1}$.
Define the lower and upper approximations to $f$
\begin{align*}
u(x) &= \begin{cases}
\frac{j}{n} & \text{if $x \in U_j \setminus U_{j+1}$ for some $j=1, \dotsc,  n-1$} \\
0 & \text{if $x \notin U_1$}
\end{cases}
\end{align*}
 and similarly 
\begin{align*}
v(x) &= \begin{cases}
\frac{j}{n} & \text{if $x \in U_{j-1} \setminus U_{j}$ for some
  $j=1,  \dotsc,  n$} \\
0 & \text{if $x \notin U_0$}
\end{cases}
\end{align*}
Note that we have the property that $u \leq f \leq v$ and moreover we
have the useful alternative defintion of $u$ and $v$
\begin{align*}
u(x) &= \frac{1}{n} \sum_{j=1}^n \characteristic{U_j}(x) \\
v(x) &= \frac{1}{n} \sum_{j=1}^{n} \characteristic{U_{j-1}}(x) \\
\end{align*}
which shows that $u,v \in C_c(X)$.

\begin{clm}$\int (v -u) \, d\mu < \epsilon$
\end{clm}

This follows by observing that $v - u = \frac{1}{n}
(\characteristic{U_0} - \characteristic{U_n}) = \frac{1}{n}
\characteristic{U_0}$ since $U_n = \emptyset$.

\begin{clm}$\int u \, d\mu \leq \Lambda(f) \leq \int v \, d\mu + \epsilon$
\end{clm}

To see this claim we decompose $f$ into a representation that is
adapted to the nested sequence $U_n \subset \dotsb \subset U_0$.  For
$j=1, \dotsc, n$ define 
\begin{align*}
\phi_j(x) &= \begin{cases}
1/n & \text{if $x \in U_j$} \\
f(x) - \frac{j-1}{n} & \text{if $x \in U_{j-1} \setminus U_{j}$} \\
0 & \text{if $x \notin U_{j-1}$}
\end{cases} \\
&=[(f(x) - \frac{j-1}{n}) \vee 0] \wedge \frac{1}{n}\\
\end{align*}
where the second representation shows that $\phi_j \in C_c(X)$ and $0 \leq \phi_j \leq \frac{1}{n}$.
Note also
that if we are given $x \in U_{j-1} \setminus U_j$ for some $j=1,
\dotsc, n$ then $\phi_i(x) = 0$ for $j < i \leq n$ and $\phi_i(x) =
\frac{1}{n}$ for $1 \leq i < j$.  Therefore we have 
\begin{align*}
\phi_1(x) + \dotsb + \phi_n(x) &= \phi_1(x) + \dotsb + \phi_j(x) \\
&= \frac{j-1}{n} + f(x) - \frac{j-1}{n} = f(x)
\end{align*}
It is clear that for $x \notin U_0$ we have $f(x) = 0$ and
$\phi_j(x) = 0$ for all $j=1, \dotsc, n$ so we have
 $f = \phi_1 + \dotsb + \phi_n$ on all of $X$.

We now need to bound $\Lambda(\phi_j)$ in terms of $\mu(U_k) = m(U_k)$ for
suitable $k=1, \dotsc, n$.  First we get a lower bound on
$\Lambda(\phi_j)$.  Let $1 \leq j \leq n$ be given.  Suppose that we have a $g \in C_c(X)$ with
$0 \leq g \leq 1$ and $supp(g) \subset U_j$.  Then by positivity of
$\phi_j$ and the fact that $\phi_j(x) = \frac{1}{n}$ on $U_j$ we see
that $g \leq \characteristic{U_j} \leq n \phi_j$ and therefore
$\Lambda(g) \leq n \Lambda(\phi_j)$.    Taking the supremum over all
such $g$ we see that $\frac{1}{n} \mu(U_j) \leq \Lambda(\phi_j)$.  Taking the
sum over all $j=1, \dotsc, n$ and using linearity of $\Lambda$ we get
\begin{align*}
\int u \, d\mu &= \frac{1}{n} \sum_{j=1}^n \mu(U_j) \leq \sum_{j=1}^n
\Lambda(\phi_j) = \Lambda(f)
\end{align*}

Now we get an upper bound on $\Lambda(\phi_j)$.   For $j=2, \dotsc, n$ we that
$n phi_j \in C_c(X)$, $0 \leq n \phi_j \leq 1$ and $supp(n \phi_j)
\subset \overline{U_{j-1}} \subset U_{j-2}$.  From the definition of
$\mu(U_{j-2}) =m(U_{j-2})$ it follows that $\Lambda(\phi_j) \leq
\frac{1}{n} \mu(U_{j-2})$.  As for $\phi_1$, we have
$n \phi_1 \in C_c(X)$ and $0 \leq n\phi_1 \leq 1$ by exactly the same
argument as for $j \geq 2$.  We also have $supp(n \phi_1) \subset
supp(f) \subset U_0$ so that $\Lambda(\phi_1) \leq \frac{1}{n}
\mu(U_0)$.  If we define $U_{-1} = U_0$ the we get
have $\Lambda(\phi_j) \leq \frac{1}{n} \Lambda(U_{j-2})$ for $j=1,
\dotsc, n$.  Again we sum and use linearity of $\Lambda$,
\begin{align*}
\Lambda(f) &= \sum_{j=1}^n \Lambda(\phi_j) \leq \frac{1}{n}
\sum_{j=1}^n \mu(U_{j-2}) = \frac{1}{n} \mu(U_{-1}) +  \frac{1}{n}
\sum_{j=1}^{n-1} \mu(U_{j-1}) \\
&\leq \frac{1}{n} \mu(U_{0}) +  \frac{1}{n}
\sum_{j=1}^{n} \mu(U_{j-1}) \leq \epsilon + \int v \, d\mu
\end{align*}

It remains to stitch together the previous claims to
show that $\Lambda(f) = \int f \, d\mu$.  Integrating the inequality
$u \leq f \leq v$ we get $\int u \, d\mu \leq f \leq \int v \, d\mu$.
Now using this fact and previous two claims we get
\begin{align*}
\Lambda(f) - \int f \, d\mu &\leq \Lambda(f) - \int v \, d\mu \leq
\int u \, d\mu  - \int u \, d\mu + \epsilon \leq 2 \epsilon
\end{align*}
and
\begin{align*}
\Lambda(f) - \int f \, d\mu &\geq \int u \, d\mu - \int f \, d\mu \geq
\int u \, d\mu - \int v \, d\mu \geq -\epsilon
\end{align*}
from which we conclude that $\abs{\Lambda(f) - \int f \, d\mu} \leq
2\epsilon$.  Since $\epsilon > 0$ was arbitrary we are done.
\end{proof}

Given a finite signed measure $\mu$ and any $\abs{\mu}$-integrable $f$ we define
\begin{align*}
\int f \, d \mu &= \int f \, d \mu_+ - \int f \, d\mu_-
\end{align*}
Note that this is well defined since $\mu_\pm$ are uniquely determined by $\mu$.  

\begin{thm}\label{BanachSpaceBoundedSignedMeasures}Let $(\Omega, \mathcal{A})$ be a measurable space space then the space
  of bounded signed measures is a Banach space with norm given by
\begin{align*}
\tvnorm{\mu} &= \frac{1}{2} \abs{\mu}(\Omega) = \frac{1}{2} (\mu_+(\Omega) + \mu_-(\Omega))
\end{align*}  
Moreover $\tvnorm{\mu} = \frac{1}{2} \sup_{\abs{f} \leq 1} \int f (x) \, \mu(dx)$.
\end{thm}
\begin{proof}
First we show that $\mu_+(\Omega) + \mu_-(\Omega) = \sup_{\abs{f} \leq 1} \int f (x) \, \mu(dx)$.  Taking the Hahn Decomposition $\Omega = A_+ \cup A_-$ so that
$\mu_\pm(A) = \pm \mu(A_\pm \cap A)$, we see that 
\begin{align*}
\mu_+(\Omega) + \mu_-(\Omega) &= \mu_+(A_+) + \mu_-(A_-) = \mu_+(A_+) - \mu_+(A_-) - \mu_-(A_+) + \mu_-(A_-) \\
&= \int (\characteristic{A_+} - \characteristic{A_-}) (x) \, \mu_+(dx) - \int (\characteristic{A_+} - \characteristic{A_-} )(x) \, \mu_-(dx) \\
&= \int (\characteristic{A_+} - \characteristic{A_-}) (x) \, \mu(dx) \leq \sup_{\abs{f} \leq 1} \int f (x) \, \mu(dx)
\end{align*}
Given an arbitrary measurable $f$ with $\norm{f} \leq 1$ we let $f_\pm = \pm f \characteristic{A_\pm}$ then $f \leq 1$ implies $f_+ \leq \characteristic{A_+}$ and
$-1 \leq f$ implies $-f_- \leq \characteristic{A_-}$.  Furthermore $f_\pm=0$ is $\mu_\mp$-almost everywhere therefore $\int f_\pm(x) \, \mu_\mp(x) = 0$ and we
get
\begin{align*}
\int f(x) \, \mu(dx) &= \int (f_+(x) - f_-(x)) \, \mu(dx) = \int f_+(x) \, \mu_+(dx) + \int f_-(x) \, \mu_-(dx) \\
&\leq \mu_+(A_+) + \mu_-(A_-) = 2 \tvnorm{\mu}
\end{align*}

Now we show that $\tvnorm{\mu}$ is in fact a norm.  If $\norm{\mu} = 0$ then we have $\mu_+ = \mu_- = 0$ and therefore
$\mu = \mu_+ - \mu_- = 0$.  From the proof of
Theorem \ref{HahnDecomposition} we see that $\mu_+(\Omega) = \sup_{A \in
  \mathcal{A}} \mu(A)$ and $\mu_-(\Omega) = -\inf_{A \in
  \mathcal{A}} \mu(A)$.  If $a > 0$ it follows that
$(a \mu)_\pm(\Omega) = a \mu_{\pm}(\Omega)$ and that if $a < 0$ it follows that $(a
\mu)_\pm(\Omega) = -a \mu_{\mp}(\Omega)$; therefore we have $\tvnorm{a\mu} =
\abs{a} \tvnorm{\mu}$.
Moreover given two finite signed measures $\nu$ and $\mu$ we have
\begin{align*}
2 \tvnorm{\mu + \nu} &= \sup_{A \in \mathcal{A}} (\mu(A) + \nu(A)) -
                   -\inf_{A \in \mathcal{A}} (\mu(A) + \nu(A)) \\
&\leq \sup_{A \in \mathcal{A}}\mu(A) + \sup_{A \in \mathcal{A}} \nu(A)) -\inf_{A \in \mathcal{A}} \mu(A) -\inf_{A \in \mathcal{A}} \nu(A) \\
&= 2 \tvnorm{\mu} + 2 \tvnorm{\nu}
\end{align*}

Now we show completeness.  Suppose that we have a Cauchy sequence $\mu_n$.  There exists an $n \in \naturals$ such that $\tvnorm{\mu_{n+m} - \mu_n} < 1$ for all $m \in \naturals$ and from this it follows that $\sup_j \tvnorm{\mu_j} \leq \tvnorm{\mu_1} \vee \dotsb \vee \tvnorm{\mu_{n-1}} \vee \tvnorm{\mu_n}+1  <\infty$.  Thus we may define
\begin{align*}
\nu &= \sum_{j=1}^\infty 2^{-j} \abs{\mu_j}
\end{align*}
to get a bounded signed measure on $\Omega$.  From $\mu_{n, \pm}(A) \leq \abs{\mu_n}(A) \leq 2^n \nu(A)$ it follows that $\mu_{n, \pm} \ll \nu$ and therefore we may apply the Radon-Nikodym Theorem \ref{RadonNikodym} to construct
a positive $\nu$-integrable $g_{n,\pm}$ such that $\mu_{n, \pm}(A) = \int_A g_{n,\pm}(x) \, \nu(dx)$.  If we define $g_n = g_{n,+} - g_{n,-}$ then $g_n$ is integrable and $\mu_{n}(A) = \int_A g_{n}(x) \, \nu(dx)$.   Observe that 
\begin{align*}
&\int \abs{g_n(x) - g_m(x)} \, \nu(dx) \\
&= \int (g_n(x) - g_m(x)) \characteristic{g_n\geq g_m}(x) \, \nu(dx) + \int (g_m(x) - g_n(x)) \characteristic{g_m > g_n}(x) \, \nu(dx) \\
&= \int \characteristic{g_n\geq g_m}(x) \, (\mu_n - \mu_m) (dx) + \int \characteristic{g_m > g_n}(x) \, (\mu_m - \mu_n) (dx) \\
&= \int (\characteristic{g_n\geq g_m} - \characteristic{g_m > g_n}) (x) \, (\mu_n - \mu_m) (dx) \\
&\leq \sup_{f \leq 1} \int f(x) \, (\mu_n-\mu_m)(dx) = 2 \tvnorm{\mu_n - \mu_m} \\
\end{align*}
which shows $g_n$ is Cauchy in $L^1(\Omega, \mathcal{A}, \nu)$.  By completeness of $L^1(\Omega, \mathcal{A}, \nu)$ we know that there is a $\nu$-integrable $g$ such that $g_n \tolp{1} g$.  Define 
$\mu = (g \cdot \nu)$.  Then $\abs{\mu}(\Omega) = \int \abs{g}(x) \, \nu(dx) < \infty$ which implies that $\mu$ is a bounded signed measure.
\begin{align*}
\tvnorm{\mu_n - \mu} &= \frac{1}{2} \sup_{\abs{f} \leq 1} \int f(x) \, (\mu_n - \mu)(dx) = \frac{1}{2} \sup_{\abs{f} \leq 1} \int f(x) (g_n(x) - g(x)) \, \nu(dx) \\
&\leq \frac{1}{2} \int \abs{g_n(x) - g(x)} \, \nu(dx)
\end{align*}
which shows that $\lim_{n \to \infty} \mu_n = \mu$.
\end{proof}



\begin{defn}Let $X$ be a locally compact Hausdorff space then a
  bounded signed
  measure $\mu$ is said to be a \emph{finite signed Radon measure} if its
  total variation $\abs{\mu} = \mu_+ + \mu_-$ is a Radon measure on $X$.
\end{defn}

\begin{thm}\label{BanachSpaceRadonMeasuresLCH}Let $X$ be a locally compact Hausdorff space then the space
  of finite signed Radon measures is a Banach space with norm given by
  $\tvnorm{\mu} = \frac{1}{2}(\mu_+(X) + \mu_-(X))$.  Moreover $\tvnorm{\mu} = \frac{1}{2}\sup_{\abs{f} \leq 1} \int f (x) \, \mu(dx)$.
\end{thm}
\begin{proof}
By Theorem \ref{BanachSpaceBoundedSignedMeasures} it suffices to show that the finite signed Radon measures are a closed subspace of the 
Banach space of bounded signed measures.
Suppose $\mu_n$ converges to $\mu$ in total variation and each $\mu_n$ is a finite Radon measure.  Since we know that $\abs{\mu}$ is in fact a finite measure we only have to show inner regularity.
TODO: Finish and show that Radon measures are a closed subset in total variation norm.
\end{proof}

TODO: The Riesz Representation Theorem itself; namely the dual space of $C_0(X)$ for a locally compact Hausdorff $X$ is 
isomorphic to the Banach space of finite signed Radon measures with
the total variation norm.
\begin{thm}\label{RieszRepresentationLCH}Let $X$ be a locally compact
  Hausdorff space then the dual space of $C_0(X)$ is isomorphic to the
  space of finite signed Radon measures under the total variation norm.
\end{thm}
\begin{proof}
TODO:
\end{proof}

\section{Regularity Properties of Borel Measures}

In this section we turn to the approximation properties of Borel measures on topological spaces.  In particular we
concern ourselves with the ability to approximate arbitrary measurable sets by compact, closed and open sets.

\begin{defn}Let $\mu$ be a measure on the Borel $\sigma$-algebra of a
Hausdorff topological space $S$.  
\begin{itemize}
\item[(i)] A Borel set $B$ is \emph{inner regular} if for
 $\mu(B) = \sup_{K \subset B} \mu(K)$ where $K$
  is compact. $\mu$ is inner regular if every Borel set is inner regular.
\item[(ii)]A Borel set $B$ is \emph{outer regular} if $\mu(B) = \inf_{U \supset B} \mu(U)$ where $U$
  is open.  A measure $\mu$ is outer regular if every Borel set
  $B$ is outer regular.
\item[(iii)] $\mu$ is \emph{locally finite} if every $x \in S$ has an
  open neighborhood $x \in U$ such that $\mu(U) < \infty$.
\item[(iv)] $\mu$ is a \emph{Radon measure} it is inner regular and
  locally finite.
\item[(v)] $\mu$ is a \emph{Borel measure} when?????  In some cases
  I've seen it required that $\mu(B) < \infty$ for all Borel sets $B$
  (reference?) and in other cases just that the Borel sets are measurable.
\item[(vi)]A Borel set  $B$ is \emph{closed regular} if $\mu(B) = \inf_{F \subset B} \mu(F)$ where $F$
  is closed (e.g. Dudley pg. 224).  A measure $\mu$ is closed regular
  if every Borel set $B$ is closed regular.
\item[(vii)] If $\mu$ is finite, then we say \emph{tight} if and only if
  X is inner regular (e.g. Dudley pg. 224).
\end{itemize}
\end{defn}

\begin{prop}\label{LocallyFiniteMeasuresLCH}Let $\mu$ be a measure on the Borel $\sigma$-algebra of a
  locally compact Hausdorff space $S$.  Then $\mu$ is locally finite
  if and only if $\mu(K) < \infty$ for all compact sets $K \subset S$.
\end{prop}
\begin{proof}
If $\mu(K) < \infty$ for all compact sets $K$ we let $x \in S$ and
pick a relatively compact neighborhood $U$ of $x$.  Then $\mu(U) \leq
\mu(\overline{U}) < \infty$ which shows $\mu$ is locally finite.  On
the other hand, suppose $\mu$ is locally finite and let $K$ be a
compact set.  For each $x \in K$ we take an open neighborhood $U_x$
such that $\mu(U_x) < \infty$ and then extract a finite subcover $U_{x_1},
\dotsc,U_{x_n}$.  By subadditivity, we have $\mu(K) \leq \mu(U_{x_1}) + \dotsb
+ \mu(U_{x_n}) < \infty$.
\end{proof}

\begin{defn}Let $\mu$ be a Borel measure on a Hausdorff topological space. A measurable set $A$ is called \emph{regular} if 
\begin{itemize}
\item[(i)]$\mu(A) = \inf_{U \supset A} \mu(A)$ where $U$ are open
\item[(ii)]$\mu(A) = \sup_{F \subset A} \mu(A)$ where $F$ are closed 
\end{itemize}
TODO: Alternative def assumes that $F$ are compact (see inner
regularity above).  If every measurable set is regular then $\mu$ is
said to be regular.  Note that if we assume the definition of
regularity uses compact inner approximations then regular measures are
inner and outer regular (although inner and outer regularity refer to
only Borel sets; is that a meaningful distinction?)  I think this use
of closed inner regularity is a bit non-standard should probably get
rid of it.
\end{defn}


TODO: Regularity of outer measures and the relationship to regularity
of measures as defined above (see Evans and Gariepy).  Note that
regularity of outer measure implies that if we take an outer measure $\mu$
and the measure on the $\mu$-measurable sets and then take the induced
outer measure we get $\mu$ back if and only $\mu$ is a regular outer
measure.  Evans and Gariepy show that Radon outer measures on
$\reals^n$ are inner
regular as measures on the $\mu$-measurable sets (I think we prove this more
generally above in the context of LCH spaces; note that every set in
$\reals^n$ is $\sigma$-bounded).  Note that inner
regular is part of the most common definition of Radon measure so
their result can be taken as showing a weaker definition of Radon
measure holds on $\reals^n$ (but also they phrase everything in terms
of outer measures...).

TODO: How much this stuff on regularity can be extended to outer
measures????  I want to understand the overlap with the results in
Evans and Gariepy.

\begin{lem}\label{InnerRegularSetsSigmaAlgebra}Let $X$ be a Hausdorff topological space, $\mathcal{A}$
  a $\sigma$-algebra on $X$ and $\mu$ a finite tight measure.  Then
\begin{align*}
\mathcal{R} &= \lbrace A \in \mathcal{A} \mid A \text { and } A^c
\text{ are $\mu$-inner regular} \rbrace
\end{align*}
is a $\sigma$-algebra.  The same is true if the condition is replaced
by sets that are $\mu$-closed inner regular (without the requirement
that $\mu$ is tight).
\end{lem}
\begin{proof}
By definition, $\mathcal{R}$ is closed under complement.  By
assumption that $\mu$ is tight we have $X \in \mathcal{R}$ so all that
needs to be shown is closure under countable union.

Assume $A_1, A_2, \dots \in \mathcal{R}$ and let $\epsilon>0$ be
given.  By finiteness of $\mu$, $\mu(\cup_{n=1}^\infty A_n) < \infty$ and
continuity of measure (Lemma \ref{ContinuityOfMeasure}) there exists $M>0$ such that $\mu(\cup_{n=1}^M
A_n) > \mu(\cup_{n=1}^\infty A_n) - \epsilon$.
 By assumption that $A_n \in \mathcal{R}$ and finiteness of $\mu$, for each
$A_n$ there exists a compact $K_n$ such that $\mu(A_n \setminus K_n) <
\frac{\epsilon}{2^n}$ and there exists compact $L_n$ such that $\mu(A_n^c \setminus L_n) <
\frac{\epsilon}{2^n}$. Let
\begin{align*}
K &= \cup_{n=1}^M K_n \\
L &= \cap_{n=1}^\infty L_n
\end{align*}
and note that both $K$ and $L$ are compact (in the latter case,
because X is Hausdorff we know that each $L$ is closed hence the
intersection is a closed subset of a compact set hence compact).
Furthermore we can compute
\begin{align*}
\mu(\cup_{n=1}^\infty A_n \setminus K) &= \mu(\cup_{n=1}^\infty A_n
\setminus \cup_{n=1}^M K_n)  \\
&= \mu(\cup_{n=1}^M A_n
\setminus \cup_{n=1}^M K_n)  + \mu(\cup_{n=1}^\infty A_n \setminus \cup_{n=1}^M A_n
\setminus \cup_{n=1}^M K_n)\\
&\leq \mu(\cup_{n=1}^M A_n \setminus K_n)  + \mu(\cup_{n=1}^\infty A_n
\setminus \cup_{n=1}^M A_n)\\
&\leq \sum_{n=1}^M(A_n \setminus K_n)  + \epsilon \\
&\leq 3 \epsilon
\end{align*}
and
\begin{align*}
\mu((\cup_{n=1}^\infty A_n)^c \setminus L) &=\mu(\cap_{n=1}^\infty
A_n^c \setminus \cap_{n=1}^\infty L_n) \\
 &=\mu(\cap_{n=1}^\infty
A_n^c \cap \cup_{n=1}^\infty L_n^c) \\
 &=\mu(\cup_{n=1}^\infty \cap_{m=1}^\infty
A_m^c \cap L_n^c) \\
 &\leq \mu(\cup_{n=1}^\infty 
A_n^c \cap L_n^c) \\
 &\leq \sum_{n=1}^\infty (
A_n^c \setminus L_n) \\
&\leq 2 \epsilon
\end{align*}

TODO: The closed inner regular case...
\end{proof}

TODO:  In metric space, tightness is equivalent to inner regularity.
Then Ulam's Theorem that finite measures on separable metric spaces
are automatically inner regular.  Also finite measures on arbitrary
metric spaces are closed inner regular as well as outer regular.

\begin{lem}\label{FiniteMeasuresOnMetricSpacesAreClosedInnerRegular}
Let $(S,d)$ be a metric space and $\mu$ be a Borel measure on $(S,
\mathcal{B}(S))$, then $\mu$ is closed inner regular.  If in addition
$\mu$ is a finite measure then it is outer regular.
\end{lem}
\begin{proof}
Let $U$ be an open set in $S$.  Then $U^c$ is closed and the
function $f(x) = d(x, U^c)$ is continuous.  If we define 
\begin{align*}
F_n &= f^{-1}([1/n, \infty))
\end{align*}
then each $F_n$ is closed, $F_1 \subset F_2 \subset \cdots$ and
$\cup_{n=1}^\infty F_n = U$.  By continuity of measure (Lemma
\ref{ContinuityOfMeasure}) we know that $\lim_{n \to \infty} \mu(F_n)
= \mu(U)$.  So this shows that every open set is inner closed
regular.  Furthermore it is trivial to note that $U^c$ is inner closed
regular because it is closed.  

By Lemma \ref{InnerRegularSetsSigmaAlgebra} we know know that 
\begin{align*}
\mathcal{B}(S) &\subset \mathcal{R} = \lbrace A \subset S \mid A
\text{ and } A^c
\text{ are inner closed regular}  \rbrace
\end{align*}

Outer regularity follows from taking complements and using the
finiteness of $\mu$.
\end{proof}

If we add the criterion that the metric space is separable, then we
can upgrade the closed inner regularity to inner regularity.
\begin{lem}\label{SeparableInnerRegularTight}Let $(S,d)$ be a
  separable metric space and $\mu$ be a finite Borel measure on $(S,
\mathcal{B}(S))$, then $\mu$ is inner regular if and only if it is tight.
\end{lem}
\begin{proof}
Clearly inner regularity implies tightness (which is just inner
regularity of the set $S$), so it suffices to show
that tightness implies inner regularity.

Suppose that $\mu$ is a tight measure.  By Lemma
\ref{InnerRegularSetsSigmaAlgebra} it suffices to show that both open
and closed sets are inner regular.

Pick $\epsilon >0$ and select $K \subset S$ a compact set such that $\mu(S \setminus K) < \frac{\epsilon}{2}$.
By Lemma \ref{FiniteMeasuresOnMetricSpacesAreClosedInnerRegular} we
know that for any Borel set $B$ there exists a closed set $F \subset
B$ such that $\mu(B \setminus F) < \frac{\epsilon}{2}$.  Note that $F
\cap K$ is compact.   We have
\begin{align*}
\mu(B \setminus (F \cap K)) &\leq \mu(B \cap F^c) + \mu(B \cap K^c) \leq \mu(B \cap F^c) + \mu(S \cap K^c) < \epsilon
\end{align*}
\end{proof}
\begin{thm}[Ulam's Theorem]\label{UlamsTheorem}Let $(S,d)$ be a
  complete separable metric space and $\mu$ be a finite Borel measure on $(S,
\mathcal{B}(S))$, then $\mu$ is inner regular.
\end{thm}
\begin{proof}
By Lemma \ref{SeparableInnerRegularTight} it suffices to show that $\mu$ is tight.  Pick
$\epsilon > 0$ and we construct a compact set $K \subset S$ such that
$\mu(S \setminus K) < \epsilon$.  Let
$\overline{B}(x,r)$ denote the closed ball of radius $r$ around $x \in
S$.  Pick
a countable dense subset $x_1, x_2, \dotsc \in S$.  For each $m \in
\naturals$, by density of $\lbrace x_n \rbrace$, we know $\cap_{n=1}^\infty \left ( S
\setminus \cup_{j=1}^n \overline{B}(x_j, \frac{1}{m}) \right ) =
\emptyset$, thus by
continuity of measure (Lemma \ref{ContinuityOfMeasure}) there exists
$N_m > 0$ such that $\mu(S
\setminus \cup_{j=1}^n \overline{B}(x_j, \frac{1}{m}) < \frac{\epsilon}{2^m}$ for
all $n \geq N_m$.
If we define
\begin{align*}
K &= \cap_{m=1}^\infty \cup_{j=1}^{N_m} \overline{B}(x_j, \frac{1}{m})
\end{align*}
we claim that $K$ is compact.  Note that $K$ is easily seen to be
closed as it is an intersection of a finite union of closed balls.
Since $S$ is complete this implies that $K$ is also complete.  Also it
is easy to see that $K$ is totally bounded since by construction we
have demonstrated a cover by a finite number of balls of radius
$\frac{1}{m}$ for each $m \in \naturals$.  So by Theorem
\ref{CompactnessInMetricSpaces} we know $K$ is compact.

To finish the result we claim $\mu(S \setminus K) < \epsilon$:
\begin{align*}
\mu(S \setminus K) 
&= \mu(S \cap \left(\cap_{m=1}^\infty
  \cup_{j=1}^{N_m} \overline{B}(x_j, \frac{1}{m})\right)^c) \\
&= \mu(S \cap \cup_{m=1}^\infty \left(
  \cup_{j=1}^{N_m} \overline{B}(x_j, \frac{1}{m})\right)^c) \\
&= \mu(\cup_{m=1}^\infty S \setminus 
  \cup_{j=1}^{N_m} \overline{B}(x_j, \frac{1}{m})) \\
&\leq \sum_{m=1}^\infty \mu( S \setminus 
  \cup_{j=1}^{N_m} \overline{B}(x_j, \frac{1}{m})) \\
&< \epsilon
\end{align*}
\end{proof}

\begin{thm}Let $\mu$ be a finite Borel measure on a metric space $S$,
  then $\mu$ is closed regular.  If $\mu$ is tight then $\mu$ is regular.
\end{thm}
TODO: Specialize the definition of Radon measure in the presence of
more assumptions on $X$ (in particular local compactness,
$\sigma$-compactness, second countability).

TODO: Are Radon measures automatically outer regular?  Finite ones are
I believe.

Tao proves Riesz representation under assumption of local compactness, Hausdorff
and $\sigma$-compactness.

Kallenberg proves Riesz representation under assumption of LCH and
second countability (this is less general than the Tao
result as lcscH implies LCH $\sigma$-compact)
and targets Radon measures.  Our results taken from Arveson are more
general as the remove the second countability assumption and the $\sigma$-compactness
assumptions.

Evans and Gariepy prove Riesz representation only on $\reals^n$ using
Radon outer measures.  This is probably subsumed by our results taken
from Arveson but I need to understand whether the use of outer
measures adds anything to the picture in general.

Arveson has some well known lecture notes that prove Riesz on general
LCH spaces
and emphasizes Radon measures (it also explores how Baire measures figure in the
picture).  I have chosen to follow these notes.  Note that Arveson
mentions that the lcscH approach of Kallenberg avoids the distinction
between Baire and Borel sets (sounds like the $\sigma$-algebras agree
in this case).

Fremlin probably has some very general account of Reisz representation
(of course).

Dudley proves Riesz representation of compact Hausdorff spaces and
phrases things in terms of Baire measures.  Dudley does not really
discuss Radon measures.  Arveson discusses the relationship between
the use of Baire and Radon measures.

Lusin's Theorem on the almost continuity of measurable functions has
the following nice extension.
\begin{thm}\label{ExtendedLusin} Let $(X, \tau)$ be a topological space and $\mu$ a finite closed regular
Borel measure.  Let $(S,d)$ be a separable metric space and let $f : X \to S$ be Borel measurable then for 
every $\epsilon > 0$ there exists a closed set $F \subset X$ with $\mu(X \setminus F) < \epsilon$ and 
$f \mid_F$ continuous.
\end{thm}
\begin{proof}
Let $s_1, s_2, \dotsc$ be a countable dense subset of $S$.  For any $x \in X$ and $m \in \naturals$ there exists a
$s_n$ such that $d(x,s_n) < 1/m$.  For every $m \in \naturals$ and $x \in X$ define 
\begin{align*}
n(x,m) &= \inf \lbrace n \in \naturals \mid d(f(x), s_n) < 1/m \rbrace
\end{align*}
and  $f_m : X \to S$ by $f_m(x) = s_{n(x,m)}$.  Observe that $f_m$ has countable range and 
\begin{align*}
f_m^{-1} \lbrace s_n \rbrace &= \cap_{j=1}^{n-1} f^{-1}(B(s_j, 1/m))^c \cap f^{-1}(B(s_n, 1/m))
\end{align*} 
hence is measurable.  
By definition we know that $\sup_{x \in X} d(f(x), f_m(x)) \leq 1/m$ and therefore
$f_m$ converges to $f$ uniformly.  We need to find a closed set of large measure on which the $f_m$ are continuous and then we will be 
able to conclude that $f$ is continuous on that set.  

\begin{clm}For every $m \in \naturals$ there exists a closed set $F_m$ with $\mu(F_m) > \mu(X) - 2^{1-m}$ such that $f_m$ is continuous on $F_m$.
\end{clm}
By density of $s_1, s_2, \dotsc$
\begin{align*}
\cap_{n=1}^\infty \lbrace d(f(x), s_n) \geq 1/m \rbrace &= \cap_{n=1}^\infty f^{-1} (B(s_n, 1/m)^c) = \emptyset
\end{align*}
hence by continuity of measure we may pick $N_m \in \naturals$ such that $\mu \left(\cap_{n=1}^{N_m} \lbrace d(f(x), s_n) \geq 1/m \rbrace \right) < 1/2^m$.  
Observe that if $x \notin \cap_{n=1}^{N_m} \lbrace d(f(x), s_n) \geq 1/m \rbrace$ if and only if $n(x,m) \leq N_m$ hence we have
\begin{align*}
\left(\cap_{n=1}^{N_m} \lbrace d(f(x), s_n) \geq 1/m \rbrace \right)^c &= \cup_{n=1}^{N_m} f_m^{-1} \lbrace s_n \rbrace
\end{align*}
From this fact and the choice of $N_m$ we have
\begin{align*}
\sum_{n=1}^{N_m}\mu \left( f_m^{-1} \lbrace s_n \rbrace \right) &= \mu \left( \cup_{n=1}^{N_m} f_m^{-1} \lbrace s_n \rbrace \right) \\
& = \mu(X) - \mu \left(\cap_{n=1}^{N_m} \lbrace d(f(x), s_n) \geq 1/m \rbrace \right) \\
&\geq \mu(X) - 1/2^m
\end{align*}
By closed regularity for each $1 \leq n \leq N_m$ we may pick a closed set $F_{mn} \subset f_m^{-1} \lbrace s_n \rbrace$ such that $\mu(f_m^{-1} \lbrace s_n \rbrace  \setminus F_{mn}) < \frac{1}{N_m 2^m}$ and therefore if we define $F_m = \cup_{n=1}^{N_m} F_{mn}$ and note that the $F_{mn}$ are disjoint for fixed $m$ we get
\begin{align*}
\mu(F_m) &= \sum_{n=1}^{N_m} \mu(F_{mn}) = \sum_{n=1}^{N_m} \mu(f_m^{-1} \lbrace s_n \rbrace) - \sum_{n=1}^{N_m} \mu(f_m^{-1} \lbrace s_n \rbrace  \setminus F_{mn})  \\
&\geq \mu(X) - 1/2^m - 1/2^m = \mu(X) - 2^{1-m}
\end{align*}
It remains to see that $f_m \mid_{F_m}$ is continuous.  For that we need the following simply fact: if $F$ and $G$ are disjoint closed sets in a topological space $X$ then because $F = (F \cup G) \cap G^c$ it follows that $F$ is open in the relative topology on $F \cup G$.  By induction it follows that for each $1 \leq  n_1 < \dotsb < n_k  \leq N_m$ the set $F_{mn_1} \cup \dotsb \cup F_{mn_k}$ is open in $F_m$.  Since $f_m$ restricted to $F_m$ takes only the finite values $s_1, \dotsc, s_{N_m}$ and by construction the inverse images of sets in $S$ are all of the form $F_{mn_1} \cup \dotsb \cup F_{mn_k}$.

To finish the proof for every $k \in \naturals$ we define $G_k = \cap_{m=k}^\infty F_m$ so that 
\begin{align*}
\mu(G_k) &= \mu(X) - \mu(G_k^c) = \mu(X) - \mu\left( \cup_{m=k}^\infty F^c_m \right) \geq \mu(X) - \sum_{m=k}^\infty 2^{1-m} = \mu(X) - 2^{2 - k}
\end{align*}
For $\epsilon > 0$ given we pick $k \in \naturals$ large enough so that $2^{2-k} < \epsilon$ then $\mu(G_k) > \mu(X) - \epsilon$ and $f_m\mid_{G_k}$ is continuous for $m \geq k$  (observe that if $f$ is continuous on $X$ then for any $A \subset X$ we have $f\mid_A$ is continuous) and, as noted at the beginning of the proof, since $f_m \to f$ uniformly we conclude that $f \mid_{G_k}$ is continuous. 
\end{proof}

\begin{defn}A separable metric space $(S,d)$ is \emph{universally measurable} if for every probability measure $\mu$ on the completion $\hat{S}$ there exist Borel measurable sets $A, B \in \mathcal{B}(\hat{S})$ such that $A \subset S \subset B$ and $\mu(A) = \mu(B)$ (i.e. $S$ is measurable in the \emph{universal $\sigma$-algebra} $\cap_{\mu \in \mathcal{P}(\hat{S})} \mathcal{B}(\hat{S})^{\mu}$).
\end{defn}

\begin{thm}\label{UniversalMeasurabilityAndTightness}A separable metric space $(S,d)$ is universally measurable if and only if every Borel probability measure on $S$ is tight.
\end{thm}
\begin{proof}
TODO:  
\end{proof}

\section{Hausdorff Measure}

\subsection{Introduction}

In this section we discuss the construction of a family of outer
measures on $\reals^n$ called \emph{Hausdorff measures}.  Note the
construction can be generalized to metric spaces.  The following is
motivation why a tool like Hausdorff measure may be useful.  Suppose
very specifically that we are
in $\reals^3$, then the Lebesgue product measure essentially
corresponds to a notion of volume.  What about the surface area of a
$2$-dimensional object or the length of a $1$-dimensional object?  As
you may have learned in advanced calculus these ideas can indeed be
describe in great generality by the notion of differential forms.
However, the formalism of forms usually has some notion of smoothness
associated with it (hence the adjective differential); a natural question to ask is whether one can fine
a purely measure theoretic approach to the problem.  Hausdorff measures
provide one answer to this question.   The broad form of the theory
is perhaps a bit more general than one might expect; for any space
there is a Hausdorff outer measure for every real number $s$.  The
case of integers
$s=1$ corresponds to arclength, $s=2$ surface area, $s=3$ volume and so
on.  Measures with $s$ non-integral are
\emph{fractal}.  On $\reals^n$, the Hausdorff measure with $s=n$ is equal to
Lebesgue measure and any Hausdorff measure with $s > n$ is trivial
(gives $0$ measure to all sets).  We'll prove all of this and more in
what follows.

\subsection{Construction of Hausdorff Measure}

TODO:  Here I am taking the path of Evans and Gariepy and normalizing
Hausdorff measure so that $\mathcal{H}^n = \lambda_n$.  I am not sure
if this winds up being inconvenient when one considers Hausdorff
measure in arbitrary metric spaces (nor do I know whether we'll bother
considering Hausdorff measures in metric spaces).

\begin{lem}Let $\lambda_n$ be Lebesgue measure on $\reals^n$, then
  $\lambda_n(B(0, 1)) = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)}$.
\end{lem}
\begin{proof}
TODO
\end{proof}

\begin{defn}Let $(S,d)$ be a metric space and $A \subset S$, the
  \emph{diameter} of $A$ is 
\begin{align*}
\diam(A) &= \sup \lbrace d(x,y) \mid x,y \in A \rbrace
\end{align*}
and we use the convention that $\diam(\lbrace x \rbrace) = 0$.
\end{defn}

\begin{defn}Let $(S,d)$ be a metric space, $0 \leq s < \infty$ and $0
  < \delta$.  Then for $A \subset S$, 
\begin{align*}
\mathcal{H}^s_\delta(A) &= \inf \lbrace \sum_{n=1}^\infty \alpha(s)
\left ( \frac{\diam(C_n)}{2}\right )^s \mid A \subset
\cup_{n=1}^\infty C_n \text{ where } \diam(C_n) \leq \delta \text{ for
  all } n\rbrace
\end{align*}
where we use the convention that $0^0=1$ and
\begin{align*}
\alpha(s) &= \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)}
\end{align*}
For $A$ and $s$ as above define
\begin{align*}
\mathcal{H}^s(A) &= \lim_{\delta \to 0} \mathcal{H}_\delta^s(A) = \sup_{\delta>0} \mathcal{H}_\delta^s(A)
\end{align*}
\end{defn}

The definition of Hausdorff measures considers covering by arbitrary sets but it is not hard to see that 
the same values occur if we limit covering to open or closed sets.
\begin{prop}\label{HausdorffMeasureByOpenClosedCoverings}Let $(S,d)$ be a metric space, $0 \leq s < \infty$ and $0
  < \delta$ then 
\begin{align*}
\mathcal{H}^s_\delta(A) &= \inf \lbrace \sum_{n=1}^\infty \alpha(s)
\left ( \frac{\diam(C_n)}{2}\right )^s \mid A \subset
\cup_{n=1}^\infty C_n \text{ where $C_n$ is open and } \diam(C_n) \leq \delta \text{ for
  all } n\rbrace \\
\intertext{and}
\mathcal{H}^s_\delta(A) &= \inf \lbrace \sum_{n=1}^\infty \alpha(s)
\left ( \frac{\diam(C_n)}{2}\right )^s \mid A \subset
\cup_{n=1}^\infty C_n \text{ where $C_n$ is closed and } \diam(C_n) \leq \delta \text{ for
  all } n\rbrace
\end{align*}
\end{prop}

\begin{thm}\label{BorelRegularityOfHausdorffMeasure}$\mathcal{H}^s$ is a Borel regular outer measure.
\end{thm}
\begin{proof}
TODO
\end{proof}

In some case Hausdorff measure is equivalent to a well-known object; here is a simple case of this phenomenon.
\begin{prop}\label{HausdoffMeasureEqualsCounting}Let $(S,d)$ be  a metric space then $\mathcal{H}^0$ is counting measure.
\end{prop}
\begin{proof}
Let $x \in S$ and $\delta>0$, then the set $\lbrace x \rbrace$ covers itself and therefore $\mathcal{H}^0(\lbrace x \rbrace) \leq (\diam(\lbrace x \rbrace))^0 = 1$.  Since every term
$(\diam(C))^0 \geq 1$ it follows from the definition that $\mathcal{H}^0_\delta \geq 1$.  Thus $\mathcal{H}^0_\delta(\lbrace x \rbrace) = 1$ and the result follows by letting $\delta \to 0$.
\end{proof}

\begin{prop}\label{HausdorffMeasureUnderIsometry}Let $(S,d)$ and $(T,d^\prime)$ be metric spaces and let $f : S \to T$ be an isometric embedding then for any $0 \leq s < \infty$ and $\delta>0$ and $A \subset S$
\begin{align*}
\mathcal{H}_\delta^s(f(A)) &= \mathcal{H}_\delta^s(A)
\end{align*}
In particular $\mathcal{H}^s(f(A)) = \mathcal{H}^s(A)$
\end{prop}
\begin{proof}
Let $\delta>0$ and $\epsilon>0$ be arbitrary.  First we show $\mathcal{H}^s_\delta (f(A)) \leq \mathcal{H}^s_\delta(A)$.  This is trivial if $\mathcal{H}^s_\delta(A) =\infty$ so we assume that $\mathcal{H}^s_\delta(A) <\infty$ and pick $U_1, U_2, \dotsc$ with $\diam(U_n) < \delta$, $A \subset \cup_{n=1}^\infty U_n$ and $\sum_{n=1}^\infty \left( \diam(C_n) \right)^s \leq \mathcal{H}^s_\delta(A)+\epsilon$.  It is elementary that $f(A) \subset \cup_{n=1}^\infty f(U_n)$ and since $f$ is an isometry $\diam(U_n) = \diam(f(U_n)) \leq \delta$ hence
\begin{align*}
\mathcal{H}^s_\delta (f(A)) &\leq \sum_{n=1}^\infty \left( \diam(f(U_n)) \right)^s = \sum_{n=1}^\infty \left( \diam(U_n) \right)^s \leq \mathcal{H}^s_\delta(A)+\epsilon
\end{align*}
Now let $\epsilon \to 0$ to conclude $\mathcal{H}^s_\delta (f(A)) \leq \mathcal{H}^s_\delta(A)$.

To see that $\mathcal{H}^s_\delta(A) \leq \mathcal{H}^s_\delta (f(A))$ arguing as above it suffices to assume $\mathcal{H}^s_\delta (f(A)) < \infty$ and to pick $U_1, U_2, \dotsc$ with $\diam(U_n) < \delta$, $f(A) \subset \cup_{n=1}^\infty U_n$ and $\sum_{n=1}^\infty \left( \diam(C_n) \right)^s \leq \mathcal{H}^s_\delta(f(A))+\epsilon$.  It is elementary that $A \subset \cup_{n=1}^\infty f^{-1}(U_n)$ (if $a \in A$ then $f(a) \in U_n$ for some $n$ hence $a \in f^{-1}(U_n)$)
and since $f$ is an isometry $\diam(f^{-1}(U_n)) \leq \diam(U_n) \leq \delta$ and therefore
\begin{align*}
\mathcal{H}^s_\delta (A) &\leq \sum_{n=1}^\infty \left( \diam(f^{-1}(U_n)) \right)^s \leq \sum_{n=1}^\infty \left( \diam(U_n) \right)^s \leq \mathcal{H}^s_\delta(f(A))+\epsilon
\end{align*}
Now let $\epsilon \to 0$ to conclude $\mathcal{H}^s_\delta (A) \leq \mathcal{H}^s_\delta(f(A))$ and therefore $\mathcal{H}^s_\delta (A) = \mathcal{H}^s_\delta(f(A))$.
 
To see that $\mathcal{H}^s(f(A)) = \mathcal{H}^s(A)$ simply take the limit as $\delta \to 0$.
\end{proof}

\begin{prop}\label{HausdorffMeasureUnderLipschitzMaps}Let $(S,d)$ and $(T,d^\prime)$ be metric spaces and let $f : S \to T$ be a Lipschitz function then for any $0 \leq s < \infty$ and $A \subset S$
\begin{align*}
\mathcal{H}^s(f(A)) &\leq \left ( \Lip(f) \right)^s \mathcal{H}^s(A)
\end{align*}
\end{prop}
\begin{proof}
If $\mathcal{H}^s(A)=\infty$ then the result is trivial so assume that $\mathcal{H}^s(A)<\infty$
Let $\delta>0$ be given and pick $\epsilon>0$, sets $C_1, C_2, \dotsc$ such that $\diam(C_n) \leq \delta$, $A \subset \cup_{n=1}^\infty C_n$ and $\sum_{n=1}^\infty \left(\diam(C_n)\right)^s < \mathcal{H}^s_\delta(A)+\epsilon$.  It is elementary that $f(A) \subset \cup_{n=1}^\infty f(C_n)$ and by the Lipschitz property of $f$ we
know
\begin{align*}
\diam(f(C_n)) &\leq \Lip(f) \diam(C_n) \leq \Lip(f) \delta
\end{align*}
Thus 
\begin{align*}
\mathcal{H}_{\Lip(f) \delta}^s (f(A)) &\leq \sum_{n=1}^\infty \left(\diam(f(C_n))\right)^s \leq \left(\Lip(f) \right)^s \sum_{n=1}^\infty\left(\diam(C_n)\right)^s \\
&\leq \left(\Lip(f) \right)^s \left(\mathcal{H}^s_\delta(A)+\epsilon\right )
\end{align*}
Now let $\epsilon \to 0$ and then $\delta \to 0$.
\end{proof}

\begin{thm}\label{IsodiametricInequality}For any $d \in \naturals$ we have 
\begin{align*}
\lambda^d(A) &\leq \alpha(d) \left( \frac{\diam(A)}{2} \right)^d
\end{align*}
\end{thm}
\begin{proof}
TODO
\end{proof}

\begin{thm}\label{HausdorffEqualsLebesgue}For any $d \in \naturals$ we have $\mathcal{H}^d =
  \lambda^d$ (or $ 2^{-d} \alpha(d) \mathcal{H}^d =\lambda^d$ depending on normalization) on $\reals^d$.
\end{thm}
\begin{proof}
TODO
\end{proof}

\section{Covering Theorems in $\reals^d$}

Since our purposes have been to understand probability theory we have
hitherto avoided making assumptions that we are dealing with
$\reals^d$.  While this decision has benefits, it has drawbacks as
well.  Among those drawbacks are that we lose sight of some history and also some very
beautiful and deep understanding of the measure theory of the reals.
TODO: Vitali and Besicovich.

\begin{thm}[Besicovich's Covering Theorem]\label{BesicovichCoveringTheorem}
Let $\mathcal{I}$ be an arbitrary index set and for each $\alpha \in \mathcal{I}$ let $\overline{B}(x_\alpha, r_\alpha)$ be a closed
ball in $\reals^d$ such that $r_\alpha > 0$.  If $\sup_{\alpha} r_\alpha < \infty$ then there exists a
constant $N_d$ depending only on $d$ and countable collections $\mathcal{J}_1, \dotsc, \mathcal{J}_{N_d}$ such
that for each $j=1, \dotsc, N_d$ the balls $\overline{B}(x_\alpha, r_\alpha)$ with $\alpha \in \mathcal{J}_j$ are disjoint and
\begin{align*}
\lbrace x_\alpha \mid \alpha \in \mathcal{I} \rbrace &\subset \cup_{j=1}^{N_d} \cup_{\alpha \in \mathcal{J}_j} \overline{B}(x_\alpha, r_\alpha)
\end{align*}
\end{thm}
\begin{proof}
TODO:
See Evans and Gariepy for the proof until I get around copying it down here (I have little to add to their proof).
\end{proof}

\begin{cor}\label{BesicovichFilling}Let $\mu$ be a Borel outer measure on $\reals^d$, $\mathcal{I}$ be an arbitrary index set and
for each $\alpha \in \mathcal{I}$ $\overline{B}(x_\alpha, r_\alpha)$ is a closed ball with $r_\alpha > 0$.  Define $A = \lbrace x_\alpha \mid \alpha \in \mathcal{I} \rbrace$
and assume that $\mu(A) < \infty$ and for every $x \in A$ we have $\inf \lbrace r_\alpha \mid x_\alpha = x \rbrace = 0$.  Then for each open set $U \subset \reals^d$ 
there exists a countable set $\mathcal{J} \subset \mathcal{I}$ such that 
\begin{itemize}
\item[(i)]$\overline{B}(x_\alpha, r_\alpha) \cap \overline{B}(x_\beta, r_\beta) = \emptyset$ for each $\alpha, \beta \in \mathcal{J}$ with $\alpha \neq \beta$
\item[(ii)]$\cup_{\alpha \in \mathcal{J}} \overline{B} (x_\alpha, r_\alpha) \subset A$
\item[(iii)]$\mu \left( (A \cap U) \setminus \cup_{\alpha \in \mathcal{J}} \overline{B} (x_\alpha, r_\alpha) \right) = 0$
\end{itemize}
\end{cor}
\begin{proof}
Let $N_d$ be the constant in Theorem \ref{BesicovichCoveringTheorem}.  Pick $\theta$ such that $1 - N_d^{-1} < \theta < 1$.  
\begin{clm}There exist a finite collection $\overline{B}(x_{\alpha_1}, r_{\alpha_1}), \dotsc, \overline{B}(x_{\alpha_{M_1}}, r_{\alpha_{M_1}})$ with $\alpha_j \in \mathcal{I}$,
$\overline{B}(x_{\alpha_j}, r_{\alpha_j}) \subset U$ for $j=1, \dotsc, M$ such that the $\overline{B}(x_{\alpha_j}, r_{\alpha_j})$ are disjoint and 
\begin{align*}
\mu \left( (A \cap U) \setminus \cup_{j=1}^{M_1} \overline{B}(x_{\alpha_j}, r_{\alpha_j}) \right) &\leq \theta \mu(A \cap U)
\end{align*}
\end{clm}
Let $\mathcal{K} = \lbrace \alpha \in \mathcal{I} \mid r_\alpha \leq 1 \text{ and } \overline{B}(x_\alpha, r_\alpha) \subset U \rbrace$.  
By the assumptions on $r_\alpha$ and the openness of $U$ we know that $A \cap U = \lbrace x_\alpha \mid \alpha \in \mathcal{K} \rbrace$.  Apply Theorem \ref{BesicovichCoveringTheorem}
to the collection $\mathcal{K}$ to construct countable sets $\mathcal{J}_1, \dotsc, \mathcal{J}_{N_d}$ with 
\begin{align*}
A \cap U &\subset \cup_{k=1}^{N_d} \cup_{\alpha \in \mathcal{J}_k} \overline{B}(x_\alpha, r_\alpha) = \cup_{k=1}^{N_d} A \cap U \cap \cup_{\alpha \in \mathcal{J}_k} \overline{B}(x_\alpha, r_\alpha) 
\end{align*}
By countable subadditivity
\begin{align*}
\mu(A \cap U) &\leq \sum_{k=1}^{N_d}\mu \left( A \cap U \cap \cup_{\alpha \in \mathcal{J}_k} \overline{B}(x_\alpha, r_\alpha)  \right) 
\leq N_d \max_{1 \leq k \leq N_d} \mu \left( A \cap U \cap \cup_{\alpha \in \mathcal{J}_k} \overline{B}(x_\alpha, r_\alpha)  \right)
\end{align*}
from which it follows that there exists $k \in \naturals$ with $1 \leq k \leq N_d$ and
\begin{align*}
\mu \left( A \cap U \cap \cup_{\alpha \in \mathcal{J}_k} \overline{B}(x_\alpha, r_\alpha)  \right) &\geq N_d^{-1} \mu(A \cap U)
\end{align*}
By continuity of regular outer measures (Proposition \ref{ContinuityOfRegularOuterMeasure}) and the fact that $1 - \theta < N_d^{-1}$ we know that there is a finite subset $\alpha_1, \dotsc, \alpha_M \in \mathcal{J}_k$ such that
\begin{align*}
\mu \left( A \cap U \cap \cup_{j=1}^M \overline{B}(x_{\alpha_j}, r_{\alpha_j})  \right) &\geq (1 - \theta) \mu(A \cap U)
\end{align*}
and by the measurability of $\cup_{j=1}^M \overline{B}(x_{\alpha_j}, r_{\alpha_j})$ we conclude
\begin{align*}
\mu \left( A \cap U \setminus \cup_{j=1}^M \overline{B}(x_{\alpha_j}, r_{\alpha_j})  \right)
&=\mu(A \cap U) - \mu \left( A \cap U \cap \cup_{j=1}^M \overline{B}(x_{\alpha_j}, r_{\alpha_j})  \right) \\
&\leq \theta \mu(A \cap U)
\end{align*}
The disjointness of the $\overline{B}(x_{\alpha_j}, r_{\alpha_j})$ follows from the disjointness of the larger family of balls $\overline{B}(x_{\alpha}, r_{\alpha})$ with $\alpha \in \mathcal{J}_k$.

Let $U_1 = U$ and we apply the claim inductively to define $M_n$, $\alpha^n_1, \dotsc, \alpha^n_{M_n}$ such that $\overline{B}(x_{\alpha^n_j}, r_{\alpha^n_j}) \subset U_n$ for all $j=1, \dotsc, M_n$,
\begin{align*}
U_{n+1} &= U_n \setminus \cup_{j=1}^{M_n} \overline{B}(x_{\alpha^n_j}, r_{\alpha^n_j}) = U \setminus \cup_{k=1}^n \cup_{j=1}^{M_k} \overline{B}(x_{\alpha^k_j}, r_{\alpha^k_j}) 
\end{align*}
and 
\begin{align*}
\mu(A \cap U_{n+1}) &= \mu \left( (A \cap U_n) \setminus \cup_{j=1}^{M_n} \overline{B}(x_{\alpha^n_j}, r_{\alpha^n_j}) \right) \leq \theta \mu(A \cap U_n)
\end{align*}
For fixed $n$ and $1 \leq i < j \leq M_n$ we have $\overline{B}(x_{\alpha^n_i}, r_{\alpha^n_i}) \cap \overline{B}(x_{\alpha^n_j}, r_{\alpha^n_j}) = \emptyset$ by the claim and for $n,m \in \naturals$ with $m < n$,
$1 \leq i \leq M_m$ and $1 \leq j \leq M_n$ we have $\overline{B}(x_{\alpha^m_i}, r_{\alpha^m_i}) \cap \overline{B}(x_{\alpha^n_j}, r_{\alpha^n_j}) = \emptyset$ since 
\begin{align*}
\overline{B}(x_{\alpha^n_j}, r_{\alpha^n_j}) &\subset U_n = U \setminus \cup_{k=1}^{n-1} \cup_{j=1}^{M_k} \overline{B}(x_{\alpha^k_j}, r_{\alpha^k_j})
\end{align*}

Let $\mathcal{J} = \lbrace \alpha^n_j \mid n \in \naturals, j=1, \dotsc, M_n \rbrace$ and note that by continuity of regular outer measures (TODO: Where do we prove this the descending case???)
\begin{align*}
\mu \left( (A \cap U) \setminus \cup_{\alpha \in \mathcal{J}} \overline{B}(x_\alpha, r_\alpha) \right)
&=\mu \left( (A \cap U) \setminus \cup_{k=1}^\infty \cup_{j=1}^{M_k} \overline{B}(x_{\alpha^k_j}, r_{\alpha^k_j})\right) \\
&= \lim_{n \to \infty} \mu \left( (A \cap U) \setminus \cup_{k=1}^n \cup_{j=1}^{M_k} \overline{B}(x_{\alpha^k_j}, r_{\alpha^k_j})\right) \\
&= \lim_{n \to \infty} \mu \left( (A \cap U_n) \setminus \cup_{j=1}^{M_n} \overline{B}(x_{\alpha^n_j}, r_{\alpha^n_j})\right) \\
&\leq \lim_{n \to \infty} \theta \mu(A \cap U_n) \\
&\leq \lim_{n \to \infty} \theta^{n-1} \mu(A \cap U)  = 0\\
\end{align*}
\end{proof}

\subsection{Derivatives of Radon Outer Measures on $\reals^d$}

In this section we apply the Besicovich covering theorem as a key ingredient in some formulas for computing Radon-Nikodym derivatives of Radon measures on $\reals^d$.

\begin{defn}Let $\mu$ and $\nu$ be Radon outer measures on $\reals^d$
  then for each $x \in \reals^d$ we define 
\begin{align*}
\overline{D}_\mu \nu(x) &= \begin{cases}
\limsup_{r \to 0} \frac {\nu(B(x, r))}{\mu(B(x,r))} & \text{if  $\mu(B(x,r)) > 0$ for all $r>0$} \\
+\infty & \text{if  $\mu(B(x,r)) = 0$ for some $r>0$} \\
\end{cases} \\
\underline{D}_\mu \nu(x) &= \begin{cases}
\liminf_{r \to 0} \frac {\nu(B(x, r))}{\mu(B(x,r))} & \text{if  $\mu(B(x,r)) > 0$ for all $r>0$} \\
+\infty & \text{if  $\mu(B(x,r)) = 0$ for some $r>0$} \\
\end{cases} \\
\end{align*}
If $\underline{D}_\mu \nu(x) = \overline{D}_\mu \nu(x) < \infty$ then we say that $\nu$ is \emph{differentiable} with respect to $\mu$ at $x$
and in this case we write $D_\mu \nu(x) = \underline{D}_\mu \nu(x) = \overline{D}_\mu \nu(x)$.
\end{defn}

Our first goal is to show that derivatives of Radon measures exist almost everywhere, then we can show that the derivative in the above sense 
is the Radon-Nikodym derivative.

\begin{lem}\label{RadonDerivativeBounds}Let $0 < \lambda < \infty$ be given then for an arbitrary subset $A \subset \reals^d$
\begin{itemize}
\item[(i)] if $A \subset \lbrace x \mid \underline{D}_\mu \nu(x) \leq \lambda \rbrace$ then $\nu(A) \leq \lambda \mu(A)$. 
\item[(ii)] if $A \subset \lbrace x \mid \overline{D}_\mu \nu(x) \geq \lambda \rbrace$ then $\nu(A) \geq \lambda \mu(A)$. 
\end{itemize}
\end{lem}
\begin{proof}
First assume that $\mu(\reals^d) < \infty$ and $\nu(\reals^d) < \infty$.  Suppose that $A \subset \lbrace x \mid \underline{D}_\mu \nu(x) \leq \lambda \rbrace$.
Let $\epsilon > 0$ and pick an open set $U$ such that $A \subset U$.  Now we define
\begin{align*}
\mathcal{A} &= \lbrace \overline{B}(a, r) \mid x \in A, \overline{B}(a,r) \subset U, \nu(\overline{B}(a, r)) \leq (\alpha  + \epsilon) \mu(\overline{B}(a, r)) \rbrace
\end{align*}
Observe that for each $a \in A$ there exists a sequence $r_n>0$ such that $r_n \downarrow 0$ and $\lim_{n \to \infty} \frac{\nu(\overline{B}(a, r_n))}{\mu(\overline{B}(a,r_n))} \leq \alpha$.  Thus for sufficiently large $n$ we have $\nu(\overline{B}(a, r_n)) \leq (\alpha + \epsilon) \mu(\overline{B}(a,r_n))$ and it follows that $\inf \lbrace r>0 \mid \overline{B}(a,r) \in \mathcal{A} = 0$.  Therefore we can apply Corollary \ref{BesicovichFilling} to
get a countable collection of disjoint balls $\overline{B}(a_1, r_1), \overline{B}(a_2, r_2), \dotsc$ in $\mathcal{A}$ such that 
\begin{align*}
\nu \left(A \setminus \cup_{n=1}^\infty \overline{B}(a_n, r_n) \right) &= 0
\end{align*}
From this it follows from measurability of the balls, subadditivity and the definition of $\mathcal{A}$
\begin{align*}
\nu(A) 
&= \nu(A \cap \cup_{n=1}^\infty \overline{B}(a_n, r_n) ) 
\leq \sum_{n=1}^\infty \nu(\overline{B}(a_n, r_n) ) 
\leq (\alpha + \epsilon) \sum_{n=1}^\infty \mu(\overline{B}(a_n, r_n) ) \\
&\leq  (\alpha + \epsilon) \mu(U)
\end{align*}
Now take the infimum over all open sets $U$ such that $A \subset U$ and use the outer regularity of $\mu$ to conclude that $\nu(A) \leq (\alpha + \epsilon) \mu(A)$.  Since $\epsilon>0$ we 
arbitrary we let $\epsilon \to 0$ to get $\nu(A) \leq \alpha \mu(A)$.

TODO: Prove (ii) using the same argument and put as an exercise.

TODO: Remove the finiteness assumption
\end{proof}

\begin{thm}\label{ExistenceMeasurabilityRadonDerivatives}Let $\mu$ and $\nu$ be Radon outer measures on $\reals^d$ then $D_\mu \nu$ exists $\mu$-almost everywhere and $D_\mu \nu$ is $\mu$-measurable.
\end{thm}
\begin{proof}
First assume that $\mu(\reals^d)<\infty$ and $\nu(\reals^d)<\infty$.
Let $A = \lbrace \underline{D}_\mu \nu = \infty$ and note that for every $\lambda$ we have  $A \subset \lbrace \underline{D}_\mu \nu \geq \lambda$ and therefore by Lemma \ref{RadonDerivativeBounds} we have $\mu(A) \leq \lambda^{-1} \nu(A)$.  Since $\nu(A) < \infty$ by assumption we may let $\lambda \to \infty$ to conclude that $\mu(A) = 0$.

Now for every $0 \leq p < q < \infty$ with $p,q \in \rationals$ consider $A_{p,q} = \lbrace \underline{D}_\mu \nu \leq p < q \leq \overline{D}_\mu \nu < \infty$.  By two applications
of Lemma \ref{RadonDerivativeBounds} we see that 
\begin{align*}
 q \mu(A_{p,q}) &< \nu(A_{p,q}) < p \mu(A_{p,q})
\end{align*}
and therefore since $p < q$ we conclude that $\mu(A_{p,q}) = 0$.  Since $\lbrace D_\mu \nu \text{ not exists} \rbrace = A \cup \cup_{q,p \in \rationals_+} A_{p,q}$ we conclude that $D_\mu \nu$ exists $\mu$-almost everywhere.

\begin{clm}Let $\mu$ be an arbitrary Radon measure on $\reals^d$.  For every $r>0$ and $x \in \reals^d$ we have $\limsup_{y \to x} \mu(\overline{B}(y, r)) \leq \mu(\overline{B}(x, r))$ (i.e. $\mu(\overline{B}(x,r))$ is an upper semicontinuous function of $x$).
\end{clm}
Pick a sequence $y_n$ such that $\lim_{n \to \infty} y_n = x$.  Suppose that $\characteristic{\overline{B}(x,r)}(y) = 0$ i.e. we are given $y$ such that $\norm{y-x}>r$.  We can pick $N$ such that $\norm{y_n-x} < \norm{y-x}-r$ for all $n \geq N$; it follows that $\norm{y-y_n} \geq \norm{y-x} - \norm{y_n-x} > r$ for all $n \geq N$ which is to say that $\characteristic{\overline{B}(y_n,r)}(y) = 0$ which shows $\lim_{n \to \infty} \characteristic{\overline{B}(y_n,r)}(y) = 0 = \characteristic{\overline{B}(x,r)}(y)$.  If on the other hand $\characteristic{\overline{B}(x,r)}(y)=1$ it follows trivially that $\limsup_{n \to \infty} \characteristic{\overline{B}(y_n,r)}(y) \leq \characteristic{\overline{B}(x,r)}(y)$.  We conclude that
\begin{align*}
\limsup_{n \to \infty} \characteristic{\overline{B}(y_n,r)} &\leq \characteristic{\overline{B}(x,r)}
\end{align*}
which is to say 
\begin{align*}
1 - \liminf_{n \to \infty} \characteristic{\overline{B}(y_n,r)} &\geq 1 - \characteristic{\overline{B}(x,r)}
\end{align*}
Now we apply Fatou's Lemma to conclude
\begin{align*}
\mu(\overline{B}(x,2r)) - \mu(\overline{B}(x,r)) 
&= \int_{\overline{B}(x, 2r)} (1- \characteristic{\overline{B}(x,r)}) \, d\mu 
\leq \int_{\overline{B}(x, 2r)} (1 - \liminf_{n \to \infty} \characteristic{\overline{B}(y_n,r)}) \, d\mu \\
&\leq \liminf_{n \to \infty} \int_{\overline{B}(x, 2r)} (1 - \characteristic{\overline{B}(y_n,r)}) \, d\mu \\
&\leq \liminf_{n \to \infty} ( \mu(\overline{B}(x, 2r)) - \mu(\overline{B}(y_n,r))) \\
&= \mu(\overline{B}(x, 2r)) - \limsup_{n \to \infty} \mu(\overline{B}(y_n,r)) 
\end{align*}
Since $\mu$ is a Radon measure we know that $\mu(\overline{B}(x, 2r))<\infty$ so we can cancel terms and the claim follows.

For each $r > 0$ define
\begin{align*}
f_r(x) &= \begin{cases}
\frac{\nu(\overline{B}(x,r))}{\mu(\overline{B}(x,r))} & \text{ if $\mu(\overline{B}(x,r))>0$} \\
\infty & \text{ if $\mu(\overline{B}(x,r))=0$} 
\end{cases}
\end{align*}
and observe from the previous claim and the Borel measurability of upper semicontinous functions (TODO: Where do we show this????) we know that $f_r$ is Borel measurable (hence $\mu$-measurable).  Since $D_\mu \nu = \lim_{n \to \infty} f_{1/n}$ $\mu$-almost everywhere it follows that $D_\mu \nu$ is $\mu$-measurable (TODO:  Something subtle about concluding $\mu$-measurability but not Borel measurability; understand it).

TODO: Remove the finiteness assumption
\end{proof}

Now we are ready to show that the derivative defined above is indeed the Radon-Nikodym derivative.
\begin{thm}\label{RadonNikodymDerivativeOfRadonOuterMeasuresReals}Let $\mu$ and $\nu$ be Radon outer measure on $\reals^d$ then for all $\mu$-measurable sets $A$ we have
\begin{align*}
\nu(A) &= \int_A D_\mu \nu \, d\mu
\end{align*}
\end{thm}
\begin{proof}
Let $A$ be a $\mu$-measurable set.  Since Radon outer measures are Borel regular we can find a Borel set $B$ such that $A \subset B$ and $\mu(B) = \mu(A)$.

TODO: Finish
\end{proof}
\subsection{Differentiation of Lipschitz Functions}

\begin{thm}[Rademacher's Theorem]\label{RademachersTheorem}Let $f : \reals^d \to \reals^m$ be a locally Lipschitz map then $f$ is 
Frechet differentiable $\lambda^d$-a.e.  Moreover the Frechet derivative $Df$ is Borel measurable.
\end{thm}
\begin{proof}
Since each coordinate function $f_i : \reals^d \to \reals$ is locally Lipschitz the case of general $m$ follows from the case of $m=1$ by Proposition \ref{FrechetDerivativeProductSpaces}.

The outline of the proof is to show that successively that $f$ has directional derivatives almost surely, is G\^{a}teaux differentiable almost surely and then that it is Frechet differentiable almost surely.  

G\^{a}teaux differentiability in the case of $d=1$ is essentially a corollary of the Fundamental Theorem of Calculus.
\begin{clm}Let $f : \reals \to \reals$ be a locally Lipschitz map then $f$ is differentiable $\lambda$-a.e.
\end{clm}
Since $f$ is locally Lipschitz it is absolutely continuous and therefore of bounded variation (Lemma \ref{AbsoluteContinuityImpliesContinuousBoundedVariation}); we know that $g(t)$ is differentiable $\lambda$-almost everywhere (by Theorem  \ref{BoundedVariationAsDifferenceOfMonotone} write $g(t)$ as a difference of monotone functions then apply the Fundamental Theorem of Calculus \ref{FundamentalTheoremOfCalculus}).  


\begin{clm}Let $v$ be a unit vector in $\reals^d$ then the directional derivative $df(x,v)$ exists for $\lambda^d$-almost all $x$.
\end{clm}
Let $A$ be a rotation of $\reals^d$ such that $v = A e_1$ is the first standard basis vector and define $f_A(x) = f (A x)$.  Note that 
\begin{align*}
\abs{f_A(x) - f_A(y)} &= \abs{f(Ax) - f(Ay)} \leq \Lip(f) \abs{Ax - Ay} = \Lip(f) \abs{x - y} 
\end{align*}
and therefore $f_A$ is Lipschitz.  Also
\begin{align*}
df(x,v) &= \lim_{t \to 0} \frac{f(x+tv) - f(x)}{t} = \lim_{t \to 0} \frac{f(A (A^{-1} x+te_1)) - f(A (A^{-1} x))}{t} = df_A(A^{-1} x, e_1)
\end{align*}
Since $\lambda^d$ is rotation invariant (Corollary \ref{LesbegueRotationInvariance}) it follows that it suffices to consider the case of $v=e_1$. 

By the continuity of $f$ we have
\begin{align*}
\limsup_{t \to 0} \frac{f(x+te_1) - f(x)}{t} &= \lim_{n \to \infty} \sup_{0 < t < 1/n} \frac{f(x+te_1) - f(x)}{t} = \lim_{n \to \infty} \sup_{\substack{0 < t < 1/n \\ t \in \rationals}} \frac{f(x+te_1) - f(x)}{t} 
\end{align*} 
hence by Lemma \ref{LimitsOfMeasurable} we know that $\limsup_{t \to 0} \frac{f(x+te_1) - f(x)}{t}$ is Borel measurable and the same argument shows $\liminf_{t \to 0} \frac{f(x+te_1) - f(x)}{t}$
is Borel measurable as well.  It follows that $A = \lbrace \liminf_{t \to 0} \frac{f(x+te_1) - f(x)}{t} < \limsup_{t \to 0} \frac{f(x+te_1) - f(x)}{t} \rbrace$ is a Borel measurable set (and shows that when the limit exists it is Borel measurable). For a fixed $x \in \reals^d$ we consider the function $g(t) = f(x+te_1)$ which is Lipschitz and therefore differentiable $\lambda$-a.e. by the first claim.   Writing $\reals^d = \reals \times \reals^{d-1}$ and letting $\pi : \reals^d \to \reals^{d-1}$ be the projection on to the last $d-1$ coordinates we see that for every $y \in \reals^{d-1}$, $\lambda(A \cap \lbrace \pi_2(x)=y \rbrace) = 0$.  Now by Tonelli's Theorem \ref{Fubini} we see that
\begin{align*}
\lambda^d(A) &= \int \lambda(A \cap \lbrace \pi_2(x)=y \rbrace) \, \lambda^{d-1}(dy) = 0
\end{align*}

By the previous claim the partial derivatives $\frac{\partial f}{\partial x_1}, \dotsc, \frac{\partial f}{\partial x_d}$ exist $\lambda^d$-almost everywhere and therefore the gradient 
\begin{align*}
\nabla f(x) &= \left( \frac{\partial f}{\partial x_1}, \dotsc, \frac{\partial f}{\partial x_d}\right)
\end{align*}
exists $\lambda^d$-almost everwhere.
\begin{clm} $f$ is almost everywhere G\^{a}teaux differentiable and for $v \in \reals^d$ we have $df(x,v) = \langle v, \nabla f(x) \rangle$.
\end{clm}
Let $g(x)$ be a compactly supported $C^\infty$ function, by change of variables and the translation invariance of Lebesgue measure we get
\begin{align*}
\int \left( \frac{f(x+tv) - f(x)}{t} \right) g(x) \, dx &= t^{-1} \int f(x+tv) g(x) \, dx - t^{-1} \int f(x) g(x) \, dx \\
&= t^{-1} \int f(x) g(x-tv) \, dx - t^{-1} \int f(x) g(x) \, dx \\
&= - \int f(x) \left( \frac{g(x) - g(x -tv)}{t} \right) \, dx\\
\end{align*}
Let $K$ be a compact set containing the support of $g$ so that $\norm{g}_\infty = \sup_{x \in K} \abs{g(x)} < \infty$ and note that by continuity of $f$ we have $\sup_{x \in K} \abs{f(x)} < \infty$
\begin{align*}
\abs{\left(\frac{f(x+tv) - f(x)}{t} \right) g(x)} &\leq t^{-1} \Lip(f) \abs{x+tv - x} \norm{g}_\infty = \Lip(f) \abs{v} \norm{g}_\infty 
\end{align*}
and by the Mean Value Theorem (more specifically the corollary Proposition \ref{LagrangeFormRemainderBanachSpaces})
\begin{align*}
\abs{f(x) \left( \frac{g(x) - g(x-tv)}{t} \right)} &\leq t^{-1} \norm{dg(x,v)}_\infty \abs{x+tv - x} \sup_{x \in K} \abs{f(x)} \\
&= \norm{dg(x,v)}_\infty \sup_{x \in K} \abs{f(x)}< \infty
\end{align*}
and therefore we may use Dominated Convergence, Proposition \ref{PartialDerivativesBanachSpaces} and the previous claim to see
\begin{align*}
\int df(x,v) g(x) \, dx &= \lim_{t \to 0} \int \left( \frac{f(x+tv) - f(x)}{t} \right) g(x) \, dx \\
&= - \lim_{t \to 0} \int  f(x) \left( \frac{g(x) - g(x -tv)}{t} \right) \, dx \\
&= - \int f(x)  dg(x,v) \, dx \\ 
&= - \sum_{i=1}^d v_i \int  f(x) \frac{\partial g}{\partial x_i} \, dx \\ 
&= - \sum_{i=1}^d v_i \int  f(x) \frac{\partial g}{\partial x_i} \, dx \\ 
&= - \sum_{i=1}^d v_i \lim_{t \to 0} \int  f(x) \left(\frac{g(x) - g(x-te_i)}{t} \right) \, dx \\ 
&= \sum_{i=1}^d v_i \lim_{t \to 0} \int \left( \frac{f(x+te_i) - f(x)}{t} \right) g(x) \, dx \\ 
&= \sum_{i=1}^d v_i \int \frac{\partial f}{\partial x} g(x) \, dx \\ 
&= \int \langle v, \nabla f(x) \rangle g(x) \, dx \\ 
\end{align*}
Now if we choose non-negative compactly supported $C^\infty$ functions $g_n$ such that $g_n \uparrow 1$ (e.g. Lemma \ref{ExistenceOfBumpFunction}) by Monotone Convergence we get
\begin{align*}
\int \abs{df(x,v) - \langle v, \nabla f(x) \rangle} \, dx &= \lim_{n \to \infty} \int \abs{df(x,v) - \langle v, \nabla f(x) \rangle} g_n(x) \, dx = 0
\end{align*}
and it follows that $df(x,v) = \langle v, \nabla f(x) \rangle$ $\lambda^n$-a.e. by Lemma \ref{ZeroIntegralImpliesZeroFunction}.

Pick a countable dense subset of the unit sphere $v_1, v_2, \dotsc$ and let 
\begin{align*}
A_n &= \lbrace x \in \reals^d \mid df(x,v_n), \nabla f(x) \text{ exist  and } df(x,v_n) = \nabla f(x) \rbrace
\end{align*}
From the previous claims, we know that $\lambda^d(A^c_n) = 0$ and therefore if we define $A = \cap_{n=1}^\infty A_n$ we have $\lambda^d(A^c) = 0$.  
\begin{clm} $f$ is Frechet differentiable on $A$ and $Df(x) v = \langle v, \nabla f(x) \rangle$ for all $x \in A$ and $v \in \reals^d$.
\end{clm}
Let $x \in A$  and $\epsilon>0$ be given.  Since the unit sphere in $\reals^d$ is compact it is totally bounded (Theorem \ref{CompactnessInMetricSpaces}) hence there
exists an $N>0$ such that for every $v \in \reals^d$ with $\norm{v}=1$ there exists a $v_i$ with $i=1, \dotsc, N$ and $\norm{v-v_i} < \epsilon/2(1+\sqrt{d})\Lip(f)$ (of course the
terms in the denominator are just there so we a nice clean $\epsilon$ at the end; their source will be made clear).  By the previous claim and the finiteness of $N$ there exists a
$\delta>0$ such that 
\begin{align*}
\abs{\frac{f(x+tv_i) - f(x)}{t} - \langle v_i, \nabla f(x) \rangle} &\leq \epsilon/2
\end{align*}
for all $i=1, \dotsc, N$ and $0 < \abs{t} < \delta$.  Now for an arbitrary $h \in \reals^d$ with $0 < \norm{h}<\delta$ we pick $v_i$ with $i=1, \dotsc, N$ and $\norm{v_i - h/\norm{h}} < \epsilon/2(1+\sqrt{d})\Lip(f)$
and compute 
\begin{align*}
&\frac{\abs{f(x + h) - f(x) - \langle h, \nabla f(x) \rangle}}{\norm{h}}  \\
&= \frac{\abs{f(x + h) - f(x + \norm{h} v_i) + f(x+\norm{h} v_i) - f(x) - \langle\norm{h} v_i, \nabla f(x) \rangle - \langle h-\norm{h} v_i, \nabla f(x) \rangle}}{\norm{h}} \\
&\leq \abs{\frac{ f(x+\norm{h} v_i) - f(x)}{\norm{h}} - \langle v_i, \nabla f(x) \rangle } + \frac{\abs{f(x + h) - f(x + \norm{h} v_i)}}{\norm{h}} + \frac{\abs{\langle h-\norm{h} v_i, \nabla f(x) \rangle}}{\norm{h}}\\
&\leq \epsilon/2 + \Lip(f) \norm{h/\norm{h} -  v_i} + \norm{\nabla f(x)} \norm {h/\norm{h}- v_i} \\
&\leq \epsilon/2 + (\Lip(f) + \norm{\nabla f(x)}) \epsilon/2(1+\sqrt{d})\Lip(f) \leq \epsilon
\end{align*}
where in the last line we have used the fact that $\norm{\nabla f(x)} \leq \sqrt{d} \Lip(f)$ which follows from 
\begin{align*}
\abs{\frac{\partial f}{\partial x_i}} &= \lim_{t \to 0} \frac{\abs{f(x+te_i)-f(x)}}{t} \leq \Lip(f)
\end{align*}
\end{proof}

\subsection{The Area Formula}

\begin{defn}Let $L : \reals^d \to \reals^m$ be a linear map then the \emph{Jacobian} of $L$ is defined to be
\begin{align*}
\exdet{A} &= \begin{cases}
\det(L^T L) & \text{if $d \leq m$} \\
\det(L L^T) & \text{if $d > m$} \\
\end{cases}
\end{align*}
\end{defn}

It is very useful to be able to compute the Jacobian of a matrix using the singular value decomposition
\begin{lem}\label{JacobianViaSVD}Let $L : \reals^d \to \reals^m$ be a linear map and let $L=U \Sigma V^T$ be a
singular value decomposition of $L$ then 
\begin{align*}
\exdet{L} = \prod_{j=1}^{d \minop m} \Sigma_{jj} = \det(\Sigma[1:d \minop m, 1:d \minop m])
\end{align*}
\end{lem}
\begin{proof}
TODO
\end{proof}

The reason why we bring up Jacobians of matrices is the following result which is a generalization of Corollary \ref{LebesgueLinearChangeOfVariables}
which shows how Lebesgue measure transforms under linear maps.
\begin{lem}Let $d \leq m$ and $L : \reals^d \to \reals^m$ be a linear map then for all $A \subset \reals^d$
\begin{align*}
\mathcal{H}^d(L(A)) &= \frac{2^d \exdet{L} }{\alpha(d)} \lambda^d(A)
\end{align*}
\end{lem}
\begin{proof}
TODO: the proof in Evans and Gariepy seems a bit more complicated than it has to be (argues with balls and then uses Radon derivatives to generalize); presumably this is because they aren't assuming the change of measure formula for Lebesgue measure under linear change of coordinates????  Can't we argue like this:
Let $L = U \Sigma V^T$ be a singular value decomposition and write 
\begin{align*}
\Sigma = \begin{bmatrix}
\IdentityMatrix_d \\
0_{m-d}
\end{bmatrix}
\hat{\Sigma}
\end{align*}
where $\hat{\Sigma}$ is $d \times d$ diagonal matrix with $\det(\hat{\Sigma}) = \exdet{L}$.  Since 
\begin{align*}
\hat{U} &=  U \begin{bmatrix}
\IdentityMatrix_d \\
0_{m-d}
\end{bmatrix}
\end{align*}
is an isometric embedding of $\reals^d$ into $\reals^m$ we have $\mathcal{H}^d(\hat{U}(A)) =\mathcal{H}^d(A)  = \frac{2^d}{\alpha(d)} \lambda^d(A)$
by Proposition \ref{HausdorffMeasureUnderIsometry} and Theorem \ref{HausdorffEqualsLebesgue}.

Now by Corollary \ref{LebesgueLinearChangeOfVariables}
\begin{align*}
\mathcal{H}^d(L(A)) &= \mathcal{H}^d(\hat{U} \hat{\Sigma} V^T(A)) = \frac{2^d}{\alpha(d)} \lambda^d(\hat{\Sigma} V^T(A)) \\
&= \frac{2^d}{\alpha(d)} \det(\hat{\Sigma}) \det(V^T) \lambda^d(A) = \frac{2^d \exdet{L} }{\alpha(d)} \lambda^d(A)
\end{align*}
\end{proof}

The following quantity is fundamental to the change of variables formula.
\begin{defn}Let $f : \reals^d \to \reals^m$ be function then we call the map $y \mapsto \mathcal{H}^0( A \cap f^{-1} (y))$ the \emph{multiplicity function}.
\end{defn}

We need to establish some basic properties of multiplicity functions before we proceed.

The first lemma is completely elementary.
\begin{lem}\label{SupportOfMultiplicityFunction}The support of the multiplicity function is $f(A)$ and $\characteristic{f(A)} \leq \mathcal{H}^0( A \cap f^{-1} (y))$.
\end{lem}
\begin{proof}
Suppose $y \in f(A)$, then there exists $x \in A$ such that $f(x) = y$ hence $A \cap f^{-1}(y) \neq \emptyset$.  Since $\mathcal{H}^0$ is the counting measure (Proposition \ref{HausdoffMeasureEqualsCounting}) we see that $\mathcal{H}^0(A \cap f^{-1}(y)) \geq 1$ which shows $\characteristic{f(A)}(y) \leq \mathcal{H}^0(A \cap f^{-1}(y)) \geq 1$ for all $y$ (and shows that $f(A)$ is contained in the support of $\mathcal{H}^0(A \cap f^{-1}(y))$).
Now we suppose that $\mathcal{H}^0(A \cap f^{-1}(y)) \neq 0$; since $\mathcal{H}^0$ is counting measure this implies $A \cap f^{-1}(y) \neq 0$ hence there is an $x \in A$ such that $f(x) = y$.
\end{proof}

We also need some measurability properties and estimates for Lipschitz $f$.
\begin{lem}\label{MeasurabilityOfMultiplicityFunction}Let $A \subset \reals^d$ be $\lambda^d$-measurable and let $f : \reals^d \to \reals^m$ with $d \leq m$ be Lipschitz then
\begin{itemize}
\item[(i)] $f(A)$ is $\mathcal{H}^d$-measurable
\item[(ii)] the multiplicity function $\mathcal{H}^0( A \cap f^{-1} (y))$ is a $\mathcal{H}^d$-measurable on $\reals^m$.
\item[(iii)] $\int \mathcal{H}^0( A \cap f^{-1} (y)) \, \mathcal{H}^d(dy) \leq \left( \Lip(f) \right)^d \lambda^d(A)$.
\end{itemize}
\end{lem}
\begin{proof}
We first prove (i).  First we assume that $A$ is bounded.  By the inner regularity of Lebesgue measure we find compact sets $K_n \subset A$ such that $\lambda^d(K_n) \geq \lambda^d(A) - 1/n$.
Since $\lambda^d(A)<\infty$ and finite additivity it follows that $\lambda^d(A \setminus K_n) \leq 1/n$.  Since $f$ is continuous we know that $f(K_n)$ is compact (Theorem \ref{ContinuousImageOfCompact}) and therefore since $\mathcal{H}^d$ is Borel regular (Theorem \ref{BorelRegularityOfHausdorffMeasure}) we know that $f(K_i)$ is $\mathcal{H}^d$-measurable and therefore $\cup_{n=1}^\infty f(K_n)$ is $\mathcal{H}^d$-measurable.  On the other hand by subadditivity of $\mathcal{H}^d$, Proposition \ref{HausdorffMeasureUnderLipschitzMaps} and Theorem \ref{HausdorffEqualsLebesgue} we 
get
\begin{align*}
\mathcal{H}^d\left(f(A) \setminus \cup_{n=1}^\infty f(K_n) \right) &\leq \mathcal{H}^d\left(f\left(A \setminus \cup_{n=1}^\infty K_n\right ) \right) \\
&\leq \left( \Lip(f) \right)^d \mathcal{H}^d \left(A \setminus \cup_{n=1}^\infty K_n\right ) = \left( \Lip(f) \right)^d \lambda^d \left(A \setminus \cup_{n=1}^\infty K_n\right ) =0
\end{align*}
which shows us that $f(A) \setminus \cup_{n=1}^\infty f(K_n)$ is $\mathcal{H}^d$-measurable.  To remove the assumption that $A$ is bounded simply write $f(A) = \cup_{n=1} f(A \cap B(0,n))$ and apply (i) to each $A \cap B(0,n)$.

To see (ii), for each $n \in \naturals$ we write $\reals^d$ as the disjoint union of cubes with sides of length $1/n$:
\begin{align*}
\mathcal{C}_n &= \lbrace (k_1/n, (k_1+1)/n] \times \dotsb \times (k_d/n, (k_d+1)/n] \mid k_1, \dotsc, k_d \in \integers \rbrace
\end{align*}
By (i) for each cube $C \in \mathcal{C}_n$ we have $f(A \cap C)$ is $\mathcal{H}^d$-measurable so it follows that the function
\begin{align*}
g_n(y)  &= \sum_{C \in \mathcal{C}_n} \characteristic{f(A \cap C)}(y)
\end{align*}
is $\mathcal{H}^d$-measurable.  
\begin{clm} $\lim_{n \to \infty} g_n(y) = \mathcal{H}^0( A \cap f^{-1} (y))$
\end{clm}
To see this, first observe that $g_n(y)$ is equal to the number of cubes $C \in \mathcal{C}_n$ such that $f^{-1}(y) \cap A \cap C \neq \emptyset$.  If $\mathcal{H}^0( A \cap f^{-1} (y))=0$ then $A \cap f^{-1} (y)$ is empty and we see that $g_n(y)=0$ for all $n$.  If $\mathcal{H}^0( A \cap f^{-1} (y)) < \infty$ let $x_1, \dotsc, x_{\mathcal{H}^0( A \cap f^{-1} (y)) }$ be an enumeration of $A \cap f^{-1}(A)$ and if $\mathcal{H}^0( A \cap f^{-1} (y)) < \infty$ let $x_1, x_2, \dotsc$ be an enumeration of an arbitrarily chosen countable subset of $A \cap f^{-1} (y)$.  Let $M \in \naturals$ be given then define
\begin{align*}
D_M &= \inf \lbrace \abs{x_i-x_j}  \mid  1 \leq i < j \leq M \minop \mathcal{H}^0( A \cap f^{-1} (y)) \rbrace
\end{align*} 
and observe that $0 < D_M \leq \infty$ (as usual $\inf \emptyset = \infty$).  Now we can pick $N$ large enough so that $\diam(C) < D_M$ for every $n \geq N$ and $C \in \mathcal{C}_n$ (concretely since $\diam(C) = \sqrt{d}/n$ for $C \in \mathcal{C}_n$ we may pick $N = \ceil{\sqrt{d}/ D_M}$ if $D_M<\infty$ or $N=1$ if $D_M=\infty$).  It follows that for every $n \geq N$ we have $g_n(y) = M$.  This shows that $g_n(y) \uparrow \mathcal{H}^0( A \cap f^{-1} (y))$.  

$\mathcal{H}^d$-measurability of $\mathcal{H}^0( A \cap f^{-1} (y))$ follows from Lemma \ref{LimitsOfMeasurable}.

To see (iii) we use Monotone Convergence, Proposition \ref{HausdorffMeasureUnderLipschitzMaps}, Theorem \ref{HausdorffEqualsLebesgue} and countable additivity of Lebesgue measure to compute
\begin{align*}
\int \mathcal{H}^0( A \cap f^{-1} (y)) \, \mathcal{H}^d(dy) &= \lim_{n \to \infty} \int g_n(y) \, \mathcal{H}^d(dy) \\
&=\lim_{n \to \infty} \sum_{C \in \mathcal{C}_n} \mathcal{H}^d(f(A \cap C)) \\
&\leq \lim_{n \to \infty} \left( \Lip(f) \right)^d \sum_{C \in \mathcal{C}_n} \mathcal{H}^d(A \cap C) \\
&= \limsup_{n \to \infty} \left( \Lip(f) \right)^d \sum_{C \in \mathcal{C}_n} \lambda^d(A \cap C) \\
&= \left( \Lip(f) \right)^d \lambda^d(A) \\
\end{align*}
\end{proof}

Next we need an kind of inverse function theorem for Lipschitz maps (TODO: Check the variational analysis literature to see if there are related results; in variational analysis, things are expressed in terms of generalized derivatives which in the Lipschitz case can be defined as limits of the a.e. defined Frechet derivative.  Clarke has a paper on this and I'll bet Rockafellar and Wets has something).

\begin{lem}\label{AreaFormulaLipschitz:FunctionBounds}Let $d \leq m$, $f : \reals^d \to \reals^m$ be a locally Lipschitz map, $t>1$ and
\begin{align*}
A &= \lbrace x \in \reals^d \mid Df(x) \text{ exists and } Jf(x) > 0\rbrace
\end{align*}
There exists a countable set of Borel sets $A_1, A_2, \dotsc$ and symmetric invertible linear map $T_n : \reals^d \to \reals^d$ such that 
\begin{itemize}
\item[(i)] $A = \cup_{n=1}^\infty A_n$
\item[(ii)] $f \mid_{A_n}$ is injective for each $n \in \naturals$
\item[(iii)] For each $n \in \naturals$
\begin{align}\label{eq:AreaFormulaLipschitz:LipschitzConstants}
\Lip((f \mid_{A_n}) \circ T_n^{-1}) \leq t, \qquad \Lip(T_n \circ (f
  \mid_{A_n})^{-1}) \leq t
\end{align}
\item[(iv)] For each $n \in \naturals$
\begin{align}\label{eq:AreaFormulaLipschitz:JacobianBounds}
t^{-d} \abs{\det(T_n)} \leq Jf\mid_{A_n} \leq t^{d} \abs{\det(T_n)} 
\end{align}
\end{itemize}
\end{lem}
\begin{proof}
Pick $\epsilon >0$ small enough so that 
\begin{align*}
\frac{1}{t} + \epsilon < 1 < t - \epsilon
\end{align*}
First recall that every subset of a separable metric space is separable in the relative topology; it is obviously true that every subset of a second countable topological space is second countable in the relative topology (every base of a topology is a base of the relative topology as well) and in metric spaces separability and second countability are equivalent (Lemma \ref{SeparabilitySecondCountabilityMetricSpaces}).  Therefore we may 
select $B$, a countable dense subset of $A$, and $\mathbf{S}$, a countable dense subset of the set of symmetric invertible linear maps of $\reals^d$.  For each $i \in \naturals$, $x \in B$ and $T \in \mathbf{S}$ we let $A(x,T,i)$ be the set of $y \in A \cap B(x, 1/i)$ such that
\begin{align*}
\left( \frac{1}{t} + \epsilon \right) \norm{Tv} &\leq \norm{Df(y) v} \leq (t-\epsilon)\norm{Tv} \text{ for all $v \in \reals^d$} \\
\intertext{and}
\norm{f(z) - f(y) - Df(x) \cdot (z-y)} &\leq \epsilon \norm{T(z-y)} \text{ for all $z \in B(y, 2/i)$}
\end{align*}

By continuity of $f$, the linear maps $T$, $Df(y)$ and $Df(a)$ we may equivalently define as the countable intersection
\begin{align*}
A(x,T,i) &= \cap_{v \in \rationals^d} \lbrace y \mid \left( \frac{1}{t} + \epsilon \right) \norm{Tv} \leq \norm{Df(y) v} \leq (t-\epsilon)\norm{Tv} \rbrace \bigcap \\
&\cap_{z \in B(0, 2/i) \cap \rationals^d} \lbrace y \mid \norm{f(z+y) - f(y) - Df(y) \cdot z} \leq \epsilon \norm{Tz} \rbrace 
\end{align*}
Continuity of $f$ and Borel measurability of $Df$ (Theorem \ref{RademachersTheorem}) then shows that $A(x,T,i)$ is a Borel subset of $\reals^d$.

We now show (iv) holds on the sets $A(x,T,i)$.
\begin{clm}For every $y \in A(x,T,i)$ we have
\begin{align*}
\left( \frac{1}{t} + \epsilon \right)^d \abs{\det T} &\leq Jf(y) \leq (t-\epsilon)^d \abs{\det(T)}
\end{align*}
\end{clm}
Take the polar decomposition $Df(y) = U \circ S$ with $U : \reals^d \to \reals^m$ orthogonal and $S : \reals^d \to \reals^d$ symmetric.  Therefore $Jf(y) = \exdet{Df(y)} = \abs{\det(S)}$ and we
have
\begin{align*}
\left( \frac{1}{t} + \epsilon \right) \norm{Tv} &\leq \norm{Df(y) v} = \norm{(U \circ S) v } =  \norm{S v} \leq (t-\epsilon)\norm{Tv} \text{ for all $v \in \reals^d$}
\end{align*}
and therefore
\begin{align*}
\left( \frac{1}{t} + \epsilon \right) \norm{v} &\leq \norm{(S \circ T^{-1}) v} \leq (t-\epsilon)\norm{v} \text{ for all $v \in \reals^d$}
\end{align*}
From the right hand inequality here we conclude $(S \circ T^{-1}) B(0,1) \subset B(0, t-\epsilon)$ and therefore by Corollary \ref{DeterminantOfProducts} and  Corollary \ref{LebesgueLinearChangeOfVariables}
\begin{align*}
\abs{\det(S)} \abs{\det(T)}^{-1} \lambda^d(B(0,1)) &= \abs{\det(S \circ T^{-1})} \lambda^d(B(0,1)) \\
&= \lambda^d((S \circ T^{-1}) B(0,1)) \leq \lambda^d(B(0, t-\epsilon)) = (t-\epsilon)^d \lambda^d(B(0,1)) 
\end{align*}
Thus 
\begin{align*}
\abs{\det(S)}  &\leq (t-\epsilon)^d \abs{\det(T)}
\end{align*}
From the left hand inequality we conclude $B(0, \frac{1}{t} + \epsilon) \subset (S \circ T^{-1}) B(0,1)$ and by the same reasoning
\begin{align*}
\left( \frac{1}{t} + \epsilon \right)^d \lambda^d(B(0,1)) &= \lambda^d(B(0, \frac{1}{t} + \epsilon) ) \leq \lambda^d((S \circ T^{-1}) B(0,1)) \\
&= \abs{\det(S \circ T^{-1})} \lambda^d(B(0,1)) = \abs{\det(S)} \abs{\det(T)}^{-1} \lambda^d(B(0,1))
\end{align*}
which shows $\left( \frac{1}{t} + \epsilon \right)^d \abs{\det T} \leq \abs{\det(S)}$.  

Now we show (i)
\begin{clm} $\cup_{i=1}^\infty \cup_{T \in \mathbf{S}} \cup_{x \in B} A(x,T,i) = A$.
\end{clm}
Note that for any linear map $T$ we have 
\begin{align*}
\Lip(T) = \sup_{x \neq y} \frac{\norm{Ty - Tx}}{\norm{y-x} } = \sup_{x \neq y}\frac{\norm{T(y - x)}}{\norm{y-x}} = \sup_{v \neq 0}\frac{\norm{Tv}}{\norm{v}} = \norm{T}
\end{align*}
Therefore if $S, S_1, S_2, \dotsc$ are invertible linear maps such that $S_n \to S$ in the operator norm then from 
\begin{align*}
\abs{\norm{ S_n \circ S^{-1}} - 1} &\leq \abs{\norm{ S_n \circ S^{-1} - \IdentityMatrix} + \norm{\IdentityMatrix} - 1} = \norm{S_n \circ S^{-1} - \IdentityMatrix}  \leq \norm{S^{-1}} \norm{S_n - S}
\end{align*}
it follows that
$\Lip(S_n \circ S^{-1} ) \to 1$.  Similarly since $S_n \to S$ implies $\norm{S_n} \to \norm{S} \neq 0$ we get
\begin{align*}
\abs{\norm{ S \circ S_n^{-1}} - 1} &\leq \norm{S_n^{-1}} \norm{S - S_n}
\end{align*}
from which it follows that $\Lip(S \circ S_n^{-1} ) \to 1$.  Let $x \in A$ and write the polar decomposition $Df(x) = U \circ S$.  By the above argument and the density of $\mathbf{S}$ we may
find a $T \in \mathbf{S}$ such that 
\begin{align*}
\Lip(T \circ S^{-1}) &\leq \left( \frac{1}{t} + \epsilon \right)^{-1}, \qquad \Lip(S \circ T^{-1}) \leq t-\epsilon
\end{align*}
which gives us
\begin{align*}
\left( \frac{1}{t} + \epsilon \right) \norm{Tv} &\leq \left( \frac{1}{t} + \epsilon \right) \Lip(T \circ S^{-1}) \norm{S v} \leq \norm{Sv} = \norm{Df(x) v}
\end{align*}
and
\begin{align*}
\norm{Df(x) v} &= \norm{Sv} \leq \Lip(S \circ T^{-1}) \norm{T v} \leq (t -\epsilon) \norm{Tv}
\end{align*}
By definition of the Frechet derivative pick $i \in \naturals$ large enough so that
\begin{align*}
\abs{f(y) - f(x) - Df(x) \cdot (y-x)} &\leq \frac{\epsilon}{\Lip(T^{-1})} \norm{y-x} \leq \epsilon\norm{T(y-x)} \text{ for all $y \in B(x,2/i)$}
\end{align*}
Lastly by density of $B$ in $A$ we may pick $z \in B$ such $\norm{z - y} < 1/i$.  Now note that we have all three conditions that show $x \in A(z,T,i)$.

To prove (ii) and (iii) we need the following estimates
\begin{clm} For all $y,z \in A(x,T,i)$
\begin{align*} 
\frac{1}{t} \norm{T(z-y)}  &\leq \norm{f(z) - f(y)}  \leq  t\norm{T(z-y)} 
\end{align*}
\end{clm}
From the definition of $A(x,T,i)$ if we suppose $y \in A(x,T,i)$ and $z \in B(y,2/i)$ then
\begin{align*}
\norm{f(z) - f(y)} &\leq \norm{f(z) - f(y) - Df(y) \cdot (z-y)} + \norm{Df(y) \cdot (z-y)} \\
&\leq \epsilon \norm{T(z-y)} + (t - \epsilon) \norm{T(z-y)} \\
&= t\norm{T(z-y)}
\end{align*}
and
\begin{align*}
\left( \frac{1}{t} + \epsilon \right) \norm{T(z-y)} &\leq \norm{Df(y) \cdot (z-y)} \\
&\leq \norm{f(z) - f(y) - Df(y) \cdot (z-y)} + \norm{f(z) - f(y)} \\
&\leq \epsilon \norm{T(z-y)} + \norm{f(z) - f(y)} 
\end{align*}
which we summarize as
\begin{align*}
\frac{1}{t} \norm{T(z-y)}  &\leq \norm{f(z) - f(y)}  \leq  t\norm{T(z-y)}
\end{align*}
However by construction $A(x,T,i) \subset B(x,1/i) \subset B(y,2/i)$ for all $y \in A(x,T,i)$ and therefore 
\begin{align*}
\frac{1}{t} \norm{T(z-y)}  &\leq \norm{f(z) - f(y)}  \leq  t\norm{T(z-y)} \text{ for all $y,z \in A(x,T,i)$}
\end{align*}

To see (ii) simply note that injectivity $f \mid_{A(x,T,i)}$ follows
immediately from the left hand inequality of the previous claim.  As for (iii) if $T^{-1} y, T^{-1} z \in A(x,T,i)$ then by 
the right hand inequality of the previous claim
\begin{align*}
\frac{\norm{f(T^{-1} z) - f(T^{-1} y)}}{\norm{z - y}} \leq  t\frac{\norm{T(T^{-1} z-T^{-1} y)}}{\norm{z - y}} = t
\end{align*}
so $\Lip(f \mid_{A(x,T,i)} \circ T^{-1}) \leq t$.   If $y,z \in f(A(x,T,i))$ then by the left hand inequality of the previous claim
\begin{align*}
\frac{\norm{T (f^{-1}(z) - T(f^{-1} y)}}{\norm{z - y}} \leq  t\frac{\norm{f(f^{-1} z)-f(f^{-1} y)}}{\norm{z - y}} = t
\end{align*}
so $\Lip(T \circ (f\mid_{A(x,T,i)})^{-1}) \leq t$
\end{proof}

\begin{thm}[Area Formula]\label{AreaFormulaLipschitz}Let $d \leq m$ and $f : \reals^d \to \reals^m$ be Lipschitz.  For each $\lambda^d$-measurable set $A \subset \reals^d$
\begin{align*}
\int_A Jf(x) \, \lambda^d(dx) &= \int_{\reals^m} \mathcal{H}^0(A \cap f^{-1}(y)) \, \mathcal{H}^d(dy)
\end{align*}
\end{thm}
\begin{proof}
Let
\begin{align*}
B &= \lbrace x \in \reals^d \mid Df(x) \text{ exists} \rbrace
\end{align*}
By Rademacher's Theorem \ref{RademachersTheorem} we know that $\lambda^d(B^c) = 0$ and therefore
$\int_{A} Jf(x) \, \lambda^d(dx) = \int_{A \cap B} Jf(x) \lambda^d(dx)$.  Moreover we know since $\mathcal{H}^0$ is counting measure (Proposition \ref{HausdoffMeasureEqualsCounting})
and by Lemma \ref{MeasurabilityOfMultiplicityFunction}
\begin{align*}
&\int_{\reals^m} \mathcal{H}^0(A \cap B \cap f^{-1}(y)) \, \mathcal{H}^d(dy) \\
&\leq \int_{\reals^m} \mathcal{H}^0(A \cap f^{-1}(y)) \, \mathcal{H}^d(dy) \\
&= \int_{\reals^m} \mathcal{H}^0(A \cap B \cap f^{-1}(y)) \, \mathcal{H}^d(dy) + \int_{\reals^m} \mathcal{H}^0(A \cap B^c \cap f^{-1}(y)) \, \mathcal{H}^d(dy) \\
&\leq \int_{\reals^m} \mathcal{H}^0(A \cap B \cap f^{-1}(y)) \, \mathcal{H}^d(dy) +\left( \Lip(f) \right)^d \lambda^d(A \cap B^c) \\
&=\int_{\reals^m} \mathcal{H}^0(A \cap B \cap f^{-1}(y)) \, \mathcal{H}^d(dy) 
\end{align*} 
Therefore $\int_{\reals^m} \mathcal{H}^0(A \cap f^{-1}(y)) \, \mathcal{H}^d(dy) = \int_{\reals^m} \mathcal{H}^0(A \cap B \cap f^{-1}(y)) \, \mathcal{H}^d(dy)$ and it suffices to assume that $Df$ (hence $Jf$) exists everywhere on $A$.

We first assume that $A \subset \lbrace Jf > 0 \rbrace$.  Let $t > 1$ be arbitrary and choose Borel sets
$A_1, A_2, \dotsc$ and symmetric invertible linear maps $T_1, T_2, \dotsc$ as in Lemma \ref{AreaFormulaLipschitz:FunctionBounds}.
Let $\mathcal{C}_n$ be as in the proof of Lemma \ref{MeasurabilityOfMultiplicityFunction} so that the sets
$A \cap A_i \cap C$ for $i \in \naturals$ and $C \in \mathcal{C}_n$ are disjoint and for any $n \in \naturals$
\begin{align*}
A &= \cup_{i=1}^\infty \cup_{C \in \mathcal{C}_n} A \cap A_i \cap C
\end{align*}

Since $\diam(A_i \cap C) \leq \diam(C) \to 0$ as $n \to \infty$ by the same argument as in Lemma \ref{MeasurabilityOfMultiplicityFunction} we conclude
\begin{align}\label{eq:AreaFormula:SumOverCubes}
\lim_{n \to \infty} \sum_{i=1}^\infty \sum_{C \in \mathcal{C}_n} \mathcal{H}^d(A \cap A_i \cap C) = \int \mathcal{H}^0(A \cap f^{-1}(y)) \, \mathcal{H}^d(dy)
\end{align}
TODO: Maybe factor that argument out into a separate lemma.

By choice of $A_i$ and \eqref{eq:AreaFormulaLipschitz:LipschitzConstants} of Lemma \ref{AreaFormulaLipschitz:FunctionBounds}, Proposition \ref{HausdorffMeasureUnderLipschitzMaps} 
and Theorem \ref{HausdorffEqualsLebesgue} we have
\begin{align*}
\mathcal{H}^d(f(A \cap A_i \cap C)) &= \mathcal{H}^d((f\mid_{A_i} \circ T_i^{-1} \circ T_i)(A \cap A_i \cap C)) \leq t^d \mathcal{H}^d(T_i(A \cap A_i \cap C)) \\
&=  t^d \lambda^d(T_i(A \cap A_i \cap C))
\end{align*}
and similarly 
\begin{align*}
\lambda^d(T_i(A \cap A_i \cap C) &= \mathcal{H}^d((T_i \circ (f\mid_{A_i})^{-1} \circ f\mid_{A_i})(A \cap A_i \cap C) \leq t^d \mathcal{H}^d(f(A \cap A_i \cap C)
\end{align*}
Using these two facts together with \eqref{eq:AreaFormulaLipschitz:JacobianBounds} of Lemma \ref{AreaFormulaLipschitz:FunctionBounds} we get
\begin{align*}
t^{-2d} \mathcal{H}^d(f(A \cap A_i \cap C)) &\leq t^{-d} \lambda^d(T_i(A \cap A_i \cap C)) \\
&= t^{-d} \abs{\det(T_i)} \lambda^d(A \cap A_i \cap C) \\
&\leq \int_{A \cap A_i \cap C} Jf(x) \, \lambda^d(dx) \\
&\leq t^{d} \abs{\det(T_i)} \lambda^d(A \cap A_i \cap C) \\
&= t^{d} \lambda^d(T_i(A \cap A_i \cap C)) \\
&\leq t^{2d} \mathcal{H}^d(f(A \cap A_i \cap C)
\end{align*}
Summing over $i \in \naturals$ and $C \in \mathcal{C}_n$ yields
\begin{align*}
t^{-2d} \sum_{i=1}^\infty \sum_{C \in \mathcal{C}_n} \mathcal{H}^d(f(A \cap A_i \cap C)) 
&\leq \sum_{i=1}^\infty \sum_{C \in \mathcal{C}_n} \int_{A \cap A_i \cap C} Jf(x) \, \lambda^d(dx) \\
&= \int_A  Jf(x) \, \lambda^d(dx) \\
&\leq t^{2d} \sum_{i=1}^\infty \sum_{C \in \mathcal{C}_n}\mathcal{H}^d(f(A \cap A_i \cap C) 
\end{align*}
Now let $n \to \infty$ and use \eqref{eq:AreaFormula:SumOverCubes} then let $t \downarrow 1$.

Now we assume that $A \subset \lbrace Jf = 0 \rbrace$.  Somewhat surprisingly we can reduce this case to case in which $A \subset \lbrace Jf = 0 \rbrace$.
In order to do that we let $0 < \epsilon < 1$ be arbitrary and define $g : \reals^d \to \reals^m \times \reals^d$ by $g(x) = (f(x), \epsilon x)$.  If we let $\pi : \reals^m \times \reals^d \to \reals^d$ be the projection on the second coordinate then if follows that $f = \pi \circ g$.  Furthermore since 
\begin{align*}
\norm{g(y) - g(x)} &= \sqrt{\norm{f(y) - f(x)}^2 + \epsilon^2\norm{y-x}^2} \leq \sqrt{(\Lip(f))^2\norm{y - x}^2 + \epsilon^2\norm{y-x}^2} \\
&= \sqrt{(\Lip(f))^2+ \epsilon^2} \norm{y-x}
\end{align*}
we see that $g$ is Lipschitz.  It also follows from Proposition \ref{PartialDerivativesBanachSpaces} that $g$ is Frechet differentiable whenever $f$ is Frechet differentiable and moreover
\begin{align*}
Dg(x) &= \begin{bmatrix}
Df(x) \\
\epsilon \IdentityMatrix_d
\end{bmatrix}
\end{align*}
From the Cauchy-Binet Theorem \ref{CauchyBinetTheorem} we see that
\begin{align*}
(Jg(x))^2 &= \sum_{\psi \in Psi} \det((Dg(x))_\psi)^2
\end{align*}
where $\Psi$ is the set of all strictly increasing functions $\lbrace 1, \dotsc, d\rbrace$ to $\lbrace 1, \dotsc, m+d \rbrace$.  
To get a lower bound for $Jg(x)$ note the function $\psi(i) = i+m$ is in $\Psi$ and for this $\psi$ we have $(Dg(x))_\psi = \epsilon \IdentityMatrix$ and it follows that
$Jg(x) \geq \epsilon^{d}>0$.  For an upper bound on $Jg(x)$ first note that by a second application of Cauchy-Binet we have
\begin{align*}
(Jg(x))^2 &= \sum_{\psi \in Psi} \det((Dg(x))_\psi)^2 = \sum_{\substack{\psi \in Psi \\ \psi(d) \leq m}} \det((Dg(x))_\psi)^2 + \sum_{\substack{\psi \in Psi \\ \psi(d) > m}} \det((Dg(x))_\psi)^2 \\
&= \sum_{\substack{\psi \in Psi \\ \psi(d) \leq m}} \det((Df(x))_\psi)^2 + \sum_{\substack{\psi \in Psi \\ \psi(d) > m}} \det((Dg(x))_\psi)^2 \\
&= (Jf(x))^2 + \sum_{\substack{\psi \in Psi \\ \psi(d) > m}} \det((Dg(x))_\psi)^2 
\end{align*}

\begin{clm}There is a constant $C$ independent of $x \in A$ such that $Jg(x) \leq \epsilon C$.
\end{clm}
By the above calculation using the Cauchy-Binet Theorem and the finiteness of $\Psi$ it suffices to show that $\abs{\det((Dg(x))_\psi}  \leq \epsilon C$ for every $\psi \in \Psi$ with $\psi(d)>m$
and some $C$ independent of $x \in A$.   We need a few simple facts about determinants and operator norms of matrices. Note for a general $d \times d$ matrix $M$ we have $\abs{\det(M)} \leq \norm{M}^d$; for example this follows from taking a Singular Value Decomposition (Theorem \ref{SingularValueDecomposition}) $M = U \Sigma V^T$, recalling that $\Sigma_{11} = \norm{M}$ and noting that 
\begin{align*}
\abs{\det(M)} &= \det(\Sigma) = \prod_{i=1}^d \Sigma_{ii} \leq \Sigma_{11}^d = \norm{M}^d
\end{align*}
Also $M$ is a $d \times m$ matrix and $N$ is a $c \times m$ matrix then $\norm{\begin{bmatrix} M \\ N \end{bmatrix}} \leq \norm{M} + \norm{N}$.
For any $\psi$ with $\psi(d) > m$ let $k \geq 1$ be largest integer such that $\psi(d-k+1) > m$ then we can write $(Dg(x))_\psi = \epsilon^k \pi \circ \norm{\begin{bmatrix} Df(x) \\ \IdentityMatrix\end{bmatrix}}$ for an $m \times d$ projection matrix $P$.  Using the linear algebra facts mentioned, the fact that $0 < \epsilon \leq 1$, $\norm{\pi} =1$ and $\norm{Df(x)} \leq \Lip(f)$ we get the bound
\begin{align*}
\abs{\det((Dg(x))_\psi)} &\leq \norm{(Dg(x))_\psi} = \epsilon^{k} \norm{P \begin{bmatrix} Df(x) \\ \IdentityMatrix\end{bmatrix}} \\
&\leq \epsilon \norm{P} ( \norm{Df(x)} + \norm{\IdentityMatrix} ) \leq \epsilon(\Lip(f) + 1)
\end{align*}

Since $\pi : \reals^m \times \reals^d \to \reals^d$ is a projection operator it is Lipschitz and $\Lip(\pi) = \norm{\pi} = 1$.  Thus from Proposition \ref{HausdorffMeasureUnderLipschitzMaps},
Lemma \ref{SupportOfMultiplicityFunction}
and the part of this theorem already proven
\begin{align*}
\mathcal{H}^d(f(A)) &= \mathcal{H}^d(\pi(g(A))) \leq \mathcal{H}^d(g(A)) \\
&\leq \int \mathcal{H}^0(A \cap g^{-1}(y,z)) \, \mathcal{H}^d(dy,dz) \\
&=\int_A Jg(x) \, \lambda^d(dx) \\
&\leq \epsilon C \lambda^d(A)
\end{align*}
Since $\epsilon>0$ was arbitrary we let $\epsilon \to 0$ and conclude $\mathcal{H}^d(f(A))=0$.  Since the support of $\mathcal{H}^0(A \cap f^{-1}(y))$ equals $f(A)$ (Lemma \ref{SupportOfMultiplicityFunction}) we also get
\begin{align*}
\int \mathcal{H}^0(A \cap f^{-1}(y)) \, \mathcal{H}^d(dy) &= 0 = \int_A Jf(x) \, \lambda^d(dx)
\end{align*}

The proof is finished by taking a general $A$, writing $A = (A \cap \lbrace Jf>0 \rbrace) \cup (A \cap \lbrace Jf = 0 \rbrace)$ and using the fact that both $\int_A Jf(x) \, \lambda^d(dx)$ and
$\int \mathcal{H}^0(A \cap f^{-1}(y)) \, \mathcal{H}^d(dy)$ are finitely additive over disjoint sets.
\end{proof}

\subsection{The Coarea Formula}

The following generalizes the Fundamental Theorem of Calculus \ref{FundamentalTheoremOfCalculus} to Radon measures on $\reals^d$.
\begin{thm}\label{FundamentalTheoremOfCalculusRadonOuterMeasuresReals}Let $\mu$ be a Radon measure on $\reals^d$ and $f$ be locally $\mu$ integrable then 
\begin{align*}
\lim_{r \to 0} \frac{1}{\mu(B(x,r))} \int_{B(x,r)} f(y) \, \mu(dy) &= f(x)
\end{align*}
for $\mu$-almost every $x \in \reals^d$.
\end{thm}
\begin{proof}
For every Borel set $B \subset \reals^d$ let $\nu_{\pm}(B) = \int_B f_{\pm}(y) \mu(dy)$.  Then create the outer measures 
\begin{align*}
\nu_{\pm}(A) &= \inf \lbrace \nu_{\pm}(B) \mid B \subset \reals^d \text{ Borel} \rbrace
\end{align*}
These are Radon outer measures (Borel regularity is immediate from the construction and local finiteness follows from the local integrability of $f$).  It is also clear that each of $\nu_{\pm}$ is absolutely continuous with respect to $\mu$ and therefore we have by the Radon-Nikodym Theorem \ref{RadonNikodymDerivativeOfRadonOuterMeasuresReals}
\begin{align*}
\nu_{\pm}(A) &= \int_A D_\mu \nu_{\pm} \, d\mu = \int_A f_{\pm} \, d\mu \text{ for all measurable $A \subset \reals^d$}
\end{align*}
From this it follows that $D_\mu \nu_{\pm} = f_{\pm}$.  Therefore by the definition of $D_\mu \nu_{\pm}$ we get for $\lambda^d$ almost every $x \in \reals^d$
\begin{align*}
\lim_{r \to 0} \frac{1}{\mu(B(x,r))} \int_{B(x,r)} f(y) \, \mu(dy) &= \lim_{r \to 0} \frac{1}{\mu(B(x,r))} \left(\nu_+(B(x,r)) - \nu_-(B(x,r)) \right) \\
&=\lim_{r \to 0}  (D_\mu \nu_+(B(x,r)) - D_\mu \nu_-(B(x,r))) \\
&=f_+(x) - f_-(x) = f(x) 
\end{align*}
\end{proof}

\begin{cor}\label{MeasureTheoreticInteriorAlmostEverywhere} Let $A \subset \reals^d$ be measurable then 
\begin{align*}
\lim_{r \to 0} \frac{\lambda^d(A \cap B(x,r))}{\lambda^d(B(x,r))} &= 1 \text{ for $\lambda^d$ almost every $x \in A$}
\intertext{and}
\lim_{r \to 0} \frac{\lambda^d(A \cap B(x,r))}{\lambda^d(B(x,r))} &= 0 \text{ for $\lambda^d$ almost every $x \in \reals^d \setminus A$}
\end{align*}
\end{cor}
\begin{proof}
Simply apply Theorem \ref{FundamentalTheoremOfCalculusRadonOuterMeasuresReals} to the measurable function $\characteristic{A}$ and the Radon measure $\lambda^d$.
\end{proof}

\begin{lem}\label{DifferentiationOfInversesLipschitz}
\begin{itemize}
\item[(i)] Let $f : \reals^d \to \reals^m$ be locally Lipschitz and $Z = \lbrace x \in \reals^d \mid f(x) = 0 \rbrace$ then $Df(x) = 0$ $\lambda^d$-a.e. on $Z$.
\item[(ii)] Let $f,g : \reals^d \to \reals^d$ are locally Lipschitz and $Y = \lbrace x \in \reals^d \mid g(f(x)) = 0 \rbrace$ then $Dg(f(x)) Df(x) = \IdentityMatrix$ $\lambda^d$-a.e. on $Y$.
\end{itemize}
\end{lem}
\begin{proof}
TODO: Can we derive the first part from Lemma \ref{DifferentiationOnNullSets}?  The only trick seems to be use of separability to get a countable intersection of null sets.  Below we give the Evans-Gariepy proof.

To see (i), by Rademacher's Theorem and Corollary \ref{MeasureTheoreticInteriorAlmostEverywhere} we know that $\lambda^d$-almost everywhere on $Z$, $Df(x)$ exists and 
\begin{align*}
\lim_{r \to 0} \frac{\lambda^d(A \cap B(x,r))}{B(x,r)} &= 1
\end{align*}
(i.e. $x$ is in the measure theoretic interior of $Z$).  Suppose that $x$ is in the measure theoretic interior of $Z$ and $Df(x) \neq 0$ exists; we will derive a contradiction.  By definition of $Df(x)$   there exists $\delta>0$ such that for all $w \in \reals^d$ with $0 < \norm{w} < \delta$ we have
\begin{align*}
\abs{f(x + w) - \langle w, Df(x) \rangle} &\leq \norm{w} \norm{Df(x)}{4} 
\end{align*}
Let $S$ be the unit vectors $v$ such that the angle between $v$ and $Df(x)$ is between $-60^\circ$ and $60^\circ$ (i.e. $\langle v, Df(x) \rangle \geq \norm{Df(x)}/2$).  It follows that
for any $0 < r < \delta$,
\begin{align*}
\abs{f(x+rv)} &\geq r \abs{\langle v, Df(x) \rangle} - \abs{f(x + rv) - r \langle v, Df(x) \rangle} &\geq r \norm{Df(x)}{2} - r \norm{Df(x)}{4} = r \norm{Df(x)}{4} > 0
\end{align*}
and therefore $r S \cap Z = \emptyset$ for all $0 < r < \delta$.  This yields the bound
\begin{align*}
\lim_{r \to 0} \frac{\lambda^d(Z \cap B(x,r))}{\lambda^d(B(x,r))} &\leq \lim_{r \to 0} \frac{\lambda^d(B(x,r) \setminus rS)}{\lambda^d(B(x,r))} < 1
\end{align*}
which contradicts the fact that $x$ is in the measure theoretic interior of $Z$.

To see (ii) we reduce to the case of (i) by considering $g \circ f - \IdentityMatrix$ but we have to be careful to track a few $\lambda^d$ null sets.  We consider the points in $Y$ where $Dg(f(x)) Df(x)$ is defined; let
\begin{align*}
X &= Y \cap \domain{Df} \cap f^{-1}(\domain{Dg})
\end{align*}
\begin{clm}$Y \setminus X \subset (\reals^d \setminus \domain{Df}) \cup g(\reals^d \setminus \domain{Dg})$
\end{clm}
If $x \in Y \setminus X$ then either $x \notin \domain{Df}$ or $x \notin f^{-1}(\domain{Dg})$.  Since the first case is equivalent to $x \in \reals^d \setminus \domain{Df}$ it suffices to assume 
$x \in Y \setminus f^{-1}(\domain{Dg})$.   In this case $f(x) \notin \domain{Dg}$ hence $g(f(x)) \in g(\reals^d \setminus \domain{Dg})$; since $x \in Y$ we know that $x = g(f(x))$ and the claim follows.

From the claim and Rademacher's Theorem applied to $f$ and $g$ we know that $\lambda^d(Y \setminus X) = 0$.  Now if we assume $x \in X$ then, by definition of $X$, $Dg(f(x))$ and $Df(x)$ exist and
therefore by the Chain Rule (Proposition \ref{ChainRuleBanachSpaces}) we know that $D(g \circ f)(x)$ exists and $D(g \circ f)(x) = Dg(f(x)) Df(x)$.  Now we simply note that $Y$ is contained in the zero set of $g \circ f - \IdentityMatrix$ and apply (i).
\end{proof}

\begin{lem}\label{ExtensionOfLipschitzMaps}Let $A \subset \reals^d$ and $f : A \to \reals^m$ be a Lipschitz map then there exists a Lipschitz map $\overline{f} : \reals^d \to \reals^m$ such that
\begin{itemize}
\item[(i)] $\overline{f} \mid_A = f$
\item[(ii)] $\Lip(\overline{f}) \leq \sqrt{m} \Lip(f)$
\end{itemize}
\end{lem}
\begin{proof}
First assume that $m=1$.  Define 
\begin{align*}
\overline{f}(x) &= \inf_{a \in A} (f(a) + \Lip(f) \norm{x - a})
\end{align*}
\begin{clm} If $x \in A$ then $\overline{f}(x) = f(x)$
\end{clm}
Note that for all $a \in A$
\begin{align*}
f(x) - f(a) &\leq \abs{f(x) - f(a)} \leq \Lip(f) \norm{x-a}
\end{align*}
hence $f(x) \leq f(a) + \Lip(f) \norm{x-a}$ and taking the infimum over all $a \in A$ we conclude $f(x) \leq \overline{f}(x)$.  On the other hand since $x \in A$ it is immediate from the definition
of $\overline{f}$ that $\overline{f} \leq f(x)$.

\begin{clm} $\Lip(\overline{f}) = \Lip(f)$
\end{clm}
Given $x,y \in \reals^d$ we get
\begin{align*}
\overline{f}(x) &= \inf_{a \in A} (f(a) + \Lip(f) \norm{x-a}) \leq \inf_{a \in A} (f(a) + \Lip(f)\norm{y-a} + \Lip(f) \norm{x-y}) \\
&= \overline{f}(y) + \Lip(f) \norm{x-y}
\end{align*}
By symmetry we also have $\overline{f}(y) \leq \overline{f}(x) + \Lip(f) \norm{x-y}$ and it follows that $\abs{f(x) -f(y)} \leq \Lip(f) \norm{x-y}$ hence $\Lip(\overline{f}) \leq \Lip(f)$.  The fact that $\Lip(f) \leq \Lip(\overline{f})$ follows from the fact that $\overline{f} \mid_A = f$.

Now to handle the case of general $m$ let $f = (f_1, \dotsc, f_m)$ and construct $\overline{f} = (\overline{f}_1, \dotsc, \overline{f}_m)$ and note that
\begin{align*}
\norm{\overline{f}(x) - \overline{f}(y)}^2 &= \sum_{j=1}^m \abs{\overline{f}_j(x) - \overline{f}_j(y)}^2 \leq m (\Lip(f))^2 \norm{x-y}
\end{align*}
\end{proof}

The first lemma proves the coarea formula for linear maps.

\begin{lem}\label{CoareaFormulaLinearMaps}Let $d \geq m$, $L : \reals^d \to \reals^m$ be linear and $A \subset \reals^d$ be $\lambda^d$-measurable then
\begin{itemize}
\item[(i)] The map $y \to \mathcal{H}^{d-m}(A \cap L^{-1}(y))$ is $\lambda^m$-measurable
\item[(ii)] $\int \mathcal{H}^{d-m}(A \cap L^{-1}(y)) \, \lambda^m(dy) = \frac{2^{d-m}} {\alpha(d-m)}\exdet{L} \lambda^d(A)$.
\end{itemize}
\end{lem}
\begin{proof}
First suppose that $\dim(L) < m$.  In this case $\lambda^m(L(\reals^d)) = 0$ and  $A \cap L^{-1}(y) = \emptyset$ for all $y \in \reals^m \setminus L(\reals^d)$.  From this it follows that
$\mathcal{H}^{d-m}(A \cap L^{-1}(y)) = 0$ $\lambda^m$-almost everywhere which implies $\mathcal{H}^{d-m}(A \cap L^{-1}(y))$ is $\lambda^m$-measurable and $\int \mathcal{H}^{d-m}(A \cap L^{-1}(y)) \, \lambda^m(dy) = 0$.  On the other hand $\exdet{L}^2 = \det(L \circ L^T) = 0$ and (ii) follows.  

We begin by handling the case of the orthogonal projection $P(x_1, \dotsc, x_d) = (x_1, \dotsc, x_m)$; i.e. if we write $\reals^d = \reals^m \times \reals^{d-m}$ then $P$ is just the first coordinate projection.  In this case $A \cap P^{-1}(y)$ is just $\lbrace y \rbrace \times A_y$ where $A_y$ is 
the section $\lbrace x \in \reals^{d-m} \mid (x,y) \in A \rbrace$.  Furthermore for any $y \in \reals^m$ the map $x \mapsto (y,x)$ from $\reals^{d-m} \to \reals^d$ is an isometric embedding hence
by Proposition \ref{HausdorffMeasureUnderIsometry} (TODO: the $\sigma$-algebra of $\lambda^d$-measurable sets is actually bigger than the product $\sigma$-algebra yet we have only proven Fubini for the product $\sigma$-algebra; we must address the completion!  Note that sections of $\lambda^d$-measurable sets are only almost surely measurable; as an example let $A$ be a non-measurable set in $\reals$ and then consider $\lbrace 0 \rbrace \times A \subset \reals^2$ it is a Lebesgue null set hence $\lambda^2$-measurable but has a non-measurable section)  and Theorem \ref{HausdorffEqualsLebesgue} we know that 
\begin{align*}
\mathcal{H}^{d-m}(A \cap P^{-1}(y)) &= \mathcal{H}^{d-m}(\lbrace y \rbrace \times A_y) = \mathcal{H}^{d-m}(A_y) = \frac{2^{d-m}}{\alpha(d-m)} \lambda^{d-m}(A_y) 
\end{align*}
The $\lambda^m$-measurability of $\mathcal{H}^{d-m}(A \cap P^{-1}(y))$ follows from Lemma \ref{MeasurableSections} and Tonelli's Theorem shows us that
\begin{align*}
\int \mathcal{H}^{d-m}(A \cap P^{-1}(y)) \, \lambda^m(dy) &= 2^{d-m} \alpha(d-m) \int \lambda^{d-m}(A_y) \, \lambda^m(dy) =  \frac{2^{d-m}} {\alpha(d-m)} \lambda^d(A)
\end{align*}
Since $\exdet{P} = 1$ in this case the result is proven.

Now we will handle the case of a general linear map $L$ in which $\dim(L)  = m$.  Take the singular value decomposition and use the fact that $d \geq m$ to write $L = U \Sigma P V^T$ where
$U$ and $V$ are orthogonal, $P$ is the projection matrix above and $\Sigma$ is the $m \times m$ diagonal matrix of singular values (in particular $\det(\Sigma) = \exdet{L}$ by Lemma \ref{JacobianViaSVD}).  
Now using Corollary \ref{LebesgueLinearChangeOfVariables}, the case of the current lemma for the matrix $P$ and Proposition \ref{HausdorffMeasureUnderIsometry}
\begin{align*}
\frac{2^{d-m}} {\alpha(d-m)} \lambda^d(A) &= \frac{2^{d-m}} {\alpha(d-m)} \lambda^d( V^T (A)) \\
&= \int \mathcal{H}^{d-m}(V^T(A) \cap P^{-1}(y)) \, \lambda^m(dy) \\
&= \int \mathcal{H}^{d-m}(V^T(A \cap (V \circ P^{-1})(y))) \, \lambda^m(dy) \\
&= \int \mathcal{H}^{d-m}(A \cap (V \circ P^{-1})(y)) \, \lambda^m(dy) 
\end{align*}
Now we use this equality and the Area Formula with the transformation $U \circ \Sigma : \reals^m \to \reals^m$ and the function $\mathcal{H}^{d-m} (A \cap (V \circ P^{-1})(y))$ to calculate (TODO: get all the constants right)
\begin{align*}
\int \mathcal{H}^{d-m} (A \cap L^{-1}(z)) \, \lambda^m(z) &= \int \mathcal{H}^{d-m} (A \cap (V \circ P^{-1} \circ \Sigma^{-1} \circ U^T)(z)) \, \lambda^m(z) \\
&=2^{-m} \alpha(m) \int \sum_{y \in (\Sigma^{-1} \circ U^T)(z)} \mathcal{H}^{d-m} (A \cap (V \circ P^{-1})(y)) \, \mathcal{H}^m(z) \\
&= \abs{\det(U \circ \Sigma)} \int \mathcal{H}^{d-m} (A \cap (V \circ P^{-1})(y)) \, \lambda^m(y) \\
&= \frac{2^{d-m}} {\alpha(d-m)} \exdet{L} \lambda^d(A) 
\end{align*}
(Here we use the invertibility of $U \circ \Sigma$ to see that $(\Sigma^{-1} \circ U^T)(z)$ is a singleton set).

TODO: The measurability of $\mathcal{H}^{d-m}(A \cap L^{-1}(y))$ for general full-rank $L$ is also supposed to follow from measurability of sections by essentially the same argument as that for $P$.
\end{proof}

Now we address the measurability of the mapping $y \mapsto \mathcal{H}^{d-m} (A \cap f^{-1}(y))$ for Lipschitz $f$.
\begin{lem}\label{CoareaFormulaMeasurability}Let $d \geq m$, $f : \reals^d \to \reals^m$ be Lipschitz
  and $A \subset \reals^d$ be $\lambda^d$-measurable.  Then
\begin{itemize}
\item[(i)] The set $A \cap f^{-1}(y)$ is $\mathcal{H}^{d-m}$-measurable for $\lambda^m$ almost everywhere.
\item[(ii)] The function $y \mapsto \mathcal{H}^{d-m} (A \cap f^{-1}(y))$
\item[(iii)] $\int \mathcal{H}^{d-m} (A \cap f^{-1}(y)) \, \lambda^m(dy) \leq 2^{d-m} \alpha(m) (\Lip(f))^m \lambda^d(A)$.
\end{itemize}
\end{lem}
\begin{proof}
We first prove a variant of (iii) in which the integral is replaced by an outer integral (use of the latter means we can avoid showing measurability).  Now by a basic property of
Lebesgue outer measure (TODO: Where do we show this; perhaps this will come up in the proof of Theorem \ref{HausdorffEqualsLebesgue}????  In fact follows from Theorem \ref{HausdorffEqualsLebesgue} once we show that Hausdorff measure can be calculated using coverings by closed balls)  we know that for every $n \in \naturals$ there exist closed balls $B^n_1, B^n_2, \dotsc$ such that
$A \subset \cup_{j=1}^\infty B^n_j$, $\diam(B^n_j) \leq 1/n$ and $\lambda^d(A) \leq \sum_{j=1}^n \lambda^d(B^n_j) \leq \lambda^d(A) + 1/n$.  This covering of $A$ shows
that for each $y \in \reals^m$ we have
\begin{align*}
\mathcal{H}^{d-m}_{1/n}(A \cap f^{-1}(y)) &\leq \sum_{\substack{1 \leq j < \infty \\ B^n_j \cap f^{-1}(y) \neq \emptyset}} (\diam(B^n_j))^{d-m} = \sum_{j=1}^\infty (\diam(B^n_j))^{d-m} \characteristic{f(B^n_j)}(y)
\end{align*}
Since the ball $B^n_j$ is compact and $f$ is continuous it follows that $f(B^n_j)$ is compact hence $\lambda^m$-measurable.  Taking the limit of both sides we get
\begin{align*}
\mathcal{H}^{d-m}(A \cap f^{-1}(y)) &= \lim_{n \to \infty} \mathcal{H}^{d-m}_{1/n}(A \cap f^{-1}(y)) &\leq \liminf_{n \to \infty} \sum_{j=1}^\infty (\diam(B^n_j))^{d-m} \characteristic{f(B^n_j)}(y)
\end{align*} 
thus we have a measurable majorant of
the mapping $y \mapsto \mathcal{H}^{d-m}(A \cap f^{-1}(y))$.  Applying the above majorant, Fatou's Lemma, Monotone Convergence (specifically Corollary \ref{TonelliIntegralSum}), the Isodiametric Inequality (Theorem \ref{IsodiametricInequality}) and the fact that $\lambda^d(B^n_j) = (\diam(B^n_j)/2)^d$ we get
\begin{align*}
\int^* \mathcal{H}^{d-m}(A \cap f^{-1}(y))  \, \lambda^m(dy) 
&\leq \int \liminf_{n \to \infty} \sum_{j=1}^\infty (\diam(B^n_j))^{d-m} \characteristic{f(B^n_j)}(y) \, \lambda^m(dy) \\
&\leq \liminf_{n \to \infty} \int \sum_{j=1}^\infty (\diam(B^n_j))^{d-m} \characteristic{f(B^n_j)}(y) \, \lambda^m(dy) \\
&= \liminf_{n \to \infty} \sum_{j=1}^\infty (\diam(B^n_j))^{d-m} \lambda^m(f(B^n_j)) \\
&\leq \liminf_{n \to \infty} \sum_{j=1}^\infty (\diam(B^n_j))^{d-m} \alpha(m) \left( \frac{\diam(f(B^n_j))}{2} \right)^m \\
&\leq \liminf_{n \to \infty} \sum_{j=1}^\infty (\diam(B^n_j))^{d} 2^{-m} (\Lip(f))^m \alpha(m) \\
&= 2^{d-m} (\Lip(f))^m \alpha(m)\liminf_{n \to \infty} \sum_{j=1}^\infty \lambda^d(B^n_j) \\
&= 2^{d-m} (\Lip(f))^m \alpha(m) \lambda^d(A) \\
\end{align*}

We first prove (ii).  We break this down into several different cases depending on the characteristics of $A$.
\begin{cas} $A$ is compact
\end{cas}
Fix $t \geq 0$.  For every $n \in \naturals$ let $U_n$ be the set of $y \in \reals^m$ such that there exists $k \in \naturals$ and open sets 
$V_1, \dotsc, V_k$ satisfying
\begin{itemize}
\item[(i)] $A \cap f^{-1}(y) \subset \cup_{j=1}^k V_j$
\item[(ii)] $\diam(V_j) \leq 1/n$ for $j=1, \dotsc, k$
\item[(iii)] $\sum_{j=1}^k (\diam(V_j))^{d-m} \leq t + 1/n$
\end{itemize}
\begin{clm}$U_n$ is open 
\end{clm}
In fact given $y \in U_n$ and open sets $V_1, \dotsc, V_k$ there is an open neighborhood $W$ of $y$
such that $A \cap f^{-1}(z) \subset \cup_{j=1}^k V_j$ for all $z \in W$.  If this were not true then set could find a sequence $y_i \to y$ such
that $A \cap f^{-1}(y_i) \setminus \cup_{j=1}^k V_j \neq \emptyset$.  By using the compactness of $A$ to pass to a subsequence if necessary
we may pick $x_i \in A \cap f^{-1}(y_i) \setminus \cup_{j=1}^k V_j$ such that $x_i \to x$ with $x \in A \setminus \cup_{j=1}^k V_j$.  By continuity of $f$
we know that 
\begin{align*}
f(x) &= \lim_{i \to \infty} f(x_i) = \lim_{i \to \infty} y_i = y
\end{align*}
hence $x \in A \cap f^{-1}(y) \subset \cup_{j=1}^k V_j$ which is a contradiction.

\begin{clm}The map $y \mapsto \mathcal{H}^{d-m}(A \cap f^{-1}(y))$ is Borel measurable in fact
\begin{align*}
\lbrace y \in \reals^m \mid \mathcal{H}^{d-m}(A \cap f^{-1}(y)) \leq t \rbrace &= \cap_{n=1}^\infty U_n
\end{align*}
\end{clm}
We first show the inclusion $\lbrace y \in \reals^m \mid \mathcal{H}^{d-m}(A \cap f^{-1}(y)) \leq t \rbrace = \cap_{n=1}^\infty U_n$.
Since $\mathcal{H}^{d-m}(A \cap f^{-1}(y)) \leq t$ and $\mathcal{H}^{d-m}(A \cap f^{-1}(y)) = \sup_{\delta>0} \mathcal{H}_\delta^{d-m}(A \cap f^{-1}(y))$ we know that
$\mathcal{H}_\delta^{d-m}(A \cap f^{-1}(y)) \leq t$ for all $\delta > 0$.  By Proposition \ref{HausdorffMeasureByOpenClosedCoverings} for any $n \in \naturals$ we pick $0 < \delta < 1/n$ and some open sets $V_1, V_2, \dotsc$ such that
\begin{itemize}
\item[(i)] $A \cap f^{-1}(y) \subset \cup_{j=1}^\infty V_j$
\item[(ii)] $\diam(V_j) \leq \delta < 1/n$ for $j \in \naturals$
\item[(iii)] $\sum_{j=1}^\infty (\diam(V_j))^{d-m} < t + 1/n$
\end{itemize}
Since $f$ is continuous we know that $f^{-1}(y)$ is closed hence $A \cap f^{-1}(y)$ is a closed subset of a compact set hence compact (Corollary \ref{ClosedSubsetsCompact}).  Thus we may find
a finite subcover $V_1, \dotsc, V_k$ of $A \cap f^{-1}(y)$ which shows $y \in U_n$.  

Now suppose that $y \in \cap_{n=1}^\infty U_n$.  By the definitions of $U_n$ and $\mathcal{H}^{d-m}$ we know that $y \in U_n$ implies $\mathcal{H}_{1/n}^{d-m}(A \cap f^{-1}(y)) \leq t + 1/n$; therefore we know that $\mathcal{H}_{1/n}^{d-m}(A \cap f^{-1}(y)) \leq t + 1/n$ for all $n \in \naturals$.  Now take the limit as $n \to \infty$ to conclude that $\mathcal{H}^{d-m}(A \cap f^{-1}(y)) \leq t$.

\begin{cas} $A$ is open.
\end{cas}

For each $n \in \naturals$ define 
\begin{align*}
K_n &= \left((A^c)){1/n}\right)^c \cap \overline{B}(0,n) = \lbrace x \mid d(x,A^c) \geq 1/n \rbrace \cap \overline{B}(0,n)
\end{align*}
which is a closed bounded set and therefore compact.  By the definition of $K_n$ and the openness of $A$ we have $K_1 \subset K_2 \subset \dotsb \subset A$ and $\cup_{n=1}^\infty K_n = A$.  By Borel regularity of $\mathcal{H}^{d-m}$ 
and continuity of regular outer measures  (Proposition \ref{ContinuityOfRegularOuterMeasure}) we have
\begin{align*}
\mathcal{H}^{d-m} (A \cap f^{-1}(y)) &= \lim_{n \to \infty} \mathcal{H}^{d-m} (K_n \cap f^{-1}(y))
\end{align*}
which shows that $y \mapsto \mathcal{H}^{d-m} (A \cap f^{-1}(y))$ is open.

\begin{cas} $\lambda^d(A) < \infty$.
\end{cas}
By the outer regularity of $\lambda^d$ (or the Lebesgue measurability of $A$) we can find open sets $V_1 \supset V_2 \supset \dotsb \supset A$ such that
$\lim_{n \to \infty} \lambda^d(V_n \setminus A) = 0$ and $\lambda^d(V_1) < \infty$.  By the subadditivity of $\mathcal{H}^{d-m}$ we get
\begin{align*}
\mathcal{H}^{d-m}(V_n \cap f^{-1}(y)) 
&\leq 
\mathcal{H}^{d-m}(A \cap f^{-1}(y)) +
\mathcal{H}^{d-m}((V_n \setminus A) \cap f^{-1}(y)) \\
\end{align*}
Now put this bound together with the already proven version of (iii) with outer integrals
\begin{align*}
0 &\leq \limsup_{n \to \infty} \int^* \abs{\mathcal{H}^{d-m}(V_n \cap f^{-1}(y))  - \mathcal{H}^{d-m}(A \cap f^{-1}(y))} \, \lambda^m(dy) \\
&\leq \limsup_{n \to \infty} \int^* \mathcal{H}^{d-m}((V_n \setminus A) \cap f^{-1}(y)) \, \lambda^m(dy) \\
&\leq 2^{d-m} \alpha(m) (\Lip(f))^m \limsup_{n \to \infty} \lambda^m(V_n \setminus A) = 0\\
\end{align*}
which shows (TODO: be careful to go through the details of this argument since we have an outer integral) $\lim_{n \to \infty} \mathcal{H}^{d-m}(V_n \cap f^{-1}(y)) = \mathcal{H}^{d-m}(A \cap f^{-1}(y))$
$\lambda^m$-almost everywhere.  Thus by the Borel measurablility of $\mathcal{H}^{d-m}(V_n \cap f^{-1}(y))$ for $V_n$ open we conclude that $\mathcal{H}^{d-m}(A \cap f^{-1}(y))$ is $\lambda^m$-measurable.  Furthermore this shows 
\begin{align*}
\mathcal{H}^{d-m}((\cap_{n=1}^\infty V_n \setminus A) \cap f^{-1}(y)) &= \lim_{n \to \infty} \mathcal{H}^{d-m}((V_n \setminus A) \cap f^{-1}(y)) = 0
\end{align*} 
so $(\cap_{n=1}^\infty V_n \setminus A) \cap f^{-1}(y)$ is $\mathcal{H}^{d-m}$-measurable.
Since we can write 
\begin{align*}
A \cap f^{-1}(y) &= \cap_{n=1}^\infty V_n \setminus (\cap_{n=1}^\infty V_n  \setminus A) \cap f^{-1}(y) = 
\cap_{n=1}^\infty V_n \cap f^{-1}(y) \setminus (\cap_{n=1}^\infty V_n  \setminus A) \cap f^{-1}(y)
\end{align*}
and $\cap_{n=1}^\infty V_n \cap f^{-1}(y)$ is Borel measurable (hence $\mathcal{H}^{d-m}$-measurable) we see that $A \cap f^{-1}(y)$ is also $\mathcal{H}^{d-m}$ measurable for $\lambda^m$-almost every $y$.

\begin{cas} $\lambda^d(A) = \infty$.
\end{cas}
We write $A = \cup_{n=1}^\infty (A \cap B(0,n))$ and use the previous case to see that $A \cap f^{-1}(y)$ is $\mathcal{H}^{d-m}$ measurable.  From continuity of regular outer measures (Proposition \ref{ContinuityOfRegularOuterMeasure}) we have
\begin{align*}
\mathcal{H}^{d-m}(A \cap f^{-1}(y)) &= \lim_{n \to \infty} \mathcal{H}^{d-m}(A \cap B(0,n) \cap f^{-1}(y)) 
\end{align*}
which in turns shows the $\mathcal{H}^{d-m}$ measurability of $y \mapsto \mathcal{H}^{d-m}(A \cap f^{-1}(y))$.

To complete the proof note that (i) and (ii) together with the version of (iii) proven for outer integrals shows (iii) (TODO: where do we show that the outer integral of a measurable function equals the integral).
\end{proof}

\begin{lem}\label{CoareaFormulaLipschitz:FunctionBounds}Let $f : \reals^d \to \reals^d$ be a Lipschitz map, $t>1$ and
\begin{align*}
A &= \lbrace x \in \reals^d \mid Df(x) \text{ exists and } Jf(x) > 0\rbrace
\end{align*}
There exists a countable set of Borel sets $A_1, A_2, \dotsc$ and symmetric invertible linear map $T_n : \reals^d \to \reals^d$ such that 
\begin{itemize}
\item[(i)] $\lambda^d(A \setminus \cup_{n=1}^\infty A_n)$
\item[(ii)] $f \mid_{A_n}$ is injective for each $n \in \naturals$
\item[(iii)] For each $n \in \naturals$
\begin{align}\label{eq:CoareaFormulaLipschitz:LipschitzConstants}
\Lip( T_n^{-1} \circ (f \mid_{A_n})) \leq t, \qquad \Lip((f
  \mid_{A_n})^{-1} \circ T_n) \leq t
\end{align}
\item[(iv)] For each $n \in \naturals$
\begin{align}\label{eq:CoareaFormulaLipschitz:JacobianBounds}
t^{-d} \abs{\det(T_n)} \leq Jf\mid_{A_n} \leq t^{d} \abs{\det(T_n)} 
\end{align}
\end{itemize}
\end{lem}
\begin{proof}
We apply Lemma \ref{AreaFormulaLipschitz:FunctionBounds} to $f$, $t$ and $A$ to construct Borel sets $B_1, B_2, \dotsc$ and
symmetric invertible maps $S_1, S_2, \dotsc$ such that $A = \cup_{n=1}^\infty B_n$, $f \mid_{B_n}$ is injective for each $n \in \naturals$
and for all $n \in \naturals$
\begin{align*}
\Lip((f \mid_{B_n}) \circ  S_n^{-1} ) \leq t, \qquad \Lip(S_n \circ (f  \mid_{B_n})^{-1} ) \leq t \\
t^{-d} \abs{\det(S_n)} \leq Jf\mid_{A_n} \leq t^{d} \abs{\det(S_n)}
\end{align*}
Note that we have for all $n \in \naturals$ and $x,y \in f(B_n)$
\begin{align*}
\norm{ (f  \mid_{B_n})^{-1}(x) -  (f  \mid_{B_n})^{-1}(y)} &= \norm{ S_n^{-1} \circ S_n \circ (f  \mid_{B_n})^{-1}(x) - S_n^{-1} \circ S_n \circ (f  \mid_{B_n})^{-1}(y)} \\
&\leq \norm{S_n^{-1}} \Lip(S_n \circ (f  \mid_{B_n})^{-1} ) \norm{x-y}
\end{align*}
hence $(f  \mid_{B_n})^{-1}$ is a Lipschitz map.  By Lemma \ref{ExtensionOfLipschitzMaps} we may let $g_n : \reals^d \to \reals^d$ be a Lipschitz extension of $(f  \mid_{B_n})^{-1}$ (so 
in particular $g_n (f(x)) = x$ for all $x \in B_n$.

\begin{clm}\label{clm:CoareaFormulaLipschitz:FunctionBounds:JacobianOfExtendedLocalInverse} $Jg_n > 0$ $\lambda^d$-almost everywhere on $f(B_n)$.
\end{clm}
We know that $g_n \circ f = \IdentityMatrix$ on $B_n$ hence by Lemma \ref{DifferentiationOfInversesLipschitz} we know that $D g_n (f(x)) Df(x) = \IdentityMatrix$ $\lambda^d$ almost
everywhere on $B_n$.  Therefore by multiplicativity of determinants (Corollary \ref{DeterminantOfProducts}) we know $J g_n (f(x)) Jf(x) = 1$ $\lambda^d$ almost
everywhere on $B_n$ and in particular $J g_n (f(x)) > 0$ $\lambda^d$ almost everywhere on $B_n$.  Since $f$ is Lipschitz the claim follows by Proposition \ref{HausdorffMeasureUnderLipschitzMaps} (TODO: Is there an obvious Lebesgue measure version of this or do we need to appeal to Theorem \ref{HausdorffEqualsLebesgue} as well?).

For each $n \in \naturals$ we apply Lemma \ref{AreaFormulaLipschitz:FunctionBounds} to $g_n$ and $t$ to construct Borel sets
$C^n_1, C^n_2, \dotsc$ and symmetric invertible maps $R^n_1, R^n_2, \dotsc$ such that 
\begin{align*}
\lbrace x \in \reals^d \mid Dg_n(x) \text{ exists and } Jg_n(x) > 0\rbrace &= \cup_{j=1}^\infty C^n_j
\end{align*}
$g_n \mid_{C^n_j}$ is injective and 
\begin{align*}
\Lip((g_n \mid_{C^n_j}) \circ  (R^n_j)^{-1} ) \leq t, \qquad \Lip(R^n_j \circ (g_n  \mid_{R^n_j})^{-1} ) \leq t \\
t^{-d} \abs{\det(R^n_j)} \leq Jg_n\mid_{C^n_j} \leq t^{d} \abs{\det(R^n_j)}
\end{align*}
for $j \in \naturals$.  By the previous claim 
\begin{align*}
\lambda^d\left(g(B_n) \setminus \cup_{j=1}^\infty C^n_j \right) &= 0
\end{align*}

Now we are ready to define our final sets and injective maps.  For $j,n \in \naturals$ let
$A^n_j = B_n \cap f^{-1}(C^n_j)$ and $T^n_j = (R^n_j)^{-1}$.

\begin{clm}\label{clm:CoareaFormulaLipschitz:FunctionBounds:SimpleFormulas} For all $n,j \in \naturals$ 
\begin{itemize}
\item[(i)] $x \in A^n_j$ we have $f(x) = (g_n \mid_{C^n_j})^{-1}(x)$ 
\item[(ii)] for all $y \in f(A^n_j)$ we have $g_n(y) = (f \mid_{A^n_j})^{-1}(y)$
\end{itemize}
\end{clm}
To see (i) first note that the statement makes sense.  In particular we need to make sure the domain of $(g_n \mid_{C^n_j})^{-1}$ of the domain contains $A^n_j$.
If $x \in A^n_j$ then
$x \in B_n$ and $f(x) \in C^n_j \cap f(B_n)$.  Since by construction $g_n \mid_{f(B_n)} = (f \mid_{B_n})^{-1}$ we know that
$g_n(f(x)) = (f \mid_{B_n})^{-1}(f(x)) = x$ which shows us that $x \in g_n(C^n_j) = \domain{(g_n \mid_{C^n_j})^{-1}}$.  By injectivity of $g_n$ on $C^n_j$ the fact that $g_n(f(x)) =x$ also shows us that
$f(x) = (g_n \mid_{C^n_j})^{-1}(x)$ on $A^n_j$.

The claim (ii) follows from (i) by simply writing $y=f(x)$ for $x \in A^n_j$ to see that 
\begin{align*}
g_n(y) &= g_n(f(x)) = x = (f \mid_{A^n_j})^{-1}(y)
\end{align*}


\begin{clm} $\lambda^d\left( A \setminus \cup_{n=1}^\infty \cup_{j=1}^\infty A^n_j \right) = 0$.
\end{clm}
For each $n \in \naturals$
\begin{align*}
g_n \left(f(B_n) \setminus \cup_{j=1}^\infty C^n_j \right) 
&= (f \mid_{B_n})^{-1} \left(f(B_n) \setminus \cup_{j=1}^\infty C^n_j \right) \\
&=B_n \setminus \cup_{j=1}^\infty f^{-1}(C^n_j) \\
&=B_n \setminus \cup_{j=1}^\infty A^n_j \\
\end{align*}
Therefore
\begin{align*}
\lambda^d\left(B_n \setminus \cup_{j=1}^\infty A^n_j  \right) 
&\leq \Lip(g_n) \lambda^d \left( f(B_n) \setminus \cup_{j=1}^\infty C^n_j \right) = 0
\end{align*}
and
\begin{align*}
\lambda^d\left( A \setminus \cup_{n=1}^\infty \cup_{j=1}^\infty A^n_j \right)
&= \lambda^d \left( \cup_{n=1}^\infty B_n \setminus \cup_{n=1}^\infty \cup_{j=1}^\infty A^n_j \right) \\
&\leq \sum_{n=1}^\infty \lambda^d \left(B_n \setminus \cup_{k=1}^\infty \cup_{j=1}^\infty A^k_j \right) \\
&\leq \sum_{n=1}^\infty \lambda^d \left(B_n \setminus \cup_{j=1}^\infty A^n_j \right) = 0
\end{align*}

\begin{clm} For every $n,j \in \naturals$ $f \mid_{A^n_j}$ is injective
\end{clm}
This follows immediately from the fact that $f \mid_{B_n}$ is injective and $A^n_j \subset B_n$.

\begin{clm} For every $n,j \in \naturals$ we have
\begin{align*}
\Lip( (T^n_j)^{-1} \circ (f \mid_{A^n_j})) \leq t, \qquad \Lip((f  \mid_{A^n_j})^{-1} \circ T^n_j) \leq t \\
t^{-d} \abs{\det(T^n_j)} \leq Jf\mid_{A^n_j} \leq t^{d} \abs{\det(T^n_j)} 
\end{align*}
\end{clm}
Using Claim \ref{clm:CoareaFormulaLipschitz:FunctionBounds:SimpleFormulas} we know that $(g_n \mid_{C^n_j})^{-1}$ is an
extension of $f \mid_{A^n_j}$ hence
\begin{align*}
\Lip((T^n_j)^{-1} \circ (f \mid_{A^n_j})) &= \Lip(R^n_j \circ (f \mid_{A^n_j})) \leq \Lip(R^n_j \circ (g_n \mid_{C^n_j})^{-1}) \leq t
\end{align*}
and similarly since $g_n \mid_{C^n_j}$ is an extension of $(f \mid_{A^n_j})^{-1}$ 
\begin{align*}
\Lip((f \mid_{A^n_j})^{-1} \circ T^n_j) &= \Lip((f \mid_{A^n_j})^{-1} \circ (R^n_j)^{-1}) \leq \Lip((g_n \mid_{C^n_j} \circ (R^n_j)^{-1}) \leq t
\end{align*}

From the proof of claim \ref{clm:CoareaFormulaLipschitz:FunctionBounds:JacobianOfExtendedLocalInverse}  we know that
$J g_n(f(x)) J f(x) = 1$ $\lambda^d$-almost everywhere on $A^n_j$ therefore
\begin{align*}
t^{-n} \abs{\det(S^n_j)} &=t^{-n} \abs{\det(R^n_j)}^{-1} \leq (J g_n \mid_{C^n_j})^{-1} = J f \mid_{A^n_j}
\end{align*}
and
\begin{align*}
J f \mid_{A^n_j} &= (J g_n \mid_{C^n_j})^{-1} \leq t^n \abs{\det(R^n_j)}^{-1} = t^{n} \abs{\det(S^n_j)}
\end{align*}
\end{proof}

\begin{thm}[Coarea Formula]\label{CoareaFormulaLipschitz}Let $d \geq m$, $f : \reals^d \to \reals^m$ be a Lipschitz function and $A \subset \reals^d$ be a
$\lambda^d$-measurable set then
\begin{align*}
\int_A Jf(x) \, \lambda^d(dx) &= \int \mathcal{H}^{d-m}(A \cap f^{-1}(y)) \, \lambda^m(dy)
\end{align*}
\end{thm}
\begin{proof}
\begin{clm}If suffices to prove the theorem for $A$ on which $Df(x)$ and $Jf(x)$ exist everwhere and for which $\lambda^d(A) < \infty$.
\end{clm}
By Rademacher's Theorem we know that $\lambda^d(\reals^d \setminus \lbrace Df(x) \text{ exists} \rbrace) =0$.  From this and the additivity of $\lambda^d$ we know that
\begin{align*}
\int_A Jf(x) \, \lambda^d(dx) &= \int_{A \cap \lbrace Df(x) \text{ exists} \rbrace}  Jf(x) \, \lambda^d(dx) + \int_{A \setminus \lbrace Df(x) \text{ exists} \rbrace}  Jf(x) \, \lambda^d(dx) \\
&= \int_{A \cap \lbrace Df(x) \text{ exists} \rbrace}  Jf(x) \, \lambda^d(dx) 
\end{align*}
and by the subadditivity of $\mathcal{H}^{d-m}$ and Lemma \ref{CoareaFormulaMeasurability}
\begin{align*}
&\int \mathcal{H}^{d-m}(A \cap \lbrace Df(x) \text{ exists} \rbrace \cap  f^{-1}(y)) \, \lambda^m(dy) \\
&\leq \int \mathcal{H}^{d-m}(A \cap f^{-1}(y)) \, \lambda^m(dy) \\
&\leq \int \mathcal{H}^{d-m}(A \cap \lbrace Df(x) \text{ exists} \rbrace \cap f^{-1}(y)) \, \lambda^m(dy) 
+ \int \mathcal{H}^{d-m}(A \setminus \lbrace Df(x) \text{ exists} \rbrace \cap f^{-1}(y)) \, \lambda^m(dy) \\
&\leq \int \mathcal{H}^{d-m}(A \cap \lbrace Df(x) \text{ exists} \rbrace \cap f^{-1}(y)) \, \lambda^m(dy)  + 
2^{d-m} \alpha(m) (\Lip(f))^m \lambda^d(A \setminus \lbrace Df(x) \text{ exists} \rbrace \cap f^{-1}(y)) \\
&=\int \mathcal{H}^{d-m}(A \cap \lbrace Df(x) \text{ exists} \rbrace \cap f^{-1}(y)) \, \lambda^m(dy)
\end{align*}
hence $\int \mathcal{H}^{d-m}(A \cap f^{-1}(y)) \, \lambda^m(dy)  = \int \mathcal{H}^{d-m}(A \cap \lbrace Df(x) \text{ exists} \rbrace \cap f^{-1}(y)) \, \lambda^m(dy)$.
From these two facts it follows that we may replace $A$ by $A \cap \lbrace Df(x) \text{ exists} \rbrace$ and assume that $Df(x)$ and $Jf(x)$ exists everywhere on $A$.

To see the reduction to the case in which $\lambda^d(A) < \infty$ assume the result holds for such sets and define $A_n = A \cap B(0,n)$.  Applying Proposition \ref{ContinuityOfRegularOuterMeasure} and 
Montone Convergence we see
\begin{align*}
\int_A Jf(x) \, \lambda^d(dx) 
&= \lim_{n \to \infty}\int_{A_n} Jf(x) \, \lambda^d(dx) 
= \lim_{n \to \infty} \int \mathcal{H}^{d-m}(A_n \cap f^{-1}(y)) \, \lambda^m(dy) \\
&= \int \mathcal{H}^{d-m}(A \cap f^{-1}(y)) \, \lambda^m(dy)
\end{align*}

\begin{cas} $A \subset \lbrace Jf > 0 \rbrace$
\end{cas}
Let $\lambda : \lbrace 1, \dotsc, m-d \rbrace \to \lbrace 1, \dotsc, d \rbrace$ be a strictly increasing function.  Let $P_\lambda : \reals^d \to \reals^{d-m}$ be defined by
$P_\lambda(x_1, \dotsc, x_d) = (x_{\lambda(1)}, \dotsc, x_{\lambda(m-d)})$.  Now define $h_\lambda : \reals^d \to \reals^m \times \reals^{d-m}$ by $h_\lambda(x) = (f(x), P_\lambda(x))$ and
$q : \reals^{m} \times \reals{d-m}$ by $q(y,z) = y$; it follows that $f = q \circ h_\lambda$.  Let
\begin{align*}
A_\lambda &= \lbrace x \in A \mid \det(Dh_\lambda(x)) \neq 0 \rbrace = \lbrace x \in A \mid P_\lambda \mid_{\ker Df(x)} \text{ is injective} \rbrace
\end{align*}
(the second equality follows since $\det(Dh_\lambda(x)) \neq 0$ if and only if $Dh_\lambda(x) = \begin{bmatrix} Df(x) \\ P_\lambda \end{bmatrix}$ is injective if and only if $P_\lambda$ is injective on $\ker Df(x)$).  Since $Jf(x) > 0$ we know that $\ker Df(x)$ has dimension $d-m$; it follows that there exists some $\lambda$ such that $P_\lambda$ is injective (e.g. pick a basis in reduced row-echelon form and pick the columns of the leading coefficients to define $\lambda$).  Hence $A = \cup_{\lambda} A_\lambda$ and by $\mathcal{H}^{d-m}$-measurability of the 
sets $A \cap f^{-1}(y)$ and $A_\lambda \cap f^{-1}(y)$ we have $\mathcal{H}^{d-m}(A \cap f^{-1}(y)) = \sum_\lambda \mathcal{H}^{d-m}(A_\lambda \cap f^{-1}(y))$ and it suffices to 
handle the case of an $A_\lambda$.

Let $t>1$ be given and apply Lemma \ref{CoareaFormulaLipschitz:FunctionBounds} to the function $h_\lambda$ to construct (TODO: disjoint?) Borel measurable sets $A_1, A_2, \dotsc$ and invertible symmetric
linear maps $T_1, T_2, \dotsc$ such that
\begin{itemize}
\item[(i)] $\lambda^d(\lbrace Jh_\lambda>0\rbrace \setminus \cup_{n=1}^\infty A_n) = 0$
\item[(ii)] $h_\lambda \mid_{A_n}$ is injective for $n \in \naturals$
\item[(iii)] For each $n \in \naturals$
\begin{align}
\Lip( T_n^{-1} \circ (h_\lambda \mid_{A_n})) \leq t, \qquad \Lip((h_\lambda  \mid_{A_n})^{-1} \circ T_n) \leq t
\end{align}
\item[(iv)] For each $n \in \naturals$
\begin{align}
t^{-d} \abs{\det(T_n)} \leq Jh_\lambda\mid_{A_n} \leq t^{d} \abs{\det(T_n)} 
\end{align}
\end{itemize}
Let $B_n = A_\lambda \cap A_n$.
\begin{clm} $t^{-d} \exdet{q \circ T_n} \leq Jf \mid_{B_n} \leq t^n \exdet{ q \circ T_n }$
\end{clm}
Writing $f = q \circ h_\lambda$ and using the linearity of $q$ and the Chain Rule
\begin{align*}
Df(x) &= q \circ Dh_\lambda(x) = q \circ T_n \circ (T_n)^{-1} \circ Dh_\lambda(x) = q \circ T_n \circ D(\circ (T_n)^{-1} h_\lambda)(x)
\end{align*}
By property (iii) above we know that $t^{-1} \leq \Lip( T_n^{-1} \circ (h_\lambda \mid_{B_n})) \leq t$.  Take Polar Decompositions (TODO: Should we do SVDs?)
\begin{align*}
Df(x) = S \circ U^T, \quad q \circ T_n = T \circ V^T
\end{align*}
it follows that 
\begin{align*}
S &= T \circ V^T \circ D((T_n)^{-1} \circ h_\lambda)(x) \circ U
\end{align*}
Since $Jf(x) = \abs{\det(S)} \neq 0$ we also know that $\det(T) \neq 0$ hence $T$ is invertible and
\begin{align*}
\norm{T^{-1} \circ S} &= \norm{V^T \circ D( (T_n)^{-1} \circ h_\lambda)(x) \circ U}
\leq \norm{D((T_n)^{-1} \circ h_\lambda)(x)} \\
&= \Lip(D((T_n)^{-1} \circ h_\lambda)(x)) \\
&\leq \Lip((T_n)^{-1} \circ  h_\lambda\mid_{B_n}) \text{ TODO: ?????}
\leq t
\end{align*}
hence $T^{-1}(S(\overline{B}(0,1))) \subset \overline{B}(0,t)$ and 
\begin{align*}
\abs{\det(T)^{-1} \det(S) } \lambda(\overline{B}(0,1)) &= \lambda^d(T^{-1}(S(\overline{B}(0,1)))) \leq \lambda^d(\overline{B}(0,t)) = t^d \lambda(\overline{B}(0,1)) 
\end{align*}
Therefore
\begin{align*}
Jf(x) &= \abs{\det(S)} \leq t^d \abs{\det(T)} = t^d \exdet{q \circ T_n}
\end{align*}

TODO: Finish
\end{proof}
\section{Integration in Banach Spaces}

Our prior development of measure and integration theory made use of the
special properties of the reals in various places and as a
result the theory does not hold for functions with values in arbitrary
vector spaces.  As we shall soon see it is useful to be able to
integrate functions with vector space values (in particular Banach
space values) so we need an integration theory.  As it turns out there are a
couple of directions that one can go.  In the simplest case that will
suffice for many of our needs, we develop the theory of Riemann
integrals.  The primary loss of generality is
that the domains of functions in the Riemann integral case must be 
functions of a real variable.  For our purposes we shall only be
requiring the Riemann theory for a single real variable so that shall
suit us fine.  For problems in which the domain in an arbitrary
measurable space we need a Lebesgue-like theory that was developed by
Bochner.  The reader may want to be made aware that in addition to these integrals there is also an
integral due to Gelfand and Pettis that we shall not discuss.

\subsection{Riemann Integrals}
As mentioned we shall only bother to develop the Riemann integral for
a single real variable.
\begin{defn}Let $a \leq b$ be real numbers then a \emph{partition} of
  the interval $[a,b]$ is a finite sequence of real numbers $a=a_0
  \leq a_1 \leq \dotsb \leq a_n = b$.  Let $X$ be a Banach space then
  a map $f : [a,b] \to X$ is said to be a \emph{step map with repsect
    to P} if there
  exists a partition $P=\lbrace a_j \rbrace_{j=0}^n$ and elements $w_1,
  \dotsc, w_n \in X$ such that $f(t) = w_j$ for $a_{j-1} < t < a_j$.
  A \emph{step map} is any map $f$ such that for which there exists a partition
  $P$ for which $f$ is a step map with respect to $P$.
  The \emph{integral} of a step map with repsect to $P$ is
\begin{align*}
I_P(f) &= 
\sum_{j=1}^n (a_j - a_{j-1}) w_j
\end{align*}
\end{defn}
Note that a step map has it values constrained on the open intervals
$(a_{j-1},a_j)$ but not at the points $a_j$.

With all of these elementary definitions in hand we come to our first
task which is to show that the integral of a step map is well defined.
\begin{prop}Let $X$ be a Banach space and let $f : [a,b] \to X$ be a
  step map with respect to partitions $P$ and $Q$ then it follows that
  $I_P(f) = I_Q(f)$.
\end{prop}
\begin{proof}
Given a partition $P$ of the form $a=a_0 \leq \dotsb \leq a_n=b$ let
$c \in [a,b]$ and let the refinement $P_c$ represent the partition
obtained by adding $c$ to the set of $a_j$.  It is clear that $f$ is
still a step map with respect to $P_c$ and that $I_P(f)
= I_{P_c}(f)$.  A partition $R$ is said to be a refinement of $P$ if
it is a subset of $P$; by induction we see that $I_P(f) = I_R(f)$
whenever $R$ is a refinement of $P$.  Now given arbitrary partitions
$P$ and $Q$ as in the hypotheses we simply find a common refinement
(e.g. take the union of $P$ and $Q$) and the result follows.
\end{proof}

Now we extend the integral by a limiting procedure.  To do this we use 
somewhat abstract language of Banach space theory.  First let us set
up the Banach space in which we operate.

\begin{prop}Let $X$ be a normed vector space, let $S$ be an arbitrary
  set and let $\mathfrak{B}(S, X)$
  represent the set of bounded functions $f : S \to X$.  Let
  $\abs{x}$ denote the norm on $X$.  If we
  define $\norm{f} = \sup_{s \in S} \abs{f(s)}$ then $\norm{f}$
  makes $\mathfrak{B}(S, X)$ into a normed vector space.
\end{prop}
\begin{proof}
We first observe that $\mathfrak{B}(S, X)$ is a vector space.  
This follows from the fact that if $f$ is bounded by $C$ then for all
$a \in \reals$ we have $af$ is bounded by $\abs{a} C$ if both $f$ and
$g$ are bounded by $C_1$ and $C_2$ respectively then using the
triangle inequality in $X$ we see that $f+g$ is bounded by $C_1 + C_2$.

Next we prove that we have defined a norm.  The fact that $\norm{f}
\geq 0$ and $\norm{0} =0$ follow immediately from the definition and
the fact that $\abs{\cdot}$ is a norm on $X$.   If $\norm{f}
= 0$ the it follows that $\abs{f(s)} = 0$ for all $s \in S$
and therefore $f=0$.  Let $c \in \reals$ then since $\abs{cf(s)} =
\abs{c} \abs{f(s)}$ it follows that $\norm{cf} \leq \abs{c}\norm{f}$.
On the other hand, let $\epsilon > 0$ be given then we may find an $s
\in S$
such that $\norm{f} - \epsilon < \abs{f(s)}$.  It follows that 
\begin{align*}
\abs{c} \norm{f} - \abs{c} \epsilon < \abs{c} \abs{f(s)} = \abs{cf(s)}
\end{align*}
Now $\epsilon$ was chosen arbitrarily so we may let $\epsilon \to 0$
and we get the inequality $\abs{c} \norm{f} \leq
\abs{cf(s)}$.  Now take the supremum over $s \in S$ to get opposite
inequality $\abs{c} \norm{f} \leq \norm{cf}$ and it follows that
$\abs{c} \norm{f} = \norm{cf}$.  The triangle inequality follows in a
similar way.  Given an $f$ and $g$ we see using the triangle
inequality in $X$ that for all $s \in S$ we
have $\abs{f(s) + g(s)} \leq \abs{f(s)} + \abs{g(s)} \leq \norm{f}
+\norm{g}$; taking the supremum over $s \in S$ we get $\norm{f+g} \leq
\norm{f} + \norm{g}$.
\end{proof}

Now we have the following extension result
\begin{lem}Let $X$ be a normed vector space and let $Y$ be a Banach
  space.  Suppose that $V \subset X$ is a subspace and $A : V \to Y$
  is a bounded linear map, then $A$ has a unique extension
  $\overline{A} : \overline{V} \to Y$ from the closure of $V$ into
  $Y$.  Moreover if $C$ is a bound on $A$ the $C$ is also a bound on $\overline{A}$.
\end{lem}
\begin{proof}
TODO:
\end{proof}

As is usual to compute with integrals it is imperative to connect the
integration with differentiation.  Since we are dealing with the
Riemann integral we must use relatively strong hypotheses however
these results will suffice for our applications and the proof are very
simple.  We start with the Fundamental Theorem of Calculus.

\begin{thm}[Fundamental Theorem of
  Calculus]\label{FundamentalTheoremOfCalculusForBanachSpaceRiemannIntegrals}Let
 $X$ be a Banach space and let $f : [a,b] \to X$ be continuously
 differentiable then 
\begin{align*}
f(b) - f(a) &= \int_a^b Df(t) dt
\end{align*}
\end{thm}
\begin{proof}
First we let $g(t)$ be a regulated function from $[a,b]$ to $X$ and
consider the integral $\int_a^s g(t) \, dt$.  Suppose that $g$ is
continuous at $c \in [a,b]$ and let $\epsilon > 0$ be given.  By
right continuity we may find $\delta > 0$ such that $\abs{g(c+h) -
  g(c)} < \epsilon$ for all $\abs{h} < \delta$.  If we let $G(s) = \int_a^s g(t) \,
dt$ then if $\abs{h} < \delta$ we have
\begin{align*}
\abs{ \frac{G(c + h) - G(c)}{h} - g(c)} &= \abs{\frac{1}{h}
                                          \int_c^{h+c} g(t)\, dt  -
                                          \frac{1}{h} \int_c^{h+c}
                                          g(c) \, dt} \\
&= \abs{\frac{1}{h}  \int_c^{h+c} (g(t) - g(c)) \, dt} \\
&\leq \frac{1}{\abs{h}} \abs{h} \sup_{c \leq s \leq c+h} \abs{g(t) -
  g(c)} 
=\sup_{c \leq s \leq c+h} \abs{g(t) -  g(c)} 
\end{align*}
By continuity
TODO
\end{proof}

\begin{prop}\label{NormRiemannIntegralBanachSpace}Let $X$ be a
  Banach space and let $f : [a,b] \to L(X,Y)$ be regulated then it
  follows that 
\begin{align*}
\norm{\int_a^b f(t) \, dt} &\leq \int_a^b \norm{f(t)} \, dt
\end{align*}
\end{prop}
\begin{proof}
TODO
\end{proof}

\begin{prop}\label{RiemannIntegralOfContinuousMaps}Let $X$ and $Y$ be
  Banach spaces and let $f : [a,b] \to L(X,Y)$ be regulated then it
  follows that for every $x \in X$ we have
\begin{align*}
\int_a^b f(t) x \, dt &= \int_a^b f(t) \, dt \cdot x
\end{align*}
\end{prop}
\begin{proof}
TODO
\end{proof}

\begin{defn}Let $X$ and $Y$ be Banach spaces then an \emph{unbounded
    operator} is $A$ a linear
  map of a subspace of $X$ into $Y$.  We let $\domain{A}$ be the
  domain of $A$.  We say that $A$ is a \emph{closed
  operator} if its graph is a closed linear subspace of $X \times Y$;
  that is to say if $v_n \to v$ in $X$ and $A v_n \to w$  in $Y$ then
  $v \in \domain{A}$ and $y = A v_n$.
\end{defn}

The domain of a linear operator is a crucial part of its definition
and there is no small amount of pain in having to be careful about
handling to be careful when dealing with these domains.  In
particular, if one is given two linear operators $A : X \to Y$ and $B
: X \to Y$ then we
can define $A+B : X \to Y$ where $\domain{A+B} = \domain{A} \cap
\domain{B}$.  
Given two linear operators $A : X \to Y$ and $B
: Y \to Z$ then we
can define $B \circ A: X \to Z$ where $\domain{B \circ A} = \lbrace v
\in \domain{A} \mid Av \in \domain{B} \rbrace$.

\begin{prop}\label{ClosedOperatorOfRiemannIntegral}Let $X$ and $Y$ be
  Banach spaces, let $A$ be a closed linear operator from $X$ to $Y$ and
  let $f : [a,b] \to X$ be continuous such that
\begin{itemize}
\item[(i)] $f(t) \in \domain{A}$ for all $a \leq t \leq b$
\item[(ii)] $Af : [a,b] \to Y$ is continuous
\end{itemize}
then it follows that $\int_a^b f(t) \, dt \in \domain{A}$ and
\begin{align*}
A \int_a^b f(t) \, dt &= \int_a^b A f(t) \, dt
\end{align*}
\end{prop}
\begin{proof}

TODO
\end{proof}

\subsection{Bochner Integrals}

TODO: We develop Bochner integrals on $\sigma$-finite measure spaces;
does it exist without that assumption?  What if one uses nets for
convergence of simple functions instead of sequences?

TODO: Preliminaries on norming subspaces.

TODO:
The Bochner integral is the analogue of a vector valued integral using
general measure theory.  Though it can be developed in a bit more
generality, we define it here for Banach space valued functions.  
The first thing to do is to observe that the defintion of simple functions
extends trivially to this context.
\begin{defn}Given a set $(\Omega, \mathcal{A})$, a Banach space $X$
  sets $A_1, \dotsc, A_n \subset \Omega$ and $v_1, \dotsc, v_n \in X$ a
  linear combination $v_1
  \characteristic{A_1} + \cdots + v_n \characteristic{A_n}$ is called
  a \emph{simple function}.
\end{defn}

\begin{prop}\label{prop:SimpleVectorValuedFunctions}A function $f : \Omega \to X$ is simple if and only it
  takes a finite number of values.  A simple function is measurable if
  an only if $f^{-1}(v_j)$ is measurable for each of its distinct
  values $v_j \in X$.
\end{prop}
\begin{proof}
The proof of Proposition \ref{prop:SimpleFunctions} applies here with essentially
no changes.
\end{proof}

From this point on we will tacitly assume that all simple functions
are measurable. Integrals of simple functions can be defined in the obvious way.

The natural way to proceed would be to observe that $X$ can be given
the Borel $\sigma$-algebra and thus we can talk of measurable
functions.  One might then try to show that all measurable functions
can be approximated by simple functions and the integral can be
extended by use of such approximations.  In fact that is a little too much to hope for in a
non-separable Banach space (not all measurable functions can be thus
approximated) 
and we have to restrict ourselves to the class of functions that can be approximated
by simple functions.  We'll have to spend a bit of time understanding
this class of functions. 

\begin{defn}Let $(\Omega,\mathcal{A})$ be a measurable space and let
  $X$ be a Banach space then a function $f : \Omega \to X$ is said to
  be \emph{strongly measurable} if and only if there exist simple
  functions $f_n : \Omega \to X$ such that $f_n(\omega)$ converges to 
$f(\omega)$ for  all $\omega \in \Omega$.
\end{defn}

Intuitively strongly measurable functions must have some kind of
countability restriction since they are a limit of a countable number
of finite valued functions.  This is indeed true and is at the heart
of matter why can't approximate a general Borel measurable function
with simple functions.  To make this precise we need a couple of
definitions before stating a theorem that provides a descriptive
characterization of strongly measurable functions.

\begin{defn}Let $\Omega$ be a set, a function $f : \Omega \to X$ is
  \emph{separably valued} if there exists a closed separable subspace $V
  \subset X$ such that $f(\Omega) \subset V$.  
\end{defn}

\begin{defn}Let $(\Omega, \mathcal{A})$ be a measurable space, a function $f : \Omega \to X$ is
  \emph{weakly measurable} if for every $\lambda \in X^*$ the function
  $\lambda \circ f : \Omega \to \reals$ is measurable. 
\end{defn}

\begin{thm}Let $(\Omega, \mathcal{A})$ be a measurable space and $X$
  be a Banach space then $f$ is strongly measurable if and only if $f$
  is separably valued and weakly measurable.  In fact it suffices to
  show that $f$ is separably valued and $\lambda \circ f$ is
  measurable for $\lambda$ in a norming subspace of $E^*$.
\end{thm}
\begin{proof}
TODO
\end{proof}

It is useful to note that one cannot approximate any more functions by
using strongly measurable functions rather than just simple functions.
\begin{cor}A pointwise limit of strongly measurable functions is strongly measurable.
\end{cor}
\begin{proof}
TODO
\end{proof}

We also have the following useful consequence that shows that a
strongly measurable function is a separably valued Borel measurable
function.  In particular, all measurable functions with values in a
separable Banach space are strongly measurable.

Now we introduce a $\sigma$-finte measure and consider the measure
space $(\Omega, \mathcal{A}, \mu)$.  
\begin{defn}Given a set $(\Omega, \mathcal{A})$, a Banach space $X$,
  sets $A_1, \dotsc, A_n \in \mathcal{A}$ with $\mu(A_j) < \infty$ for
  $j=1, \dotsc, n$ and $v_1, \dotsc, v_n \in X$ a
  linear combination $v_1
  \characteristic{A_1} + \cdots + v_n \characteristic{A_n}$ is called
  a \emph{$\mu$-simple function}.
\end{defn}

\section{Differentiation in Banach Spaces}

TODO:
\begin{itemize}
\item Absolute convergence of a infinite series in a Banach space
\item Define space of linear maps with operator norm
\item Show that Frechet deriviative is equal to Jacobian matrix on
  finite dimensional spaces
\end{itemize}

\begin{prop}Let $X$ be a Banach space then if $\sum_{j=0}^\infty a_j$
  converges absolutely then $\sum_{j=0}^\infty a_j$ converges in $X$.
\end{prop}
\begin{proof}
By completeness of $X$ it suffices to show that $S_n = \sum_{j=0}^n
a_j$ is a Cauchy sequence.  Let $\epsilon > 0$ be given and pick $n >
0$ such that $\sum_{j=n}^\infty \norm{a_j} < \epsilon$.  Then for all
$m \geq n$ we have
\begin{align*}
\norm{S_m - S_n} &\leq \norm{\sum_{j=n}^{m-1} a_j} \\
&\leq \sum_{j=n}^{m-1}\norm{ a_j} \leq \sum_{j=n}^{infty}\norm{ a_j} < \epsilon
\end{align*}
and we are done.
\end{proof}

\begin{prop}Let $X$ be a Banach space.  The set of invertible maps in $L(X)$ is open, moreover
  for any invertible map $A \in L(X)$ and any $\norm{A-B} <
  \norm{A}^{-1}$ we have 
\begin{align*}
B^{-1} &= \sum_{n=0}^\infty A^{-n-1} (A-B)^n 
\end{align*}
In particular, the inversion map is continuously differentiable on its domain.
\end{prop}
\begin{proof}
We first assume that $A= I$ is the identity map.  If we let $\norm{B}
< 1$ then note that 
\begin{align*}
\norm{\sum_{n=0}^m B^n} &\leq \sum_{n=0}^m \norm{ B}^n <
                          \sum_{n=0}^\infty \norm{ B}^n =
                          \frac{1}{1-\norm{B}} < \infty
\end{align*}
which shows that $\sum_{n=0}^\infty B^n$ converges absolutely and
is well defined in $L(X)$.  Moreover we have
\begin{align*}
\norm{(1- B) \sum_{n=0}^\infty B^n }
\end{align*}
TODO: Finish....
\end{proof}

We present some of the basic results on differentiation in Banach
spaces.

\begin{defn}Let $X$ and $Y$ be Banach spaces, let $U \subset X$ be
  open and let $f : U \to Y$ be
  a map.  We say that $f$ is Frechet differentiable at $x \in U$ if there
  exists a bounded linear map $L : X \to Y$ such that
\begin{align*}
\lim_{h \to 0} \frac{f(x + h) - f(x) -Lh}{\norm{h}} = 0
\end{align*}
We call the linear map $L$ the \emph{Frechet derivative} of $f$ at $x$
and denote it $Df(x)$.
\end{defn}

As it stand, we have been a little loose in defining \emph{the}
Frechet derivative as we have not ruled out the possibility that
multiple linear maps may satisfy the defining property.   The first
task is to show that in fact the Frechet derivative is uniquely
defined provided it exists.

\begin{prop}Suppose $A$ and $B$ are bounded linear maps satisfying the
  defining property of the Frechet derivative then $A = B$.
\end{prop}
\begin{proof}
Let $\epsilon > 0$ be given and pick $\delta > 0$ so that we have
$\norm{f(x+h) - f(x) -Ah} < \epsilon \norm{h}$ for $\norm{h} < \delta$
and similarly for $B$.  It then follows that
\begin{align*}
\norm{Ah - Bh} &\leq \norm{f(x+h) - f(x) -Ah} + \norm{f(x+h) - f(x)
                 -Bh} < 2 \epsilon \norm{h}
\end{align*}
so by linearity we see that $\norm{A - B} < 2 \epsilon$.  Since
$\epsilon>0$ was arbitrary it follows that $\norm{A -B} = 0$ and
therefore $A = B$.
\end{proof}

There are weaker forms of derivative that one can consider.  For the
most part we shall be concerned with only the Frechet derivative but
it can be helpful to be aware of the alternatives if for no other
reason than to refine one's understanding of the nature of the Frechet derivative.
\begin{defn}Let $X$ and $Y$ be Banach spaces, let $U \subset X$ be
  open and let $f : U \to Y$ be
  a map.  Let $v \in X$, the we say that $f$ has a directional
  derivative at $x$ in the direction of $v$ if the limit 
\begin{align*}
df(x,v) &= \lim_{t \to
    0} \frac{f(x + tv) - f(x)}{t}
\end{align*} 
exists.  We say that $f$ is
  \emph{G\^{a}teaux differentiable at $x$} if it has directional
  derivatives at all $v \in X$.
\end{defn}

Note that some authors reserve the term G\^{a}teaux differentiable for
functions for which the directional derivatives are a linear form.
Note that under any circumstances we have homogeneity.  We will later show an example of
a nonlinear G\^{a}teaux derivative.

TODO: Example of nonlinear G\^{a}teaux derivative

\begin{prop}\label{GateauxDerivativeHomogeneous}Let $X$ and $Y$ be
  Banach spaces, let $U \subset X$ be open and let $f : U \to Y$ be
  a map such that $f$ is G\^{a}teaux differentiable at $x \in U$, then
  for all $c \in \reals$ and $v \in X$, $df(x,cv) = cdf(x,v)$.
\end{prop}
\begin{proof}
This follows by a simple change of variable in the limit
\begin{align*}
df(x,cv) &= \lim_{t \to  0} \frac{f(x + t c v) - f(x)}{t} = \lim_{t \to  0} \frac{f(x + t v) - f(x)}{t/c} = c \lim_{t \to  0} \frac{f(x + t v) - f(x)}{t} = c df(x,v)
\end{align*} 
\end{proof}


Now we observe that Frechet differentiability implies G\^{a}teaux differentiability.
\begin{prop}\label{FrechetDifferentiableImpliesGateauxDifferentiable}Let $X$ and $Y$ be Banach spaces, let $U \subset X$ be
  open and let $f : U \to Y$ be
  a Fr\'{e}chet differentiable at $x \in U$.  Then $f$ is G\^{a}teaux
  differentiable at $x$ and the directional derivative at $v$ is equal
  to $Df(x)v$.
\end{prop}
\begin{proof}
Let $\epsilon >0$ be given and pick $\delta>0$ such
that $\norm{f(x+h) - f(x) -Df(x)h} \leq \epsilon \norm{h}$ for all
$\norm{h} < \delta$.  With $v \in X$ fixed and suppose that $\norm{v}
= 1$; we note that for all
$\abs{t} < \delta$ we have $\norm{f(x+tv) - f(x) - t Df(x) v}
\leq \epsilon \abs{t} $ and thus 
\begin{align*}
\norm{\frac{f(x+tv) - f(x) }{t} - Df(x) v} < \epsilon
\end{align*}
so that $df(x,v) = Df(x) v$.  Now it is a simple matter to validate
that $df(x, tv) = t df(x,v) = Df(x) \cdot tv$ for all $t \in \reals$.
\end{proof}
In general G\^{a}teaux derivatives need not be linear (i.e. even
though $df(x,tv) = tdf(x,v)$ it is not necessarily the case that
$df(x,v+w) = df(x,v) + df(x,w)$) and even if
linear need not be bounded.  Somewhat more surprising is that even if
the G\^{a}teaux derivative exists and is bounded and linear the
Fr\'{e}chet derivative may not exist.  What is necessary is that the
limits $\lim_{t \to 0} \frac{f(x+tv) -f(x)}{t}$ converge uniformly for
$v$ in the unit sphere.

We calculate some trivial Frechet derivatives.
\begin{examp}Let $f : X \to Y$ be a constant map $f(x) = y$ for some
  fixed $y \in Y$, then $f$ is differentiable at every point $x \in X$
  and moreover $Df(x) = 0$.
\end{examp}
\begin{examp}Let $A : X \to Y$ be a bounded linear map, then $A$ is differentiable at every point $x \in X$
  and moreover $Df(x) = A$.
\end{examp}

The following example generalizes the product rule of calculus.
\begin{examp}Let $A : X_1  \times \dotsm \times X_n \to Y$ be a bounded
  multilinear map, then $A$ is differentiable at every point $x \in
  X_1 \times \dotsm \times X_n$
  and moreover 
\begin{align*}
Df(x_1, \dotsc, x_n)(h_1, \dotsc, h_n) = A(h_1, x_2, \dotsc, x_n) +
  A(x_1, h_2, x_3, \dotsc, x_n) + \dotsm + A(x_1, x_2, \dotsc, h_n)
\end{align*}
\end{examp}

Another important case is the behavior of deriviative when composing
with a linear map.
\begin{examp}\label{FrechetDerivativeCompositionWithLinearMap}Let $X$, $Y$ and $Z$ be Banach spaces, let $U \subset X$
  be open, let $f : U \to Y$ be differentiable and let $A : Y \to Z$
  be a bounded linear map, then $D (A \circ f)(x) = A \circ Df(x)$.

Note that this would follow from the Chain Rule below (Proposition
\ref{ChainRuleBanachSpaces}) but is worth showing this directly to get
some practice with the definitions.  Let $\epsilon > 0$ be given and
pick $\delta>0$ such that $\norm{f(x+h) - f(x) - Df(x)h} \leq
\frac{\epsilon}{\norm{A}} \norm{h}$ for all $\norm{h} < \delta$.  Note
that
\begin{align*}
\norm{Af(x+h) - Af(x) - A Df(x)h} &\leq \norm{A} \norm{f(x+h) - f(x) -
                                    Df(x)h} \leq \epsilon \norm{h}
\end{align*}
for all $\norm{h} < \delta$ which shows the result.
\end{examp}

\begin{prop}\label{DifferentiabilityImpliesContinuity}Let $X$ and $Y$ be Banach spaces, let $U \subset X$ be
  open and let $f : U \to Y$ be differentiable at $x \in U$ then $f$
  is continuous at $x$.
\end{prop}
\begin{proof}
Let $\epsilon > 0$ be given and define $0 < \delta < \frac{\epsilon}{1 + \norm{Df(x)}}$ small enough so
that $\norm{f(x+h) - f(x) - Df(x)h} \leq \norm{h}$ for all $\norm{h} <
\delta$ then 
\begin{align*}
\norm{f(x + h) - f(x)} &\leq \norm{f(x + h) - f(x) - Df(x)h} +
                         \norm{Df}\norm{h} \\
&\leq \norm{h} + \norm{Df}\norm{h} < \epsilon
\end{align*}
and continuity is proven.
\end{proof}

\begin{prop}[Chain Rule]\label{ChainRuleBanachSpaces}Let $X$, $Y$ and $Z$ be
  Banach spaces, let $U \subset X$ and $f : U \to Y$ be differentiable
  at $x \in U$, let $V \subset Y$ with $f(U) \subset V$, $g : V \to Z$ be
  differentiable at $f(x)$ then $g \circ f : U \to Z$ is
  differentiable at $x$ and moreover
\begin{align*}
D(g \circ f)(x) &= Dg(f(x)) \circ Df(x)
\end{align*}
\end{prop}
\begin{proof}
Let $\epsilon$ be given.  Let $\tilde{\delta} > 0$ be chosen so that
$\norm{g(f(x) + h) - g(f(x)) - Dg(f(x)) h} < \frac{1}{2} \epsilon
\norm{h}$ for all $\norm{h} < \tilde{\delta}$.  By continuity of $f$
at $x$ we can choose $\delta_1>0$ such that $\norm{f(x+h) - f(x)} <
\tilde{\delta}$ for all $\norm{h} < \delta_1$ and by differentiability
we may choose $\delta_2>0$ such that $\norm{f(x+h) - f(x) - Df(x)h} <
\frac{\epsilon}{\epsilon + 2\norm{Dg(f(x))}} \norm{h}$ for all
$\norm{h} < \delta_2$.  Let $\delta = \delta_1 \vee \delta_2$ and then
for $\norm{h} < \delta$ we compute
\begin{align*}
&\norm{g(f(x+h)) - g(f(x)) - Dg(f(x)) Df(x) h} \\
&\leq \norm{g(f(x+h)) -
                                                g(f(x)) - Dg(f(x))
                                                (f(x+h)-f(x)) }  \\
&+\norm{Dg(f(x))(f(x+h)-f(x))
                                                - Dg(f(x)) Df(x) h}\\
&\leq \frac{1}{2} \epsilon\norm{f(x+h)-f(x)}  +\norm{Dg(f(x))}\norm{f(x+h)-f(x) - Df(x) h}\\
&\leq \frac{1}{2} \epsilon \norm{h}  +(\frac{\epsilon}{2} +
  \norm{Dg(f(x))}) \frac{\epsilon}{\epsilon + 2\norm{Dg(f(x))}}
  \norm{h} = \epsilon \norm{h}\\
\end{align*}
and we're done.
\end{proof}

\subsection{Higher Order Derivatives and Taylor's Theorem}

\begin{thm}[Mean Value Theorem]\label{MeanValueTheoremBanachSpaces}Let
  $X$ and $Y$ be  Banach spaces, $U \subset X$ be open and let $f : U
  \to Y$ be continuously differentiable.  Suppose $x \in U$ and $y \in
  X$ such that $x + ty \in U$ for all $0 \leq t \leq 1$ then
\begin{align*}
f(x + y) - f(x) &= \int_0^1 Df(x + ty) y dt = \int_0^1 Df(x + ty) dt
                  \cdot y
\end{align*}
\end{thm}
\begin{proof}
Define $g(t) = f(x + ty)$.  Then by the Chain Rule it follows that
$g(t)$ is continuously differentiable and $Dg(t) = Df(x + ty) y$.
Since $Dg(t)$ is continuous we may apply the Fundamental Theorem of
Calculus (Theorem
\ref{FundamentalTheoremOfCalculusForBanachSpaceRiemannIntegrals}) to
conclude that 
\begin{align*}
f(x+y) - f(x) &= g(1) - g(0) = \int_0^1 Df(x+ty) y dt = \int_0^1
                Df(x+ty) dt \cdot y
\end{align*}
where in the last inequality we have use Proposition \ref{RiemannIntegralOfContinuousMaps}.
\end{proof}

Higher order derivatives are defined by iterating Frechet derivatives.  For
example if we assume that the map $f : U \to Y$ differentiable on all
of $U$ then the second derivative is obtained by
taking the derivative of the map $Df : U \to L(X,Y)$ whereever it
exists.  Thus the second derivative is a map $D^2f : U \to
L(X,L(X,Y))$.  

\begin{examp}Let $A : X \to Y$ be a bounded linear map then $D^2A = 0$.
\end{examp}

Based on the definition via induction we think of $D^nf$ as a map from
$U$ to $L(X, \dotsb ,L(X, Y) \dotsb)$.  The range here actually has a
more convenient representation as the space of multilinear maps $X
\times \dotsm \times X \to Y$.  For example given an element in $f \in
L(X,L(X,Y))$ we may define $\tilde{f}(u,v) = f(u) v$ and note that
\begin{align*}
\tilde{f}(au + bv, w) &= f(au + bv) w = a f(u) w + b f(v) w = a
\tilde{f}(u,w) + b \tilde{f}(v,w)
\end{align*} and 
\begin{align*}
\tilde{f}(u, av + bw) &= f(u)(av+bw) = a f(u) v + b f(u) w = a
                        \tilde{f}(u,v) + b\tilde{f}(u,w)
\end{align*}
so that $\tilde{f}$ is indeed bilinear.  It is easy to see that this
is an isomorphism and that the construction extends to general $n$.

TODO: Do this in the required excruciating detail...

In the sequel, it will be convenient to view higher derivatives as maps
from $U$ to the space of multilinear maps.  It turns out that higher
derivatives are not arbitrary multilinear maps but also have the
property of being symmetric.
\begin{prop}Let $U \subset X$ be open and $f : U \to Y$ be $C^p$ then
  $D^pf(x)$ is multilinear and symmetric for every $x \in U$.
\end{prop}
\begin{proof}
TODO:
We first consider the case $p=2$.  Let $u,v \in X$ and consider
\begin{align*}
D^2f(x) (u,v) = 
\end{align*}
\end{proof}

A more complicated but important example is the computation of the
derivative of the inverse in a Banach algebra.
\begin{prop}\label{FrechetDerivativeOfInverse}The map $\phi(A) = A^{-1}$ on $L(X,X)$ is $C^\infty$ on the open set of
  invertible maps.  In fact we have
\begin{align*}
D^n\phi(A)(h_1, \dotsc, h_n) &= (-1)^n\sum_\sigma A^{-1} h_{\sigma_1}
                               A^{-1} \dotsm h_{\sigma_n} A^{-1}
\end{align*}
where the summation is over all permutations of $\lbrace 1, \dotsc, n \rbrace$.
\end{prop}
\begin{proof}
We first compute the first derivative of $\phi$.  Let $A$ be invertible and
observe that for $\norm{h} < \norm{A^{-1}}^{-1}$ we know that $I +
A^{-1} h$ is invertible and moreover
\begin{align*}
(A + h)^{-1} &= A^{-1} (I + h A^{-1})^{-1} = A^{-1} \sum_{n=0}^\infty
              (-1)^n h^n A^{-n}
\end{align*}
and therefore using the absolute convergence of the series on the
right we get
\begin{align*}
\norm{(A + h)^{-1} - A^{-1} + A^{-1} h A^{-1}} &\leq \sum_{n=2}^\infty
                                                 \norm{h}^n
                                                 \norm{A^{-1}}^n \\
&= \frac{\norm{h}^2\norm{A^{-1}}^2}{1 - \norm{h}\norm{A^{-1}}} < \norm{h}^2\norm{A^{-1}}^2
\end{align*}
which shows us that $D\phi(A)h = -A^{-1} h A^{-1}$ (for $\epsilon > 0$
let $\delta < \epsilon \norm{A^{-1}}^{-2}$).

Now to see that $\phi$ is in fact $C^{\infty}$, we do an induction.
TODO: Finish
\end{proof}

With the defintion of higher derivatives available we are now able to
extend the Mean Value Theorem to Taylor's Theorem in Banach spaces.  
\begin{thm}[Taylor's Theorem]\label{TaylorsTheoremBanachSpaces}Let $X$
  and $Y$ be Banach spaces and let $U \subset X$ be open and of class
  $C^p$.  Suppose that $x \in U$ and $y \in X$ such that $x + ty \in
  U$ for all $0 \leq t \leq 1$ then we have
\begin{align*}
f(x+y) &= f(x) + Df(x) y + \dotsm + \frac{D^{p-1}f(x)
         y^{(p-1)}}{(p-1)!} + \int_0^1 \frac{(1-t)^{p-1}}{(p-1)!}
         D^pf(x + ty) y^{(p)} dt
\end{align*}
where $y^{(k)} = (y, \dotsc, y) \in X^k$.
\end{thm}
\begin{proof}
TODO
\end{proof}

It is worth noting that in the case $Y=\reals$ that Theorem
\ref{TaylorsTheoremBanachSpaces} can be proven using the one
dimensional version Theorem \ref{TaylorsTheorem} and the chain rule
Proposition \ref{ChainRuleBanachSpaces}.  We'll show this in the proof
of the Lagrange form of the remainder term below.

We've presented Taylor's Theorem in Banach spaces with the integral
form of the remainder term.  There are several different versions of
the remainder and estimates derived therefrom that are useful to
note.  The first that we mention is applicable in the important case
in which $Y = \reals$; the Lagrange form of the remainder.
\begin{prop}\label{LagrangeFormRemainderBanachSpaces}There is a number $c \in
  (0,1)$ such that 
\begin{align*}
\int_0^1 \frac{(1-t)^{p-1}}{(p-1)!}
         D^pf(x + ty) y^{(p)} dt &= 
\frac{D^pf(x + cy) y^{(p)}}{p!}
\end{align*}
\end{prop}
\begin{proof}
We derive this from the one dimensional Taylor's Theorem.  Note that
$g(t) = f(x + ty)$ is $C^p$ from $[0,1]$ to $\reals$ and by the chain
rule we have $g^\prime(t) = Df(x + ty) y$.  Now since evaluation $A \to A y$
is a bounded linear map on $L(X,Y)$, an induction argument using either Example \ref
{FrechetDerivativeCompositionWithLinearMap}
or the chain rule shows that $g^{(k)}(t) = D^kf(x+ty) y^{(k)}$.
Now apply Theorem \ref{TaylorsTheorem} to see there is a $0 < c < 1$ such that 
\begin{align*}
f(x+y) &= g(1) = g(0) + g^\prime(0) + \dotsm + \frac{g^{(p-1)} (0)}{(p-1)!} +
         \frac{g^{(p)} (c)}{p!}  \\
&=f(x) + Df(x) y + \dotsm + \frac{D^{p-1}f(x) y^{(p-1)}}{(p-1)!} +
         \frac{D^{p}f(x + cy) y^{(p)}}{p!}
\end{align*}
\end{proof}

TODO: Analytic functional calculus and beyond.

Frechet derivatives of functions that map into product spaces are easily computed.
\begin{prop}\label{FrechetDerivativeProductSpaces}Let $f : U \to Y \times Z$ be a function then $f$ is Frechet differentiable at $x \in U$ if and only if the coordinate functions $f_1 : U \to Y$ and
$f_2 : U \to Z$ are Frechet differentiable at $x \in U$ and in this case $Df(x)v = (Df_1(x)v, Df_2(x)v)$.
\end{prop}
\begin{proof}
Let $\pi_1$ and $\pi_2$ be the projections from $Y \times Z$ to $Y$ and $Z$ respectively.  Suppose that $f$ is Frechet differentiable at $x$ then by the Chain Rule and the linearity of projections $\pi_i$ we know that $f_i = \pi_i \circ f$ is Frechet differentiabe and $D f_i = \pi_i \circ Df$.

Suppose that $D f_1(x)$ and $D f_2(x)$ both exist.  Then for any $\epsilon>0$ there exists $\delta>0$ such that $\norm{f_i(x+h) - f_i(x) - Df_i(x) h} < \epsilon \norm{h}/\sqrt{2}$ for any $\norm{h} < \delta$ and $i=1,2$.   The derivative of $f$ follows from 
\begin{align*}
\norm{f(x+h) - f(x) - (Df_1(x) h, Df_2(x) h)} &\leq \left(\norm{f_1(x+h) - f_1(x) - Df_1(x) h}^2 +  \norm{f_2(x+h) - f_2(x) - Df_2(x) h}^2 \right)^{1/2} \\
&\leq \epsilon \norm{h}^{-1}
\end{align*}
\end{proof}

There is an generalization of the notion of partial derivatives to the Frechet differentiation on Banach spaces.
Recall that given Banach spaces $X_1, X_2, \dotsc, X_n$ the cartesian product $X_1 \times \dotsb \times X_n$ with norm
$\norm{(v_1, \dotsc, v_n)} = \norm{v_1} \vee \dotsb \vee \norm{v_n}$ is a Banach space.  
\begin{defn}Let $Y,X_1, X_2, \dotsc, X_n$ be Banach spaces and let $U_i \subset X_i$ be open for $i=1, \dotsc, n$ and $f : U_1 \times \dotsb U_n \to Y$ be a map.  Let 
$v=(v_1, \dotsc, v_n) \in U_1 \times \dotsb U_n$ then we say that $f$ has a partial derivative with respect to $i$ at $v$ if there exists a bounded linear map $L_i : X_i \to Y$ 
such that 
\begin{align*}
\lim_{h \to 0} \frac{f(v_1, \dotsc, v_{i-1}, v_i + h, v_{i+1}, \dotsc, v_n) - f(v_1, \dotsc, v_n) - L_i h}{\norm{h}} &= 0
\end{align*}
We say that $L_i$ is the $i^{th}$ partial derivative and is denoted $D_i f (v)$.
\end{defn}

In general the existence of partial derivatives does not guarantee the existence of total Frechet deriviative however in the presence of continuity it does.
\begin{prop}\label{PartialDerivativesBanachSpaces}Let $Y,X_1, X_2, \dotsc, X_n$ be Banach spaces and let $U_i \subset X_i$ be open for $i=1, \dotsc, n$ and $f : U_1 \times \dotsb \times U_n \to Y$ be a map then $f$ is continuously differentiable on $U_1 \times \dotsb \times U_n$ if and only if $D_i f : U_1 \times \dotsb \times U_n \to L(X_i,Y)$ exists and is continuous for each $i=1, \dotsc,n$.
In this case,
\begin{align*}
D f (v) h = \sum_{i=1}^n D_i f( v) h_i
\end{align*}
\end{prop}
\begin{proof}
Suppose that continuous partial derivatives exist, let $v=(v_1, \dotsc, v_n) \in U_1 \times \dotsb \times U_n$ and let $h=(h_1, \dotsc, h_n) \in X_1 \times \dotsb \times X_n$.  
For each $i=0, \dotsc, n$ let $h^i$ be obtained by $h$ by setting the first $i$ coordinates to $0$; in particular $h^0=h$ and $h^n=0$ and for $i=1, \dotsc, n$ $h^{i-1}-h^{i}$ is non-zero only in the $i^{th}$ coordinate and that coordinate is $h_i$.
Writing a telescoping sum and using the Mean Value Theorem \ref{MeanValueTheoremBanachSpaces}
\begin{align*}
f(v + h) - f(v) &= \sum_{i=1}^{n} (f(v + h^{i-1}) - f(v+h^{i})) = \sum_{i=0}^{n-1} \int_0^1 D_i f(v+h^i + t(h^{i-1}-h^i)) h_i \, dt \\
&=\sum_{i=0}^{n-1} \int_0^1 D_i f (v) h_i \, dt + \int_0^1 \sum_{i=0}^{n-1} \int_0^1 (D_i f(v+h^i + t(h^{i-1}-h^i)) h_i - D_i f (v) h_i )\, dt \\
&=\sum_{i=0}^{n-1} D_i f (v) h_i + \int_0^1 \sum_{i=0}^{n-1} \int_0^1 (D_i f(v+h^i + t(h^{i-1}-h^i)) h_i - D_i f (v) h_i )\, dt \\
\end{align*}
It suffices to show that each term $\int_0^1 (D_i f(v+h^i + t(h^{i-1}-h^i)) h_i - D_i f (v) h_i )\, dt$ is $o(h)$.  To see this recall $\norm{h_i} \leq \norm{h_1} \vee \dotsb \norm{v_n} = \norm{v}$ and thus we have the simple bound
\begin{align*}
&\frac{\norm {\int_0^1 (D_i f(v+h^i + t(h^{i-1}-h^i)) h_i - D_i f (v) h_i )\, dt}}{h} \\
&\leq \frac{\norm {\int_0^1 (D_i f(v+h^i + t(h^{i-1}-h^i)) h_i - D_i f (v) h_i )\, dt}}{h_i} \\
&\leq \frac{\int_0^1 \norm { D_i f(v+h^i + t(h^{i-1}-h^i)) h_i - D_i f (v) h_i }\, dt}{h_i} \\
&\leq \frac{\norm{h_i} \sup_{0 \leq t \leq 1} \norm { D_i f(v+h^i + t(h^{i-1}-h^i)) - D_i f (v) }}{h_i} \\
&=  \sup_{0 \leq t \leq 1} \norm { D_i f(v+h^i + t(h^{i-1}-h^i)) - D_i f (v) }\\
\end{align*}
Since $D_i f$ is continuous at $v$, for every $\epsilon > 0$ we may find a $\delta > 0$ such that $\norm{D_i f(v + u) - D_i f(v)} < \epsilon$ for every $\norm{u} < \delta$.  Thus if $\norm{h} < \delta$ then $\norm{h^i + t(h^{i-1}-h^i)} = \abs{t} \norm{h_i} \leq \norm{h} < \delta$ and therefore $\sup_{0 \leq t \leq 1} \norm { D_i f(v+h^i + t(h^{i-1}-h^i)) - D_i f (v) }\leq \epsilon$.  Thus 
\begin{align*}
&\lim_{h \to 0} \frac{\norm {\int_0^1 (D_i f(v+h^i + t(h^{i-1}-h^i)) h_i - D_i f (v) h_i )\, dt}}{h} \\
&\leq \lim_{h \to 0} \sup_{0 \leq t \leq 1} \norm { D_i f(v+h^i + t(h^{i-1}-h^i)) - D_i f (v) }= 0
\end{align*}

To see the other direction suppose that $Df$ exists and is continuous on $U_1 \times \dotsb \times U_n$.  Let $\epsilon > 0$ be given.  Since $Df(v)$ exists we may find $\delta > 0$ so that $\norm{h} < \delta$ we have $\norm{f(v+h) - f(v) - Df(v) h} < \norm{h} \epsilon$.   Let $v=(v_1, \dotsc, v_n) \in U_1 \times \dotsb \times U_n$ and $h \in X_i$ and write $h_i$ be vector in $U_1 \times \dotsb \times U_n$ with $h$ the $i^{th}$ coordinate and $0$ in the others.   Note that $\norm{h} = \norm{h_i}$.  Thus for all $\norm{h} < \delta$
\begin{align*}
&\norm{f(v_1, \dotsc, v_{i-1}, v_i+h, v_{i+1}, \dotsc, v_n) - f(v_1, \dotsc, v_n) - Df(v) h_i} \\
&= \norm{f(v + h_i) - f(v) - Df(v) h_i} \leq \norm{h_i} \epsilon = \norm{h} \epsilon
\end{align*}
which shows that $D_i f(v)$ exists and $D_i f(v) h = D_i f(v) h_i$.  Continuity of $D_i f$ follows from the continuity of $Df$ and the continuity 
\end{proof}

\subsection{Inverse and Implicit Function Theorems}

\begin{thm}\label{InverseFunctionTheoremBanachSpaces}Let $X$ and $Y$ be Banach spaces let $U \subset X$ be an
  open subset of $X$ and suppose that $f : U \to Y$ is continuously
  differentiable and $Df(x)$ is invertible at $x \in U$.  There is an
  open set $V \subset U$ containing $x$ and an open set $W \subset Y$
  containing
  $f(x)$ such that $f : V \to W$ is a bijection and $f^{-1}$ is
  continuously differentialble on $W$.
\end{thm}

We'll be a bit redundant a  provide both the general proof in Banach spaces as
well as a proof for the finite dimensional case that is more verbose
but is very elementary.

For the finite dimensional proof we use the following simple
consequence of the mean value theorem
that shows a continuously differentiable function is Lipschitz
continuous on a bounded domain.
\begin{lem}\label{IFT:BoundedDerivativeImpliesLipschitz}Let $f : \reals^n \to \reals^n$ be differentiable on an
  open rectangle $R = (a_1,b_1) \times \dotsm \times (a_n, b_n)$ such that 
\begin{align*}
\abs{\frac{\partial f_i}{\partial x_j}(x)} \leq M
\end{align*}
for all $1 \leq i,j \leq n$ and $x \in R$ then it follows that
$\norm{f(y)-f(x)} \leq M \cdot n^2 \cdot \norm{y-x}$ for all $x,y \in R$.
\end{lem}
\begin{proof}
By expanding as a telescoping sum and the one
dimensional mean value theorem we
get for every $i=1, \dotsc, n$ 
\begin{align*}
f_i(y) - f_i(x) &= \sum_{j=1}^n f_i(y_1, \dotsc, y_j, x_{j+1},
                     \dotsc, x_n) - 
f_i(y_1, \dotsc, y_{j-1}, x_{j},
                     \dotsc, x_n)\\
&= \sum_{j=1}^n \frac{\partial f_i}{\partial x_j} (y_1, \dotsc, y_{j-1}, y^*_j, x_{j+1},
                     \dotsc, x_n) (y_j - x_j)\\
\end{align*}
where $a_j < y^*_j  < b_j$ (in fact $x_j \leq y^*_j \leq y_j$ when
$x_j \leq y_j$ and similarly when $y_j < x_j$).  Now by the triangle
inequality and the bound on partials of $f$ we get
\begin{align*}
\norm{f(y) - f(x)} &\leq \sum_{i=1}^n \abs{f_i(y) - f_i(x)} \\
&= \sum_{i=1}^n \sum_{j=1}^n \abs{\frac{\partial f_i}{\partial x_j} (y_1, \dotsc, y_{j-1}, y^*_j, x_{j+1},
                     \dotsc, x_n)} \abs{ y_j - x_j } \\
&\leq \sum_{i=1}^n \sum_{j=1}^n M \norm{y - x} = M \cdot n^2 \cdot
  \norm{y -x}
\end{align*}
\end{proof}

Now we can proceed with the proof of the theorem in the finite
dimensional case.
\begin{proof}
We first make a reduction to the case in which $Df(x)$ is the
identity.  If the result is proven in that case then for general $f$
we can define $Df(x)^{-1} \circ f : X \to X$ where from the Chain Rule
it follows that
$D(Df(x)^{-1} \circ f)(x)$ is the identity.  Applying the inverse
function theorem we see there exists open
sets $V$ and $\tilde{W}$ containing $x$ and $Df(x)^{-1}  f(x)$ respectively such
$Df(x)^{-1} \circ f$ is a bijection from $V$ to $\tilde{W}$ with $(Df(x)^{-1} \circ f)^{-1}$
continuously differentiable.  Now we define $W = Df(x)(\tilde{W})$
which is open by continuity of $Df(x)^{-1}$ and contains $f(x)$.
Since $f^{-1} = Df(x) \circ Df(x)^{-1} \circ f$ it follows by the
Chain Rule that $f^{-1}$ is continuously differentiable on $W$.

\begin{clm}There is an open ball $B(x, \delta) \subset U$ such that $f$ is
injective on the closure of $B(x, \delta)$, $Df(y)$ is invertible for all $y \in B(x,
\delta)$ and 
\begin{align}
\abs{\frac{\partial f_i}{\partial x}(y) - \frac{\partial
    f_i}{\partial x}(x)} 
&< \frac{1}{2n^2} \text{ for all $1 \leq i,j \leq n$ and $y \in
  B(x,\delta)$}
\end{align}\label{IFT:Partials}
\end{clm}

By the the openness of $U$, triangle inequality and the fact that $Df(x)$ is the
identity we know that we can find $\delta > 0$
such that $B(x,\delta) \subset U$ and 
\begin{align*}
\norm{f(x+h) - f(x)} &= \norm{f(x+h) - f(x) - h + h} \geq \norm{h} -
                       \norm{f(x+h) - f(x) - h} \\
&\geq \frac{1}{2}\norm{h}
\end{align*}
so injectivity on $B(x, \delta)$ follows; by continuity of $f$ the
bound and hence the injectivity extends to the closure of $B(x,\delta)$.  Since the invertible linear maps are an open
subset of $L(X,Y)$ and $Df$ is continuous we may also assume that
$\delta > 0$ is chosen so that $Df$ is invertible on $B(x,\delta)$.
Similarly continuity of $Df$ implies the continuity of each partial
derivative $\frac{\partial f_i}{\partial x}$ and therefore
\eqref{IFT:Partials} follows for sufficiently small $\delta > 0$.

The next claim should be thought of as asserting that the inverse of
$f$ is Lipschitz.  As it turns out the estimate is useful in showing
that $f^{-1}$ exists.

\begin{clm}$\norm{y - z}
\leq 2 \norm{f(y) - f(z)}$ for all $y,z \in B(x, \delta)$.
\end{clm}

Define $g(x) = f(x) - x$ on $B(x, \delta)$.  Because $Df(x)$ is the
identity we know that $\frac{\partial f_i}{\partial x}(x) =
\delta_{ij}$ and therefore 
\begin{align*}
\abs{\frac{\partial g_i}{\partial x}(y)} &=
\abs{\frac{\partial f_i}{\partial x}(y) - \frac{\partial f_i}{\partial
                                           x}(x)} 
\leq \frac{1}{2n^2}
\end{align*}
Since $y$ and $z$ are contained in some open rectangle that is a
subset of $B(x,\delta)$ we can apply Lemma \ref{IFT:BoundedDerivativeImpliesLipschitz} and the
triangle inequality to
conclude that 
\begin{align*}
\frac{1}{2} \norm{y - z} &\geq \norm{g(y) - g(z)} \\
&=\norm{f(y) - y - g(z) + z} \\
&\geq \norm{y - z} - \norm{f(y) - g(z)}
\end{align*}
and the claim follows by collecting terms.

The next step in the proof is to validate that the image of $B(x,
\delta)$ under $f$ contains an open set (on which we will then have a
bijection).  Consider the function $f(y) - f(x)$.  It is
continuous and by compactness of the boundary $\partial B(x,\delta)$
and the injectivity of $f$ on the closed ball we know that there
exists an $\epsilon > 0$ such that $g(y) \geq \epsilon$ on $\partial
B(x,\delta)$.  Define $W = B(f(x), \epsilon/2)$ and notice that by the
choice of $\epsilon$, the triangle inequality and the previous claim
we have for all $z \in W$ and $y \in \partial B(x, \delta)$
\begin{align}
\norm{z - f(y)} &\geq \norm{f(x) - f(y)} - \norm {f(x) - z} \geq d -
                  \frac{d}{2} \\
&= \frac{d}{2} > \norm{z - f(x)}
\end{align}\label{IFT:InteriorMinimumEstimate}

This estimate is used to construct an open set in the image of $f$.

\begin{clm}For every $z \in W$ there is a unique $y \in B(x, \delta)$ such
that $f(y) = z$.
\end{clm}

To see existence we let $z \in W$ be given and we define the function
$h(y) = \norm{f(y) - z}^2 = \sum_{j=1}^n (f_j(y) - z_j)^2$.  Differentiability of $h$ follows from the
differentiability of $f$ and the chain rule.  In
particular, $h$ is continuous and therefore by compactness of the
closed ball $\overline{B}(x, \delta)$ it attains its minimum.  By the
estimate \eqref{IFT:InteriorMinimumEstimate} we see that the minimum
must occur in the interior of the ball.  Therefore we know that the
derivative of $h$ must vanish at the minimum so by the Chain Rule we
know that for all $v \in X$
\begin{align*}
0 &=D \norm{f - z}^2 (y) \cdot v = 2 \norm{Df(y)\cdot v}{f(y) - z}
\end{align*}
and by the invertibility of $Df(y)$ it follows that we must have $f(y)
= z$ at the minimum.

The uniqueness of $y$ follows from the injectivity of $f$.

Now we define $V = f^{-1}(W) \cap B(x, \delta)$ and it follows that
$f$ is a bijection from $V$ to $W$. Now that $f^{-1}$ is well defined
we immediately get its continuity.  

\begin{clm}$f^{-1}$ is continuous on $W$.
\end{clm}

The second claim proved the bound $\norm{y - z} \leq 2 \norm{f(y) -
  f(z)}$ on $B(x, \delta)$ which certainly shows that $\norm{f^{-1}(z)
  - f^{-1}(w)} \leq 2 \norm{z -w}$ on $W$ so that $f^{-1}$ is
Lipschitz in particular continuous.

It remains to show that $f^{-1}$ is differentiable.

\begin{clm}$f^{-1}$ is continuously differentiable on $W$.
\end{clm}

In fact we show (as would follow from the Chain Rule) that $Df^{-1}
(z) = \left[ Df(f^{-1}(z) )\right]^{-1}$ for all $z \in W$.  Note that
this is well defined since $Df$ is invertible on all of $V$.  To clean
up the notation a bit let $A = Df(f^{-1}(z) )$.  Let $\epsilon > 0$ be
given.  Using differentiability of $f$ at $f^{-1}(z)$ we choose
$\tilde{\eta} > 0$ such that
\begin{align*}
\norm{f(f^{-1}(z) +
  h) -
  f(f^{-1}(z)) - Ah} &< \frac{\epsilon}{2\norm{A^{-1}}} \norm{h}
                       \text{ for all $\norm{h} < \tilde{\eta}$}
\end{align*}
By continuity of $f^{-1}$ at $z$ we choose $\eta > 0$ such that
$\norm{f^{-1}(z+h) - f^{-1}(z)} < \tilde{\eta}$ for
all $\norm{h} < \eta$.  Pick $h \in Y$ with $\norm{h}<\eta$ and
compute using the Lipschitz continuity of $f^{-1}$
\begin{align*}
&\norm{f^{-1}(z + h) - f^{-1}(z) - A^{-1} h} \\
&=\norm{A^{-1} \left(f(f^{-1}(z+h)) - f(f^{-1}(z)) - A(f^{-1}(z + h) -
                                              f^{-1}(z)) \right)} \\
&\leq \norm{A^{-1}} \norm{f(f^{-1}(z+h)) - f(f^{-1}(z)) - A(f^{-1}(z + h) -
                                              f^{-1}(z))} \\
&\leq \frac{1}{2} \epsilon \norm{f^{-1}(z + h) - f^{-1}(z)} \leq \epsilon \norm{h} 
\end{align*}
which gives us $Df^{-1}(z) = \left[ Df(f^{-1}(z) )\right]^{-1}$.
Continuity of $Df^{-1}$ follows from the continuity of $Df$,
continuity of $f^{-1}$ and continuity of inversion of invertible maps
in $L(X,Y)$.
\end{proof}

The proof of the Inverse Function Theorem in general Banach spaces
rests on a simple result that is of broad applicability.  The result
actually doesn't use the vector space structure and is valid in
general complete metric spaces; it provides a
very general mechanism for solving equations in such spaces.

\begin{prop}[Contraction Mapping
  Principle]\label{ContractionMappingPrinciple}Let $(S,d)$ be a complete
  metric space
  space, let $F \subset S$ be a closed subset and let $g : F \to F$ be
  a mapping such that there exists a constant $0 < K < 1$ such that 
\begin{align*}
d(g(x), g(y)) &\leq K d(x,y) \text{ for all $x,y \in F$}
\end{align*}
then there exists a unique $x_0 \in F$ such that $g(x_0) = x_0$ and
moreover given any $x \in F$ the sequence $\lbrace g^n(x) \rbrace$ is
Cauchy and converges to $x_0$.
\end{prop}
\begin{proof}
First we prove uniqueness.  Suppose there are two points $x$ and $y$
satisfying $g(x) = x$ and $g(y) = y$ then we know that
\begin{align*}
d(x,y) &= d(g(x), g(y)) \leq K d(x,y)
\end{align*}
and since $0 < K < 1$ this shows that $d(x,y) = 0$.

Now we let $x \in F$ be arbitrary and show that $g^n(x)$ is Cauchy.
Suppose that $m > n$ and observe that by a simple induction
\begin{align*}
d(g^n(x), g^m(x)) \leq K^n d(x, g^{m-n}(x))
\end{align*}
In particular, we have that $d(g^n(x), g^{n+1}(x)) \leq K^n d(x,g(x))$
and therefore by the triangle inequality 
\begin{align*}
d(x, g^n(x)) &\leq d(x, g(x)) + \dotsm + d(g^{n-1}(x), g^n(x)) \leq (1
               + \dotsm + K^{n-1}) d(x,g(x)) < \frac{d(x,g(x))}{1-K}
\end{align*}
Putting these two bounds together we see that $d(g^n(x), g^m(x)) \leq  \frac{K^n
  d(x,g(x))}{1-K}$ hence $g^n(x)$ is Cauchy.  

Since $F$ is
a closed subset of a complete metric space, it is 
complete and we know that $g^n(x)$ converges to some $x_0 \in F$; it remains to show that $x_0$
is a fixed point of $g$.  Let $\epsilon > 0$ be given and chose $N >
0$ such that $d(x_0, g^n(x)) < \epsilon$ for all $n \geq N$.  Then we
know that 
\begin{align*}
d(g(x_0), g^n(x)) &\leq K d(x_0, g^{n-1}(x)) \leq K\epsilon < \epsilon
\end{align*}
for all $n \geq N+1$ which shows that $g^n(x)$ converges to $g(x_0)$.  It
follows that $x_0 = g(x_0)$.
\end{proof}

\begin{proof}
The first step in the proof of the Inverse Function Theorem is the construction of the inverse.  In order to use the contraction mapping principle to build the inverse we have to formulate a fixed point problem.  A little bit of experimentation witht the case $X=Y$ will suggest that if we let $y \in Y$ and define $g_y(x) = x + f(x) - y$ then finding a fixed point $g_y(x) = x$ is equivalent to $f(x) = y$.  This idea is on target but doesn't yet work.  Obviously we have the restriction of the assumption $X=Y$ to lift but deeper is the observation that $g_y$ as defined above is not a contraction mapping in general.  If we calculate the Frechet derivative we
we get $Dg_y(x_0) = \IdentityMatrix + Df(x_0)$ which can be arbitrarily expansive (consider the example of $X=Y=\reals$, $x_0=y_0=0$ and $f(x) = ax$ for a large constant $a$).  The trick is to use the inverse of $Df(x_0)$ in the fixed point formulation in order to counteract the expansiveness of $f$ itself.  Doing this properly we also create a fixed point problem that makes sense in the case that $X \neq Y$.  Specifically, for each $y \in Y$ define
\begin{align*}
g_y(x) &= Df(x_0)^{-1} (Df(x_0) \cdot x + f(x) - y) = x + Df(x_0)^{-1} f (x) -  Df(x_0)^{-1} y
\end{align*}
then a simple calculation shows that $g_y(x) = x$ if and only if $f(x) = y$ (note that the use of $Df(x_0)^{-1}$ in this way is equivalent to redefining $f$ as $Df(x_0)^{-1} \circ f$ and thereby reducing to the case in which $Df(x_0) = \IdentityMatrix$).  As a quick check that we are on the right track now note that
\begin{align*}
g_{y_0}(x_0) = x_0 + Df(x_0)^{-1} f (x_0) -  Df(x_0)^{-1} y_0 = x_0 + Df(x_0)^{-1} y_0 -  Df(x_0)^{-1} y_0 = x_0
\end{align*}
as we should expect.

\begin{clm}There exists an $r > 0$ such that for all $y \in \overline{B}(y_0, r/2 \norm{Df(x_0)^{-1}})$ there exists a unique $x \in \overline{B}(x_0, r)$ such that $f(x) = y$.
\end{clm}
Define $g_y$ as above and we will show that for $y$ in a neighborhood of $y_0$ the mapping $g_y$ is a contraction on a neighborhood of $x_0$.  First note that $D g_y(x_0) = 0$ and apply continuity of $Df$ to pick an $r > 0$ such that $\norm{D g_y (x)} \leq \frac{1}{2}$ for all $x \in \overline{B}(x_0,2r)$.  By the Mean Value Theorem \ref{MeanValueTheoremBanachSpaces} and Proposition \ref{NormRiemannIntegralBanachSpace} we know that for every $x \in \overline{B}(x_0, r)$ we have 
\begin{align*}
\norm{g_y(x) - g_y(x_0)} &= \norm{\int_0^1 Dg_y(x_0 + t(x-x_0)) (x - x_0) \, dt} \leq \frac{1}{2} \norm{x - x_0}
\end{align*}
From this we compute for $y \in \overline{B}(y_0, r/2 \norm{Df(x_0)^{-1}})$ and $x \in \overline{B}(x_0, r)$
\begin{align*}
\norm{g_y(x) - x_0} &\leq \norm{g_y(x) - g_{y_0}(x)} + \norm{g_{y_0}(x) - g_{y_0}(x_0)}
&= \norm{Df(x_0)^{-1}} \norm{y - y_0} + \frac{1}{2} \norm{x - x_0} \leq r
\end{align*}
which shows that $g_y$ is a mapping from $\overline{B}(x_0, r)$ to itself.  To see that $g_y$ is a contraction mapping let $x_1, x_2 \in \overline{B}(x_0, r)$ and note that for all $0 \leq t \leq 1$, 
\begin{align*}
\norm{x_1 + t (x_2 - x_1) - x_0} &= \norm{(1-t) x_1 + t x_2 - (1-t) x_0 - t x_0}  \leq (1-t) \norm{x_1-x_0} + t \norm{x_2 - x_0} \leq r
\end{align*} 
so another application of the Mean Value Theorem \ref{MeanValueTheoremBanachSpaces}, Proposition \ref{NormRiemannIntegralBanachSpace} and the bounds on $Df(x)$ on $\overline{B}(x_0, r)$ yields
\begin{align*}
\norm{g_y(x_2) - g_y(x_1)} &= \norm{\int_0^1 Dg_y(x_1 + t(x_2 - x_1)) (x_2 - x_1) \, dt} \\
&\leq \int_0^1 \norm{ Dg_y(x_1 + t(x_2 - x_1)) (x_2 - x_1)} \, dt 
\leq \frac{1}{2} \norm{x_2 - x_1}
\end{align*}
Since a closed subset of a Banach space is a complete metric space, we may apply the Contraction Mapping Principle Proposition \ref{ContractionMappingPrinciple} to finish the proof of the claim.

Pick $r>0$ as in the claim and define $U_1 = B(x_0, r) \cap f^{-1}(B(y_0, r/2 \norm{Df(x_0)^{-1}})$.  We know from continuity of $f$ that $U_1$ is open.  Let $V_1 = f(U_1)$.  We know that $f$ is an injective map from $U_1$ to $V_1$ and therefore way define the inverse map $\phi : V_1 \to U_1$.  Note that the prior claim does not actually show that $V_1 = B(y_0, r/2 \norm{Df(x_0)^{-1}})$ since that claim leaves open the possibility that $f$ may map a point on the boundary of $\overline{B}(x_0, r)$ to a point in the interior of $\overline{B}(y_0, r/2 \norm{Df(x_0)^{-1}})$; hence we don't know that $V_1$ is an open set.  We prove that and more in the next claim.
\begin{clm}$V_1$ is an open set and $\phi$ is a continuous map. 
\end{clm}
 To prove the claim, let $x_1 \in U_1$ and define $y_1 = f(x_1)$ so that $y_1 \in B(y_0, r/2 \norm{Df(x_0)^{-1}})$.  Pick another $y_2 \in B(y_0, r/2 \norm{Df(x_0)^{-1}})$ and apply the previous claim to construct the unique $x_2 \in \overline{B}(x_0, r)$ such that $f(x_2) = y_2$.  Using the triangle inequality
\begin{align*}
\norm{x_2 - x_1} &\leq \norm{f(x_2) - f(x_1)} + \norm{x_2 - f(x_2) - (x_1 - f(x_1))} \\
&= \norm{f(x_2) - f(x_1)} + \norm{g_{y_0}(x_2) - g_{y_0}(x_1)} \\
&\leq \norm{f(x_2) - f(x_1)} + \frac{1}{2} \norm{x_2 - x_1}
\end{align*}
and therefore $\norm{x_2 - x_1} \leq 2 \norm{f(x_2) - f(x_1)}$.  From this inequality we see that $V_1$ is open; given $y_1 \in V_1$ if we pick an $\epsilon > 0$ such that $B(\phi(y_1), \epsilon) \subset U_1$ then for any $\norm{y_2 - y_1} < \epsilon/2$ we have $\norm{x_2 - x_1} < \epsilon$ for the $x_2$ with $f(x_2) = y_2$ which implies $x_2 \in U_1$ and therefore $y_2 \in V_1$.  The continuity of $\phi$ also follows from this inequality by essentially the same argument.  Given $y, y_1, y_2, \dotsc \in V_1$ with $\lim_{n \to \infty} y_n = y$ let $x = \phi(y)$ ($f(x) = y$) and $x_n = \phi(y_n)$ ($f(x_n) = y_n$).  For any $\epsilon > 0$ small enough that $B(\phi(y), \epsilon) \subset U_1$ we pick $N > 0$ such that $\norm{y_n - y} < \epsilon/2$ and therefore $\norm{phi(y_n) - \phi(y)} = \norm{x_n - x} \leq 2\norm{y_n - y} < \epsilon$ which shows $\lim_{n \to \infty} \phi(y_n) = \phi(y)$.  

It remains to show that $\phi$ is continuously differentiable on a neighborhood of $y_0$.
\begin{clm}$\phi$ is $C^1$ on a neighborhood of $y_0$.
\end{clm}
Since $f$ is continuously differentiable on a neighborhood of $y_0$, $Df(x_0)$ is invertible and the set of invertible linear maps is an open set of $L(X,Y)$ we may pick an $\epsilon>0$ such that $B(Df(x_0), \epsilon)$ are invertible maps and the define $U_2 = U_1 \cap Df^{-1}(B(Df(x_0), \epsilon))$ which an open neighborhood of $x_0$ such that $Df(x)$ is invertible for all $x \in U_2$.  Define $V_2 = \phi^{-1}(U_2)$ which is an open neighborhood of $y_0$ by continuity of $\phi$ and note that $f$ is a continuous bijection of $U_2$ to $V_2$ with continuous inverse $\phi$ and we still have the inequality $\norm{x_2 - x_1} \leq 2 \norm{f(x_2) - f(x_1)}$ for all $x_1, x_2 \in U_2$.  Given $x_1\in U_2$ we pick $\psi$ such that $f(x) - f(x_1) - Df(x_1) (x - x_1) = \norm{x - x_1} \psi(x - x_1)$ and $\lim_{h \to 0} \psi(h) = 0$.  Let $y_1 = f(x_1) \in V_2$ then
\begin{align*}
\norm{\phi(y) - \phi(y_1) - Df(x_1)^{-1} (y - y_1)} &= \norm{x - x_1 - Df(x_1)^{-1} (f(x) - f(x_1))} \\
&= \norm{x - x_1 - Df(x_1)^{-1} (Df(x_1) (x - x_1) + \norm{x-x_1} \psi(x-x_1))}\\
&= \norm{Df(x_1)^{-1} \norm{x-x_1} \psi(x-x_1))}\\
&\leq \norm{Df(x_1)^{-1}} \norm{x-x_1} \norm{\psi(x-x_1)} \\
&\leq 2 \norm{Df(x_1)^{-1}} \norm{f(x) - f(x_1) } \norm{\psi(\phi(y) -\phi(y_1))} \\
\end{align*}
By continuity of $\phi$ and the fact that $\lim_{h \to 0} \psi(h) = 0$ we have $\lim_{y \to y_1} 2 \norm{Df(x_1)^{-1}} \norm{\psi(\phi(y) -\phi(y_1))} = 0$ which shows that $D\phi(y_1) = Df(\phi(y_1))^{-1}$.  The continuity of $D\phi$ thus follows from the continuity of $\phi$, $Df$ and Proposition \ref{FrechetDerivativeOfInverse}.
\end{proof}

The Inverse Function Theorem has the following equally important
consequence that is known at the Implicit Function Theorem.

\begin{thm}[Implicit Function
  Theorem]\label{ImplicitFunctionTheorem}Let $X$, $Y$ and $Z$ be a Banach
  spaces, let $U \subset X \times Y$ be an open set  and let $f : U
  \to Z$ be $C^p$.  
Suppose $(x_0,y_0) \in U$, that $f(x_0,y_0) = 0$
  and $Df(x_0,y_0)(0,v)$ defines an invertible map from $Y \to Z$, then
  there exists an open set $V \subset X$ such that $x_0 \in V$, an
  open set $W \subset Y$ such that $y_0 \in W$ and a function $g : V \to
  W$ such that $g$ is continuously differentiable, $f(x, g(x)) = 0$
  for all $x \in V$ and $f(x,y) = 0$ if and only if $y = g(x)$ for all
  $(x,y) \in V \times W$.
\end{thm}
\begin{proof}
First define the map $g : U \to X \times Z$ by $\psi(x,y) = (x,
f(x,y))$.  We claim that $g$ has a local inverse at $(x_0,y_0)$.  For this we apply 
Proposition \ref{PartialDerivativesBanachSpaces} to calculate the derivate
\begin{align*}
D\psi(x_0,y_0) (u,v) &= 
\begin{bmatrix}
\IdentityMatrix & 0 \\
D_1 f(x_0, y_0) & D_2 f(x_0, y_0)
\end{bmatrix} 
\begin{bmatrix} 
u \\
v
\end{bmatrix}
= (u, D_1 f(x_0, y_0) u + D_2f(x_0,y_0) v)
\end{align*}
Since $D_2 \psi (x_0,y_0)$ is assumed invertible it is easy to see that
\begin{align*}
D\psi(x_0,y_0)^{-1} &=
\begin{bmatrix}
\IdentityMatrix & 0 \\
-D_2 f(x_0, y_0)^{-1}  \circ D_1 f(x_0, y_0) & D_2 f(x_0, y_0)^{-1}
\end{bmatrix} 
\end{align*}
and therefore we may apply the Inverse Function Theorem \ref{InverseFunctionTheoremBanachSpaces}
to conclude that there exists an open set $W \subset X \times Z$ and a $\phi : W \to U$ with $(x_0,0) \in W$, $\phi$ continuously differentiable on $W$ and
$\psi(\phi(x,z)) = (x,z)$ on $W$.  It follows that $\phi(x, z) = (x, \phi_2(x,z))$ for a continuously differentiable $\phi_2$.  Pick an open ball $V$ containing $x_0$ such that $V \times \lbrace 0 \rbrace \subset W$ and define $g(x) = \phi_2(x,0)$ on $V$ then
$(x,0) = \psi(\phi(x,0)) = (x, f(x, g(x)))$; so $f(x, g(x)) = 0$ as required.  Note that $g(x)$ is the unique point in $\phi(W) \cap \lbrace x \rbrace \times Y$ satisfying $f(x, g(x)) = 0$ for $ x \in V$.   This follows since $\psi$ is injective on $\phi(W)$ and therefore if $f(x, a) = f(x, b)$ then $\psi(x,a) = (x, f(x,a)) = (x, f(x,b)) = \psi(x, b)$ which imples $a=b$.
\end{proof}

\section{Linear Algebra}

This is a little refresher on linear algebra with more of a focus on matrix factorizations.

\begin{defn}Let $\mathds{F}$ be a field, then a \emph{vector space} over $\mathds{F}$ is an abelian group $(V,+,0)$ together with 
a multiplication operator $\mathds{F} \times V \to V$ such that $a (v + w) = av + aw$.
\end{defn}

\begin{defn}If $W \subset V$ is a vector space then we say that $W$ is a \emph{subspace} of $V$.  We say that a set of elements  $v_1, \dotsc, v_n$ of $V$ is \emph{linearly independent} if and only if $a_1 v_1 + \dotsb + a_n v_n = 0$ implies
$a_1 = \dotsb = a_n = 0$.  If a set is not linearly independent then we say it is  \emph{linearly dependent}.  The \emph{dimension} of a vector space $V$ is the supremum of cardinalities of linearly independent sets in $V$.  Given elements $v_\alpha$ in $V$ the \emph{linear span} is the intersection of all subspaces of $V$ containing the $v_\alpha$.  
\end{defn}

\begin{prop}Let $V$ be a vector space of $\mathds{F}$. 
\begin{itemize}
\item[(i)]Let $W_\alpha$ be a collection of subspaces of $V$, then $W = \cap_\alpha W_\alpha$ is a subspace.  
\item[(ii)] Let $v_\alpha$ be elements in $V$ then the span of $v_\alpha$ is a subspace and moreover the span is precisely the set of finite linear combinations of elements of $v_\alpha$.  
\item[(iii)]The dimension of the span of $\lbrace v_1, \dotsc, v_n \rbrace$ is less than or equal to $n$.  It is equal to $n$ if and only if $v_1, \dotsc, v_n$ are linearly independent.
\item[(iv)]$\dim \lbrace v_1, \dotsc, v_{n+1} \rbrace \leq \dim \lbrace v_1, \dotsc, v_n \rbrace + 1$.
\end{itemize}
\end{prop}
\begin{proof}
The fact that $W$ is a subspace is simple and left to the read.  The fact that a linear span is a subspaces follows from the first assertion as it was defined as an intersection of subspaces.  

To see that the set of finite linear combinations is indeed the span, first note that it is clear all finite linear combinations belong to every subspace containing the $v_\alpha$.  It suffices to show that the set of finite linear combinations is a vector space.  Given $u$ and $w$ which are finite linear combinations with index sets $A \subset \Lambda$ and $B \subset \Lambda$ respectively.  By use of zero coefficients we express both $u$ and $w$ using the index set $A \cup B$.  So to be concrete we write $u = \sum_{i=1}^n u_i v_{\alpha_i}$ and $w = \sum_{i=1}^n w_i v_{\alpha_i}$.  Let $a,b \in \mathds{F}$ and we compute $au + bw = \sum_{i=1}^n (au_i + bw_i) v_{\alpha_i}$.

To see (iii) we use induction.  For $n=1$ it is clear that $\dim\lbrace v_1 \rbrace = 1$ if $v_1 \neq 0$ and $0$ if $v_1 = 0$ and that $v_1 \neq 0$ if and only if $\lbrace v_1 \rbrace$ is linearly independent.  Suppose the result is true for $n-1$ and consider $v_1, \dots, v_n$.  Suppose $w_1, \dotsc, w_m$ is a linearly independent set in the span of the $v_i$.  Write
$w_i = \sum_{j=1}^n a_{ij} v_j$ for $1 \leq i \leq m$.  If $a_{in}=0$ for all $1 \leq i \leq n$

TODO: Finish
\end{proof}


\begin{defn}Let $V$ and $W$ be vector spaces over $\mathds{F}$ then a function $A : V \to W$ is said to be a \emph{linear map} if $A (av + bw) = aAv + bAw$ for all $v,w \in V$ and $a,b \in \mathds{F}$.  In the special case that $W = \mathds{F}$ we may say that $A$ is a \emph{linear functional}.  The set of linear functionals on $V$ is denoted $V^*$ and is called the \emph{dual space} to $V$.  Given any linear map $A : V \to W$ we define the \emph{dual map} $A^* : W^* \to V^*$ by $A^*(\lambda)(v)= \lambda(Av)$.  
\end{defn}
Note that the dual map is well defined since 
\begin{align*}
A^*(\lambda)(av + bw) &= \lambda(A(av+bw)) = \lambda(aAv + b A w) = a \lambda(Av) + b\lambda(Aw) = aA^*(\lambda)(v) + b A^*(\lambda)(w)
\end{align*}
shows that $A^*(\lambda)$ is a linear functional.  The dual space is easily seen to be a vector space and the dual map is easily shown to be a linear map.

\begin{prop}$V^*$ is a vector space over $\mathds{F}$ with addition and scalar multiplication defined pointwise as $(a\lambda + b\mu)(v) = a\lambda v + b \mu(v)$.  With repsect to this vector space structure the dual map $A^*$ is linear.  If $V$ is finite dimensional then $V^*$ is finite dimensional and $\dim V = \dim V^*$.
\end{prop}
\begin{proof}
The proof that $V^*$ is a vector space is elementary and left to the reader.  To see that $A^*$  is a linear map we just compute using the definitions
\begin{align*}
A^*( a \lambda + b \mu)(v) &= (a \lambda + b\mu)(Av) = a \lambda (Av) + b \mu (Av) = (a A^*(\lambda) + b A^*(\mu)) (v) \text{ for all $v \in V$}
\end{align*}

Now if $V$ is finite dimensional we can select a basis $v_1, \dotsc, v_n$.  Define $v_i^* \in V^*$ by $v_i^*(v_j) = \delta_{ij}$ for $1 \leq i,j \leq n$.  We claim that $v^*_i$ is a basis for $V^*$.  Clearly $v^*_i$ spans $V^*$ since if we are given an arbitrary $\lambda$ then by linearity
\begin{align*}
\left( \sum_{i=1}^n \lambda(v_i) v^*_i \right) v &= \sum_{i=1}^n \lambda(v_i) v^*_i (\sum_{j=1}^n a_j v_j) = \sum_{i=1}^n a_i \lambda(v_i) = \lambda(v)
\end{align*}
Moreover the $v^*_i$ are seen to be linearly independent since if $\sum_{i=1}^n a_i v^*_i = 0$ then for each $1 \leq j \leq n$ we have $0 = \left(\sum_{i=1}^n a_i v^*_i \right)v_j = a_j$.
Thus $v^*_i$ is a basis hence $\dim V  = \dim V^* = n$.
\end{proof}

The basis $v^*_i$ constructed from the basis $v_i$ in the above proof is referred to as the \emph{dual basis}.  

\begin{defn}Let $A$ be an $m \times n$ real matrix, then a triple comprising an $m \times m$ orthogonal matrix $U$, an $n \times n$ orthogonal matrix $V$ and a $m \times n$ diagonal matrix $\Sigma$ with $\Sigma_{11} \geq \dotsb \geq \Sigma_{pp}$ where $p = m \wedge n$ such that $A = U \Sigma V^T$ is called a \emph{singular value decomposition}.
\end{defn}

\begin{thm}\label{SingularValueDecomposition}Singular value decompositions exist.
\end{thm}
\begin{proof}
The result is trivially true if $A = 0$ (let $\Sigma=0$, $U$ and $V$ be identity matrices), so assume that $A \neq 0$.  Let $\sigma_1$ be the $L^2$ operator norm of $A$.   By compactness of the unit sphere we can find a unit vector $x_1 \in \reals^n$ such that $0 \neq \sigma_1 = \norm{Ax_1}$.  Define $y_1 = \sigma^{-1}_1 Ax_1$ so that $y_1$ is a unit vector in $\reals^m$.  We can now find an orthonormal basis $\lbrace x_2, \dotsc, x_n \rbrace$ of $x_1^{\perp}$ and an orthonormal basis $\lbrace y_2, \dotsc, y_m \rbrace$ of $y_1^{\perp}$.  Define the orthogonal matrices $U_1 = [y_1, \dotsc, y_m]$ and $V_1 = [x_1, \dotsc, x_n]$.  From the fact that $Ax_1 = \sigma_1 y_1$ we can write 
\begin{align*}
U_1^T A V_1 &=
\begin{bmatrix}
\sigma_1 & w^T \\
0 & B
\end{bmatrix}
\end{align*}
where $w \in \reals^{n-1}$ and $B$ is an $(m-1) \times (n-1)$ matrix.
Observe that
\begin{align*}
\norm{U_1^T A V_1} &= \sup_{v \neq 0} \frac{\norm{U_1^T A V_1 v}}{\norm{v}} \geq \frac{\norm{U_1^T A V_1 \begin{bmatrix} \sigma_1 \\ w\end{bmatrix}}}{\norm{\begin{bmatrix} \sigma_1 \\ w\end{bmatrix}}} 
=\frac{\norm{\begin{bmatrix} \sigma_1^2 + w^T w \\ 
B w
 \end{bmatrix}}}{\sqrt{\sigma_1^2 + w^T w}} \\
&= \frac{\sqrt{(\sigma_1^2 + w^T w )^2 + w^TB^TBw}}{\sqrt{\sigma_1^2 + w^T w}} \geq \sqrt{\sigma_1^2 + w^T w}
\end{align*}
On the other hand, by orthogonality of $U_1$ and $V_1$ we know that
$\norm{U_1^T A V_1} = \norm{A} = \sigma_1$ and therefore we see that $w^Tw = 0$ hence $w = 0$.    
Now use the induction hypothesis to conclude that there exist $U_2$ and $V_2$ such that
$U_2^T B V_2 = \Sigma_2$ is diagonal with nonincreasing entries on the diagonal and note that if we define 
\begin{align*}
U &= 
U_1 \begin{bmatrix}
1 & 0 \\
0 & U_2
\end{bmatrix}, &
V &= V_1 
\begin{bmatrix}
1 & 0 \\
0 & V_2
\end{bmatrix}
\end{align*}
Then 
\begin{align*}
U^T A V 
&=
\begin{bmatrix}
1 & 0 \\
0 & U_2^T
\end{bmatrix} U_1^T A V_1 
\begin{bmatrix}
1 & 0 \\
0 & V_2
\end{bmatrix} 
= 
\begin{bmatrix}
1 & 0 \\
0 & U_2^T
\end{bmatrix}
\begin{bmatrix}
\sigma_1 & 0 \\
0 & B
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
0 & V_2
\end{bmatrix} 
= 
\begin{bmatrix}
\sigma_1 & 0 \\
0 & \Sigma_2
\end{bmatrix} 
\end{align*}
Lastly note that embedding $\reals^{n-1}$ into $\reals^n$ as the vectors of the form $\begin{bmatrix}0 \\ v\end{bmatrix}$ we get
\begin{align*}
\sigma_1 &= \norm{A} = \norm{U_1^T A V_1} 
= \sup_{\substack {w \in \reals^n \\ w \neq 0}} \frac{\norm{\begin{bmatrix}
\sigma_1 & 0 \\
0 & B
\end{bmatrix}w}} {\norm{w}} \\
&\geq \sup_{\substack {v \in \reals^{n-1} \\ v \neq 0}} \frac{\norm{Bv}} {\norm{v}} 
= \norm{B} = (\Sigma_2)_{11}\\
\end{align*}
and therefore the diagonal entries of $\Sigma=\begin{bmatrix}
\sigma_1 & 0 \\
0 & \Sigma_2
\end{bmatrix}$ are nonincreasing and we are done.
\end{proof}

The fact that we may perform matrix factorizations in linear algebra in a Borel measurable way is easy to verify with the following elegant method relying on the ``principle of measurable choice''.
\begin{thm}\label{PrincipleOfMeasurableChoice}Let $S$ and $T$ be separable complete metric spaces and let $A \subset S \times T$ be closed and $\sigma$-compact.  Then $\pi_1(A)$ is Borel and there exists a Borel measurable function $f : \pi_1(A) \to T$ such that the graph of $f$ is contained in $A$.
\end{thm}
\begin{proof}
We start with the following
\begin{clm}Let $F$ be a closed set in $T$ then $\pi_1(A \cap S\times F)$ is Borel measurable.
\end{clm}
Write $A = \cup_n K_n$ with $K_n$ compact.  Since $F$ be a closed set it follows that $A \cap S\times F$ is closed $A \cap S\times F =  \cup_n K_n \cap S \times F$ where each $K_n \cap S \times F$ is compact.  Since $\pi_1$ is continuous each $\pi_1 (K_n \cap S \times F)$ is compact in $S$ and $\pi_1 (A \cap S\times F) = \cup_n \pi_1(K_n \cap S \times F)$ is therefore Borel.  

Applying the claim with $F = T$ we see that $\pi_1(A)$ is Borel.

Take a countable dense subset $\lbrace y_n \rbrace$ of $T$.  We define $f$ by an iterative approximation scheme.  
For $x \in \pi_1(A)$ define $f_1(x)$ to be the first $y_n$ (in index order) such that $A \cap \lbrace x \rbrace \times \overline{B}(y_n, 1/2) \neq \emptyset$ where $\overline{B}(z,r)$ represents the closed ball of radius $r$ centered at $z$.  Clearly $f_1$ is well defined since there exists an $(x,y) \in A$ and by density of $\lbrace y_n \rbrace$ in $T$ for any $r > 0$ there exists a $y_n$ with $(x,y) \in \lbrace x \rbrace \times \overline{B}(y_n,r)$.

Next observe that $f_1(x)$ is Borel measurable.  To see this, for each $r > 0$ and $n \in \naturals$ we define 
\begin{align*}
C_{n,r} &= \pi_1(A \cap S \times \overline{B}(y_n,r) )\\
&= \lbrace x \in S \mid A \cap \lbrace x \rbrace \times \overline{B}(y_n,r) \neq \emptyset \rbrace
\end{align*}
which is Borel by the claim.  Moreover it follows that $f_1^{-1}(y_n) = C_{1, 1/2}^c \cap \dotsb \cap C_{n-1, 1/2}^c \cap C_{{n}, 1/2}$ is Borel.  Since $f_1$ is countably valued it follows that $f_1$ is Borel measurable.

Now define $f_k(x)$ be the first $y_n$ such that $A \cap \lbrace x \rbrace \times \overline{B}(y_n, 1/2^k) \neq \emptyset$ and $d(f_{k-1}(x), y_n) \leq 1/2^{k-2}$; again density of the $\lbrace y_n \rbrace$ shows that $f_k(x)$ is well defined.  Borel measurability of $f_k$ follows by a simple induction as the Borel measurability of $f_{k-1}$ and Lipschitz continuity of $d$ imply that $D_{n,r} = \lbrace x \mid d(f_{k-1}(x), y_n) \leq r \rbrace$ is Borel measurable for every $n \in \naturals$ and $r > 0$.  Since 
\begin{align*}
&f_k^{-1}(y_n) = \\
&(C_{1,1/2^k} \cap D_{1, 1/2^{k-2}})^c \cap \dotsb \cap (C_{n-1,1/2^k} \cap D_{n-1, 1/2^{k-2}})^c \cap  C_{n,1/2^k} \cap D_{n, 1/2^{k-2}}
\end{align*}
and $f_k$ is countably valued it follows that $f_k$ is Borel measurable.

For a fixed $x$ note that for $j > k$ we have the triangle inequality
\begin{align*}
d(f_k(x), f_j(x)) &\leq \sum_{i=k}^{j-1} d(f_i(x), f_{i+1}(x)) \leq \sum_{i=k}^{j-1} 1/2^{i-1} \leq \sum_{i=k}^\infty 1/2^{i-1} = 1/2^{k-2}
\end{align*}
which shows that $f_k(x)$ is Cauchy.  By completeness of $T$, we can take 
the limit of $f_k$ which is a Borel measurable function (Lemma \ref{LimitsOfMeasurableMetricSpace}).  Since $A$ is closed and $\lim_{k \to \infty} d(f_k(x), A) = 0$ it follows that $(x,f(x)) \in A$ and we are done.

TODO: Do we ever use Polishness of $S$?  Note the reference Azoff ``Borel Measurability in Linear Algebra'' in the Proceedings of the AMS who in turn references Bourbaki.
\end{proof}

We now illustrate how matrix factorizations (and canonical forms) may be shown to be Borel measurable by using the singular value decomposition as an example.  
\begin{cor}\label{BorelMeasurabilitySVD}There exists a Borel measurable function $f (A) = (U,\Sigma,V)$ from  $\reals^{m\times n}$ to $\reals^{m \times m} \times \reals^{m \times n} \times \reals^{n \times n}$ such that $A = U \Sigma V^T$ is a singular value decomposition.
\end{cor}
\begin{proof}
Let 
\begin{align*}
F &= \lbrace (A, U,\Sigma, V) \mid \text{$U, V$ are orthogonal, $\Sigma$ is diagonal and $A = U \Sigma V^T$} \rbrace
\end{align*}
and note that $F$ is closed (because the spaces of orthogonal and diagonal matrices are closed and matrix multiplication is continuous).  $F$ is also $\sigma$-compact because the ambient space is (just write $F = \cup_n F \cap \overline{B}(0,n)$).  Since singular value decompositions exist (Theorem \ref{SingularValueDecomposition}) we know that $\pi_1(F) = \reals^{m \times n}$ and therefore the result follows immediately from Theorem \ref{PrincipleOfMeasurableChoice}.
\end{proof}

B\'{e}la Sz Nagy also seems to prove measurability of eigenvalues and eigenvectors using the minimax criterion in ``Harmonic Analysis of Operators on Hilbert Space''.  From this one can bootstrap up to the measurability of the Schur decomposition and then get to the measurability of the SVD by considering Schur decompositions of $A^TA$ and $AA^T$.  There are also some more powerful section theorems that may have some relevance.

\begin{defn}Let $A$ be a $d \times d$ matrix and let $\Sigma_d$ be the set of permutations of $\lbrace 1, \dotsc, d \rbrace$ then the \emph{determinant} of $A$ is the number $\det(A) = \sum_{\sigma \in \Sigma_d} \sgn(\sigma) A_{1\sigma(1)} \dotsb A_{d \sigma(d)}$.
\end{defn}
The formula we have used in the definition of the determinant is called Leibnitz's formula.

\begin{prop}\label{DeterminantUnderPermutationTranspose}Let $A$ be a $d \times d$ matrix, 
\begin{itemize}
\item[(i)] $\det(A) = \det(A^T)$
\item[(ii)] let $\tau \in \Sigma_d$ and define $(A^\tau)_{ij} = A_{i \tau(j)}$ then $\det(A^\tau) = \sgn(\tau) \det(A)$. 
\item[(iii)] let $\tau \in \Sigma_d$ and define $(A_\tau)_{ij} = A_{\tau(i) j}$ then $\det(A_\tau) = \sgn(\tau) \det(A)$. 
\item[(iv)] If any two columns of $A$ are identical or any two rows of $A$ are identical then $\det(A) = 0$
\end{itemize}
\end{prop}
\begin{proof}
To see (i)
\begin{align*}
\det(A^T) &= \sum_{\sigma \in \Sigma_d} \sgn(\sigma) A^T_{1\sigma(1)} \dotsb A^T_{d \sigma(d)} = \sum_{\sigma \in \Sigma_d} \sgn(\sigma) A_{\sigma(1) 1} \dotsb A_{\sigma(d) d} \\
&= \sum_{\sigma \in \Sigma_d} \sgn(\sigma) A_{\sigma(1) \sigma^{-1}(\sigma(1))} \dotsb A_{\sigma(d) \sigma^{-1}(\sigma(d))} 
= \sum_{\sigma \in \Sigma_d} \sgn(\sigma^{-1}) A_{1 \sigma^{-1}(1)} \dotsb A_{d \sigma^{-1}(d)}  \\
&= \det(A)
\end{align*}
 
To see (ii) we note that by properties of permutations we have
\begin{align*}
\det(A^\tau) &= \sum_{\sigma \in \Sigma_d} \sgn(\sigma) A^\tau_{1\sigma(1)} \dotsb A^\tau_{d \sigma(d)} 
= \sum_{\sigma \in \Sigma_d} \sgn(\sigma) A_{1 \tau(\sigma(1))} \dotsb A_{d \tau(\sigma(d))} \\
&= \sum_{\sigma \in \Sigma_d} \sgn(\tau^{-1} \circ \tau \circ \sigma) A_{1 \tau(\sigma(1))} \dotsb A_{d \tau(\sigma(d))}  \\
&= \sum_{\sigma \in \Sigma_d} \sgn(\tau^{-1} ) \sgn( \tau \circ \sigma) A_{1 \tau(\sigma(1))} \dotsb A_{d \tau(\sigma(d))} \\
&= \sgn(\tau )\sum_{\sigma \in \Sigma_d} \sgn( \sigma) A_{1 \sigma(1)} \dotsb A_{d \sigma(d)} = \sgn(\tau) \det(A)
\end{align*}

To see (iii) we simply note that $A_\tau = ((A^T)^\tau)^T$ and using (i) and (ii).

To see (iv) suppose $1 \leq j < k \leq d$ and that columns $j$ and $k$ of $A$ are equal; if we let $(j k)$ be the permutation that exchanges $j$ and $k$ then we restate the hypothesis as $A^{(j k)} = A$.  Now apply (ii) and use the fact that $\sgn( j k) = -1$.  The case of equal rows follow by transposition (or equivalently by using (iii) with the appropriate transposition).
\end{proof}

\begin{thm}[Cauchy-Binet Theorem]\label{CauchyBinetTheorem} Let $A$ be a $d \times m$ matrix and let $B$ be an $m \times d$ matrix.  Let $\Psi$ be the set of all increasing injective functions from $\lbrace 1, \dotsc, d \rbrace$ to $\lbrace 1, \dotsc, m \rbrace$ and for every $\psi \in \Psi$ let 
\begin{align*}
A_\psi &= A( : , [\psi(1) \dotsb \psi(d)])
\end{align*}
and let 
\begin{align*}
B_\psi &= B( [\psi(1) \dotsb \psi(d)], :)
\end{align*}
then 
\begin{align*}
\det(AB) &= \sum_{\psi \in \Psi} \det(A_\psi) \det(B_\psi)
\end{align*}
\end{thm}
\begin{proof}
By the formula for matrix multiplication note that the $i^{th}$ row of $AB$ is a linear combination of the $m$ rows of $B$; specifically $(AB)(i, :) = \sum_{k=1}^m A_{ik} B(k, :)$.  Iteratively using this fact together with the linearity of the determinant we get
\begin{align*}
\det(AB) &=  \det
\begin{bmatrix}
(AB)(1,  :)\\
\dotsb \\
 (AB)(d, :)
\end{bmatrix}
= \sum_{k_1 = 1}^m 
A_{1 k_1} \det \begin{bmatrix}
B(k_1,  :) \\
(AB)(2, :)\\
\dotsb \\
(AB)(d,  :)
\end{bmatrix} \\
&= \sum_{k_1 = 1}^m \dotsb \sum_{k_d=1}^m
A_{1 k_1}  \dotsb A_{d k_d} \det \begin{bmatrix}
B(k_1, :)\\
\dotsb \\
B(k_d, :)
\end{bmatrix} \\
&=\sum_{(k_1, \dotsc, k_d) \in \lbrace 1, \dotsc, m \rbrace^d}
A_{1 k_1}  \dotsb A_{d k_d} \det B([k_1 \dotsb k_d], :)\\
&=\sum_{\substack{(k_1, \dotsc, k_d) \in \lbrace 1, \dotsc, m \rbrace^d \\ k_i \text{ distinct} }}
A_{1 k_1}  \dotsb A_{d k_d} \det B([k_1 \dotsb k_d], :)\\
\end{align*}
where in the last line we have used the fact that for any set of indices $(k_1, \dotsc, k_d)$ for which there exists $1 \leq i < j \leq d$ and $k_i=k_j$ we have $\det B([k_1 \dotsb k_d], :) = 0$ (assertion (iv) of Proposition \ref{DeterminantUnderPermutationTranspose}).  

Given any $d$-tuple $(k_1, \dotsc, k_d)$ with $k_i$ distinct there is a unique permutation $\sigma$ of $\lbrace 1 , \dotsc, d \rbrace$ such that $1 \leq k_{\sigma(1)} < \dotsb < k_{\sigma(d)} \leq m$. These increasing sequences $1 \leq k_{\sigma(1)} < \dotsb < k_{\sigma(d)} \leq m$ are precisely the ranges of functions $\psi \in \Psi$.   Thus if we let $\Sigma_d$ be set of permutations of $\lbrace 1, \dotsc, d \rbrace$ then every $d$-tuple $(k_1, \dotsc, k_d)$ with $k_i$ distinct can be written uniquely as $(\psi(\sigma(1)), \dotsc, \psi(\sigma(d)))$ where $\psi \in \Psi$ and $\sigma \in \Sigma_d$.  Now we compute using Proposition \ref{DeterminantUnderPermutationTranspose}
\begin{align*}
\det(AB) 
&= \sum_{\psi \in \Psi} \sum_{\sigma \in \Sigma_d} A_{1 \psi(\sigma(1))}  \dotsb A_{d \psi(\sigma(d))} \det B([\psi(\sigma(1)) \dotsb \psi(\sigma(d)) ], :) \\
&= \sum_{\psi \in \Psi} \sum_{\sigma \in \Sigma_d} (A_\psi)_{1 \sigma(1)}  \dotsb (A_\psi)_{d \sigma(d)} \det (B_\psi) ([\sigma(1) \dotsb \sigma(d) ], :) \\
&= \sum_{\psi \in \Psi} \sum_{\sigma \in \Sigma_d} (A_\psi)_{1 \sigma(1)}  \dotsb (A_\psi)_{d \sigma(d)} \sgn(\sigma) \det (B_\psi) \\
&= \sum_{\psi \in \Psi} \det(A_\psi) \det(B_\psi)
\end{align*}
\end{proof}

\begin{cor}\label{DeterminantOfProducts}Let $A$ and $B$ be $d \times d$ matrices then $\det(AB) = \det(A) \det(B)$.
\end{cor}
\begin{proof}
Apply the Cauchy-Binet formula in the case $d=m$ (here $\Psi$ comprises on the identity map).
\end{proof}

\begin{cor}Let $A$ be an orthogonal matrix then $\abs{\det(A)} = 1$.
\end{cor}
\begin{proof}
By the previous Corollary and  Proposition \ref{DeterminantUnderPermutationTranspose}
\begin{align*}
1 &= \det(\IdentityMatrix) = \det(A^T A) = \det(A^T) \det(A) = \det(A)^2
\end{align*}
\end{proof}

\section{The Fell Topology and Painlev\'{e}-Kuratowski Convergence}

\begin{defn}Let $X$ be a Hausdorff topological space the \emph{Fell topology} on the set of closed subsets of $X$ the the topology generated by sets of the form
$\lbrace F \subset X \mid F \cap U \neq \emptyset \rbrace$ for $U \subset X$ non-empty and open and $\lbrace F \subset X \mid F \cap K = \emptyset \rbrace$ for $K \subset X$ non-empty and compact.
\end{defn}

\begin{thm}\label{CompactnessOfFellTopology}Let $X$ be Hausdorff then the set of closed subsets under the Fell topology is a compact space.  The set of closed subsets is Hausdorff in the Fell topology if and only if $X$ is locally compact.
\end{thm}
\begin{proof}
We will be applying the Alexander Subbase Theorem \ref{AlexanderSubbaseTheorem} to the defining subbase of the Fell topology.  Therefore suppose that we have index sets $\mathcal{A}$ and $\mathcal{B}$ and non-empty open sets $U_\alpha$ for $\alpha \in \mathcal{A}$ and non-empty compact sets $K_\beta$ for $\beta \in \mathcal{B}$ such that the $\lbrace F \mid F \cap U_\alpha \neq \emptyset \rbrace$
and  $\lbrace F \mid F \cap K_\beta = \emptyset \rbrace$ are an open cover of the set of closed subsets.  Note that both $\mathcal{A}$ and $\mathcal{B}$ are non-empty since $\emptyset$ cannot be a member of any set of the form $\lbrace F \mid F \cap U_\alpha \neq \emptyset \rbrace$ whereas $X$ cannot be a member of any set of the form $\lbrace F \mid F \cap K_\beta = \emptyset \rbrace$.

\begin{clm}There exists $\beta_0 \in \mathcal{B}$ such that $K_{\beta_0} \subset \cup_{\alpha \in \mathcal{A}} U_\alpha$.
\end{clm}
We argue by contradiction.  Suppose the claim is not true then for each $\beta \in \mathcal{B}$ we may select $x_\beta \in K_{\beta_0} \setminus \cup_{\alpha \in \mathcal{A}} U_\alpha$.  Define $G$
to be the closure of $\lbrace x_\beta \mid \beta \in \mathcal{B} \rbrace$.  By construction and Corollary \ref{DisjointOpenSetsDisjointClosure} we know that $G$ is disjoint from $\cup_{\alpha \in \mathcal{A}} U_\alpha$ hence $G$ is not contained in any $\lbrace F \mid F \cap U_\alpha \neq \emptyset \rbrace$ for $\alpha \in \mathcal{A}$.  By construction we have $x_\beta \in G \cap K_\beta$ which
shows that $G$ is not contained in $\lbrace F \mid F \cap K_\beta = \emptyset \rbrace$ for every $\beta \in \mathcal{B}$.  Thus we have a contradiction to the fact that we have an open cover of the closed subsets of $X$.

Pick $K_{\beta_0}$ as in the claim and use the compactness of $K_{\beta_0}$ to select a finite subcover $U_{\alpha_1}, \dotsc, U_{\alpha_n}$ of $K_{\beta_0}$.
\begin{clm} $\lbrace F \mid F \cap K_{\beta_0} = \emptyset \rbrace, \lbrace F \mid F \cap U_{\alpha_1} \neq \emptyset \rbrace, \cdots , \lbrace F \mid F \cap U_{\alpha_n} \neq \emptyset \rbrace$ is a cover of the closed subsets of $X$.
\end{clm} 
Suppose that $F$ is a closed set and $F \cap U_{\alpha_j} = \emptyset$ for each $j=1, \dotsc, n$; it follows that $F \cap \cup_{j=1}^n U_{\alpha_j}  = \emptyset$ and therefore since $K_{\beta_0} \subset \cup_{j=1}^n U_{\alpha_j}$ we conclude $F \cap K_{\beta_0} = \emptyset$ and the claim is proved.

We have shown that every cover of the closed subsets of $X$ by subbase elements has a finite subcover and therefore compactness follows from the Alexander Subbase Theorem \ref{AlexanderSubbaseTheorem}.



Suppose that $X$ is locally compact. 
TODO: Do the Hausdorff hyperspace if and only if $X$ locally compact part.
\end{proof}

\begin{thm}\label{MetrizabilityOfFellTopology}Let $X$ be Hausdorff then the following are equivalent
\begin{itemize}
\item[(i)] $X$ is locally compact and second countable
\item[(ii)] The set of closed sets under the Fell topology is metrizable
\item[(ii)] The set of closed sets under the Fell topology is Polish
\end{itemize}
\end{thm}
\begin{proof}
Suppose that $X$ is locally compact and second countable.  There exists a countable base $\mathcal{B}$ of the topology on $X$ comprising relatively compact sets.  We claim that the sets 
$\lbrace F \cap U \neq \emptyset \rbrace$ and $\lbrace F \cap \overline{U} = \emptyset \rbrace$ for $U \in \mathcal{B}$ are a subbase for the Fell topology.  First assume that $U$ is open in $X$ and
$F \cap U \neq \emptyset$.  Pick $x \in F \cap U$ and choose $V \in \mathcal{B}$ such that $x \in V \subset U$ it follows that $F \cap V \neq \emptyset$.  Next assume that $K$ is compact in $X$ and $F \cap K = \emptyset$.  Let $x \in K$ and note that there exists an open set $U$ such that $x \in U$ and $U \cap F = \emptyset$; 


Suppose that $X$ is locally compact second countable and Hausdorff.  By Corollary \ref{LCSCHIsMetrizable} of the Urysohn Metrization Theorem it follows that $X$ is metrizable and moreover we may select a metric $d$ with the property that every closed ball is compact.  
\begin{clm}For every $x \in X$ the function $d_x(F) = d(x, F)$ is continuous in the Fell topology.
\end{clm}
By Proposition \ref{ContinuityViaSubbase} it suffices to show that both $d_x^{-1}(0, r)$ and $d_x^{-1}(r, \infty)$ are open for every $r > 0$ (to see we have a subbase note just write $(a,b) = (a,\infty) \cap (0,b)$ noting that the finite intervals are a base for the topology on $(0,\infty)$).  This follows by the set identities $r > 0$,
\begin{align*}
d_x^{-1}(r, \infty) &= \lbrace F \mid \overline{B}(x,r) \cap F = \emptyset \rbrace \\
d_x^{-1}(-\infty, r) &= \lbrace F \mid B(x,r) \cap F \neq \emptyset \rbrace \\
\end{align*}
where both sets on the right hand side are open in the Fell topology (here we use the properties of the metric $d$ so that $\overline{B}(x,r)$ is compact).

\begin{clm}Suppose that $\net{F}{\alpha}$ is a net, $F$ is closed and $d(x,F_\alpha) \to 0$ for some $x \in X$.  There exist $x_\alpha \in F_\alpha$ such that $x_\alpha \to x$.
\end{clm}
For each $n \in \naturals$ there exists $\alpha_n$ such that $d(x,F_\alpha) < 1/n$ for all $\alpha \succ \alpha_n$.  Moreover by replacing $\alpha_{n+1}$ with any $\alpha \succ \alpha_{n+1}$ and $\alpha \succ \alpha_{n}$ we may assume that $\alpha_{n+1} \succ \alpha_n$ for all $n \in \naturals$.  First suppose that there exists $\alpha_\infty$ such that $\alpha_\infty \succ \alpha_n$ for all $n \in \naturals$.  It follows that $d(x,F_{\alpha}) = 0$ for all $\alpha \succ \alpha_\infty$.  This means that $x \in F_{\alpha}$ for all $\alpha \succ \alpha_\infty$.  Now define $\net{x}{\alpha}$ to be equal to $x$ for all $\alpha \succ \alpha_\infty$ and arbitrarily otherwise; it is immediate that $x_\alpha \to x$.  Therefore we now assume there is no such $\alpha_\infty$.  For any $\alpha$ for which there is no $\alpha_n$ with $\alpha \succ \alpha_n$ we define $x_\alpha$ arbitrarily.  For other $\alpha$ we pick the maximum $n \in \naturals$ such that $\alpha \succ \alpha_n$ (well defined by the non existence of $\alpha_\infty$) and select $x_\alpha \in F_\alpha$ with $d(x,x_\alpha) < 1/n$.  Again by construction it is clear that $x_\alpha \to x$.

\begin{clm}The Fell topology is generated by functions $d_x(F) = d(x,F)$ for $x \in X$.
\end{clm}
The previous claim shows that the Fell topology is finer than the topology generated by the functions $d_x$.  To show the reverse inclusion of topologies, by Corollary \ref{UniquenessOfConvergenceClasses} it suffices to show that every net $\net{F}{\alpha}$ converging in the topology generated by the $d_x$ also converges in the Fell topology.
Therefore suppose that $\net{F}{\alpha}$ is a net, $F \subset X$ is closed and $d(x, F_\alpha) \to d(x,F)$ for all $x \in X$.   Let $U$ be open in $X$ such that $F \cap U \neq \emptyset$.  Pick $x \in F \cap U$ and use the previous claim to select $x_\alpha \in F_\alpha$ such that $x_\alpha \to x$.  Since $U$ is an open neighborhood of $x$ there exists $\alpha$ such that $x_\beta \in U$ for all $\beta \succ \alpha$.  It follows that $F_\alpha \cap U \neq \emptyset$ for all $\alpha \succ \beta$ which shows that $F_\alpha$ is in the Fell neighborhood defined by $U$.  Next suppose that $K$ is compact with $K \cap F = \emptyset$.  For every $x \in K$, since $x \notin F$ the open ball $B(x, \frac{1}{2}d(x,F))$ satisfies $B(x,\frac{1}{2}d(x,F)) \cap F = \emptyset$.   By compactness we may extract a finite
subcover $B(x_1, \frac{1}{2}d(x_1,F)), \dotsc, B(x_n,\frac{1}{2}d(x_n, F))$.  For $j=1, \dotsc, n$.  For each $x_j$ we know that $d(x_j, F_\alpha) \to d(x_j, F) > 0$ and therefore there is an $\alpha_j$ such that for all $\alpha \succ \alpha_j$ we have
\begin{align*}
\abs{d(x_j, F_\alpha) - d(x_j, F)} &< \frac{1}{2}d(x_j, F)
\end{align*}
Now pick any $x \in X$ such that $d(x,x_j) < \frac{1}{2} d(x_j, F)$.  For all $\alpha \succ \alpha_j$ it follows by a few applications of the triangle inequality
\begin{align*}
d(x, F_\alpha) &> d(x_j, F_\alpha) - d(x,x_j) > d(x_j, F) - \abs{d(x_j, F_\alpha) - d(x_j, F)} - d(x,x_j) \\
&> d(x_j, F) - \frac{1}{2}d(x_j, F) - \frac{1}{2} d(x_j, F) = 0
\end{align*}
which is to say that $B(x_j, \frac{1}{2}d(x_j,F)) \cap F_\alpha = \emptyset$ for all $\alpha \succ \alpha_j$.  Let $\alpha$ be such that $\alpha \succ \alpha_j$ for $j=1, \dotsc, n$ it follows that for
all  $\beta \succ \alpha$ we have $\cup_{j=1}^n B(x_j, \frac{1}{2}d(x_j,F)) \cap F_\beta = \emptyset$ hence $K \cap F_\beta = \emptyset$.


If $X$ is second countable and metrizable it is separable and therefore we can pick a countable dense subset $x_1, x_2, \dotsc$.  
\begin{clm} The Fell topology is generated by $d_{x_j}(F) = d(x_j,F)$ for $j \in \naturals$
\end{clm}
We have ready proven that the Fell topology is generated by $d_x$ for all $x \in F$ so by Corollary \ref{UniquenessOfConvergenceClasses} it suffices to show that every net that converges in the
topology generated by the $d_{x_j}$ for $j \in \naturals$ also converges in the Fell topology.  Therefore suppose we have a net $\net{F}{\alpha}$ and a closed set $F$ such that $d(x_j, F_\alpha) \to d(x_j,F)$ for all $j \in \naturals$.  Let $x \in X$ be an arbitrary point and let $\epsilon > 0$ be given then pick $x_j$ such that $d(x,x_j) < \epsilon/2$ and then pick $\alpha$ such that
$\abs{d(x_j, F_\beta) - d(x_j, F)} < \epsilon/2$ for all $\beta \succ \alpha$.  By the triangle inequality if follows that 
\begin{align*}
\abs{d(x, F_\beta) - d(x, F)} &\leq \abs{d(x_j, F_\beta) - d(x_j, F)} + d(x_j,x) < \epsilon \text{ for all $\beta \succ \alpha$}
\end{align*}
hence $d(x, F_\alpha) \to d(x,F)$ which by the previous claim shows that $F_\alpha \to F$ in the Fell topology.

Now we can define $\phi(F) = (d(x_1, F), d(x_2, F), \dotsc)$ into $[0,\infty]^\infty$ which is easily seen to be continuous and injective.  In fact by Lemma \ref{TopologyGeneratedByFunctionsAndOpenMappings} and the previous claim it follows that $\phi$ is an open mapping onto it image in $[0,\infty]^\infty$.  Since the latter space is second countable and metrizable those properties for the space of closed sets in the Fell topology as well.

Also note that by Lemma \ref{TopologyGeneratedByFunctionsAndOpenMappings}  the Fell topology is generated by sets of the form $d_{x_n}^{-1}(0,r)$ and $d_{x_n}^{-1}(r, \infty)$ for $r \in \rationals$ which is to say sets of the form $\lbrace F \mid F \cap B(x_n,r) \neq \emptyset \rbrace$ and $\lbrace F \mid F \cap \overline{B}(x_n,r) = \emptyset \rbrace$ for $r \in \rationals$.
\end{proof}


The following are from Beer.   It appears that Painlev\'{e}-Kuratowski convergence is equivalent to the convergence in the Fell topology when a space is first countable.

\begin{defn}Let $\net{A}{\alpha}$ be a net of sets in $X$.  A \emph{limit point} of $\net{A}{\alpha}$ is point $x \in X$ such that for every open neighborhood of $x$ there exists $\alpha_0$ such that $A_\alpha \cap U \neq \emptyset$ for every $\alpha \succ \alpha_0$. A \emph{cluster point} of $\net{A}{\alpha}$ is point $x \in X$ such that for every open neighborhood of $x$ and every $\alpha_0$ there exists $\alpha \succ \alpha_0$ such that $A_\alpha \cap U \neq \emptyset$.   We define $\liminf_\alpha A_\alpha$ to be the set of limit points of  $\net{A}{\alpha}$ and $\limsup_\alpha A_\alpha$ to be the set of cluster points of  $\net{A}{\alpha}$.
\end{defn}

TODO: Show that $\liminf_\alpha A_\alpha$ and $\limsup_\alpha A_\alpha$ are closed.

\begin{prop}\label{PainleveKuratowskiFirstCountable}Let $X$ be a first countable Hausdorff space and let $F_1, F_2, \dotsc$ be a sequence of closed sets.
\begin{itemize}
\item[(i)] $x \in \liminf_{n \to \infty} F_n$ if and only if there exists and $N \in \naturals$ and $x_n \in F_n$ for $n \geq N$ such that $\lim_{n \to \infty} x_n = x$
\item[(ii)] $x \in \limsup_{n \to \infty} F_n$ if and only if there exists and a subsequence $n_j$ of $\naturals$ and $x_{n_j} \in F_{n_j}$ such that $\lim_{j \to \infty} x_{n_j} = x$
\item[(iii)] $F = \liminf_{n \to \infty} F_n = \limsup_{n \to \infty} F_n$ if and only if $F_n \to F$ in the Fell topology.
\end{itemize}
\end{prop}
\begin{proof}
To see (i), if there exists a sequence $x_n \in F_n$ with $x_n \to x$ then for any open neighborhood $U$ of $x$ the $x_n$ are eventually in $U$ hence $U \cap F_n \neq \emptyset$.  On the other hand uppose $x \in \liminf_{n \to \infty} F_n$  and let $U_1, U_2, \dotsc$ be a countable local base at $x$.  If necessary we can redefine $U_n = U_1 \cap \dotsb \cap U_n$ and therefore we may assume that $U_1 \supset U_2 \supset \dotsb$.  For each $n \in \naturals$ pick $N_n$ such that $U_n \cap F_k \neq \emptyset$ for $k \geq N_n$ and $N_1 < N_2 < \dotsb$.  For $N_n \leq m < N_{m+1}$ we pick a 
point $x_m \in U_n \cap F_m$ and it follows that $x_m \to x$.

To see (ii), if there exists a a subsequence $x_{n_j} \in F_{n_j}$ with $x_{n_j} \to x$ then for any open neighborhood $U$ of $x$ the $x_{n_j}$ are eventually in $U$ hence $U \cap F_n \neq \emptyset$ frequently.  On the other hand uppose $x \in \limsup_{n \to \infty} F_n$  and let $U_1, U_2, \dotsc$ be a countable local base at $x$ with $U_1 \supset U_2 \supset \dotsb$.  For each $j \in \naturals$ pick $n_j$ such that $U_n \cap F_{n_j} \neq \emptyset$ and $n_1 < n_2 < \dotsb$.  For each $n_j$ pick a 
point $x_{n_j} \in U_n \cap F_{n_j}$ and it follows that $x_{n_j} \to x$.

TODO:
\end{proof}

Even though Painlev\'{e}-Kuratowski is not always equivalent to the Fell topology it shares compactness properties in general.  TODO: Does Painlev\'{e}-Kuratowski convergence define a convergence class (i.e. a topology).
\begin{prop}Let $X$ be a Hausdorff space and let $F_\alpha$ be a net then $F_\alpha$ has convergent subnet.  If $X$ is second countable then a sequence $F_n$ has a convergent subsequence.
\end{prop}
\begin{proof}
TODO:
\end{proof}

In the lcscH case the upper and lower limits can be described nicely by 

