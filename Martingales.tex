\section{Martingales and Optional Times}
TODO:  First introduce discrete time martingales then do stopping
times and lastly extend to continuous time martingales (at least the
basics).

We first begin with a very general notion of \emph{stochastic process}
which we rather quickly specialize.  
\begin{defn}Suppose one has a measurable space $(S, \mathcal{S})$ and an
  index set $T$.  We let $S^T$ denote the set of all functions $f : T
  \to S$.  Then $\mathcal{S}^T$ is the $\sigma$-algebra generated by
  all the evaluation maps $\pi_t : S^T \to S$ defined by $\pi_t(f) =
  f(t)$.  That is to say
\begin{align*}
\mathcal{S}^T &= \sigma \left ( \lbrace \lbrace f \mid f(t) \in U
  \rbrace \mid t \in T \text{, } U \in
  \mathcal{S} \rbrace\right )
\end{align*}
\end{defn}
Measurability with respect to the $\sigma$-algebra $\mathcal{S}^T$ has a useful
alternative characterization.  First we establish some notation.  If
we consider a set function $X : \Omega \to S^T$ then can equivalently
view this as a set function $\tilde{X} : \Omega \times T \to S$ via the
identification $\tilde{X}(\omega, t) = X(\omega)(t)$ (the process of
transforming $\tilde{X}$ to $X$  is called
\emph{currying} in computer science).  We can also curry $\tilde{X}$ on $\Omega$
 to get an element $\hat{X} : T \to S^\Omega$.  It is customary to
write $\hat{X}(t)$ as $X_t$.
\begin{lem}\label{ProcessMeasurableProjections}Suppose one has a probability space $(\Omega,
  \mathcal{A})$, a measurable space $(S, \mathcal{S})$, an
  index set $T$ and a subset $U \subset S^T$.  Then $X : \Omega \to U$
  is $U \cap \mathcal{S}^T$-measurable if and only if $X_t : \Omega
  \to S$ is $\mathcal{S}$-measurable for all $t \in T$.
\end{lem}
\begin{proof}
We know by definition of $\mathcal{S}^T$ that every projection $\pi_t
: S^T \to S$ is measurable.  Moreover, we know that $X_t = \pi_t \circ
X$.  Therefore if we assume $X$ is
$\mathcal{S}^T$-measurable then $X_t$ is a composition of measurable
functions and it follows from Lemma
\ref{CompositionOfMeasurable} that $X_t$ is measurable.

In the opposite direction, assume that each $X_t$ is measurable.  Let
$A \in \mathcal{S}$ and $t \in T$ and consider the set $\pi_t^{-1}(A)
\in \mathcal{S}^T$.  By definition we can see that
\begin{align*}
X^{-1}(\pi_t^{-1}(A)) &= \lbrace \omega \in \Omega \mid
\pi_t(X(\omega)) \in A \rbrace = X_t^{-1}(A)
\end{align*}
which is measurable by assumption.  Since sets of the form
$\pi_t^{-1}(A)$ generate $\mathcal{S}^T$ application of Lemma \ref{MeasurableByGeneratingSet}
shows that $X$ is measurable.
\end{proof}

It can be useful to know what operations on set functions are
measurable with respect to the product topology on $S^T$.  Here we
record a simple fact that we will use.
\begin{lem}\label{MeasurableFunctionGroup}Let $G$ be a measurable
  group and $T$ be an index set, with group operations defined pointwise, $(G^T, \mathcal{G}^T)$ is a  measurable group.
\end{lem}
\begin{proof}
With the identity defined by the constant function $f(t) = e$, the
fact that $G^T$ is a group is immediate.  To see measurability of the
group operation, let $A \in \mathcal{G}$ and pick $t \in T$.  Note
that $(\pi_t, \pi_t) : \mathcal{G}^T \otimes \mathcal{G}^T / \mathcal{G} \otimes \mathcal{G}$ - measurable by
definition of the product $\sigma$-algebra (both on $G^T$ and
generically) and we know the group operation is $\mathcal{G} \otimes
\mathcal{G}/\mathcal{G}$-measurable therefore $\lbrace (f,g) \mid (f
\cdot g)(t) \in A \rbrace$ is $\mathcal{G}^T \otimes
\mathcal{G}^T$-measurable.  The proof for the inverse operation
follows similarly.
\end{proof}

\begin{defn}Suppose one has a probability space $(\Omega,
  \mathcal{A}, \mu)$, a measurable space $(S, \mathcal{S})$, an
  index set $T$ and a subset $U \subset S^T$.  A $U \cap
  \mathcal{S}^T$-measurable $X : \Omega \to U$ is called a
  \emph{stochastic process}.
\end{defn}

Note that we do not require $U$ to be a measurable subset of $S^T$
(and in most case that we consider it will not be).
According to this definition, a stochastic process is simply a random
element in subset of a path space $(U, U \cap \mathcal{S}^T)$.  As such it has a
distribution $\pushforward{X}{\mu}$ which is a measure on $U$; as usual we will say
that two stochastic processes $X$ and $Y$ are equal in distribution
when their laws are equal.  Because of the nature of the
$\sigma$-algebra on $\mathcal{S}^T$ there is a simple way to measure
whether two processes are equal in distribution.

TODO: Build some intuition about the definition of a process.  In
particular, the reason for considering subsets $U \subset S^T$ is
clear because $S^T$ is just too big.  It is very rare for one to be
interested in arbitrary set functions; almost always one wants some
kind of condition to be imposed such as continuity or at least some
restriction on the discontinuities that can occur (e.g. allowing jump
discontinuities but outlawing
oscillatory discontinuities is common in stochastic processes).  These
subsets very often come with additional structure that either implies or constrains their measure theoretic
structure (e.g. a metric topology that implies a Borel
$\sigma$-algebra).  A subtle point that shall come up is that one will
want the implied measure theoretic structure be compatible with the
measure theoretic structure of the general definition.
TODO: Is there something deep about the use of the product
$\sigma$-algebra in this context or is a technical convenience/least
common denominator that allow one to prove general results?  Well,
arguably it is made in this way so that a stochastic process is just a
family of random elements $X_t$ indexed by $T$; where do we even state
this fact?

\begin{lem}\label{RestrictionOfSigmaAlgebra}Let $(S, \mathcal{S})$ be a measurable space an let $U
  \subset S$ be a (not necessarily measurable) subset, then $U
  \cap \mathcal{S}$ is a $\sigma$-algebra on $U$.  Furthermore if
  $\mathcal{C} \subset 2^S$ is a set of subsets of $S$ that
  generates $\mathcal{S}$ then $\mathcal{D} = \lbrace U \cap C \mid C
  \in \mathcal{C} \rbrace$ generates $U \cap \mathcal{S}$.  Lastly
  given a measurable space $(T, \mathcal{T})$ and an $\mathcal{S}/\mathcal{T}$-measurable
  function $f : S \to T$, the restriction $f \vert_U : U \to T$ is $U
  \cap \mathcal{S}/\mathcal{T}$-measurable.
\end{lem}
\begin{proof}
The fact that $U \cap \mathcal{S}$ is a $\sigma$-algebra follows
easily from the fact that $\mathcal{S}$ is a $\sigma$-algebra and the set theoretic identities $\cap_{i=1} (U \cap A_i) = U
\cap \cap_{i=1} A_i$ and $U \setminus (U \cap A) = U \cap (U \cap A)^c
= U \cap A^c$.

Given the generating set $\mathcal{C}$ for $\mathcal{S}$ and
$\mathcal{D}$ defined as above it is immediate from the fact that
$\mathcal{C} \subset \mathcal{S}$ that we have $\mathcal{D} \subset U
\cap \mathcal{S}$.  As we have just proven that $U \cap \mathcal{S}$
is a $\sigma$-algebra it follows that $\sigma(\mathcal{D}) \subset U
\cap \mathcal{S}$.  

On the other hand, let $\mathcal{E} = \lbrace A
\subset S \mid U \cap A \in \sigma(\mathcal{D}) \rbrace$.  We
claim that $\mathcal{E}$ is a $\sigma$-algebra.  Indeed if $A, A_1, A_2,
\dotsc \in \mathcal{E}$ then we have $U \cap \cup_{i=1}^\infty A_i =
\cup_{i=1}^\infty U \cap A_i \in \sigma(\mathcal{D})$ which implies
$\cup_{i=1}^\infty A_i \in \mathcal{E}$ and $U \cap A^c
= (U \cap U^c) \cup (U \cap A^c) = U \cap (U \cap A)^c \in
\mathcal{D}$ which implies $A^c \in \mathcal{E}$.  By the definition
of $\mathcal{D}$, we know
$\mathcal{C} \subset \mathcal{E}$ and therefore $\mathcal{S} =
\sigma(\mathcal{C}) \subset \sigma(\mathcal{E}) = \mathcal{E}$.  Thus we have shown the
reverse inclusion $U \cap
\mathcal{S} \subset \sigma(\mathcal{D})$ and we have $U \cap
\mathcal{S} = \sigma(\mathcal{D})$.

Lastly, $U \cap \mathcal{S}/\mathcal{T}$-measurability of the
restriction of a
$\mathcal{S}/mathcal{T}$-measurable $f$ follows from the identity
$(f\vert_U)^{-1}(A) = \lbrace s \in S \mid f(s) \in A \text{ and } s
\in U \rbrace = U \cap f^{-1}(A)$ which shows $(f\vert_U)^{-1}(A) \in U \cap
\mathcal{S}$ whenever $f^{-1}(A) \in \mathcal{S}$.
\end{proof}

\begin{lem}\label{ProcessLawsAndFDDs}   Let $X$ be a stochastic process
  with values in $U \subset S^T$, then for every $t_1,
  \dotsc, t_n \in T$ then $(X_{t_1}, \dotsc, X_{t_n}) \in S^n$ is
  $\mathcal{S}^{\otimes n}$-measurable and the measures
  $\pushforward{(X_{t_1}, \dotsc, X_{t_n})}{\mu}$ are called the
  \emph{finite dimensional distributions} of $X$. Given $U \subset
  S^T$ then any two probability measures on $(U, U \cap
  \mathcal{S}^T)$ are equal if and only if their finite
  dimensional distributions are equal.  In particular, if $X$ and $Y$ are
  two stochastic processes with values in $U \subset S^T$ then $X \eqdist Y$ if and only if their
  finite dimensional distributions are equal (written $X \eqfdd Y$).
  It is also that the case that $X \eqfdd Y$ if and only if
  $\pushforward{(X_{t_1}, \dotsc, X_{t_n})}{\mu} =
  \pushforward{(Y_{t_1}, \dotsc, Y_{t_n})}{\mu}$ for all $t_1, \dotsc,
  t_n \in T$ with the $t_j$ distinct.
\end{lem}
\begin{proof}
Suppose that $t_1, \dotsc, t_n$ are given and define the $n$-dimensional projection
$(\pi_{t_1}, \dotsc, \pi_{t_n}) : S^T \to S^n$.   We claim that 
$(\pi_{t_1}, \dotsc, \pi_{t_n})$ is 
$\mathcal{S}^T/\mathcal{S}^{\otimes n}$ measurable.  Indeed if we let $A_1 \times \dotsb \times A_n \in
\mathcal{S}^{\otimes n}$ then $(\pi_{t_1}, \dotsc, \pi_{t_n})^{-1}(A_1
\times \dotsb \times A_n) = 
\pi_{t_1}^{-1}(A_1) \cap \dotsb \cap \pi_{t_n}^{-1}(A_n) $, hence $(\pi_{t_1}, \dotsc, \pi_{t_n})^{-1}(A_1
\times \dotsb \times A_n) \in
\mathcal{S}^T$ by the measurability of each $\pi_{t_j}^{-1}(A_j)$ for
$j=1, \dotsc, n$.  Since sets of the form $A_1 \times \dotsb \times
A_n$ generate $\mathcal{S}^{\otimes n}$ 
we see that $(\pi_{t_1}, \dotsc, \pi_{t_n})$ is measurable by
application of Lemma \ref{MeasurableByGeneratingSet}.

The $\mathcal{S}^{\otimes n}$-measurability of $(X_{t_1}, \dotsc,
X_{t_n})$ now follows directly from Lemma
\ref{CompositionOfMeasurable} and \ref{RestrictionOfSigmaAlgebra} since we can write $(X_{t_1}, \dotsc,
X_{t_n}) = (\pi_{t_1}, \dotsc, \pi_{t_n}) \circ X$ as a composition of
a $U \cap \mathcal{S}^T/\mathcal{S}^{\otimes n}$-measurable function
$(\pi_{t_1}, \dotsc, \pi_{t_n})\vert_U$ and $U \cap \mathcal{S}^T$-measurable function $X$.

Suppose that $\mu$ and $\nu$ are probability measures on $(U, U \cap
\mathcal{S}^T)$ whose finite dimensional projections are equal; that
is to say for every $t_1, \dotsc, t_n \in T$ we have 
$\pushforward{(\pi_{t_1},\dotsc, \pi_{t_n})}{\mu} =
\pushforward{(\pi_{t_1},\dotsc, \pi_{t_n})}{\nu}$.  This fact that
shows that $\mu$ and $\nu$ agree on all sets of the form
$(\pi_{t_1},\dotsc, \pi_{t_n})^{-1}(A)$ where $n > 0$, $t_1, \dotsc,
t_n \in T$ and $A \subset \mathcal{S}^{\otimes n}$.  Let the
set of sets of this form be called $\mathcal{C}$.  We claim
$\mathcal{C}$ generates $U \cap \mathcal{S}^T$.  Indeed it is the case
that sets of the form $\pi_t^{-1}(A)$ for $t \in T$ and $A \subset
\mathcal{S}$ generate $U \cap \mathcal{S}^T$.  One can see this by
observing that $\pi_t^{-1}(A) = U \cap \tilde{\pi}_t^{-1}(A)$ where
$\tilde{\pi}_t : S^T \to S$ is the evaluation map extended to the entireity of $S^T$.  By
definition $\mathcal{S}^T$ is generated by the sets
$\tilde{\pi}^{-1}(A)$ and therefore by Lemma
\ref{RestrictionOfSigmaAlgebra} we conclude $U \cap \mathcal{S}^T$ is
generated by sets of the form $U \cap \tilde{\pi}_t^{-1}(A) =
\pi_t^{-1}(A)$.

Next we claim that $\mathcal{C}$ is a $\pi$-system.  This follows
immediately as we can write $(\pi_{t_1},\dotsc, \pi_{t_n})^{-1}(A)
\cap (\pi_{s_1},\dotsc, \pi_{s_m})^{-1}(B) = (\pi_{t_1},\dotsc,
\pi_{t_n}, \pi_{s_1},\dotsc, \pi_{s_m})^{-1}(A \times B)$.  Now we may
conclude $\mu = \nu$
by a monotone class argument (specifically Lemma \ref{UniquenessOfMeasure}).

The statement about stochastic processes follows by applying the fact just proven the laws
of $X$ and $Y$.

It is trivial that if $X \eqdist Y$ then the finite dimensional
distributions with distinct $t_j$ are equal.  To see the converse note
that the projection $(\pi_{t_1},\dotsc,  \pi_{t_n})$ for not necessarily distinct $t_j$ may be written
as a composition $i \circ (\pi_{s_1}, \dotsc, \pi_{s_m})$ with $s_1,
\dotsc, s_m$ the set of distinct $t_j$ and $i : S^m \to S_n$ that
depends only on the $t_j$.  Now the result follows from functoriality
of the pushforward of a measure.
\end{proof}

The previous result shows that the finite dimensional distributions
uniquely characterize the distribution of a stochastic process.  We
now turn an associated existence problem.  Namely given a family of
distributions that are candiates to be the finite dimensional
distributions of a stochastic process, is there in fact a stochastic
process with these FDDs.  In general this is not the case and the
result requires topological assumptions.  It is sufficient to assume
that the spaces involved are Borel.

\begin{defn}Let $(S_1, \mathcal{S}_1), (S_2, \mathcal{S}_2), \dotsc$ be a sequence of measurable spaces,
  and for each $n \in \naturals, $ let  $\mu_n$ be a probability
  measure on on $S_1 \times \dotsb \times S_n$.  We say that the
  sequence of measures $\mu_1, \mu_2, \dotsc$ is \emph{projective} if
  for every $n \in \naturals$ and every $A \in \mathcal{S}_1 \otimes
  \dotsb \otimes \mathcal{S}_n$ we have $\mu_{n+1}(A \times S_{n+1}) =
  \mu_n(A)$.
\end{defn}

\begin{thm}[Daniell Theorem]\label{DaniellExtension}Let $(S_1,
  \mathcal{S}_1), (S_2, \mathcal{S}_2), \dotsc$ be a sequence of
  measurable spaces, with $S_2, S_3 , \dotsc$ Borel and let $\mu_1,
  \mu_2, \dotsc$ be a projective sequence of measures then there exist
  random elements $\xi_n$ in $S_n$ for $n \in \naturals$ such that
  $\mathcal{L}(\xi_1, \dotsc, \xi_n) = \mu_n$ for all $n \in
  \naturals$.  In particular, there exists a probability measure $\mu$
  on $S_1 \times S_2 \times \dotsb$ such that for every $n \in
  \naturals$ and $A \in \mathcal{S}_1 \otimes \dotsb \otimes
  \mathcal{S}_n$ we have $\mu(A \times S_{n+1} \times \dotsb) = \mu_n(A)$.
\end{thm}
\begin{proof}
Trivially we can create $\xi_1$ with $\mathcal{L}(\xi_1) = \mu_1$
(just take $\Omega = S_1$ and define $\xi_1$ to be the identity).  Now
by extending $\Omega$ to $S_1 \times [0,1]$ we applying Lemma
\ref{ReproductionOfUniform} we can find independent $U(0,1)$ random
variables $\vartheta_2, \vartheta_3, \dotsc$ which are also
independent of $\xi_1$.  We construct the remaining $\xi_2, \xi_3,
\dotsc$ by an induction argument using Lemma \ref{Transfer}.  

Suppose that we have constructed $\xi_1, \dotsc, \xi_n$ where for each
$m > 1$ there exists a measurable function $f_m$ such that $\xi_m =
f_m(\xi_1, \vartheta_2, \dotsc, \vartheta_m)$.  Let $\eta_1, \dotsc,
\eta_{n+1}$ we arbitrary random elements such that
$\mathcal{L}(\eta_1, \dotsc, \eta_{n+1}) = \mu_{n+1}$ (e.g. define
$\tilde{\Omega} = S_1 \times \dotsb \times S_{n+1}$ with probability
measure $\mu_{n+1}$ and define $\eta_m(s_1, \dotsc, s_{n+1}) = s_m$).
By the induction hypothesis and the projective property of the sequence $\mu_n$ we have for each $A
\in \mathcal{S}_1 \otimes \dotsb \otimes \mathcal{S}_n$
\begin{align*}
\probability{(\eta_1, \dotsc, \eta_n) \in A} &= \probability{(\eta_1,
  \dotsc, \eta_n, \eta_{n+1}) \in A \times S_{n+1}} = \mu_{n+1}(A
\times S_{n+1}) \\
&= \mu_n(A) = \probability{(\eta_1, \dotsc, \eta_n) \in A} 
\end{align*}
and therefore $(\eta_1, \dotsc, \eta_n) \eqdist (\xi_1, \dotsc,
\xi_n)$.  Now we may apply Lemma \ref{Transfer} to conclude that there
is a measurable function $g : S_1 \times \dotsb \times S_n \times
[0,1] \to S_{n+1}$ such that $\xi_{n+1} = g(\xi_1, \dotsc, \xi_n,
\vartheta_{n+1})$ satisfies 
\begin{align*}
\mathcal{L}(\xi_1, \dotsc, \xi_{n+1}) &=
\mathcal{L}(\eta_1, \dotsc, \eta_{n+1})  = \mu_{n+1}
\end{align*}
Moreover we may define 
\begin{align*}
f_{n+1}(x_1, \dotsc, x_{n+1}) &= g(x_1,
f_2(x_1, x_2), \dotsc, f_n(x_1, \dotsc, x_n), x_{n+1})
\end{align*} so that
$\xi_{n+1} = f_{n+1}(\xi_1, \vartheta_2, \dotsc, \vartheta_{n+1})$.

For the last part of the theorem, define $\mu = \mathcal{L}(\xi_1,
\xi_2, \dotsc)$.  It then follows that for every $n \in \naturals$ and
$A \in \mathcal{S}_1 \otimes \dotsb \otimes \mathcal{S}_n$ we have
\begin{align*}
\mu(A \times S_{n+1} \dotsb) &= \probability{(\xi_1, \xi_2, \dotsc)
  \in A \times S_{n+1} \dotsb} \\
&=  \probability{(\xi_1, \dotsc, \xi_n)
  \in A } = \mu_n(A) 
\end{align*}
\end{proof}

We now generalize the Daniell Theorem to arbitrary index sets $T$.
First we generalize the notion of a projective sequence of measures to
a projective family on an arbitrary index set.
\begin{defn}Let $T$ be a set and suppose we are given a measurable
  space $(S_t, \mathcal{S}_t)$ for every $t \in T$ and for every
  finite subset $I \subset T$ we are given a probability measure
  $\mu_I$ on $\times_{t \in I} S_t$.  For any subset $U \subset T$
  define $(S_U, \mathcal{S}_U) = (\times_{t \in U} S_t , \otimes{t \in
    U} \mathcal{S}_t)$.  We say that $\lbrace \mu_I
  \rbrace$ is a \emph{projective family} if for every
  finite subset $J \subset T$ and $I \subset J$ we have $\mu_J( \cdot
  \times S_{J \setminus I}) = \mu_I( \cdot)$. If in the definition
  above we replace the set of finite subsets of $T$ by the set of
  countable subsets of $T$ the we say $\mu_I$ is a \emph{countable projective family}.
\end{defn}

Before we attack the theorem we give a description of the structure of the
infinite product $\sigma$-algebra that will prove useful in the proof
of the extension theorem.
\begin{lem}\label{ProductSigmaAlgebraAsCountableCylinderSets}Let $T$ be a set and $(S_t, \mathcal{S}_t)$ be a family of
  measurable spaces then the $\sigma$-algebra $\otimes_{t \in T}
  \mathcal{S}_t$ is precisely the set of sets of the form $A \times
  S_{T \setminus U}$ where $U \subset T$ is a countable subset and $A
  \in \otimes_{t \in U} \mathcal{S}_t$.
\end{lem}
\begin{proof}
We claim that 
\begin{align*}
\mathcal{C} &= \lbrace A \times
  S_{T \setminus U} \mid U \subset T \text{ is countable and } A
  \in \otimes_{t \in U} \mathcal{S}_t \rbrace
\end{align*}
is a $\sigma$-algebra.  Obviously $\mathcal{C}$ is non-empty.  To see that $\mathcal{C}$ is closed under set
complement take $A \times S_{T \setminus U}$ with $U \subset T$
countable and $A \in \mathcal{S}_U$.  Then $A^c  \in \mathcal{S}_U$
and moreover $(A \times S_{T \setminus U})^c = A^c \times S_{T
  \setminus U} \in \mathcal{C}$.  
Given a sequence $C_1, C_2, \dotsc \in \mathcal{C}$
with $C_j = A_j \times S_{T \setminus U_j}$, again by passing to the
union $\cup_{j=1}^\infty U_j$ we may assume that the $U_j$ are all the
same countable subset of $T$ and therefore $C_j = A_j \times S_{T
  \setminus U}$ with $A_j \in \mathcal{S}_U$.  It follows that
$\cup_{j=1}^\infty A_j \in \mathcal{S}_U$ and therefore
$\cup_{j=1}^\infty C_j \in \mathcal{C}$.  Closure under countable
intersection follows by De Morgans Law.  Since it is clear that each
$\pi_t$ is $\mathcal{C}$-measurable we see that $\otimes_{t \in T}
\mathcal{S}_t \subset \mathcal{C}$.

To see the reverse conclusion fix a countable subset $U \subset T$ and
we need to show that for every $A \in \mathcal{S}_U$ we have $A \times
S_{T \setminus U} \in \otimes_{t \in T}
\mathcal{S}_t$.  We note that the set of all  $A \subset S_U$ such that $A \times
S_{T \setminus U} \in \mathcal{S}_T$ is a $\sigma$-algebra since 
it is precisely the pullback and pushforward of $\mathcal{S}_U$ under the projection
$\pi_U : S_T \to S_U$ (Lemma \ref{SigmaAlgebraPullback}).  It clearly
contains all sets of the form $B \times S_{U \setminus \lbrace t
  \rbrace}$ for $t \in U$ and $B \in \mathcal{S}_t$.  Such sets
generate the $\sigma$-algebra $\mathcal{S}_U$  and therefore we
conclude that $A \times S_{T \setminus U} \in \mathcal{S}_T$.
\end{proof}

\begin{thm}[Daniell-Kolmogorov
  Theorem]\label{DaniellKolmogorovExtension}Let $T$ be a set, $(S_t,
  \mathcal{S}_t)$ for $t \in T$ be a family of Borel sets and $\mu_I$
  be a projective family of probability measures.  There exists a
  random element $\xi_t$ in $S_t$ for all $t \in T$ such that for
  every $I \subset T$ we have $\mathcal{L}(\xi_I) = \mu_I$.
\end{thm}
\begin{proof}
Let $\overline{T}$ be the set of countable subsets of $T$.  It is
clear that the restriction of the projective family $\mu_I$ to any
subset $U \subset T$ is a projective sequence and therefore we can
apply Theorem \ref{DaniellExtension} to construct a probability
measure $\mu_U$ on $S_U$ such that for every finite subset $J \subset
U$ we have $\mu_U(\cdot \times S_{U \setminus J}) = \mu_J(\cdot)$.  

Now assume that we have a \emph{countable} subset $V \subset U$ and
consider the probability measure $\mu_U( \cdot \times S_{U \setminus
  V}$ on $S_V$.  From what we have just shown, for every finite subset
$J \subset V$ and every $A \in \mathcal{S}_J$ we have
\begin{align*}
\mu_U(A \times S_{V \setminus J} \times S_{U \setminus V}) &= \mu_U(A
\times S_{U \setminus J}) = \mu_J(A)
\end{align*}
and therefore $\mu_U(\cdot \times S_{U \setminus V})$ and $\mu_V$ have
the same finite dimensional distributions and therefore by Lemma
\ref{ProcessLawsAndFDDs} we know that $\mu_U(\cdot \times S_{U
  \setminus V}) =\mu_V$.  Thus we have extended the projective
family $\mu_I$ to a countable projective family.  Since by Lemma
\ref{ProductSigmaAlgebraAsCountableCylinderSets} we know that
$\otimes_{t \in T} \mathcal{S}_t$ is the precisely the set of
countable cylinder sets, we can define a measure as a set function on
said sets.  Pick $U \subset T$ and let $A \in \mathcal{S}_U$ then we
define $\mu$ by $\mu(A \times S_{T \setminus U} ) = \mu_U(A)$.  We first
claim that the $\mu$ is well defined.  Suppose we have countable
subset $U,V \subset T$, $A \in \mathcal{S}_U$ and $B \in
\mathcal{S}_V$ such that $A \times S_{T setminus U} = B \times S_{T
  \setminus V}$.  We can write 
\begin{align*}
A \times S_{T setminus U} &= A \times S_{V \setminus U} \times S_{T
  setminus (U \cup V)}
\intertext{and}
B \times S_{T setminus V} &= B \times S_{U \setminus V} \times S_{T
  setminus (U \cup V)}
\end{align*}
from which it follows that $A \times S_{V \setminus U}  =  B \times
S_{U \setminus V}$.  Using this equality along with projectivity we get
\begin{align*}
\mu(A \times S_{T \setminus U}) &= \mu_U(A) = \mu_{U \cup V}(A \times
S_{V \setminus U}) \\
&=\mu_{U \cup V}(B \times S_{U \setminus V}) = \mu_V(B) = \mu(B \times S_{T \setminus V}) 
\end{align*}
which shows that $\mu$ is well defined.

It is clear that $\mu(\emptyset) = \mu_I(\emptyset)$ for any $I
\subset T$ and therefore $\mu(\emptyset) = 0$.  

To see countable additivity of $\mu$ suppose we are given set $A_1
\times S_{T \setminus U_1}, A_2 \times S_{T \setminus U_2}, \dotsc$
where each $U_j \subset T$ is a countable subset and $A_j \in
\mathcal{S}_{U_j}$ for $j \in \naturals$.  If we define $U =
\cup_{j=1}^\infty U_j$ and redefine $A_j$ as $A_j \times S_{U
  \setminus U_j}$ then we may assume that the $U_j$ are all the same.
  Therefore we now have by countable additivity of $\mu_U$,
\begin{align*}
\mu(\cup_{j=1}^\infty (A_j \times S_{T\setminus U})) &=
\mu((\cup_{j=1}^\infty A_j) \times S_{T\setminus U}) \\
&=\mu_U(\cup_{j=1}^\infty A_j) = \sum_{j=1}^\infty \mu_U(A_j) =
\sum_{j=1}^\infty \mu(A_j \times S_{T \setminus U}) 
\end{align*}

Having defined $\mu$ on $(S_T, \mathcal{S}_T)$ we now define $\xi_t$
to be the projection $\xi_t : S_T \to S_t$ for each $t \in T$.  It is
clear from the definition of $\mu$ that for every finite subset of $I
\subset T$ and $A \in \mathcal{S}_I$ we have
\begin{align*}
\probability{ \xi_I \in A} &= \mu(A \times S_{T \setminus I}) = \mu_I(A)
\end{align*}
and therefore $\mathcal{L}(\xi_I) = \mu_I$.
\end{proof}

There are a great many things to be said about stochastic processes in
general, however we will wait a bit to travel that road and instead begin to
look at a special subclass of stochastic processes.

The first specialization is to assume our index set $T \subset \overline{\reals}$ (e.g. $\integers$,
$\reals$).  A good intuition here is that $T$ represents time and that
$X_t$ repsesents the dynamics of a time-varying random variable.

Remaining in the land of intuition, we know that as time progress we
learn from our experience; more things become known (or at least
knowable).  If we translate the term ``knowable'' into the term
``measurable'' we get a mathematically precise description of the
increasing flow of information with time.
\begin{defn}Suppose we have a probability space $(\Omega,
  \mathcal{A})$.  A collection of $\sigma$-algebras $\mathcal{F}_t
  \subset \mathcal{A}$ for $t
  \in T$ is called a \emph{filtration} if $\mathcal{F}_s \subset
  \mathcal{F}_t$ for all $s < t$.
\end{defn}

Given a stochastic process one can easily construct a filtration
associated with observations of said process.
\begin{defn}Given a probability space $(\Omega,  \mathcal{A})$, an
  index set $T \subset \overline{\reals}$ and a stochastic process $X
  : \Omega \to U$, the filtration \emph{generated by} $X$ is 
\begin{align*}
\mathcal{F}_t &= \sigma(\lbrace \sigma(X_s) \mid s \leq t \rbrace)
\end{align*}
\end{defn}

We then need to tie back the notion of a stochastic process with the
notion of a filtration.  In particular one wants to call out the case
in which a filtration contains enough information to be able to
measure the values of the process (i.e. contains at least as much
information as the knowledge of the values of the process itself).
\begin{defn}Given a probability space $(\Omega,  \mathcal{A})$, an
  index set $T \subset \overline{\reals}$, a
  filtration $\mathcal{F}_t$ for $t \in T$ and a stochastic process $X
  : \Omega \to U$, we say that $X$ is \emph{adapted} to $\mathcal{F}$
  if $X_t$ is $\mathcal{F}_t$-measurable for every $t \in T$.
\end{defn}

\begin{examp}$X$ is adapted to its generated filtration (and the
  generated filtration is the smallest filtration adapted to $X$).
\end{examp}

Now we are able to define the special class of stochastic processes
with which well spend some time.
\begin{defn}Given a probability space $(\Omega,  \mathcal{A})$, an
  index set $T \subset \overline{\reals}$ and a
  filtration $\mathcal{F}_t$ for $t \in T$, a stochastic process $M :
  \Omega \to \reals^T$ is called an \emph{$\mathcal{F}$-martingale} if 
\begin{itemize}
\item[(i)]$M_t$ is integrable for all $t \in T$
\item[(ii)]$M$ is adapted to $\mathcal{F}$
\item[(iii)]$\cexpectation{\mathcal{F}_s}{M_t} = M_s$ a.s. for all
  $s,t \in T$ with $s \leq t$.
\end{itemize}
If we replace the condition (iii) by the condition
$M_s \leq \cexpectation{\mathcal{F}_s}{M_t}$ a.s., then $M$ is said to
be a \emph{submartingale} and if we replace it with
$M_s \geq \cexpectation{\mathcal{F}_s}{M_t} $ a.s. then $M$ is said to be a
\emph{supermartingale}.
\end{defn}

A entire class of examples of martingales can be constructed via the
following Lemma.  
\begin{lem}\label{ClosedMartingales}Given a probability space $(\Omega,  \mathcal{A})$, an
  index set $T \subset \overline{\reals}$, a
  filtration $\mathcal{F}_t$ for $t \in T$ and a integrable random
  variable $\xi$, the process $M_t = \cexpectation{\mathcal{F}_t}{\xi}$ is
  an $\mathcal{F}$-martingale.
\end{lem}
\begin{proof}Integrability $\mathcal{F}$-adaptedness of $M_t$ follows from the definition of
  conditional expectation.  Since for $s,t \in T$ with $s \leq t$ we
  have $\mathcal{F}_s \subset \mathcal{F}_t$, the chain rule for
  conditional expectation shows 
\begin{align*}
\cexpectation{\mathcal{F}_s}{M_t} &=
\cexpectation{\mathcal{F}_s}{\cexpectation{\mathcal{F}_t}{\xi}} =
\cexpectation{\mathcal{F}_s}{\xi} = M_s
\end{align*}
\end{proof}
A martingale that can be expressed in the form given by the Lemma is
referred to as a \emph{closed} martingale.  We call the reader's attention the the fact
that the index set in the above Lemma is allowed to include $\infty$.
For in that case, we might as well assume that $\xi$ is
$\mathcal{F}_\infty$-measurable (or equivalently assume that $\xi =
M_\infty$ a.s.)  This consideration points to an
arguably less transparent defintion
definition of a closed martingale as one for which $\sup T \in T$ (see
Kallenberg for example; what we call a closed martingale he calls a
\emph{closable} martingale).  Also thinking about the case  in which
$\infty \ in T$ (or more generally when $\sup T \in T$) suggests a
relationship between a closing $\xi$ and a limit $\lim_{t \to \infty}
M_t$.  Such a relationship indeed exists and is explained in
Martingale convergence theorems that follow.

The unbiased random walk provides one of the simplest examples of a martingale.
\begin{examp}\label{RandomWalkAsMartingale}Suppose we are given a collection of independent random
  variables $\xi_1, \xi_2, \dots$ with $\expectation{\xi_n} = 0$ for
  all $n > 0$.  Define the filtration
  $\mathcal{F}_0 = \lbrace \emptyset, \Omega \rbrace$ and
  $\mathcal{F}_n = \sigma(\xi_1, \dots, \xi_n)$ for $n > 0$ and define
  the process $M_0=0$ and $M_n = \xi_1 + \cdots + \xi_n$.  Then $M_n$
  is an $\mathcal{F}$-martingale.
\end{examp}
From the point of view of gambling, if we think of each $\xi_n$ as
representing the outcome of a fair game based on a bet of one dollar,
then $M_n$ represents the wealth at time $n$ of a gambler that places a one dollar
bet on every game.  The gambling interpretation of martingales doesn't
really depend on the random walk structure of the example.  Given any
martingale we can interpret $M_n$ as the wealth at time $n$ and then
use use a telescoping sum
\begin{align*}
M_n &= M_0 + \sum_{j=1}^n (M_j - M_{j-1})
\end{align*}
to represent the wealth at time $n$ as the initial wealth $M_0$ plus
the sum of the return $M_j - M_{j-1}$ on the first $j$ bets.

The second example shows how one can make a martinagale out of the
variance of a base martinagle.
\begin{examp}Suppose we have the setup of Example
  \ref{RandomWalkAsMartingale} except that we also assume a constant variance
  $\expectation{\xi_n^2} = \sigma^2$ for all $n > 0$.  Then $M_n^2 - n
  \sigma^2$ is an $\mathcal{F}$-martingale.  Integrability and
  $\mathcal{F}$-adaptedness are immediate from our assumptions.  The
  martingale property requires a small computation
\begin{align*}
\cexpectationlong{\mathcal{F}_{n-1}}{M_n^2 - n \sigma^2} &=
\cexpectationlong{\mathcal{F}_{n-1}}{M_{n-1}^2 + 2 M_{n-1} \xi_n +
  \xi_n^2 - n \sigma^2} \\
&=M_{n-1}^2 +  2 M_{n-1} \cexpectationlong{\mathcal{F}_{n-1}}{\xi_n}
+  \cexpectationlong{\mathcal{F}_{n-1}}{ \xi_n^2} - n \sigma^2 \\
&=M_{n-1}^2 +  2 M_{n-1} \expectation{\xi_n}
+  \expectation{ \xi_n^2} - n \sigma^2 \\
&= M_{n-1}^2 - (n-1)\sigma^2
\end{align*}
\end{examp}

Returning to our gambling interpretation of martingales we discussed in Example
\ref{RandomWalkAsMartingale}, one can 
ask whether the ``unit bet'' assumption can be relaxed.  That is we
think of each increment $M_n - M_{n-1}$ as the return on a game in
which one has wagered on dollar.  It
would be very interesting indeed to know whether there is a betting
strategy that could make a fair game into an advantageous game (either
for the gambler or the house).  As manifested in our view of the world
as a wealth process and a returns process, the bet on the $n^{th}$
game is simply a multiplier $A_n$ applied to the return $M_n -
M_{n-1}$.  Thus the betting strategy is also a stochastic process.  To
model reality, there is an important
constraint on a betting strategy.  A bet on the $n^{th}$ game must be
made prior to the $n^{th}$ game being played and therefore should only
should only be able to make use of information about the outcome of
the first $n-1$ games.  Thus a betting strategy must not only bet
adapted the filtration $\mathcal{F}$ but satisfy the stonger condition
of the following definition.
\begin{defn}Given a filtration $\mathcal{F}_0 \subset \mathcal{F}_1
  \subset \cdots$, we say a process $A_n$ is \emph{$\mathcal{F}$-previsible} or
  \emph{$\mathcal{F}$-non-anticipating} if $A_n$ is $\mathcal{F}_{n-1}$-measurable.
\end{defn}
We make the assumption that a betting strategy is previsible and model
the strategy as providing the amount that a gambler will bet.  Of
interest is that we allow the gambler to ``short'' the bet (i.e. bet a
negative amount).  It turns out that under reasonable conditions
betting strategies alone cannot alter the fairness of a game.
\begin{lem}\label{DiscreteTimeMartingaleTransform}Let $M_n$ be a martingale and let $A_n$ be an
  $\mathcal{F}$-previsible process with each $A_n$ bounded and $A_0 =
  1$.  
Define the \emph{martingale transform}
  $\tilde{M}_n = \sum_{j=0}^n A_j\left(M_j - M_{j-1}\right )$ (we
  define $M_{-1} = 0$ for simplicity).  Then $\tilde{M}_n$ is a martingale.
\end{lem}
\begin{proof}
Clearly $\tilde{M}_n$ is $\mathcal{F}_n$-measurable as $M_j$ and
$A_j$ are for each $j\leq n$.  Integrability
of $\tilde{M}_n$ follows from the integrability of the $M_n$ and the
boundedness of $A_n$.  The martingale property follows from a simple
computation
\begin{align*}
\cexpectationlong{\mathcal{F}_{n-1}}{\tilde{M}_n} &= \sum_{j=0}^n
\cexpectationlong{\mathcal{F}_{n-1}}{A_j(M_j - M_{j-1})} \\
&=A_n \cexpectationlong{\mathcal{F}_{n-1}}{(M_n - M_{n-1})}  +
\sum_{j=0}^{n-1} A_j(M_j - M_{j-1})\\
&= \tilde{M}_{n-1}
\end{align*}
\end{proof}

\begin{lem}Let $M_t$ be a martingale then $\expectation{M_t}$ is
  constant in $t \in T$.
\end{lem}
\begin{proof}For $s,t \in T$ with $s < t$, by the
  martingale property and the chain rule of
  conditional expectations we have
$\expectation{M_s} = \expectation{\cexpectation{\mathcal{F}_s}{M_t}} = \expectation{M_t}$.
\end{proof}

\begin{defn}Given a set $T \subset \overline{\reals}$, we call a $T
  \cup \lbrace \sup T \rbrace$-valued random variable a \emph{random
    time}.  A random time is called an
  \emph{$\mathcal{F}$-optional time} (also called an \emph{$\mathcal{F}$-stopping time}) if and only if $\lbrace \tau \leq
  t \rbrace \in \mathcal{F}_t$ for all $t \in T$.
\end{defn}

An $\mathcal{F}$-optional time $\tau$ represents a random decision
rule of when to stop a game such that the decision  to stop at time $t$can be made based only on
information acccumulated up to and including time $t$ (i.e. without
seeing the future).  Note that we allow a random time to take the value $\sup T$ (think of
this as infinity) but the condition of being an optional time does not
place a condition on what happens at $\sup T$.

Provided with an optional time there is a $\sigma$-algebra of events
that is associated with it.
\begin{defn}Given an optional time $\tau$, we define 
\begin{align*}
F_\tau &= \lbrace
  A \in \mathcal{A} \mid A \cap \lbrace \tau \leq t \rbrace\in
    \mathcal{F}_t \text{ for all } t \in T \rbrace
\end{align*}
\end{defn}

 Note that we have not taken the generated $\sigma$-algebra in the
above definition, because of the following.
\begin{lem}\label{StoppedFiltration}Given an optional time $\tau$, $\mathcal{F}_\tau$ is a
  $\sigma$-algebra.  Furthermore, $\tau$ is $\mathcal{F}_\tau$-measurable.
\end{lem}
\begin{proof}Since $\Omega \cap \lbrace \tau \leq t \rbrace  = \lbrace
  \tau \leq t \rbrace \in \mathcal{F}_t$ by definition of optional
  time, we see that $\Omega \in \mathcal{F}_\tau$.  If we suppose that
  $A \in \mathcal{F}_\tau$ then for all $t \in T$, we apply elementary
  Boolean  algebra and $\sigma$-algebra properties of $\mathcal{F}_t$ to see $A^c \cap \lbrace
  \tau \leq t \rbrace = (A \cap \lbrace
  \tau \leq t \rbrace)^c \cap \lbrace
  \tau \leq t \rbrace \in \mathcal{F}_t$.  Lastly, given $A_1, A_2,
  \dots \in \mathcal{F}_\tau$, we have $(\cap_{n=1}^\infty A_n)
  \cap \lbrace
  \tau \leq t \rbrace = \cap_{n=1}^\infty (A_n
  \cap \lbrace
  \tau \leq t \rbrace ) \in \mathcal{F}_t$ and thus $\mathcal{F}_\tau$
  is a $\sigma$-algebra.

For every $s,t \in T$, we have 
$\lbrace \tau \leq s \rbrace \cap \lbrace \tau \leq t \rbrace =
\lbrace \tau \leq s \wedge t \rbrace \in \mathcal{F}_{s \wedge t}
\subset \mathcal{F}_t$ which shows every set $\lbrace \tau \leq s
\rbrace \in \mathcal{F}_\tau$ for $s \in T$.  Now for $s \in \reals
\setminus T$,
$\lbrace \tau \leq s \rbrace = \cup_{t \in T ; t < s} \lbrace \tau
\leq t \rbrace$; the trick is that this is an uncountable
union so we have to be a bit more careful in handling this
case.  Let $\tilde{s} = \sup \lbrace t \leq s \mid t \in T \rbrace$.  The
first thing to note is that $\lbrace \tau \leq s \rbrace = \lbrace
\tau \leq \tilde{s} \rbrace$.  The inclusion $\supset$ is obvious
since $s \geq \tilde{s}$.  To see the inclusion $\subset$ note that we
cannot have $\tilde{s} < \tau(\omega) \leq s$ since $\tau(\omega) \in
T$.  If $\tilde{s} \in T$ then we have show $\lbrace \tau \leq s
\rbrace \in \mathcal{F}_\tau$.  Lets assume that $\tilde{s} \notin T$.
By definition, we can find an increasing sequence $s_n \leq \tilde{s}$
such that $s_n \in T$ and $\lim_{n
  \to \infty} s_n = \tilde{s}$.  Now we claim that $\cup_n \lbrace \tau \leq
s_n \rbrace = \lbrace \tau \leq \tilde{s} \rbrace$.  The inclusion
$\subset$ follows since $s_n \leq \tilde{s}$.  To see the other
inclusion, suppose $\tau(\omega) \leq \tilde{s}$.  Because we have
assumed $\tilde{s} \notin T$ then in fact $\tau(\omega) < \tilde{s}$
and we can find $s_n$ such that $\tau(\omega) < s_n < \tilde{s}$
showing $\omega \in \cup_n \lbrace \tau \leq
s_n \rbrace$.  Putting the two equalities together
\begin{align*}
\lbrace \tau \leq s \rbrace &= \lbrace \tau \leq \tilde{s} \rbrace =
\cup_n \lbrace \tau \leq s_n \rbrace \in \mathcal{F}_\tau
\end{align*}
and we have shown that for all $s \in \reals$, $\lbrace \tau \leq s
\rbrace \in \mathcal{F}_\tau$.
This suffices to show $\mathcal{F}_\tau$-measurability by Lemma \ref{MeasurableByGeneratingSet}.
\end{proof}

Conceptually, one thinks of the $\sigma$-algebra $\mathcal{F}_\tau$ as being
events $A$ such that if $\tau \leq t$ then one only needs information
available at time $t$ to determine whether $A$ has occurred or not.
More suggestively one may say that $\mathcal{F}_\tau$ as being the
events that happen before $\tau$.

\begin{lem}Let $\sigma$ and $\tau$ be optional times with $\sigma \leq
  \tau$, then $\mathcal{F}_\sigma \subset \mathcal{F}_\tau$.
 \end{lem}
\begin{proof}Suppose we have an $A \in \mathcal{F}_\sigma$.  Because
  $\sigma \leq \tau$, we know that$\lbrace \tau \leq t
  \rbrace \subset \lbrace \sigma \leq t \rbrace$  for all $t \in T$.  Take a 
  $t \in T$, then $A \cap \lbrace \tau \leq t \rbrace = (A \cap
  \lbrace \sigma \leq t \rbrace) \cap \lbrace \tau \leq t \rbrace \in \mathcal{F}_t$.
\end{proof}

\begin{lem}Let $T \subset \overline{\reals}$ be a countable subset of
  the extended reals, let $\mathcal{F}_t$ be a filtration and $\tau :
  \Omega \to T$ be a random time.  Then $\tau$ is an optional time if
  and only if $\lbrace \tau = t \rbrace \in \mathcal{F}_t$ for every
  $t \in T$.
\end{lem}
\begin{proof}
Suppose that $\lbrace \tau = t \rbrace \in \mathcal{F}_t$ then we see
that 
\begin{align*}
\lbrace \tau \leq t \rbrace &= \cup_{s \leq t} \lbrace \tau = s \rbrace
\end{align*}
which is a countable union of sets $\lbrace \tau = s \rbrace \in
\mathcal{F}_s \subset \mathcal{F}_t$ hence is in $\mathcal{F}_t$.

Now if $\tau$ is $\mathcal{F}$-optional then similarly we may write
\begin{align*}
\lbrace \tau = t \rbrace &= \lbrace \tau \leq t \rbrace \cap \left(
  \cup_{s < t} \lbrace \tau \leq s \rbrace\right )^c
\end{align*}
which shows that $\lbrace \tau = t \rbrace \in \mathcal{F}_t$.
\end{proof}

If we think of an optional time as a random stopping rule for a game, then a
useful construct is the random stopping element associated with a
process and the stopping rule.  An interesting aspect of the proof is
that it shows stopped processes can be represented as martingale transforms.
\begin{lem}Let $\tau$ be an $\mathcal{F}$-optional time on a countable index set $T
  \subset \overline{\reals}$ and let $X$ be a stochastic
  process on $T$ adapted to $\mathcal{F}$. Then the random element $X_\tau$ is $\mathcal{F}_\tau$-measurable.
\end{lem}
\begin{proof}
TODO
\end{proof}

The $\sigma$-algebra maybe thought of as being constructed by patching
together the individual $\sigma$-algebras $\mathcal{F}_t$ of the
filtration; many arguments make use of this idea.  A precise statement
that allows localization of conditional expectations with respect to
$\mathcal{F}_\tau$ is given here.  The reader should translate the
following lemma into the intuitively obvious prose assertion that ``given
that $\tau = t$, an event $A$ happens before
$\tau$ if and only if $A$ happens before $t$''.
\begin{lem}\label{LocalizationOfStoppedFiltration}Given a filtration $\mathcal{F}_t$ and an
  $\mathcal{F}$-optional time $\tau$, for every $t \in T$, the $\sigma$-algebras
  $\mathcal{F}_t$ and $\mathcal{F}_\tau$ agree on the set $\lbrace
  \tau = t\rbrace$.
\end{lem}
\begin{proof}
Suppose $A \in \mathcal{F}_\tau$ and $A \subset \lbrace \tau = t
\rbrace$.  Then by definition of $\mathcal{F}_\tau$ we know that $A = A
\cap \lbrace \tau \leq t \rbrace \in \mathcal{F}_t$.  On the other
hand, if $A \in \mathcal{F}_t$ we know that for all $s \in T$,
\begin{align*}
A \cap \lbrace \tau \leq s \rbrace &= A \cap \lbrace \tau = t
\rbrace \cap\lbrace \tau \leq s \rbrace = \begin{cases}
A & \text{if $s \geq t$} \\
\emptyset & \text{if $s < t$}
\end{cases}
\in \mathcal{F}_s
\end{align*}
\end{proof}

Another useful fact is 
\begin{prop}\label{StoppedAlgebraMinOfOptionalTimes}Let $\sigma$ and $\tau$ be $\mathcal{F}$-optional times
  then $\mathcal{F}_\sigma \cap \lbrace \sigma \leq \tau \rbrace \subset
  \mathcal{F}_{\sigma \wedge \tau} = \mathcal{F}_\sigma \cap \mathcal{F}_\tau$.
\end{prop}
\begin{proof}
TODO:
\end{proof}

\subsection{Discrete Time Martingales}
For the special case of index set $T = \integers_+$, we often call a
martingale a \emph{discrete time martingale}.  Discrete martingales are well
undertsood objects and as it turns out many important results about discrete
martingales can be used to prove corresponding results for general martingales via approximation
arguments.  Thus, we will start our study of martingales by studying
discrete martingales.

The first thing to note is a simple observation that the definition
for the special case of discrete martingales can be simplified.
\begin{lem}Let $\mathcal{F}_n$ be a filtration and $M_n$ be a sequence
  of $\mathcal{F}$-adapted integrable random variables.  If
  $\cexpectation{\mathcal{F}_{n-1}}{M_n} = M_{n-1}$ for $n > 0$ then $M_n$
  is an $\mathcal{F}$-martingale.
\end{lem}
\begin{proof}We only have to show that
  $\cexpectation{\mathcal{F}_m}{M_n} = M_m$ for all $m \leq n$.
  Because we know $M_n$ is $\mathcal{F}_n$-measurable then we have
  $\cexpectation{\mathcal{F}_n}{M_n} = M_n$.  If $m < n-1$, then we
  proceed by induction
  assuming the result is true for $m+1$,
\begin{align*}
\cexpectation{\mathcal{F}_m}{M_n} &=
\cexpectation{\mathcal{F}_m}{\cexpectation{\mathcal{F}_{m+1}}{M_n}} \\
&=\cexpectation{\mathcal{F}_m}{M_{m+1}} & & \text{by induction hypothesis}\\
&= M_m & & \text{by hypothesis}
\end{align*}
\end{proof}

Furthermore in discrete time we have a simple version of a
construction of a useful class of optional times.
\begin{defn}Let $\mathcal{F}$ be a filtration on $\integers_+$ and let
  $X_n$ be an $\mathcal{F}$-adapted process with values in a meaurable
  space $(S, \mathcal{S})$.  For every $A \in \mathcal{S}$ be can
  define the \emph{hitting time} by
\begin{align*}
\tau_A &= \min \lbrace n \mid X_n \in A \rbrace
\end{align*}
where by convention we assume the minimum of the empty set is positive infinity.
\end{defn}

For the moment the only thing we want to record about hitting times is
that they are indeed optional times.  They will soon thereafter start to prove their utility.
\begin{lem}\label{HittingTimesDiscrete}A hitting time is an
  $\mathcal{F}$-optional time.
\end{lem}
\begin{proof}
Simply write for every finite $n$,
\begin{align*}
\lbrace \tau_A \leq n \rbrace &= \cup_{0 \leq m \leq n} \lbrace X_m
\in A \rbrace
\end{align*}
and note that by $\mathcal{F}$-adaptedness of $X$, we have $\lbrace X_m
\in A \rbrace \in \mathcal{F}_m \subset \mathcal{F}_n$.
\end{proof}

\begin{lem}\label{ExpectationStoppedMartingaleDiscrete}Let $M_n$ be a martingale and let $\tau$ be an optional
  time such that $\tau \leq C < \infty$, then $\expectation{M_\tau} =
  \expectation{M_0}$.
\end{lem}
\begin{proof}
\begin{align*}
\expectation{M_\tau} &= \sum_{n=0}^C \expectation{M_n ; \tau=n} \\
&=\sum_{n=0}^C \expectation{\cexpectationlong{\mathcal{F}_n}{M_C} ;
  \tau=n} \\
&=\sum_{n=0}^C \expectation{M_C ;
  \tau=n} \\
&=\expectation{M_C ;
  \cup_{n=0}^C \tau=n} = \expectation{M_C}\\
\end{align*}

Therefore the result follows from the case of a constant deterministic
time.  This latter case is just a simple induction on $n$.
\end{proof}


\begin{thm}[Optional Sampling Theorem]\label{OptionalSamplingDiscrete}Let $\sigma$ and $\tau$ be bounded
  $\mathcal{F}$-optional times and
  let $M_n$ be a martingale, then 
\begin{align*}
\cexpectationlong{\mathcal{F}_\sigma}{M_\tau} &\geq M_{\sigma \wedge
  \tau}
\text{ a.s.}
\end{align*}
TODO: The assumption that $\sigma$ is bounded can be removed (see
Kallenberg's proof for a demonstration of that).  How to fix up the
proof below or amend them?

TODO: This result assumes that we have a martingale on $\integers$;
the result holds with arbitrary countable index sets.
\end{thm}
\begin{proof}
We warn the reader that the following proof is a bit longer than many
you'll see in the literature.  It intentionally avoids any of the
tricks that make for short proofs in hopes of making a clearer
explanation for why the result is in fact true.

We first begin with a simple special case with $\sigma$ deterministic that captures the essence of
the result.  Suppose $\tau$ is $\mathcal{F}$-optional and there exist
constants $k, m$ such that $k \leq \tau \leq m$.  We need to prove
that $\cexpectationlong{\mathcal{F}_k}{M_\tau} = M_k$.  We do this by
induction on $m-k$.  For $m-k=0$, the result is trivial since in this
case $M_\tau = M_k$.  For the induction step suppose we have $k \leq
\tau \leq m$ with $m-k >0$ and note that we can use the induction
hypothesis on the stopping time $k+1 \leq \tau\vee k+1 \leq m$.  We
get
\begin{align*}
\cexpectationlong{\mathcal{F}_k}{M_\tau} &=
\cexpectationlong{\mathcal{F}_k}{M_{\tau \vee k+1}} +
\cexpectationlong{\mathcal{F}_k}{(M_k-M_{k+1})\characteristic{\tau=k}}
\\
&=\cexpectationlong{\mathcal{F}_k}{\cexpectationlong{\mathcal{F}_{k+1}}{M_{\tau
      \vee k+1}} } +
\characteristic{\tau=k}\cexpectationlong{\mathcal{F}_k}{(M_k-M_{k+1})}
\\
&=\cexpectationlong{\mathcal{F}_k}{M_{k+1}} + 0 = M_k
\end{align*}

To get the general result, we suppose that we are given $\sigma, \tau
\leq N < \infty$
and we suppose we are given $A \in \mathcal{F}_\sigma$.  Note that we
can write $A = \cup_{n=0}^N A \cap \lbrace \sigma=n\rbrace$ where
$A \cap \lbrace \sigma=n\rbrace \in \mathcal{F}_n$ for all $0 \leq n
\leq N$.
\begin{align*}
\expectation{M_\tau ; A} &= \sum_{n=0}^N \sum_{m=0}^N \expectation{M_n
  \characteristic{\tau=n} \characteristic{\sigma=m}
  \characteristic{A}} \\
&= \sum_{n=0}^N \left (
\sum_{m=n}^N \expectation{M_m
  \characteristic{\tau=m} \characteristic{\sigma=n} \characteristic{A}}+
\sum_{m=n+1}^N \expectation{M_n
  \characteristic{\tau=n} \characteristic{\sigma=m}\characteristic{A}}
\right) \\
&=\sum_{n=0}^N \expectation{(M_{\tau \vee n} - M_n
  \characteristic{\tau < n})\characteristic{\sigma=n}
  \characteristic{A}} + 
\expectation{M_n
  \characteristic{\tau=n} \characteristic{\sigma \geq
    n+1}\characteristic{A}} \\
&=\sum_{n=0}^N \expectation{M_n  \characteristic{\tau\geq n}\characteristic{\sigma=n}
  \characteristic{A}} + 
\expectation{M_n
  \characteristic{\tau=n} \characteristic{\sigma \geq
    n+1}\characteristic{A}} \\
&=\sum_{n=0}^N \expectation{M_n \characteristic{\tau \wedge \sigma =
    n}  \characteristic{A}} \\
&= \expectation{M_{\tau \wedge \sigma} ; A}
\end{align*}
and therefore by the defining property of conditional expectations we
are done.

Here is another rather direct proof that seems quite transparent and
is completely self contained.
Suppose $\sigma, \tau \leq N$.  Pick $A \in \mathcal{F}_\sigma$ and
compute
\begin{align*}
\expectation{M_\tau ; A} &= \sum_{n=0}^N \sum_{m=0}^N \expectation{M_n
  ; A \cap \lbrace \tau = n \rbrace \cap \lbrace \sigma = m\rbrace} \\
&=\sum_{n=0}^{N-1} \sum_{m=n+1}^N \expectation{M_n
  ; A \cap \lbrace \tau = n \rbrace \cap \lbrace \sigma = m\rbrace} + \sum_{n=0}^{N} \sum_{m=n}^N \expectation{M_m
  ; A \cap \lbrace \tau = m \rbrace \cap \lbrace \sigma = n\rbrace}\\
&=\sum_{n=0}^{N-1} \expectation{M_n
  ; A \cap \lbrace \tau = n \rbrace \cap \lbrace \sigma > n\rbrace} + \sum_{n=0}^{N} \sum_{m=n}^N \expectation{M_N
  ; A \cap \lbrace \tau = m \rbrace \cap \lbrace \sigma = n\rbrace}\\
&=\sum_{n=0}^{N-1} \expectation{M_n ; A \cap \lbrace \tau = n \rbrace
  \cap \lbrace \sigma > n\rbrace} + 
\sum_{n=0}^{N} \expectation{M_N  ; A \cap \lbrace \tau \geq n \rbrace \cap \lbrace \sigma = n\rbrace}\\
&=\sum_{n=0}^{N-1} \expectation{M_n ; A \cap \lbrace \tau = n \rbrace
  \cap \lbrace \sigma > n\rbrace} + 
\sum_{n=0}^{N} \expectation{M_n  ; A \cap \lbrace \tau \geq n \rbrace
  \cap \lbrace \sigma = n\rbrace}\\
&=\sum_{n=0}^{N} \expectation{M_n  ; A \cap \lbrace \tau \wedge \sigma = n \rbrace}\\
&=\expectation{M_{\tau \wedge \sigma} ; A}
\end{align*}
\end{proof}

\begin{cor}Let $M_n$ be a martingale and let $\tau$ be an optional
  time, then $M_{\tau \wedge n}$ is a martingale.
\end{cor}
\begin{proof}
This is an immediate consequence of Optional Sampling as $\tau \wedge
n$ and $n-1$ are both bounded optional times and therefore 
\begin{align*}
\cexpectationlong{\mathcal{F}_{n-1}}{M_{\tau \wedge n}} &= M_{\tau
  \wedge n \wedge (n-1)} = M_{\tau \wedge (n-1)}
\end{align*}

Note that this can also be proven by a direct computation using the
fact that $\lbrace \tau \geq n \rbrace = \lbrace \tau \leq n-1
\rbrace^c \in \mathcal{F}_{n-1}$:
\begin{align*}
\cexpectationlong{\mathcal{F}_{n-1}}{M_{\tau \wedge n}} &=
\sum_{m=0}^{n-1} \cexpectationlong{\mathcal{F}_{n-1}}{M_m
  \characteristic{\tau=m}} + \cexpectationlong{\mathcal{F}_{n-1}}{M_n
\characteristic{\tau \geq n}} \\
&= \sum_{m=0}^{n-1} M_m  \characteristic{\tau=m} + M_{n-1}
\characteristic{\tau \geq n} \\
&= \sum_{m=0}^{n-2} M_m  \characteristic{\tau=m} + M_{n-1}
\characteristic{\tau \geq n-1} = M_{\tau \wedge (n-1)}\\
\end{align*}
\end{proof}

\begin{lem}[Doob Decomposition]\label{DoobDecompositionDiscrete}Let
  $X_n$ be a submartingale, then there exists a martingale $M_n$ and
  an almost surely increasing non-negative $\mathcal{F}$-previsible process $A_n$ such that $X_n
  = X_0 + M_n + A_n$.
\end{lem}
\begin{proof}
We start with $M_0 = A_0 = 0$ and proceed to define $M_n$ by induction
for $n >0$ in the most natural way possible
\begin{align*}
M_n &= X_n - \cexpectationlong{\mathcal{F}_{n-1}}{X_n} + M_{n-1} \\
A_n &= X_n - M_n - X_0 = \cexpectationlong{\mathcal{F}_{n-1}}{X_n} - M_{n-1}
+ X_0
\end{align*}
a simple induction validating that $M_n$ is
$\mathcal{F}_n$-measurable, $A_n$ is $\mathcal{F}_{n-1}$-measurable
and $M_n$ is integrable.

The martingale property follows immediately from the definition and
the $\mathcal{F}_{n-1}$-measurability of
$\cexpectationlong{\mathcal{F}_{n-1}}{X_n}$ and $M_{n-1}$:
\begin{align*}
\cexpectationlong{\mathcal{F}_{n-1}}{M_n} &=
\cexpectationlong{\mathcal{F}_{n-1}}{X_n} -
\cexpectationlong{\mathcal{F}_{n-1}}{\cexpectationlong{\mathcal{F}_{n-1}} {X_n}} + 
\cexpectationlong{\mathcal{F}_{n-1}}{M_{n-1}} =M_{n-1}
\end{align*}

The fact that $A_n$ is increasing follows from
\begin{align*}
A_n &= \cexpectationlong{\mathcal{F}_{n-1}}{X_n} - M_{n-1} =
\cexpectationlong{\mathcal{F}_{n-1}}{X_n} - X_{n-1} + A_{n-1}
\end{align*}
so that 
\begin{align*}
A_n - A_{n-1} &= \cexpectationlong{\mathcal{F}_{n-1}}{X_n} - X_{n-1}
\geq 0 \text{ a.s.}
\end{align*}
by the submartingale property of $X_n$.  Non-negativity of $A_n$
follows from the facts that $A_n$ is increasing and $A_0 = 0$.
\end{proof}

The Doob Decomposition is generally a useful tool to transfer results
about martingales over to submartingales.  As a first illustration we
present the following optional
sampling theorem to submartingales
\begin{cor}\label{OptionalSamplingSubmartingaleDiscrete}Let $X_n$ be a submartingale and let $\sigma$ and $\tau$ be
  bounded optional times, then
  $\cexpectationlong{\mathcal{F}_\sigma}{X_\tau} \geq X_{\sigma \wedge
    \tau}$ a.s.

TODO: See comments about relaxing boundedness hypotheses.
\end{cor}
\begin{proof}
We write $X_n = M_n + A_n + X_0$ with $M_n$ a martingale and $A_n$ positive
increasing previsible.  Applying optional sampling (Theorem
\ref{OptionalSamplingDiscrete}) and the Doob Decomposition we get
\begin{align*}
\cexpectationlong{\mathcal{F}_\sigma}{X_\tau} &=
\cexpectationlong{\mathcal{F}_\sigma}{M_\tau + A_\tau + X_0} = M_{\sigma
  \wedge \tau} + \cexpectationlong{\mathcal{F}_\sigma}{A_\tau} + X_0
\end{align*}
so by a reverse application of the Doob Decomposition 
we just need to show $\cexpectationlong{\mathcal{F}_\sigma}{A_\tau}
\geq A_{\sigma \wedge \tau}$ a.s.

To see last fact first note that the monotonicity of $A_n$ and the
fact that $\sigma \wedge \tau \leq \tau$ shows us that $A_{\sigma
  \wedge \tau} \leq A_\tau$ a.s.  
Also we know that $\mathcal{F}_{\sigma \wedge
  \tau} \subset \mathcal{F}_\sigma$ and therefore the
$\mathcal{F}_{\sigma \wedge \tau}$-measurability of $A_{\sigma \wedge
  \tau}$ implies $\mathcal{F}_\sigma$-measurability.  Therefore
applying these observations and monotonicity of conditional
expectation we get
\begin{align*}
\cexpectationlong{\mathcal{F}_\sigma}{A_\tau} - A_{\sigma \wedge \tau}
&= \cexpectationlong{\mathcal{F}_\sigma}{A_\tau - A_{\sigma \wedge
    \tau}} \geq 0 \text{ a.s.}
\end{align*}
and we are done.
\end{proof}

There are other decomposition results that are of use.  While the Doob
Decomposition shows that a submartingale is bounded below by a
martingale the following shows that an $L^1$-bounded submartingale is
bounded above by a martingale as well.
\begin{lem}[Krickeberg
  Decomposition]\label{KrickebergDecompositionDiscrete}Let $X_n$ be an
  $L^1$-bounded submartingale then there exists an $L^1$-bounded martingale $M_n$ and
  a nonnegative $L^1$-bounded supermartingale $A_n$ such that $X_n = M_n - A_n$.
\end{lem}
\begin{proof}Fix an $m \geq 0$ and for every $n\geq m$ define $M_{n,m}
  = \cexpectationlong{\mathcal{F}_m}{X_n}$.  Note that by the
  submartingale property
\begin{align*}
M_{n,m} &= \cexpectationlong{\mathcal{F}_m}{X_n}  \leq
\cexpectationlong{\mathcal{F}_m}{\cexpectationlong{\mathcal{F}_n}
  {X_{n+1}}} = \cexpectationlong{\mathcal{F}_m}
  {X_{n+1}} = M_{n+1,m} \text{ a.s.}
\end{align*}
and therefore we can define $M_m = \lim_{n \to \infty} M_{n,m}$.
Furthermore we know that 
\begin{align*}
\expectation{\abs{M_{n,m}}} &\leq
\expectation{\cexpectationlong{\mathcal{F}_m}{\abs{X_n}}} 
=\expectation{\abs{X_n}} \leq \sup_n \expectation{\abs{X_n}} < \infty
\end{align*}
and therefore we can apply the Monotone Convergence Theorem to
conclude $\expectation{\abs{M_m}} < \infty$ so $M_m$ are integrable.
Clearly by definition of conditional expectation, each $M_{n,m}$ is $\mathcal{F}_m$-measurable and
therefore by Lemma \ref{LimitsOfMeasurable} we know that $M_m$ is
$\mathcal{F}_m$-measurable showing $M_m$ is $\mathcal{F}$-adapted.
Lastly applying the monotone convergence property of conditional
expectation and the tower rule 
for conditional expectation we get
\begin{align*}
\cexpectationlong{\mathcal{F}_{m}}{M_{m+1}} &= \lim_{n \to \infty}
\cexpectationlong{\mathcal{F}_{m}}{\cexpectationlong{\mathcal{F}_{m+1}}{X_n}}
= \lim_{n \to \infty}\cexpectationlong{\mathcal{F}_{m}}{X_n} = M_m
\end{align*}
which shows that $M_m$ is indeed a non-negative martingale.
$L^1$-boundedness of $M_m$ follows from the argument that showed $M_m$
was integrable.

Now define $A_n = M_n - X_n$ and note that 
\begin{align*}
A_n = \lim_{m \to \infty} \cexpectationlong{\mathcal{F}_n}{X_m} - X_n \geq
0 \text{ a.s.}
\end{align*}
by the submartingale property of $X_n$.  To see that $A_n$ is an $L^1$-bounded
supermartingale, note that integrability and $L^1$-boundedness of $A_n$ follow by the
triangle inequality and the corresponding properties of $X_n$ and $M_n$, $\mathcal{F}$-adaptedness follows from the
$\mathcal{F}$-adaptedness of $X_n$ and $M_n$ and the supermartingale
property follows using the submartingale and martingale properties of
$X_n$ and $M_n$ respectively
\begin{align*}
\cexpectationlong{\mathcal{F}_n}{A_{n+1}} &=
\cexpectationlong{\mathcal{F}_n}{M_{n+1}} -
\cexpectationlong{\mathcal{F}_n}{X_{n+1}} \leq M_n - X_n = A_n
\end{align*}
\end{proof}

\begin{lem}Let $M_n$ be an $L^1$-bounded martingale then there exist
  non-negative martingales $Y_n^+$ and $Y_n^-$ such that $M_n = Y_n^+
  - Y_n^-$ a.s. and $\norm{Y_n^\pm}_1 \leq \norm{M_n}_1$.
\end{lem}
\begin{proof}
This is a corollary of the proof of Lemma
\ref{KrickebergDecompositionDiscrete}.  If we apply that construction
to each of the submartingales $M_n^\pm$ we get that $Y_n^\pm = \lim_{m
  \to \infty} \cexpectationlong{\mathcal{F}_m}{M_m^\pm}$ defines a
pair of nonnegative martingales.  By linearity of conditional
expectation and the martingale property of $M_n$ we see that
\begin{align*}
Y_n^+ - Y_n^- = \lim_{m \to \infty}
\cexpectationlong{\mathcal{F}_n}{M_m^+ - M_m^-} = M_n^+ - M_n^- = M_n
\text{ a.s.}
\end{align*}
\end{proof}

\subsubsection{Martingale Inequalities}
Intuitively one thinks of martingales as being essentially constant
and submartingales as essentially increasing.  These intuitions can be
helpful when thinking of the types of properties that martingales
should have.  Probably the most important such property is that
boundedness of  a martingale or submartingale implies convergence (analogous
to the fact that a bounded increasing sequence in $\reals$ must converge).  

There are several fundamental inequalities that describe
these intuitions in a precise way.  The first result we prove is a maximal
inequality that can be viewed as an analogue of Kolmogorov's Maximal
Inequality (Lemma \ref{KolmogorovMaximalInequality}) for a special
class of dependent random variables.
\begin{lem}[Doob's Maximal
  Inequality]\label{DoobMaximalInequalityDiscrete}Let $X_t$ be a
  submartingale on a countable index set $T$, then for every $\lambda
  > 0$ and $t \in T$,
\begin{align*}
\lambda \probability{\sup_{s \leq t} X_s \geq \lambda} &\leq
\expectation{X_t ; \sup_{s \leq t} X_s  \geq \lambda} \leq \expectation{X_t^+}
\end{align*}
where $X_t^+ = X_t \vee 0$.
\end{lem}
\begin{proof}
First we assume that $T$ is a finite set.  By reindexing we may as
well assume that $T = \lbrace n \mid n \leq N \text{ and } n \in
\integers_+ \rbrace$ for some $N \geq 0$.  Now pick an $n \in T$.
The first thing to note is that for any submartingale $X_n$, $n\geq m$ and $A_m
\in \mathcal{F}_m$, $\expectation{X_n ; A_m} =
\expectation{\cexpectationlong{\mathcal{F}_m}{X_n} ; A_m}
\geq \expectation{X_m ; A_m}$.

Now the event $\lbrace \sup_{0 \leq k \leq n} X_k \geq \lambda
\rbrace$ can be nicely reexpressed in terms of optional times.  Define
\begin{align*}
\tau &= \min \lbrace  n \mid X_n \geq \lambda \rbrace
\end{align*}
where we assume the minimum of the empty set is positive infinity. 
Note that $\lbrace \sup_{0 \leq k \leq n} X_k \geq \lambda
\rbrace = \lbrace \tau \leq n \rbrace$.  If we consider the stopped
process $X_\tau \characteristic{\tau \leq n} = \sum_{m=0}^n X_m
\characteristic{\tau = m}$, take expectations and use the initial
observation,
$\expectation{X_\tau \characteristic{\tau \leq n}} \leq \sum_{m=0}^n
\expectation{X_n \characteristic{\tau = m}} = \expectation {X_n
  \characteristic{\tau \leq n}}$.  But on the other hand, by defintion
of $\tau$, we know that $\expectation{X_\tau \characteristic{\tau \leq
    n}} \geq \lambda \expectation{\characteristic{\tau \leq n}} =
\lambda \probability{\sup_{0 \leq k \leq n} X_k \geq \lambda}$ which
shows the first inequality.

The second inequality is true because nonnegativity of $X_n^+$ implies 
\begin{align*}
0 \leq X_n
\characteristic{\sup_{0 \leq k \leq n} X_k \geq \lambda} \leq X_n^+
\end{align*} so we can apply
monotonicity of expectation.

Now we want to extend the result to martingales on arbitrary countable
index sets $T$.  The proof above shows that the result holds for
finite subsets of $T$.  Now note that for any finite subsets $T^\prime
\subset T^{\prime\prime}$ such that $t \in T^\prime$ we have 
\begin{align*}
\lbrace \sup_{\substack{s \leq t\\
s \in T^\prime}} X_s \geq \lambda  \rbrace &\subset
\lbrace \sup_{\substack{s \leq t\\
s \in T^{\prime\prime}}} X_s \geq \lambda  \rbrace
\end{align*}
so if we write $T$ as an increasing union of finite sets $T_0 \subset
T_1 \subset \cdots$ then by continuity of measure (Lemma
\ref{ContinuityOfMeasure}) we have 
\begin{align*}
\probability{\sup_{\substack{s \leq t \\ s \in T}} X_s \geq \lambda}
&= 
\lim_{m \to \infty} \probability{\sup_{\substack{s \leq t \\
s \in T_m}}  X_s \geq \lambda} 
\end{align*}
and by the integrability of $X_t$ and the bound $\abs{X_t
  \characteristic{\sup_{\substack{s \leq t \\ s \in T}} X_s  \geq
    \lambda}} \leq \abs{X_t}$ we can apply Dominated Convergence to conclude
\begin{align*}
\expectation{X_t ; \sup_{\substack{s \leq t \\ s \in T}} X_s  \geq \lambda} &= 
\lim_{m \to \infty} \expectation{X_t ; \sup_{\substack{s \leq t \\ s
      \in T_m}} X_s  \geq \lambda}
\end{align*}
proving the result for countable $T$.
\end{proof}

A lesser known inequality is
\begin{lem}[Doob's Minimal
  Inequality]\label{DoobMinimalInequalityDiscrete}Let $X_t$ be a
  submartingale on a countable index set $T$, then for every every
  interval $[s,t] \subset T$ and $\lambda > 0$, 
\begin{align*}
\lambda \probability{\inf_{s \leq q \leq t} X_q \leq -\lambda} &\leq
\expectation{X_t^+} - \expectation{X_s}
\end{align*}
where $X_t^+ = X_t \vee 0$.
\end{lem}
\begin{proof}
We start by assuming that $T$ is finite and as in the proof of the
Maximal inequality we assume that $T = \lbrace 0, \dots, n \rbrace$.
Let $\tau = \min \lbrace k \mid X_k \leq -\lambda \rbrace$ be the
hitting time for the interval $(-\infty, -\lambda]$ and note that it
is an optional time.  Furthermore we have by this definition $\lbrace \min_{0
  \leq k \leq n} X_k \leq -\lambda \rbrace=\lbrace \tau \leq n\rbrace$.  By
Optional Sampling Theorem \ref{OptionalSamplingDiscrete} we know that 
\begin{align*}
\expectation{X_0} &\leq \expectation{X_{\tau \wedge n}}
\end{align*}
We write $X_{\tau \wedge n} = X_\tau \characteristic{\tau \leq n}
+ X_n \characteristic{\tau > n}$ and note that $ X_n \characteristic{\tau
  > n} \leq  X_n^+ \characteristic{\tau > n} \leq X_n^+$.  Putting
these facts together,
\begin{align*}
\expectation{X_0} &\leq \expectation{X_{\tau \wedge n}} \\
&=\expectation{X_{\tau} ; \tau \leq n} + \expectation{X_n ; \tau > n}
\\
&\leq -\lambda \probability{\tau \leq n} + \expectation{X_n^+} 
\end{align*}
and the result is proven.

TODO: Extend from finite to countable as in the proof of the Maximal
Inequality Lemma \ref{DoobMaximalInequalityDiscrete}.
\end{proof}
 
Having proven a tail inequality it is often a good idea to see what it
might say about expectations via Lemma \ref{TailsAndExpectations}.  In
this case, with a bit of care we get the following result of Doob that
can be interpreted as giving a bound on the extent to which a
non-negative submartingale can deviate from being increasing.
\begin{lem}[Doob's $L^p$
  Inequality]\label{DoobLpInequalityDiscrete}Let $X_t$ be a
  non-negative submartingle on a countable index set $T$, then for all
  $p > 1$ and $t \in T$,
\begin{align*}
\norm{\sup_{s \leq t} X_s}_p &\leq \frac{p}{p-1}\norm{X_t}_p
\end{align*}
\end{lem}
\begin{proof}
As with the proof of the maximal inequality we begin by assuming that
$T$ is finite and by reindexing equal to $\lbrace n \in \integers_+
\mid n \leq N\rbrace$ for some $N \geq 0$.
We begin let us start by assuming that $\expectation{(\sup_{0 \leq k
    \leq n} X_k)^p} < \infty$.  With this assumption in place we can
apply Lemma \ref{TailsAndExpectations}, the Maximal Inequality Lemma
\ref{DoobMaximalInequalityDiscrete} and Tonelli's Theorem \ref{Fubini}
to get
\begin{align*}
\expectation{(\sup_{0 \leq k \leq n} X_k)^p} &= p \int_0^\infty
\lambda^{p-1} \probability{\sup_{0 \leq k \leq n} X_k \geq \lambda} \,
d\lambda \\
&\leq p \int_0^\infty
\lambda^{p-2} \expectation{X_n ; \sup_{0 \leq k \leq n} X_k \geq \lambda} \,
d\lambda \\
&=p \expectation{X_n \int_0^\infty
\lambda^{p-2} \characteristic{\sup_{0 \leq k \leq n} X_k \geq \lambda} \,
d\lambda} \\
&=p \expectation{X_n \int_0^{\sup_{0 \leq k \leq n} X_k}
\lambda^{p-2} \, d\lambda} \\
&=\frac{p}{p-1} \expectation{X_n (\sup_{0 \leq k \leq n} X_k)^{p-1} } \\
&\leq \frac{p}{p-1} \norm{X_n}_p \expectation{(\sup_{0 \leq k \leq n}
  X_k)^{p}}^{\frac{p-1}{p}} & & \text{by H\"{o}lder's Inequality}
\end{align*}
But now, we can divide both sides by $\expectation{(\sup_{0 \leq k \leq n}
  X_k)^{p}}^{\frac{p-1}{p}}$ to get the result.

It remains to remove the assumption that $\expectation{(\sup_{0 \leq k \leq n}
  X_k)^{p}} < \infty$.  Obviously if $\norm{X_n}_p = \infty$ then the
result is trivially true so we may assume that $\norm{X_n}_p < \infty$.
Now we have for all $k\leq n$, by the
submartingale property, Jensen's Inequality (Theorem \ref{JensenConditionalExpectation}) and the tower rule for conditional expectation
\begin{align*}
\expectation{X_k^p} \leq
\expectation{\cexpectationlong{\mathcal{F}_k}{X_n}^p} \leq 
\expectation{\cexpectationlong{\mathcal{F}_k}{X_n^p}} =
\expectation{X_n^p} < \infty
\end{align*}
which shows that $\norm{X_k}_p < \infty$ for all $0\leq k \leq n$.
But this implies that $\norm{\sup_{0 \leq k \leq n} X_k}_p < \infty$
(e.g. for any $\xi, \eta \in L^p$, write $\xi \vee \eta =
\xi\characteristic{\xi > \eta} + \eta\characteristic{\xi \leq \eta}$
and induct) and so the previous calculation proves the lemma for
finite index sets.

Now to extend the result to arbitrary countable index sets $T$, simply
observe if $t \in T^\prime \subset T^{\prime \prime}$ then 
\begin{align*}
\sup_{\substack{s \leq t \\ s \in T^\prime }} X_s &\leq
\sup_{\substack{s \leq t \\ s \in T^{\prime \prime}}} X_s
\end{align*}
so we may take finite sets $T_0 \subset T_1 \subset \cdots$ such that
$t \in T_0$ and $\cup_n T_n = T$ and use Monotone Convergence to
conclude 
\begin{align*}
\expectation{\sup_{\substack{s \leq t \\ s \in T }} X_s} &= 
\lim_{n \to    \infty} \expectation{\sup_{\substack{s \leq t \\ s \in T_n }} X_s} \leq 
\frac{p}{p-1}\norm{X_t}_p
\end{align*}
\end{proof}

It is worthwhile emphasizing that the results above cover the case in
which $\infty \in T$.  

Conceptually there are two ways that a real valued sequence can fail
to converge: either the sequence escapes to infinity or the sequence
oscillates.  Our next goal is a result that puts explicit bounds on
the expected amount of oscillation in any submartingle. More
specifically, 
assume that we have fixed two reals numbers $a < b$; then
we can focus in on the oscillations between the values $a$ and $b$.
Alternatively one can measure the number of times the value of the
submartingale pass from below the lower bound $a$ to above the upper
bound $b$; each such transition is referred to as an
\emph{upcrossing}.  To describe upcrossings precisely we first define
the times at which pass below $a$ and then the time we pass above $b$.
\begin{lem}\label{UpcrossingMeasurabilityDiscrete}Let $\mathcal{F}_n$ be a filtration, $M_n$ be a $\mathcal{F}$-
  adapted process on $\integers_+$ and let $a<b$ be real numbers.  Let $\tau_0 =0$ and for each $j \geq 0$ define
  inductively
\begin{align*}
\sigma_j &= \inf \lbrace n \mid t \geq \tau_j \text{ and } M_n \leq a \rbrace \\
\tau_{j+1} &= \inf \lbrace n \mid t \geq \sigma_j \text{ and } M_n \geq b \rbrace \\
\end{align*}
then each $\tau_j$ and $\sigma_j$ is an $\mathcal{F}$-optional time
(note that we treat the infimum of the empty set to be infinity).
Furthermore if we define 
\begin{align*}
&U_a^b(n) = \sup \lbrace m \mid \tau_m \leq n \rbrace \\
&= \sup \lbrace m \mid \exists  j_1 < k_1 < \dotsb < j_m <
k_m \leq n \text{ such that } X_{j_i} \leq a \text{ and } X_{k_i} \geq b
\text{ for all } i=1, \dotsc, m \rbrace
\end{align*}
to be the number of upcrossings of $X_m$ before $n$, then each $U_a^b(n)$
is $\mathcal{F}_n$-measurable.
\end{lem}
\begin{proof}
To see that $\tau_j$ and $\sigma_j$ is an induction.  Assume that
$\tau_j$ is $\mathcal{F}$-optional for $j \leq n$.  We write
\begin{align*}
\lbrace \sigma_n = m \rbrace &= \bigcup_{k<m}
\left ( \lbrace \tau_n = k \rbrace
\cap \bigcap_{k < l < m} \lbrace X_l > a\rbrace\right) \cap \lbrace X_m \leq a\rbrace
\end{align*}
and by $\mathcal{F}$-adaptedness of $X_n$ and the fact
that $\tau_n$ is $\mathcal{F}$-optional we see that $\lbrace
\sigma_n = m \rbrace \in \mathcal{F}_m$.  In a similar way we can
express
\begin{align*}
\lbrace \tau_{n+1} = m \rbrace &= \bigcup_{k < m} \left ( \lbrace \sigma_n = k \rbrace
\cap \bigcap_{k < l < m} \lbrace X_l < b \rbrace\right) \cap \lbrace
X_m \geq b \rbrace
\end{align*}
and by $\mathcal{F}$-adaptedness of $X_n$ and the just
proven fact
that $\sigma_n$ is $\mathcal{F}$-optional we see that $\lbrace
\tau_{n+1} = m \rbrace \in \mathcal{F}_m$.

To see the $\mathcal{F}_n$-measurability of $U_a^b(n)$ we just express for $n \in \integers_+$
\begin{align*}
\lbrace U_a^b(n) = m \rbrace &= \lbrace \tau_m \leq n \rbrace \cap
\bigcap_{k >m} \lbrace \tau_k > n\rbrace
\end{align*}
and 
\begin{align*}
\lbrace U_a^b(n) = \infty \rbrace &= \cap_{m=1}^\infty \lbrace \tau_m \leq n\rbrace 
\end{align*}
both of which are $\mathcal{F}_n$-measurable because we have just shown each $\tau_m$ is an
optional time.

To see the last equality let the $\tilde{U}_a^b(n)$ be the right hand
side of the equality to be shown.  First note that if $\tau_m \leq n$ then
taking $j_i = \sigma_{i-1}$ and $k_i = \tau_i$ we get an upcrossing sequence $j_1 <
k_1 < \dotsc < j_m < k_m \leq n$; therefore $U_a^b(n) \leq
\tilde{U}_a^b(n)$.  
On the other hand, given such an upcrossing sequence we claim that
this implies $\sigma_i \leq j_i$ and $\tau_i \leq k_i$ for $i=1,\dotsc,m$ so in
particular, $\tau_m \leq n$.  This follows from an induction argument
that has two cases.  First if
$\tau_{i-1} \leq k_i$ and $X_{j_i} \leq a$ then we clearly see
$\sigma{i} \leq j_i$.  On the other hand if $\sigma_i \leq j_i$ then
we clearly see that $\tau_i \leq k_i$.  From $\tau_m \leq n$ it follows that
$\tilde{U}_a^b(n) \leq U_a^b(n)$ so the desired equality is proven.
\end{proof}

The second definition of $U_a^b(n)$ provided in the previous result
generalizes nicely to arbitrary time indexes; in particular for
countable time indexes we get a workable definition and measurability.
\begin{cor}\label{UpcrossingMeasurabilityCountable}Let $\mathcal{F}_t$ be a filtration, $M_t$ be a $\mathcal{F}$-
  adapted process with a countable time index $T$ and let $a<b$ be real numbers.  
If we define 
\begin{align*}
&U_a^b(t) \\
&= \sup \lbrace m \mid \exists  j_1 < k_1 < \dotsb < j_m <
k_m \leq t \text{ such that } X_{j_i} \leq a \text{ and } X_{k_i} \geq b
\text{ for all } i=1, \dotsc, m \rbrace
\end{align*}
to be the number of upcrossings of $X_s$ before $t$, then each $U_a^b(t)$
is $\mathcal{F}_t$-measurable.
\end{cor}
\begin{proof}
The previous result shows the $\mathcal{F}_t$-measurability for finite time indexes
$T$ that contain $t$.  Now write $T = \cup_{n=0}^\infty T_n$ where $t
\in T_0 \subset T_1 \subset \dotsb$ is a nested sequence of finite
sets.  It is easy to see that $U(t, a,b, T) = \lim_{n \to \infty} U(t,
a,b, T_n)$ and therefore the result follows from Lemma
\ref{UpcrossingMeasurabilityDiscrete} and Lemma \ref{LimitsOfMeasurable}.
\end{proof}

\begin{lem}[Doob's Upcrossing
  Inequality]\label{UpcrossingInequalityDiscrete}Let $X_t$ be a
  submartingale with a countable time index $T$ and let $U_a^b(t)$ be the number of upcrossings up to
  time $t \in T$.  Then
\begin{align*}
\expectation{U_a^b(t)} &\leq \frac{\expectation{(X_t-a)_+}}{b-a}
\end{align*}
\end{lem}
\begin{proof}
As the first reduction note that we may assume that $T$ is in fact
finite.  To see why let us temporarily change notation to make the dependence of $U_a^b(t)$ on the
time index $T$ explicit by writing $U(t, a, b, T)$.  As noted in the
proof of Corollary \ref{UpcrossingMeasurabilityCountable} if we consider a nested set of finite time indexes $t
\in T_0 \subset T_1 \subset \dotsb$ such that $T = \cup_{n=0}^\infty
T_n$ then in fact $\lim_{n \to \infty} U(t, a, b, T_n) = U(t,
a,b,T)$.  Now by Monotone Convergence we get $\lim_{n \to \infty}
\expectation{U(t, a, b, T_n)} = \expectation{ U(t,a,b,T)}$ and the
result for $T$ will follow from the result for finite time indexes.

The second step of the proof is a reduction to a notationally simpler
case.  As the function $f(x) = (x -a)_+$ is convex and nondecreasing we know
that $(X_t - a)_+$ is a positive submartingale.  Furthermore $X_t \geq b$ if
and only if $(X_t - a)_+ \geq b-a$ and $X_t \geq a$ if and only if
$(X_t -a)_+ = 0$ and therefore the number of upcrossings of $X_t$
between $a$ and $b$ is the same as the number of upcrossings of $(X_t
- a)_+$ between $0$ and $b-a$.  Therefore the result is proven if we
show that for every positive submartingale $X_t$ and $b>0$ we have 
\begin{align*}
U_0^b (t) &\leq \frac{\expectation{X_t}}{b}
\end{align*}

To finish the proof, let $n$ be the cardinality of $T$ so that we know
$\sigma_n = \tau_n = \infty$ and we can write the finite telescoping sum
\begin{align*}
X_t &= X_{\tau_0 \wedge t} + \sum_{j=0}^n \left ( X_{\sigma_j \wedge t} - X_{\tau_j
    \wedge t} \right) + \sum_{j=0}^n \left ( X_{\tau_{j+1} \wedge t} - X_{\sigma_j
    \wedge t} \right)
\end{align*}
Taking expectations we note that from the positivity of $X_t$ we have
$\expectation{X_{\tau_0 \wedge t}}\geq 0$ and because $\sigma_j \geq
\tau_j$ and the optional sampling theorem for submartingles
(Corollary \ref{OptionalSamplingSubmartingaleDiscrete}) we have
\begin{align*}
\expectation{X_{\sigma_j \wedge t} - X_{\tau_j    \wedge t} } &=
\expectation{\cexpectationlong{\mathcal{F}_{\tau_j \wedge t}}{X_{\sigma_j \wedge t}
    - X_{\tau_j    \wedge t}}} 
\geq \expectation{X_{\tau_j \wedge t}
    - X_{\tau_j    \wedge t}} = 0\\
\end{align*}
and we also have
\begin{align*}
X_{\tau_{j+1} \wedge t} - X_{\sigma_j \wedge t} &\geq b && \text{ if  $\tau_{j+1} \leq n$} \\
X_{\tau_{j+1} \wedge t} - X_{\sigma_j \wedge t} &=X_n \geq 0 && \text{ if  $\sigma_j \leq n < \tau_{j+1}$} \\
X_{\tau_{j+1} \wedge t} - X_{\sigma_j \wedge t} &= 0 && \text{ if $n < \sigma_j$} \\
\end{align*}
so by considering only the terms in sum for which $\tau_{j+1} \leq n$
we get $\sum_{j=0}^n \left( X_{\tau_{j+1} \wedge t} - X_{\sigma_j
    \wedge t} \right ) \geq b U_0^b(n)$.   
Putting this all together
\begin{align*}
\expectation{X_n} &= \expectation{X_{\tau_0 \wedge t}} + 
\sum_{j=0}^n \expectation{ X_{\sigma_j \wedge t} - X_{\tau_j \wedge t}} + 
\sum_{j=0}^n \expectation{X_{\tau_{j+1} \wedge t} - X_{\sigma_j \wedge t}} \\
&\geq \sum_{j=0}^n \expectation{X_{\tau_{j+1} \wedge t} - X_{\sigma_j \wedge t}} \\
&\geq b \expectation{U_0^b(n)}
\end{align*}
and therefore the result is proved.
\end{proof}

TODO: Add comments about the result that $\expectation{X_{\sigma_j
    \wedge n} - X_{\tau_j    \wedge n} }\geq 0$.  Given the definition
of $\sigma_j$ and $\tau_j$ this result might seem a bit
counterintuitive since one is expecting $X_{\sigma_j} \leq a < b \leq X_{\tau_j}$.  The explanation for how this result can hold is
that in fact is very unlikely that $\sigma_j < n$; with high
probability $\sigma_j \geq n$ and moreover $X_{\sigma_j    \wedge n} = X_n \geq
X_{\tau_j \wedge n}$ and not $X_{\sigma_j    \wedge n} = X_{\sigma_j}
\leq a$.  This explanation is completely consistent with
the conceptual model that submartingales are not oscillating much and
is really one of the two main points of the result (the other main point
being the fact that a lower bound for the terms
$\expectation{X_{\tau_{j+1} \wedge n} - X_{\sigma_j   \wedge n}}$ is given by $(b-a) U_a^b(n)$).

The Upcrossing Lemma leads immediately to a proof that $L^1$-bounded
submartingales converge almost surely.  This result is usually stated
for discrete submartingales $X_n$ but with a little attention to details
we get a stronger result that applies over countable time indexes
(e.g. $\rationals_+$) and paves the way for consideration of
continuous time indexes such as $\reals_+$.
\begin{thm}[$L^1$ Submartingale Convergence
  Theorem]\label{MartingaleConvergenceBoundedL1Discrete}Let $X_t$ be a
  $\mathcal{F}$-submartingale with a countable time index $T$ such that $\sup_{t \in
    T} \norm{X_t}_1 <
  \infty$ then there exists an $A \in \mathcal{F}_\infty$ with
  $\probability{A} = 1$ such that for every increasing or
  decreasing sequence $t_n$ in $T$ there exists an integrable random variable $X$ such
  that $X_{t_n} \to X$ on $A$.
\end{thm}
\begin{proof}
The first order of business here is leverage the Doob Upcrossing
Inequality to show that $X_t$ is not oscillatory almost surely and therefore has a limit (possibly infinite) almost
surely.  To do that for every $a \in \reals$, we note the elementary inequality $(x - a)_+ \leq
\abs{x} + \abs{a}$ and therefore we can that $\expectation{(X_t -
  a)_+} \leq \sup_{t \in T} \norm{X_t}_1 + \abs{a}< \infty$.
Supposing $a,b \in \reals$ with $a<b$ and $U_a^b(t)$ be the number of upcrossings of $[a,b]$ before $t$,
we can see that $U_a^b(t)$ is positive and increasing in $t$ and Lemma
\ref{UpcrossingInequalityDiscrete} and Monotone Convergence tell us
that if we pick any sequence $t_1, t_2, \dotsc$ such that $\lim_{n \to
  \infty} t_n = \sup T$ then
\begin{align*}
\expectation{\lim_{n \to \infty} U_a^b(t_n)} &= \lim_{n \to \infty}
\expectation{U_a^b(t_n)} \leq 
\lim_{n \to \infty} \frac{\norm{X_{t_n}}_1 + \abs{a}}{b-a} \leq 
\frac{\sup_{t \in T} \norm{X_t}_1 + \abs{a}}{b-a} < \infty
\end{align*}
If we let $U_a^b(\infty) = \lim_{n \to \infty} U_a^b(t_n) = \sup_{t
  \in T} U_a^b(t)$ be the number
of upcrossing on $T$, then $U_a^b(\infty)$ is
$\mathcal{F}_\infty$-measurable by Lemma \ref{LimitsOfMeasurable}, 
$U_a^b(\infty)$ is integrable and therefore almost
surely finite.

Let $A = \cap_{\substack{a < b\\ a,b \in \rationals}} \lbrace
U_a^b(\infty) < \infty \rbrace$ which is a countable intersection of
$\mathcal{F}_\infty$-measurable sets of probability one hence is a
$\mathcal{F}_\infty$-measurable set of probability one.  Let $t_n$ be any
increasing or decreasing sequence in $T$.  For each $a,b \in \rationals$ with $a<b$ define
\begin{align*}
\Lambda_a^b &=
\lbrace \liminf_{n \to \infty} X_{t_n} < a < b < \limsup_{n \to
  \infty} X_{t_n} \rbrace
\end{align*}
and note that $\Lambda_a^b \subset \lbrace U_a^b(\infty)
= \infty \rbrace$ (we can pick subsequences $N$ and $M$ such that
$X_{t_n}$ converges to $\liminf_{n \to \infty} X_{t_n}$ along $N$ and 
$\limsup_{n \to  \infty} X_{t_n}$ along $M$ and in this way construct an
infinite number of upcrossings of $[a,b]$; it is here that we require
that the sequence $t_n$ is increasing or decreasing).  Thus
\begin{align*}
\lbrace \liminf_{n \to \infty} X_{t_n} < \limsup_{n \to  \infty}
X_{t_n} \rbrace
&= \bigcup_{
\substack{
a,b \in \rationals \\
a<b
}} \Lambda_a^b \\
&\subset \bigcup_{
\substack{
a,b \in \rationals \\
a<b
}} \lbrace U_a^b(\infty) = \infty \rbrace  \\
&=A^c
\end{align*}
and therefore $\lim_{n \to \infty} X_{t_n}$ exists on the set $A$
(in particular almost surely since $\probability {A^c} = 0$).

Let $X = \lim_{n \to \infty} X_{t_n}$  on $A$ and for concreteness
define it to be $0$ on $A^c$.  Our last task is to show that
$X$ is integrable (hence almost surely finite as well).  This follows
from Fatou's Lemma
\begin{align*}
\expectation{\abs{X}} &\leq \liminf_{n \to \infty}
\expectation{\abs{X_{t_n} }} \leq \sup_{t \in T} \norm{X_t}_1 < \infty
\end{align*}
and we are done.
\end{proof} 
Note that despite the fact that the limit of the submartingale is
integrable in the above theorem, it is not necessarily the case that the
convergence is $L^1$.
TODO: Provide example of a non-uniformly integrable martingale with
almost sure but not $L^1$ convergence.

In the martingale case we can characterize the conditions under which
the convergence to a limit is in $L^1$.  Furthermore in this case, the
martingale is closed (see Lemma \ref{ClosedMartingales} for the
definition of closed martingales).
\begin{thm}[Martingale Closure Theorem]\label{L1MartingaleConvergenceTheoremDiscrete}Let $X_n$ be a martingale then the following are equivalent
\begin{itemize}
\item[(i)]$X_n$ is uniformly
  integrable
\item[(ii)]there exists an integrable $X$  such that
  $X_n \tolp{1} X$
\item[(iii)]there exists an integrable $X$ such that
  $X_n = \cexpectationlong{\mathcal{F}_n}{X}$ almost surely.
\end{itemize}
\end{thm}
\begin{proof}
To see (i) implies (ii) we know from Lemma
\ref{UniformIntegrabilityProperties} that $X_n$ uniformly integrable
implies $L^1$ boundedness, hence we can apply Theorem
\ref{MartingaleConvergenceBoundedL1Discrete}
to conclude the existence of an integrable $X$ such that $X_n \toas
X$.  However almost sure convergence implies convergence in
probability (Lemma \ref{ConvergenceAlmostSureImpliesInProbability})
which together with uniform integrability implies $X_n \tolp{1} X$
(Lemma \ref{LpConvergenceUniformIntegrability}).

To that (ii) implies (iii) suppose that $\epsilon > 0$ is given and
let $N>0$ be such that $\norm{X_n - X}_1 = \expectation{\abs{X_n - X}}
< \epsilon$ for 
all $n \geq N$.  Pick an $m \in \integers_+$, $n \geq N \vee m$ and let $A \in
\mathcal{F}_m$.  We calculate
\begin{align*}
\abs{\expectation{X ; A} - \expectation{X_m ; A} } &=
\abs{\expectation{X ; A} - \expectation{X_n ; A} } & & \text{since
  $\cexpectationlong{\mathcal{F}_m}{X_n} = X_m$}\\
&\leq \expectation{ \abs{X - X_n} ; A} \\
&\leq \expectation{ \abs{X -   X_n}} < \epsilon
\end{align*}
and since $\epsilon$ is arbitrary, we conclude $\expectation{X ; A} =
\expectation{X_m ; A}$ and therefore
$\cexpectationlong{\mathcal{F}_m}{X} = X_m$ a.s.

To see that (ii) implies (iii), we use Lemma
\ref{UniformIntegrabilityProperties}.  First note that by contraction
property of conditional expectation, we have $\sup_n
\expectation{\abs{\cexpectationlong{\mathcal{F}_n}{X}}} \leq
\expectation{\abs{X}}$ so the first condition of the lemma holds.  To
see the second condition, let $\epsilon > 0$ be fixed and pick $R > 0$
such that $\expectation{\abs{X} ; \abs{X} > R} < \frac{\epsilon}{2}$
and pick $A$
such that $\probability{A} < \frac{\epsilon}{2R}$.  Now, for every $n$,
\begin{align*}
\abs{\expectation{\cexpectationlong{\mathcal{F}_n}{X} ; A}} &\leq
\expectation{\cexpectationlong{\mathcal{F}_n}{\abs{X}} ; A} \\
&=\expectation{\abs{X} \cdot \cexpectationlong{\mathcal{F}_n}{
    \characteristic{A}}} \\
&=\expectation{\abs{X} \cdot \cexpectationlong{\mathcal{F}_n}{
    \characteristic{A}} ; \abs{X} \leq R} + 
\expectation{\abs{X} \cdot \cexpectationlong{\mathcal{F}_n}{
    \characteristic{A}} ; \abs{X} > R} \\
&\leq R \expectation{\cexpectationlong{\mathcal{F}_n}{ \characteristic{A}}} + 
\expectation{\abs{X} ; \abs{X} > R} \\
&\leq \epsilon
\end{align*}
and therefore we have condition (ii) of Lemma
\ref{UniformIntegrabilityProperties} satisfied and uniform
integrability is shown.
\end{proof}

It should be noted that the proof of (iii) implies (i) in previous
argument did not depend on the fact that we were dealing with a
filtration; in fact we have following corollary to the proof.
\begin{cor}\label{ConditionalExpectationsUniformlyIntegrable}Suppose $\xi$ is an integrable random variable the
  collection of random variables $\cexpectationlong{\mathcal{F}}{\xi}$
  for all $\sigma$-algebras $\mathcal{F}$ is uniformly integrable.
\end{cor}
\begin{proof}For any $\mathcal{F}$ just replay the argument that (iii) implies (i) in the
  previous result.

Just for grins here is the proof that Kallenberg gives that is very
similar up to a point to the proof in the previous result but instead of
using the uniform integrability of $\xi$ to make the elementary
argument invokes some standard workhorse theorems.  The resulting
argument seems to me to be more difficult to understand.  Maybe there
is a problem with my argument but I don't see it.  He says
just as we do that for any $\mathcal{F}$,
\begin{align*}
\abs{\expectation{\cexpectationlong{\mathcal{F}}{\xi} ; A}} &\leq
\expectation{\cexpectationlong{\mathcal{F}}{\abs{\xi}} ; A} =\expectation{\abs{\xi} \cdot \cexpectationlong{\mathcal{F}}{\characteristic{A}}} \\
\end{align*}
Now he observes that if the right hand side doesn't converge to zero
uniformly in $\mathcal{F}$ as $\probability{A} \to 0$ then there
exists an $\epsilon > 0$, $\sigma$-algebras $\mathcal{F}_n$ and $A_n$
with $\lim_{n \to \infty} \probability{A_n} = 0$ such that 
\begin{align*}
\expectation{\abs{\xi} \cdot
  \cexpectationlong{\mathcal{F}_n}{\characteristic{A_n}}} \geq
\epsilon \text{ for all $n$}
\end{align*}
so in particular no subsequence can converge to zero.  Now we derive a
contradiction. We know that
$\expectation{\cexpectationlong{\mathcal{F}_n}{\characteristic{A_n}}}
= \probability{A_n} \to 0$ and therefore
$\cexpectationlong{\mathcal{F}_n}{\characteristic{A_n}} \toprob 0$
(Lemma \ref{ConvergenceInMeanImpliesInProbability}) and
$\cexpectationlong{\mathcal{F}_n}{\characteristic{A_n}} \toas 0$ along
some subsequence $N$ (Lemma
\ref{ConvergenceInProbabilityAlmostSureSubsequence}).  Now since $\xi$
is integrable it is almost surely finite and therefore
$\abs{\xi} \cexpectationlong{\mathcal{F}_n}{\characteristic{A_n}} \toas 0$ along
the subsequence $N$ and by
Dominated Convergence we get $\expectation{\abs{\xi} \cdot
  \cexpectationlong{\mathcal{F}_n}{\characteristic{A_n}}} \to 0$
along $N$ which is a contradiction.
\end{proof}

Convergence of martingales in $L^p$ spaces with $p > 1$ is equivalent
to boundedness.  An even stronger condition holds, if a martingale
converges in $L^1$ to a $p$-integrable limit then the convergence can
be upgraded to $L^p$ convergence.
\begin{thm}[$L^p$ Martingale
  Convergence]\label{LpMartingaleConvergenceDiscrete}Given a
  martingale $M_n$, then for $p > 1$, there exists an $M \in L^p$ such that $M_n
  \tolp{p} M$ if and only if $M_n$ is $L^p$ bounded.  In fact, if $M_n
  \tolp{1} M$ with $M \in L^p$ then $M_n \tolp{p} M$ and $M_n$
  is $L^p$ bounded.
\end{thm}
\begin{proof}
Suppose $M_n$ is an $L^p$ bounded martingles.  By $L^p$ boundedness,
we know that $M_n$ is uniformly integrable thus by Theorem
\ref{L1MartingaleConvergenceTheoremDiscrete}
we know there is an integrable $M$ such that $M_n \toas M$ (thus $\abs{M_n}^p \toas \abs{M}^p$) and $M_n
\tolp{1} M$.  By Doob's $L^p$ inequality, for every $n$ we have
\begin{align*}
\norm{\sup_{0 \leq k \leq n} \abs{M_k}}_p &\leq \frac{p}{p-1} \norm{M_n}_p <
\frac{p}{p-1} \sup_n \norm{M_n}_p < \infty
\end{align*}
therefore by Monotone
Convergence we have $\norm{\sup_{0 \leq k \leq \infty} \abs{M_k}}_p = \lim_{n
  \to \infty}\norm{\sup_{0 \leq k \leq n} \abs{M_k}}_p < \infty$.  Now we
clearly have $\abs{M_n}^p \leq (\sup_{0 \leq k \leq \infty} \abs{M_k})^p$ and
Dominated Convergence gives us $M_n \tolp{p} M$.

Now assume that $M_n \tolp{1} M$ with $M \in L^p$ and $p > 1$. Theorem \ref{L1MartingaleConvergenceTheoremDiscrete}
implies that $M_n = \cexpectationlong{\mathcal{F}_n}{M}$ a.s. for
every $n$.  Now convexity of $x^p$ for $p > 1$ and  Jensen's
Inequality (Theorem \ref{JensenConditionalExpectation}) imply
\begin{align*}
\expectation{\abs{M_n}^p} &=
\expectation{\abs{\cexpectationlong{\mathcal{F}_n}{M}}^p} \leq 
\expectation{\cexpectationlong{\mathcal{F}_n}{\abs{M}^p}} =
\norm{M}_p^p < \infty
\end{align*}
which shows that not only is $M_n$ $p$-integrable but that the
martingale $M_n$ is
$L^p$-bounded.  The first part of the Theorem shows that $M_n \tolp{p} M$.
\end{proof}

Martingale convergence also allows us to extend the optional sampling
theorem to unbounded optional times.
\begin{lem}Let $M_n$ be a uniformly integrable martingale and let
  $\sigma$ and $\tau$ be optional times, then $M_\tau$ is integrable
  and $\cexpectationlong{\mathcal{F}_\sigma}{M_\tau} = M_{\sigma
    \wedge \tau}$.
\end{lem}
\begin{proof}
To see integrability of $M_\tau$ we use the Martingale Convergence
Theorem \ref{L1MartingaleConvergenceTheoremDiscrete} to conclude that
there exists integrable $M_\infty$ such that $M_n =
\cexpectationlong{\mathcal{F}_n}{M_\infty}$.  By Lemma
\ref{ConditionalExpectationIsLocal} and Lemma
\ref{LocalizationOfStoppedFiltration} for every $n$ we can compute
\begin{align*}
M_\tau &= M_n = \cexpectationlong{\mathcal{F}_n}{M_\infty} =
\cexpectationlong{\mathcal{F}_\tau}{M_\infty} \text{ on
  $\lbrace \tau = n \rbrace$}
\end{align*}
and therefore $M_\tau = \cexpectationlong{\mathcal{F}_\tau}{M_\infty}
$ proving integrability.  Note that this was proven for arbitrary
optional times so in particular $M_{\tau \wedge \sigma}$ is integrable
as well.

To show the optional sampling equality we first observe by the result
in the bounded case that for every $n$, $\cexpectationlong{\mathcal{F}_\sigma}{M_{\tau
    \wedge n}} = M_{\sigma
    \wedge \tau \wedge n}$ and we just need to justify taking limits
  in the equality.  Pick $A \in
  \mathcal{F}_\sigma$.  We know that $M_n \toas M_\infty$ as well and therefore 
we have $M_{\tau
  \wedge n} \characteristic{A} \toas M_\tau \characteristic{A}$ and $M_{\tau
  \wedge \sigma \wedge n} \characteristic{A} \toas M_{\tau \wedge \sigma} \characteristic{A}$.  To
show 
\begin{align*}
\expectation{M_\tau \characteristic{A}} 
&= \lim_{n\to \infty} \expectation{M_{\tau
  \wedge n} \characteristic{A}} = \lim_{n\to \infty} \expectation{M_{\tau
  \wedge \sigma \wedge n} \characteristic{A}} 
= \expectation{M_{\tau \wedge \sigma} \characteristic{A}}
\end{align*} 
it will suffice to show that $M_{\tau \wedge n}$
is uniformly integrable for an arbitrary optional time $\tau$.  Suppose $\epsilon > 0$ is given.  By the
integrability of $M_\tau$ we can find $R_1 > 0$ such that
$\expectation{\abs{M_\tau}; \abs{M_\tau} > R_1} < \epsilon/2$ and by
uniform integrability of $M_n$ we can find $R_2 > 0$ such that $\sup_n
\expectation{\abs{M_n} ; \abs{M_n} > R_2} < \epsilon/2$.  Now let $R =
R_1 \vee R_2$ and compute
\begin{align*}
\sup_n \expectation{\abs{M_{\tau \wedge n}} ; \abs{M_{\tau \wedge n}}
  > R} &= \sup_n \expectation{\abs{M_{\tau \wedge n}} ; \abs{M_{\tau \wedge n}}
  > R \text{ and } \tau \leq n} + \\
&\sup_n \expectation{\abs{M_{\tau \wedge n}} ; \abs{M_{\tau \wedge n}}
  > R \text{ and } \tau > n} \\
&\leq \expectation{\abs{M_{\tau}} ; \abs{M_{\tau}}
  > R } + \sup_n \expectation{\abs{M_{n}} ; \abs{M_{n}}
  > R } \\
&< \epsilon
\end{align*}
\end{proof}

\begin{cor}Let $M_n$ be a uniformly integrable martingale, then the
  set of random variables $\lbrace M_\tau \mid \tau \text{ is an optional time}\rbrace$ is
  uniformly integrable.
\end{cor}
\begin{proof}
By uniform integrability there is $M_\infty$ such that $M_n \to
M_\infty$ a.s. and in $L^1$.  By the previous result we have $M_\tau =
\cexpectationlong{\mathcal{F}_\tau}{M_\infty}$ and therefore the
result follows from Corollary \ref{ConditionalExpectationsUniformlyIntegrable}.
\end{proof}

We now give a result that we'll use in the transition to continuous
time.
\begin{thm}\label{JessenConditioningLimits}Let $\xi$ be an integrable
  random variable and let $\mathcal{F}_0 \subset \mathcal{F}_1 \subset
  \cdots$ be filtration, then $\cexpectationlong{\mathcal{F}_n}{\xi}$
  converges to $\cexpectationlong{\bigvee_n \mathcal{F}_n}{\xi}$ both
  almost surely and in $L^1$.  If $\dotsb \subset \mathcal{F}_{-1}
  \subset \mathcal{F}_0$ is a filtration then as $n \to -\infty$,
  $\cexpectationlong{\mathcal{F}_n}{\xi}$ converges to $\cexpectationlong{\bigcap_n \mathcal{F}_n}{\xi}$
both  almost surely and in $L^1$.
\end{thm}
\begin{proof}
First we take the unbounded above case.  We know from the tower property of conditional expectation and
Corollary \ref{ConditionalExpectationsUniformlyIntegrable} that $\cexpectationlong{\mathcal{F}_n}{\xi}$ is a
uniformly integrable martingle and is closable and converges both
almost surely and in $L^1$.  Let $M$ be the limit and we need to show
that $\cexpectationlong{\bigvee_n \mathcal{F}_n}{\xi} = M$ almost
surely.  We know that $M$ is $\bigvee_n \mathcal{F}_n$-measurable since it is an
almost sure limit of $M_n$ each of which is $\bigvee_n
\mathcal{F}_n$-measurable.  Furthermore by Theorem
\ref{L1MartingaleConvergenceTheoremDiscrete} we also know that
$\cexpectationlong{\mathcal{F}_n}{M} =
\cexpectationlong{\mathcal{F}_n}{\xi}$ almost surely.  Now suppose
that we have $A \in \mathcal{F}_n$ for some $n$.  We have
\begin{align*}
\expectation{M ; A} &=
\expectation{\cexpectationlong{\mathcal{F}_n}{M} ; A} \\
&=\expectation{\cexpectationlong{\mathcal{F}_n}{\xi} ; A} \\
&=\expectation{\cexpectationlong{\mathcal{F}_n}{\cexpectationlong{\bigvee_n
      \mathcal{F}_n}{\xi}} ; A} \\
&=\expectation{\cexpectationlong{\bigvee_n
      \mathcal{F}_n}{\xi} ; A} \\
\end{align*}
thus $\expectation{M ; A}  = \expectation{\cexpectationlong{\bigvee_n
    \mathcal{F}_n}{\xi} ; A}$ for all $A$ belonging to the
$\pi$-system $\cup_n \mathcal{F}_n$.  By a monotone class argument
(Lemma \ref{ConditionalExpectationExtension}) we conclude that $M = \cexpectationlong{\bigvee_n
    \mathcal{F}_n}{\xi}$ almost surely.

Now we treat the unbounded below case.  As before we know that
$M_n = \cexpectationlong{\mathcal{F}_n}{\xi}$ is a uniformly integrable
martingale.  By Theorem \ref{L1MartingaleConvergenceTheoremDiscrete}
we know that there is an integrable $M_{-\infty}$ such that $\lim_{n \to -\infty}
M_n = M_{-\infty}$ a.s. and by uniform
integrability and Lemma \ref{LpConvergenceUniformIntegrability} we
know that the convergence is also in $L^1$.  We need to show that $M_{-\infty} =
\cexpectationlong{\bigcap_n \mathcal{F}_n}{\xi}$ a.s.  The first step
is to observe that since $\mathcal{F}_n$ is a filtration $\bigcap_n
\mathcal{F}_n$ is the tail $\sigma$-algebra and therefore $M_{-\infty}$ is
$\bigcap_n \mathcal{F}_n$-measurable.  If we let $A \in \bigcap_n
\mathcal{F}_n$ then for all $n \leq 0$ we have $\expectation{\xi ; A}
= \expectation{M_n;A}$.  Since $M_n$ is uniformly integrable it
follows that $M_n \characteristic{A}$ is uniformly integrable as well and therefore can conclude
that $\expectation{\xi ; A} = \lim_{n \to -\infty} \expectation{M_n;A}
= \expectation{M_{-\infty};A}$.  The result is proven.
\end{proof}

TODO: This result can be proven directly without appealing to the martingle
convergence theorems (Stroock does this).  Is there any point in doing
so here?  Should we move this result further down and put it in the
context of the discussion of approximating continuous optional times
by discrete ones?  Stroock has some other interesting consequences of
this theorem too.
Here is the proof that depends only on the Doob Maximal Inequality.
\begin{proof}
Before we begin, we can clean up the notation that follows by assuming
that $\mathcal{A} = \bigvee_n \mathcal{F}_n$.  For if $\xi$ is
integrable then we know that
$\cexpectationlong{\bigvee_n \mathcal{F}_n}{\xi}$ is also integrable
and convergence in $L^1(\Omega, \bigvee_n \mathcal{F}_n, \mu)$
implies convergence in $L^1(\Omega, \mathcal{A}, \mu)$.

First goal is to validate the following claim:
\begin{align*}
\lambda \probability{\sup_{n \in \integers_+}
  \abs{\cexpectationlong{\mathcal{F}_n}{\xi}} \geq \lambda} &\leq
\expectation{ \abs{\xi} ; \sup_{n \in \integers_+}
  \abs{\cexpectationlong{\mathcal{F}_n}{\xi}} \geq \lambda}  \leq \expectation{\abs{\xi}}
\end{align*}
\emph{Here is where Stroock reduces this to Doob's Maximal Inequality
  along the way claiming that we may assume $\xi \geq 0$.  I don't
  understand how to validate his claim about the positivity assumption
  and I am stuck trying to use
  Doob's Maximal Inequality as we've stated it. However it is easy to
  rescue the situation by adapting the proof of the
  Maximal Inequality to prove the above as you'll see}.
We first prove the claim for a finite index set.  Since we 
know from Lemma \ref{ClosedMartingales} that
$\cexpectationlong{\mathcal{F}_n}{\xi}$ is an
$\mathcal{F}$-martingale. we know from
that $\abs{\cexpectationlong{\mathcal{F}_n}{\xi}}$ is a
submartingale.  We let $\tau$ be hitting time of the interval
$[\lambda, \infty)$ and note that 
\begin{align*}
\lbrace \sup_{n \in \integers_+}
  \abs{\cexpectationlong{\mathcal{F}_n}{\xi}} \geq \lambda \rbrace =
  \cup_{0 \leq m \leq n} \lbrace \tau = m \rbrace
\end{align*}
where the union is disjoint.  Since $\tau$ is an optional time
(Lemma \ref{HittingTimesDiscrete}) we also know that $\lbrace \tau =
m \rbrace \in \mathcal{F}_m$ and therefore
\begin{align*}
\expectation{\abs{\xi} ; \tau = m} &= 
\expectation{\cexpectationlong{\mathcal{F}_m}{\abs{\xi}} ; \tau =  m} \geq 
\expectation{\abs{\cexpectationlong{\mathcal{F}_m}{\xi}} ; \tau =  m}
\geq 
\lambda \probability{\tau = m}
\end{align*}
and summing for $m$ from 0 to $n$ yields
\begin{align*}
\lambda \probability{\max_{0 \leq m \leq n}
  \abs{\cexpectationlong{\mathcal{F}_m}{\xi}} \geq \lambda} &\leq
\expectation{ \abs{\xi} ; \max_{0 \leq m \leq n}
  \abs{\cexpectationlong{\mathcal{F}_m}{\xi}} \geq \lambda} 
\end{align*}
The result is completed by taking the limit as $n$ goes to infinity
and using continuity of measure (Lemma \ref{ContinuityOfMeasure}) and
Montone Convergence.

\emph{Here is the result from Stroock}
We know from Lemma \ref{ClosedMartingales} that
$\cexpectationlong{\mathcal{F}_n}{\xi}$ is an
$\mathcal{F}$-martingale.  By Doob's Maximal Inequality (Lemma
\ref{DoobMaximalInequalityDiscrete}), the
$\mathcal{F}_n$-measurability of  $\lbrace \sup_{0 \leq k
    \leq n}\cexpectationlong{\mathcal{F}_k}{\xi} \geq \lambda \rbrace$ and another application of the
tower property we know that 
\begin{align*}
\lambda \probability{\sup_{0 \leq k \leq n}
  \cexpectationlong{\mathcal{F}_k}{\xi} \geq \lambda} &\leq
\expectation{\cexpectationlong{\mathcal{F}_n}{\xi}; \sup_{0 \leq k
    \leq n}\cexpectationlong{\mathcal{F}_k}{\xi} \geq \lambda} \\
&= \expectation{\xi; \sup_{0 \leq k
    \leq n}\cexpectationlong{\mathcal{F}_k}{\xi} \geq \lambda}
\end{align*}
By continuity of measure (Lemma \ref{ContinuityOfMeasure}) we know
that 
\begin{align*}
\probability{\sup_{k}
  \cexpectationlong{\mathcal{F}_k}{\xi} \geq \lambda} &= \lim_{n \to \infty} \probability{\sup_{0 \leq k \leq n}
  \cexpectationlong{\mathcal{F}_k}{\xi} \geq \lambda} \\
\intertext{and by Dominated Convergence}
\expectation{\xi; \sup_{k}\cexpectationlong{\mathcal{F}_k}{\xi} \geq \lambda} &=
\lim_{n \to \infty} \expectation{\xi; \sup_{0 \leq k
    \leq n}\cexpectationlong{\mathcal{F}_k}{\xi} \geq \lambda} \\
\intertext{so we have shown}
\lambda \probability{\sup_{k}
  \cexpectationlong{\mathcal{F}_k}{\xi} \geq \lambda} &\leq
\expectation{\xi; \sup_{k}\cexpectationlong{\mathcal{F}_k}{\xi} \geq \lambda}
\end{align*}
\emph{End result from Stroock}

To show almost sure convergence, we let $\mathcal{G}$ denote the set
of all integrable $\xi$ such that
$\cexpectationlong{\mathcal{F}_n}{\xi} \toas \xi$.  Note that any
$\mathcal{F}_n$-measurable $\xi$ is in $\mathcal{G}$ since the sequence
of conditional expectations is eventually almost surely constant and
equal to $\xi$.  On the other hand we know that $\cup_n
L^1(\Omega, \mathcal{F}_n, \mu)$ is dense in $L^1(\Omega, \bigvee_n
  \mathcal{F}_n, \mu) = L^1(\Omega, \mathcal{A}, \mu)$ (Lemma
  \ref{LpDensityUnionSubsigmaAlgebras}) so it suffices to show that
  $\mathcal{G}$ is closed in $L^1$.  So suppose that $\xi_n$ is a
  sequence in $\mathcal{G}$ such that $\xi_n \tolp{1} \xi$.  We show
  that $\cexpectationlong{\mathcal{F}_n}{\xi} \toas \xi$ by using
  Lemma \ref{ConvergenceAlmostSureByInfinitelyOften}.  Suppose
  $\epsilon > 0$ is given, then for every $m,n$
\begin{align*}\begin{split}
\probability{\sup_{k \geq m} \abs{\cexpectationlong{\mathcal{F}_k}{\xi} - \xi} >
  \epsilon} &\leq \probability{\sup_{k \geq m}
  \abs{\cexpectationlong{\mathcal{F}_k}{\xi-\xi_n}} >
  \frac{\epsilon}{3}} + \\
&\qquad \probability{\sup_{k \geq m}
  \abs{\cexpectationlong{\mathcal{F}_k}{\xi_n} - \xi_n} >
  \frac{\epsilon}{3}} + \probability{\abs{\xi_n - \xi} >
  \frac{\epsilon}{3}} \\
&\leq \frac{6}{\epsilon} \expectation{\abs{\xi - \xi_n} } +
\probability{\sup_{k \geq m}
  \abs{\cexpectationlong{\mathcal{F}_k}{\xi_n} - \xi_n} >
  \frac{\epsilon}{3}} \\
\end{split}
\end{align*}
where the first term is bounded by our claim at the beginning of
proof applied to $\xi_n - \xi$ and the third term is
bounded by the Markov Inequality (Lemma
\ref{MarkovInequality}).

Taking the limit as $m$ goes to infinity and using our assumption that
$\xi_n \in \mathcal{G}$ and the characterization of almost sure
convergence from Lemma
\ref{ConvergenceAlmostSureByInfinitelyOften} we see that $\lim_{m \to
  \infty} \probability{\sup_{k \geq m}
  \abs{\cexpectationlong{\mathcal{F}_k}{\xi_n} - \xi_n} >
  \frac{\epsilon}{3}} = 0$.  
Therefore
\begin{align*}
\lim_{m \to \infty} \probability{\sup_{k \geq m} \abs{\cexpectationlong{\mathcal{F}_k}{\xi} - \xi} >
  \epsilon} &\leq \frac{6}{\epsilon} \expectation{\abs{\xi - \xi_n} }
\\
\end{align*}
and by taking the limit as $n$ goes to infinity we get
\begin{align*}
\lim_{m \to \infty} \probability{\sup_{k \geq m} \abs{\cexpectationlong{\mathcal{F}_k}{\xi} - \xi} >
  \epsilon}&=0
\end{align*}
 so $\cexpectationlong{\mathcal{F}_n}{\xi} \toas \xi$ by another application of Lemma \ref{ConvergenceAlmostSureByInfinitelyOften}.

Since we know that the family $\cexpectationlong{\mathcal{F}_n}{\xi}$
is uniformly integrable by Corollary
\ref{ConditionalExpectationsUniformlyIntegrable},
$\cexpectationlong{\mathcal{F}_n}{\xi} \tolp{1} \xi$ follows from the
almost sure convergence and Lemma \ref{LpConvergenceUniformIntegrability}.
\end{proof}

\subsection {Continuous Time Martingales and Weakly Optional Times}

Our next goal is to extend the theory we've developed to a continuous
time setting.  For the most part we proceed by using approximation
arguments to reduce results to the discrete time analogues proven in
the last section.  First we have to come to grips with some subtleties
related to filtrations, optional times and measurability in continuous
time.

\begin{defn}A $T$-valued random variable is called a
  \emph{weakly $\mathcal{F}$-optional time} (also called a
  \emph{weak $\mathcal{F}$-stopping time}) if and only if $\lbrace \tau <
  t \rbrace \in \mathcal{F}_t$ for all $t \in T$.  
\end{defn}
Just as with optional times, if the filtration $\mathcal{F}$ is clear
from context, we'll simply refer to a weakly optional time.

A weakly $\mathcal{F}$-optional time $\tau$ is a decision rule to stop
at $t$ that requires an arbitrarily small amount of future information
to determine that one should stop at $t$.  Alternatively one can
characterize it as a decision rule such that $\tau + \epsilon$ is
$\mathcal{F}$-optional for all $\epsilon>0$.

Let $\mathcal{F}^+ = \cup_{s>t} \mathcal{F}_s$ (note that $\mathcal{F}
= \mathcal{F}^+$ if and only if $\mathcal{F}$ is right continuous).

One way of defining the $\sigma$-algebra associated with a weakly
$\mathcal{F}$-optional time is as a limit of the $\sigma$-algebras
associated the $\mathcal{F}$-optional times $\tau + \epsilon$
\begin{align*}
\mathcal{F}_{\tau^+} = \cup_{\epsilon > 0} \mathcal{F}_{\tau + \epsilon}
\end{align*}
\begin{lem}\label{WeaklyOptionalCharacterization}$\tau$ is $\mathcal{F}^+$-optional if and only if $\tau$ is
  weakly $\mathcal{F}$-optional.  In this case, 
\begin{align*}
\mathcal{F}^+_\tau = \mathcal{F}_{\tau^+} = \lbrace A \in \mathcal{A}
\mid A \cap \lbrace \tau < t \rbrace \in \mathcal{F}_t \text { for all
} t \in T \rbrace
\end{align*}
\end{lem}
\begin{proof}
The first thing is to notice that for any random time $\tau$ (not just
optional or weakly optional times) we have the equalities
\begin{align*}
&\lbrace \tau \leq t \rbrace = \bigcap_{\substack{
r \in \rationals \\
r >  t}} 
\lbrace \tau < r \rbrace
&\lbrace \tau < t \rbrace = \bigcup_{\substack{
r \in \rationals \\ 
r < t}} 
\lbrace \tau \leq r \rbrace
\end{align*}
TODO: Justify (but it's kinda obvious by density of $\rationals$)

Armed with these facts we proceed to show the equality 
\begin{align*}
\mathcal{F}^+_{\tau} = \lbrace A \in \mathcal{A}
\mid A \cap \lbrace \tau < t \rbrace \in \mathcal{F}_t \text { for all
} t \in T \rbrace
\end{align*} 
for any random time $\tau$.

Suppose $A \cap \lbrace \tau \leq t \rbrace \in \mathcal{F}^+_t =
\cap_{s>t} \mathcal{F}_s$ for every $t \in T$.  Then for all $t  \in T$,
\begin{align*}
A \cap \lbrace \tau < t \rbrace &= A \cap \left (\bigcup_{\substack{r \in \rationals \\ r <
  t}} \lbrace \tau \leq r \rbrace \right ) =\bigcup_{\substack{r \in \rationals \\ r <
  t}}  \left ( A \cap \lbrace \tau \leq r \rbrace \right ) \in \mathcal{F}_t\\
\end{align*}
since for any $r < t$, $\mathcal{F}^+_r \subset \mathcal{F}_t$.

On the other hand, if $A \cap \lbrace \tau < t \rbrace \in
\mathcal{F}_t$ for all $t \in T$, then
\begin{align*}
A \cap \lbrace \tau \leq t \rbrace &= A \cap \left ( \bigcap_{\substack{r \in \rationals \\ r >
  t}} \lbrace \tau < r \rbrace \right ) =  \bigcap_{\substack{r \in \rationals \\ r >
  t}} \left ( A \cap  \lbrace \tau < r \rbrace \right ) \in \mathcal{F}^+_t
\end{align*}
where the last inclusion follows from the fact that for any $r < s$, $ A \cap
\lbrace \tau < r \rbrace \subset  A \cap  \lbrace \tau < s
\rbrace$, so for any $s \in T$ with $s > t$ we in fact
have
\begin{align*}
\bigcap_{\substack{r \in \rationals \\ r >
  t}} \left ( A \cap  \lbrace \tau < r \rbrace \right ) &= 
\bigcap_{\substack{r \in  \rationals \\ 
s \geq r >  t}} \left ( A \cap  \lbrace \tau < r \rbrace \right ) \in \mathcal{F}_s
\end{align*}

Now note that by definition, $\tau$ is weakly $\mathcal{F}$-optional if and only if
$\Omega \in \lbrace A \in \mathcal{A}
\mid A \cap \lbrace \tau < t \rbrace \in \mathcal{F}_t \text { for all
} t \in T \rbrace$ and $\tau$ is $\mathcal{F}^+$-optional
if and only if $\Omega \in \mathcal{F}^+_\tau$.  Therefore the equality just shown tells us that
$\tau$ is weakly $\mathcal{F}$-optional if and only if $\tau$ is 
$\mathcal{F}^+$-optional.

We finish by showing that $\mathcal{F}^+_\tau =
\mathcal{F}_{\tau^+}$.  To see this, note that $A \in
\mathcal{F}_{\tau^+}$ if and only if $A \in
\mathcal{F}_{\tau + \epsilon}$ for all $\epsilon > 0$ which is true if and only
if $A \cap \lbrace \tau + \epsilon \leq t 
\rbrace =  A \cap \lbrace \tau \leq t - \epsilon
\rbrace\in \mathcal{F}_{t}$ for all $t \in T$, $\epsilon > 0$ which is
true if and only if $A \cap \lbrace \tau \leq t 
\rbrace\in \mathcal{F}_{t+\epsilon}$ for all $t \in T$, $\epsilon >
0$.  This last statement is simply that $A \cap \lbrace \tau \leq t 
\rbrace\in \mathcal{F}^+_t$ for all $t \in T$ so we are done.
\end{proof}

In the previous section we identified a useful class of optional times
that we called hitting times.  Hitting times can be defined in
continuous time but there are more stringent requirements on when they
are optional times.
\begin{lem}\label{HittingTimesContinuous}Let $\mathcal{F}$ be a
  filtration on $\reals_+$, let $X_t$ be an $\mathcal{F}$-adapted
  process with values in a measurable space $(S, \mathcal{S})$ where
  $S$ is topological and $\mathcal{S}$ contains the Borel $\sigma$-algebra, 
  $B \in \mathcal{S}$ and $\tau_B = \inf \lbrace t > 0 \mid X_t \in
  B\rbrace$.  Then if $S$ is a metric space, $B$ is closed and $X_t$ is continuous
$\tau_B$ is $\mathcal{F}$-optional and if $B$ is open and $X_t$ is right continuous then $\tau_B$ is weakly $\mathcal{F}$-option.
\end{lem}
\begin{proof}
To see the first case, by the countability and density of the
rationals in $\reals_+$, continuity of $X_t$ and closedness of $B$ we
know that $X_{\tau_B} \in B$ and therefore $\tau_B \leq t$ if and only if there is an $0 < s \leq t$
such that $X_s \in B$.  This latter statement is true if and only if there is an
integer $m>0$ and points $X_q$ with
$q \in \rationals$ and $1/m \leq q \leq t$ that are arbitrarily close to
$B$.  Translating this observation into set operations we get
\begin{align*}
\lbrace \tau_B \leq t \rbrace &= \cup_{m=1}^\infty \cap_{n =1}^\infty \cup_{\substack{1/m
    \leq q \leq t \\ q \in \rationals}} \lbrace d(X_q, B) < \frac{1}{n}
  \rbrace \in \mathcal{F}_t
\end{align*}
because each $\lbrace x \in S \mid d(x, B) < \frac{1}{n}  \rbrace$ is
open and thus $\lbrace d(X_q, B) < \frac{1}{n}  \rbrace \in
\mathcal{F}_t$ because $X$ is $\mathcal{F}$-adapted.
To see the second case note that by similar considerations $\tau_B < t$ if and only if there
exists a $q \in \rationals$ such that $0 \leq q < t$ with $X_q \in B$
thus
\begin{align*}
\lbrace \tau_B \leq t \rbrace &= \cup_{\substack{0 \leq q < t \\ q \in
    \rationals}} \lbrace X_q \in B \rbrace \in \mathcal{F}_t
\end{align*}

\end{proof}

When passing from discrete time results to continuous time results it
is often useful to approximate an optional time on a continuous domain
by a discrete one.  The following approximation scheme is so useful it
deserves to be called out.

\begin{lem}\label{DiscreteApproximationOptionalTimes}Let $\tau$ be a weakly optional time on $\reals_+$, then define
\begin{align*}
\tau_n = \frac{1}{2^n} \floor{2^n \tau + 1}
\end{align*}
$\tau_n$ is a sequence of optional times with values in a
countable index set such that $\tau_n\downarrow \tau$.
\end{lem}
\begin{proof}
The fact that each $\tau_n$ is an optional time follows from the
definition and the fact that $\tau$ is a weakly optional time:
\begin{align*}
\lbrace \tau_n \leq \frac{k}{2^n} \rbrace &= \lbrace \frac{k-1}{2^n}
\leq \tau < \frac{k}{2^n} \rbrace = \lbrace \tau < \frac{k-1}{2^n}
\rbrace^c \cap \lbrace
\tau < \frac{k}{2^n} \rbrace \in \mathcal{F}_{\frac{k}{2^n}}
\end{align*}

To see the fact that $\tau_n$ is decreasing, note $\tau_n =
\frac{k}{2^n}$ if and only if $\frac{k-1}{2^n} \leq \tau <
\frac{k}{2^n}$  
which implies 
\begin{align*}
\tau_{n+1} &= \begin{cases}
\frac{k}{2^n} & \text{if $\frac{2k-1}{2^{n+1}} \leq \tau < \frac{k}{2^n}$} \\
\frac{2k-1}{2^{n+1}} & \text{if $\frac{k-1}{2^{n}} \leq \tau < \frac{2k-1}{2^{n+1}}$} \\
\end{cases}
\end{align*}
Convergence to $\tau$ follows easily since $\abs{\tau - \tau_n} \leq \frac{1}{2^n}$ by definition.
\end{proof}

If we have approximation scheme for an optional time we may also want
to understand how the associated $\sigma$-algebras behave.  For the
decreasing approximation of the previous lemma, part (ii) of the
following gives us the answer.
\begin{lem}\label{InfSupStoppedFiltration}If we have a finite or countable collection of optional
  times $\tau_n$ then $\sup_n \tau_n$ is an optional time.  If we have
  a finite or countable collection of weakly optional times $\tau_n$
  then $\tau = \inf_n \tau_n$ is a weakly optional time and
  furthermore
\begin{align*}
\mathcal{F}^+_\tau &= \cap_n \mathcal{F}^+_{\tau_n}
\end{align*}
\end{lem}
\begin{proof}
If $\tau_n$ are optional times then if follows from the definition of
supremum that $\lbrace \tau \leq t \rbrace = \cap_n \lbrace \tau_n
\leq t \rbrace$ and therefore $\tau$ is an optional time.

If $\tau_n$ are weakly optional times then if follows from the definition of
infimum that $\lbrace \tau < t \rbrace = \cup_n \lbrace \tau_n
< t \rbrace$ and therefore $\tau$ is a weakly optional time.
Furthermore because $\tau \leq \tau_n$ for all $n$ we know that
$\mathcal{F}^+_\tau \subset \mathcal{F}^+_{\tau_n}$ for all $n$.  On
the other hand by Lemma \ref{WeaklyOptionalCharacterization}, if we know that $A \in \cap_n \mathcal{F}^+_{\tau_n}$
then $A \cap \lbrace \tau_n < t \rbrace \in \mathcal{F}_t$ for all $n$
and $t$.  Therefore we can write $A \cap \lbrace \tau < t \rbrace =  \cup_n A \cap \lbrace \tau_n
< t \rbrace \in \mathcal{F}_t$ which shows that $A \in
\mathcal{F}^+_\tau$ by another application of Lemma \ref{WeaklyOptionalCharacterization}.
\end{proof}

We shall have a need for the following characterization of uniform
integrability for martingales on $\integers_-$ (sometimes called a
\emph{backward submartingale}).
\begin{lem}\label{BackwardSubmartingaleBoundedUniformlyIntegrable}Let $X_n$ be an $\mathcal{F}$-submartingale on
  $\integers_-$, then $\expectation{X_n}$ is bounded if and only if $X_n$ is
  uniformly integrable.
\end{lem}
\begin{proof}
As a first simple observation, we know that since $X_n$ is a
submartingale then 
\begin{align*}
\expectation{X_n}  &=
\expectation{\cexpectationlong{\mathcal{F}_{n-1}}{X_n}} \geq
\expectation{X_{n-1}}
\end{align*}
so boundedness of $\expectation{X_n}$ is equivalent to $\lim_{n \to -\infty}
\expectation{X_n} = \inf_n \expectation{X_n} > -\infty$.

Assume that $\expectation{X_n}$ is $L^1$ bounded.  We proceed by constructing the
analogue of the Doob Decomposition for time index $\integers_-$ and
then invoking results for martingales.  Recall in the Doob
Decomposition we write a submartingale $X_n$ on $\integers_+$ as $M_n
+ A_n$ where $M_n$ is a martingale and $A_n = \sum_{m=1}^n
\cexpectationlong{\mathcal{F}_{m-1}}{X_m} - X_{m-1}$.  So to make this
work for $\integers_-$ we have to handle the fact that the desired
definitions now involve an infinite sum which must converge for things
to make sense.  To
that end, define for $n \leq 0$,
\begin{align*}
\alpha_n = \cexpectationlong{\mathcal{F}_{n-1}}{X_n} - X_{n-1} =
\cexpectationlong{\mathcal{F}_{n-1}}{X_n - X_{n-1}} \geq 0
\end{align*}
so that $\alpha_n$ is a predictable process.  Observe that by Monotone Convergence
\begin{align*}
\expectation{\sum_{n \leq 0} \alpha_n}
&= \lim_{m \to \infty} \sum_{-m \leq n \leq 0} \expectation{ \alpha_n} \\
&= \lim_{m \to \infty} \sum_{-m \leq n \leq 0}
\expectation{X_n} - \expectation{X_{n-1}} \\
&= \expectation{X_0} - \inf_n \expectation{X_n} < \infty \\
\end{align*}
Therefore we know that $\sum_{n \leq 0} \alpha_n$ is almost surely
finite.  With that in hand we can define for each $n \leq 0$
\begin{align*}
A_n &= \sum_{m \leq n} \alpha_m = \sum_{m \leq n}
\cexpectationlong{\mathcal{F}_{m-1}}{X_m} - X_{m-1}
\end{align*}
so that $A_n$ is integrable.  Moreover since $A_n$ is almost surely
increasing we know that $\sup_{n} A_n \leq A_0$ and therefore the
sequence $A_n$ is
uniformly integrable (e.g. see Example \ref{DominatedImpliesUniformlyIntegrable}).
Now we define
\begin{align*}
M_n &= X_n - A_n
\end{align*}
so that by integrability of $A_n$ we have $M_n$ is integrable and
moreover
\begin{align*}
\cexpectationlong{\mathcal{F}_{n-1}}{M_n} &=
\cexpectationlong{\mathcal{F}_{n-1}}{X_n} - A_n \\
&=\cexpectationlong{\mathcal{F}_{n-1}}{X_n} -
\cexpectationlong{\mathcal{F}_{n-1}}{X_n} + X_{n-1} - A_{n-1} = M_{n-1}\\
\end{align*}
so that $M_n$ is a martingale.  Since $M_n$ is closed we conclude from 
Theorem \ref{L1MartingaleConvergenceTheoremDiscrete} that $M_n$ is
uniformly integrable.  The uniform integrability of $A_n$ and $M_n$
together imply the uniform integrability of $X_n$ (Lemma \ref{SumsOfUniformlyIntegrable}).

Now if we assume that $X_n$ is uniformly integrable then it follows
that $X_n$ is $L^1$ bounded (Lemma
\ref{UniformIntegrabilityProperties}) and therefore
$\expectation{X_n}$ is bounded since $\abs{\expectation{X_n}} \leq \expectation{\abs{X_n}}$.
\end{proof}

 TODO: Introduce complete right continuous filtration and existence of
a cadlag version of martingales.  Say something about the nature of
the modification (e.g. preservation of f.d.d.'s) and the fact that it
is a modification.

The martingale results for discrete time tell us quite a
bit about what can happen in continuous time as well.  If we are given
a submartingale on $\reals_+$ then we can restrict it to
$\rationals_+$ and ask what we know about the restricted process; as
we'll  soon see we know quite a lot!  The first issue which we examine
gets to the heart of whether we can extrapolate from the discrete case
to the continuous case.  If there are no restrictions on the
regularity/continuity of sample paths then there is very little that we can say
about what happens on $\reals_+ \setminus \rationals_+$ based on what
is happening on $\rationals_+$.  Thus our first task is to understand
the ways in which we can modify a continuous time submartingale to get
a different submartingale that has some continuity in sample paths. Note here
that the use of the word modify is quite a bit subtle: we mean to use
the word both in its colloquial sense of \emph{how can we change
  continuous time submartingale to make it have regular sample paths}
as well as the technical sense of \emph{when are the changes that we
  make to a continuous time submartingale a modification of the
  stochastic process}.   The
specific type of regularity we aim for is that sample paths of the
submartingale are right continuous and have left limits.  It is
traditional to refer to such paths as cadlag which is an acronym
derived from the French phrase \emph{continue \`{a} droits limite
  \`{a} gauche}.  The reader may encounter the acronym derived from
the English rcll but it seems to be less popular.

The formal development of these ideas comes with a lot of technical
baggage so before we jump into the details let's step
back and think about what we can expect.  We discuss the martingale
case here even though almost all of what we say applies equally to
submartingales.  Lets suppose that we have an
$\mathcal{F}$-martingale $X$ on $\reals_+$.  If we restrict a
$X$ to $\rationals_+$ then the convergence
theorems (and at their the base the upcrossing lemma) tells us that
almost surely on
any bounded interval the restricted martingale has limits along all
sequences.  Therefore at worst the restricted martingale on
$\rationals_+$ has jump discontinuities (almost surely!).  This gives us hope that we
can modify (in the colloquial sense) $X$ to create a new process $Y$
on $\reals_+$ such that $Y$ is cadlag: simply define $Y_t$ for $t \in
\reals_+$ such that $Y_t =
\lim_{\substack{q\downarrow t \\ q \in \rationals}} X_q$.  This gives
us a process to be sure but it isn't even $\mathcal{F}$-adapted in
general: by the definition of $Y_t$ as limit of $X_q$ for $q > t$  we know that $Y_t$ is
$\mathcal{F}^+_t$-measurable but it is not necessarily $\mathcal{F}_t$-measurable.  This
introduces one of the key ideas: if we have any hope of  changing $X$
to get an adapted cadlag $Y$ we had
either be prepared to pass to the filtration $\mathcal{F}^+$ or start
with a right continuous one.  

We've already glossed over an issue that brings up a second key idea.
The construction described only works \emph{almost surely}; we have to
come up a different plan on the null set where $X_q$ is wild.  The easiest thing
to do is just to set $Y_t \equiv 0$ when this occurs.  Since the event
of $X$ on $\rationals_+$ being ill-behaved has probability zero whatever it is we decide
t do won't prevent $Y$ from being a version of $X$.  The issue is that
the event of $X$ on $\rationals_+$ being ill-behaved depends on all of
$X_t$ for all $t \geq 0$ hence is in $\mathcal{F}_\infty$; thus as we
modify $X_t$ to get $Y_t$, in accounting for the ill-behavedness of $X_t$
we are changing each $Y_t$ on an event in $\mathcal{F}_\infty$ further
destroying adaptedness of $Y$.  The good news is that we do know that
the event in question is a null event and therefore we come to the
second key idea: to get a cadlag $Y$ from $X$ we had either be
prepared to add all of the null events of $\mathcal{F}_\infty$ to each
$\mathcal{F}^+_t$ or assume that they are there to begin with.  The
filtration that is right continuous and has null sets added is
referred to as the \emph{partial augmentation} of $\mathcal{F}$.

These first two ideas are enough to get us an adapted process $Y$ but
more is true: $Y$ is a martingale with respect the partially
augmented filtration.  This is not obvious and requires checking using
discrete time discrete time results.  The remaining issue and question
is whether $Y$ is indeed a version of $X$.  The following example shows
that this may not be true without further hypotheses on $X$.

\begin{examp}\label{CadlagNotAModification}Let $\Omega = \lbrace -1, 1 \rbrace$ with the probability
  measure $\probability{1} = \probability{-1} = \frac{1}{2}$.  Let
  $\mathcal{F}_t = \lbrace \Omega, \emptyset \rbrace$ for $0 \leq t
  \leq 1$ and $\mathcal{F}_t = \lbrace \Omega, \emptyset, \lbrace 1
  \rbrace, \lbrace -1 \rbrace\rbrace$ for $t > 1$ let 
\begin{align*}
X_t(\omega) &= \begin{cases}
0 & \text{for $0 \leq t \leq 1$} \\
\omega & \text{for $t > 1$}
\end{cases}
\end{align*}
It is easy to see that $X_t$ is an $\mathcal{F}$-martingale.  Now
define 
\begin{align*}
Y_t(\omega) &= \begin{cases}
0 & \text{for $0 \leq t < 1$} \\
\omega & \text{for $t \geq 1$}
\end{cases}
\end{align*}
and note that $Y_1$ is not $\mathcal{F}_1$-measurable.  However, it is
easy to see that 
 $\mathcal{F}^+_t = \lbrace \Omega, \emptyset \rbrace$ for $0 \leq t
  < 1$,  $\mathcal{F}^+_t = \lbrace \Omega, \emptyset, \lbrace 1
  \rbrace, \lbrace -1 \rbrace\rbrace$ for $t \geq 1$ and 
  $Y_t$ is an $\mathcal{F}^+$-martingale.  Note however that
  $\probability{X_1 = Y_1} = 0 \neq 1$ and thus $Y$ is not a version
  of $X$.

Note that $X$ is not a $\mathcal{F}^+$-martingale (or
$\mathcal{F}^+$-sub/supermartingale) as for $t > 1$ we have
$\cexpectationlong{\mathcal{F}^+_1}{X_t} = X_t$ and therefore
$\cexpectationlong{\mathcal{F}^+_1}{X_t}> X_1$ with probability $1/2$
(i.e. when $\omega = 1$)
and $\cexpectationlong{\mathcal{F}^+_1}{X_t} < X_1$ with probability
$1/2$ (i.e. when $\omega = -1$).
\end{examp}

Example \ref{CadlagNotAModification} shows that there is a limit to
what we can accomplish by taking a martingale with respect to an
arbitrary filtration and trying find a version that is cadlag.  
Nonetheless, the method we've outlined to make a cadlag process $Y$
from an arbitrary process $X$ can be shown to result in a version if
$X$ is assumed to be a martingale with respect to the the augmented
filtration in the first place (plus some extra conditions if $X$ is
only assumed to be a submartingale).  Thus the impediment to the
existence of a cadlag version in Example \ref{CadlagNotAModification}
is in the final comment about $X$ not being a martingale with respect
to the right continuous filtration. 

\begin{thm}\label{CadlagModificationContinuousMartingale}Let $X_t$  be a $\mathcal{F}$-submartingale on $\reals_+$
  and let $Y_q$ denote the restriction to $\rationals_+$.  The there
  exists a set $A \subset \mathcal{F}_\infty$ with $\probability{A}=1$ on which
  $\lim_{q \to t^+} Y_q$ and $\lim_{a \to t^-} Y_q$ exist for all $t
  \in \reals_+$.  If we
  define 
\begin{align*}
Z_t(\omega) = 
\begin{cases}
\lim_{q \to t^+} Y_q(\omega) & \text{if $\omega \in A$} \\
0 & \text{if $\omega \notin A$} \\
\end{cases}
\end{align*} 
then $Z_t$ is a cadlag
  $\overline{\mathcal{F}_+}$-submartingale.
If $\mathcal{F}$ is right continuous then $X^t$ has a cadlag
modification that is a submartingale if and only if $t \mapsto
\expectation{X_t}$ is right continuous.
\end{thm}
\begin{proof}
Pick $N \in \naturals$ and note that since $Y^+_q$ is a submartingale
we have for all $q \in [0,N] \cap \rationals$
\begin{align*}
\expectation{Y^+_q} &\leq
\expectation{\cexpectationlong{\mathcal{F}_q}{Y^+_N}} = \expectation{Y^+_N}
\end{align*}
and by the same reasoning using the fact that $Y_q$ is a submartingale
we know that $\expectation{Y_0} \leq \expectation{Y_q}$.  Together
these imply 
\begin{align*}
\expectation{\abs{Y_q}} &= \expectation{Y^+_q}  + \expectation{Y^-_q}
= 2\expectation{Y^+_q}  - \expectation{Y_q} \leq 2\expectation{Y^+_N}  - \expectation{Y_0} 
\end{align*}
which implies that $Y_q$ restricted to $[0,N]$ is $L^1$-bounded.
We can apply Theorem \ref{L1MartingaleConvergenceTheoremDiscrete} to
the restricted submartingale $Y_q$ to construct $A_N \in \mathcal{F}_N$ with
$\probability{A_N}=1$ such that for all increasing and decreasing
sequences $q_n$ in $\rationals \cap [0,N]$ we have $Y_{q_n}$ converge
almost on $A_N$.  So in particular $\lim_{q \to t^-} Y_q$ and $\lim_{q
  \to t^+} Y_q$ exist for every $t \in [0,N]$ on $A_N$.  Taking the
intersection of $A = \cap_{N=1}^\infty A_N$ we see that  $\lim_{q \to t^-} Y_q$ and $\lim_{q
  \to t^+} Y_q$ exist for every $t \in \reals_+$ on $A \in
\mathcal{F}_\infty$.  Note also that the process $Z_t$ is cadlag and
$\overline{\mathcal{F}_+}$-adapted (in fact is adapted with respect to
the smaller filtration $\mathcal{G}_t = \sigma(A, \mathcal{F}^+_t)$).

Now to see that $Z$ is a submartingale, let $0 \leq s < t < \infty$ be
arbitrary and pick decreasing sequence $t_n \in \rationals_+$
such that $t_n \downarrow t$ and a decreasing sequence $s_n \in
\rationals_+$ such that $s_n < t$ for all $n$ and  $s_n \downarrow s$.
For each $n$ and $m$ we have
$Z_{s_m} \leq \cexpectationlong{\mathcal{F}_{s_m}}{Z_{t_n}}$ by the
submartingale property of $X$ (TODO: Is there anything to say about
what happens on $A^c$?).  By the Levy Downward Theorem
\ref{JessenConditioningLimits} we know that $\lim_{m \to \infty}
\cexpectationlong{\mathcal{F}_{s_m}}{Z_{t_n}} =
\cexpectationlong{\mathcal{F}^+_{s}}{Z_{t_n}}$ and therefore $Z_s \leq \cexpectationlong{\mathcal{F}_{s}}{Z_{t_n}}$.
We know that $Z_{t_n} \toas Z_t$ but as the sequence $t_n$ is bounded
we have already shown
$Z_{t_n}$ is $L^1$-bounded.  This allows us to apply Lemma
\ref{BackwardSubmartingaleBoundedUniformlyIntegrable} to conclude that
$Z_{t_n}$ is uniformly continuous hence $Z_{t_n} \tolp{1} Z_t$ by
Lemma \ref{LpConvergenceUniformIntegrability}.  Thus we have 
\begin{align*}
Z_s &\leq \lim_{n \to \infty}
\cexpectationlong{\mathcal{F}^+_s}{Z_{t_n}} =
\cexpectationlong{\mathcal{F}^+_s}{Z_{t}} =
\cexpectationlong{\overline{\mathcal{F}}^+_s}{Z_{t}}
\end{align*}
where the last equality follows by Lemma \ref{ConditionalExpectationCompletions}.

It is worth noting that the submartingale property also holds with
respect to the smaller filtration $\mathcal{G}_t$ alluded to above; the fact that the result is
expressed in terms of the augmented filtration is due to the fact that
the augmented filtration proves to be necessary in subsequent theory.

TODO: We have not shown that $Z$ is a version/modification of $X$.
This requires additional hypotheses on the filtration.
\end{proof}

\begin{lem}\label{DoobMaximalInequalityContinuous}Let $X_t$ be a
  cadlag submartingale on $\reals_+$, then for any $t$ and $\lambda$ we have
\begin{align*}
\lambda \probability{\sup_{s \leq t} X_s \geq \lambda} &\leq
\expectation{X_t ; \sup_{s \leq t} X_s \geq \lambda} \leq \expectation{X_t^+}
\end{align*}
Furthermore if $X_t$ is non-negative then for any $p > 1$ we have
\begin{align*}
\expectation{\sup_{s \leq t} X_s} &\leq \frac{p}{p-1}\norm{X_t}_p
\end{align*}
\end{lem}
\begin{proof}
Claim 1:  For any $\omega \in \Omega$ such that
$X_t(\omega)$ is cadlag, we have
\begin{align*}
\sup_{\substack{s \leq t \\ s \in \rationals \cup \lbrace t \rbrace }}
X_s(\omega) &= \sup_{\substack{s \leq t \\ s \in \reals }}
X_s(\omega)
\end{align*}
To see this note that given any $\epsilon > 0$ we can find $s \leq t$
with $s \in \reals$ such that $X_s(\omega) > \sup_{\substack{s \leq t \\ s \in \reals }}
X_s(\omega) - \frac{\epsilon}{2}$.  By right continuity and density of
rationals, we can find
$r \in \rationals \cup \lbrace t \rbrace$ such that $s \leq r \leq t$
and $\abs{X_r(\omega) - X_s(\omega)} < \frac{\epsilon}{2}$ which by
the triangle inequality tells us that $X_r(\omega) > \sup_{\substack{s \leq t \\ s \in \reals }}
X_s(\omega) - \epsilon$.  Therefore 
\begin{align*}
\sup_{\substack{s \leq t \\ s \in \rationals \cup \lbrace t \rbrace }}
X_s(\omega) &\geq \sup_{\substack{s \leq t \\ s \in \reals }}
X_s(\omega) -\epsilon
\end{align*}  Since $\epsilon > 0$ was arbitrary we can set
it to zero to get 
\begin{align*}
\sup_{\substack{s \leq t \\ s \in \rationals \cup \lbrace t \rbrace }}
X_s(\omega) &\geq \sup_{\substack{s \leq t \\ s \in \reals }}
X_s(\omega)
\end{align*}
The opposite inequality is immediate from the definition of supremum
so the claim is verified.

By the Claim 1 and the countable index set maximal inequality (Lemma
\ref{DoobMaximalInequalityDiscrete}) we get the first result.  By
Claim 1 and the countable index set $L^p$ inequality we get the second result.
\end{proof}

\begin{lem}[Doob's $L^p$
  Inequality]\label{DoobLpInequalityContinuous}Let $X_t$ be a
  non-negative submartingle on $\reals_+$ with $X_t$ and $\mathcal{F}$ right continuous, then for all
  $p > 1$ and $0 \leq t < \infty$,
\begin{align*}
\norm{\sup_{0 \leq s \leq t} X_s}_p &\leq \frac{p}{p-1}\norm{X_t}_p
\end{align*}
\end{lem}
\begin{proof}
TODO:
\end{proof}

\begin{thm}[$L^1$ Submartingale Convergence
  Theorem]\label{MartingaleConvergenceBoundedL1Continuous}Let $X_t$ be a cadlag
  $\mathcal{F}$-submartingale on $\reals_+$ such that $\sup_{0 \leq t
    < \infty} \norm{X_t}_1 <
  \infty$ then there exists an $X \in L^1$ such that $X_t \toas X$ a.s.
\end{thm}
\begin{proof}
Restricting $X_t$ to $\rationals_+$ and applying Theorem
\ref{MartingaleConvergenceBoundedL1Discrete} we know that there
exists $X$ such that $\lim_{\substack{q \to \infty \\ q \in
    \rationals_+}} X_q = X$ almost surely.  By right continuity of $X$
we also get that $\lim_{t \to \infty} X_t = X$ almost surely (let
$\epsilon > 0$ be given, for
almost every $\omega$ we pick $N_\omega$ such that $\abs{X_q(\omega) -
  X(\omega)} \leq \epsilon$ for all $q > N_\omega$ then for any $t >
N_\omega$ we have $\abs{X_t(\omega) -
  X(\omega)} = \lim_{\substack{q \downarrow t \\ q \in \rationals_+}} \abs{X_t(\omega) -
  X(\omega)} \leq \epsilon$).
\end{proof}


NOTE: It is also true that a UI submartingale is $L^1$ convergent
but it is not true that an $L^1$ convergence submartingale must be
UI (while that is true in discrete time).  Make this into a result
somewhere and provide the counterexample (I am pretty sure Rogers and
Williams has one).

\begin{thm}[Martingale Closure
  Theorem]\label{L1MartingaleConvergenceTheoremContinuous}Let $X_t$ be
  a cadlag martingale then the following are equivalent
\begin{itemize}
\item[(i)]$X_t$ is uniformly
  integrable
\item[(ii)]there exists an integrable $X$  such that
  $X_t \tolp{1} X$
\item[(iii)]there exists an integrable $X$ such that
  $X_t = \cexpectationlong{\mathcal{F}_t}{X}$ almost surely.
\end{itemize}
\end{thm}
\begin{proof}
Given the result Theorem
\ref{MartingaleConvergenceBoundedL1Continuous}, the proof is
essentially identical to the discrete time case.  
To see (i) implies (ii) we know from Lemma
\ref{UniformIntegrabilityProperties} that $X_t$ uniformly integrable
implies $L^1$ boundedness, hence we can apply Theorem
\ref{MartingaleConvergenceBoundedL1Continuous}
to conclude the existence of an integrable $X$ such that $X_t \toas
X$.  However almost sure convergence implies convergence in
probability (Lemma \ref{ConvergenceAlmostSureImpliesInProbability})
which together with uniform integrability implies $X_t \tolp{1} X$
(Lemma \ref{LpConvergenceUniformIntegrability}).

To see that (ii) implies (iii) from $X_t \tolp{1} X$ we get for any
$\sigma$-algebra $\mathcal{G}$,
\begin{align*}
\lim_{t \to \infty}
\norm{\cexpectationlong{\mathcal{G}}{X_t}
    -\cexpectationlong{\mathcal{G}}{X} }_1 &\leq
\lim_{t \to \infty}\expectation{\cexpectationlong{\mathcal{G}}{\abs{
      X_t- X}}} = \lim_{t \to \infty}\expectation{\abs{X_t -X}} = 0
\end{align*}
and therefore for any fixed $s \geq 0$ and the martingale property
$X_s = \cexpectationlong{\mathcal{F}_s}{X_t}$ a.s. we have
\begin{align*}
\norm{X_s - \cexpectationlong{\mathcal{F}_s}{X}}_1 &\leq \lim_{t \to
  \infty} \norm{X_s -
  \cexpectationlong{\mathcal{F}_s}{X_t}}_1  + \lim_{t \to
  \infty} \norm{\cexpectationlong{\mathcal{F}_s}{X_t}-
  \cexpectationlong{\mathcal{F}_s}{X}}_1 \\
&= \lim_{t \to
  \infty} \norm{\cexpectationlong{\mathcal{F}_s}{X_t}-
  \cexpectationlong{\mathcal{F}_s}{X}}_1=0
\end{align*}
and we get that $X_s = \cexpectationlong{\mathcal{F}_s}{X}$ a.s.

To see that (ii) implies (iii), we simply invoke Corollary \ref{ConditionalExpectationsUniformlyIntegrable}.
\end{proof}

\begin{thm}\label{OptionalSamplingContinuous}Let $X_t$ be an $\mathcal{F}$-submartingale on $\reals_+$
  with $X_t$ and $\mathcal{F}$ right continuous, let $\sigma$ and
  $\tau$ be optional times with $\tau$ bounded, then $X_\tau$ is
  integrable and
\begin{align*}
\cexpectationlong{\mathcal{F}_\sigma}{X_\tau} &\leq X_{\tau \wedge
  \sigma} \text{ a.s.}
\end{align*}
\end{thm}
\begin{proof}
For each $n>0$, restrict $X$ to the dyadic rationals $X_{k/2^n}$ on the
filtration $\mathcal{F}_{k/2^n}$.  Is is immediate that this is a
discrete submartingale.

Define the discrete approximations of optional times $\tau_n =
\frac{1}{2^n} \floor{2^n \tau + 1}$ and $\sigma_n = \frac{1}{2^n}
\floor{2^n \sigma + 1}$ so that $\tau_n$ and $\sigma_n$ are optional
times such that $\tau_n \downarrow \tau$ and $\sigma_n \downarrow
\sigma$ (Lemma \ref{DiscreteApproximationOptionalTimes}) and
furthermore $\mathcal{F}_\sigma = \cap_n \mathcal{F}_{\sigma_n}$ (Lemma
\ref{InfSupStoppedFiltration} and right
continuity of $\mathcal{F}$).
We can now apply the Optional Sampling Theorem Corollary
\ref{OptionalSamplingSubmartingaleDiscrete} to conclude that for each
$m,n>0$,
\begin{align*}
\cexpectationlong{\mathcal{F}_{\sigma_m}}{X_{\tau_n}} &\geq X_{\tau_n \wedge
  \sigma_m} \text{ a.s.}
\end{align*}
Now holding $n$ fixed we note that since $\sigma_m$ is decreasing in
$m$ we have $\mathcal{F}_{\sigma_1} \supset
\mathcal{F}_{\sigma_{2}} \supset \cdots$ and therefore we can apply
the downward Levy-Jessen Theorem \ref {JessenConditioningLimits} and
right continuity of $X_t$ to
conclude 
\begin{align*}
\cexpectationlong{\mathcal{F}_{\sigma}}{X_{\tau_n}} &=\lim_{m \to
  \infty} \cexpectationlong{\mathcal{F}_{\sigma_m}}{X_{\tau_n}}  \geq
\lim_{m \to \infty} X_{\tau_n \wedge  \sigma_m} 
= X_{\tau_n \wedge \sigma} \text{ a.s.}
\end{align*}

Now we need to justify taking the limit $n \to \infty$.  To do this,
we claim that the sequence of random variable $X_{\tau_n}$ 
is a backward submartingale; that is to say if we consider
$X_{\tau_{-n}}$ and the filtration
$\mathcal{F}_{\tau_{-n}}$ for every $n < 0$
then $X_{\tau_{-n}}$ is an $\mathcal{F}_{\tau_{-n}}$-submartingale on
$\integers_-$.  The fact that $X_{\tau_{-n}}$ is a submartingale
follows from the fact that $\tau_n$ is decreasing and Optional
Sampling (Corollary \ref{OptionalSamplingSubmartingaleDiscrete})
together
with our awkward indexing
\begin{align*}
\cexpectationlong{\mathcal{F}_{\tau_{-(n-1)}}}{X_{\tau_{-n}}} &\geq
X_{\tau_{-(n-1)}} \text{ a.s.}
\end{align*}
Furthermore we can show that $\expectation{X_{\tau_n}}$ is bounded
because $\tau$ is bounded.
If we pick $T > 0$ such that $0 \leq \tau \leq T$ then we have an upper bound
\begin{align*}
\expectation{X_{\tau_{n}}}
&=\expectation{\cexpectationlong{\mathcal{F}_{\tau_n}}{X_{T}}} =
\expectation{X_T} < \infty
\end{align*}
and a lower bound from 
\begin{align*}
-\infty &< \expectation{X_0} \leq
\expectation{\cexpectationlong{\mathcal{F}_0}{X_{\tau_n}}} = \expectation{X_{\tau_n}}
\end{align*}
By Lemma \ref{BackwardSubmartingaleBoundedUniformlyIntegrable} we can
now conclude that $X_{\tau_{n}}$ is uniformly integrable.
So now we pick $A \in \mathcal{F}_{\sigma}$ and by right continuity of
$X_t$ we have
\begin{align*}
\lim_{n \to \infty} X_{\tau_n} \characteristic{A} = X_\tau
\characteristic{A} \text{ a.s.}
\end{align*}
and
\begin{align*}
\lim_{n \to \infty} X_{\tau_n \wedge \sigma} \characteristic{A} =
X_{\tau \wedge \sigma}
\characteristic{A} \text{ a.s.}
\end{align*}
and therefore by uniform integrability and Lemma
\ref{LpConvergenceUniformIntegrability} we get
\begin{align*}
\expectation{X_\tau ; A} &= \lim_{n \to \infty}
\expectation{X_{\tau_n} ; A} \geq 
\lim_{n \to \infty} \expectation{X_{\tau_n \wedge \sigma} ; A} =
\expectation{X_{\tau \wedge \sigma} ; A}
\end{align*}
which shows $\cexpectationlong{\mathcal{F}_\sigma}{X_\tau} \geq
X_{\tau \wedge \sigma}$ a.s. by the defining property and monotonicity
of conditional expectation.
\end{proof}

\subsection{Progressive Measurability}
For many applications the notion of an adapted process
suffices.  However when dealing with continuous time processes there
are anomalies that can occur with such processes that are inconvenient
and it is best to define a stronger notion of measurability.  To
understand the issue we're trying to address, note that adaptedness only addresses the behavior of
$X_t(\omega)$ as a function of $\omega$ for fixed $t$.  If we take the
sample path point of view and think of $X_t(\omega)$ as a function of
$t$ for fixed $\omega$ then there little constraint on how horribly it
can behave.  In fact the general definition of a process allows $T$ to
be an arbitrary set so it isn't even in scope to talk about an type of
regularity of sample paths.  

For the special case of processes indexed by $\reals_+$ we can discuss
measurability, continuity and even differentiability of sample paths.
For the moment, there is a
very mild restriction that we make.

\begin{defn}Let $(\Omega, \mathcal{A})$, $(S, \mathcal{S})$  and
  $(T, \mathcal{T})$  be measurable spaces.  A process $X$ on $T$ with
  values in $S$ is said to be \emph{jointly
    measurable} or simply \emph{measurable} if $X : \Omega \times T
  \to S$ is $\mathcal{A} \otimes \mathcal{T}/\mathcal{S}$ measurable.
\end{defn}


\begin{lem}\label{JointMeasurabilityOfSampleContinuous}Let $S$ be a metric space and let $T$ be a separable metric
  space both given the Borel $\sigma$-algebra.  Suppose a process $X$
  on $T$ with values in $S$ has continuous sample paths, then $X$ is
  jointly measurable.
\end{lem}
\begin{proof}
Let $\lbrace t_n \rbrace$ be a countable dense set of points in $T$.
We use this dense set to provide a sequence of approximations to $X$.
To this end, for each $n\geq 1$ and $k \geq 1$ define
\begin{align*}
B_{n,k} &= \lbrace t \in T \mid d(t, t_k) < 1/n \rbrace \\
V_{n,k} &= B_{n,k} \setminus \cup_{j=1}^{k-1} B_{n,j}
\end{align*}
Clearly the $V_{n,k}$ are Borel measurable since the $B_{n,k}$ are open.
By construction they are disjoint and by density of the $t_k$ they
cover $T$.  Define
\begin{align*}
X^n_t (\omega) &= X_{t_k}(\omega) \text{ for $t \in V_{n,k}$}
\end{align*}
Since for any $A \in \mathcal{B}(S)$ we have $\lbrace (\omega, t) \mid
X^n_{t}(\omega) \in A \rbrace = \cup_{k=1}^\infty V_{n,k} \times \lbrace
X_{t_k} \in A \rbrace$ we see that $X^n$ is jointly measurable.

Now by density of $X^n$ and the continuity of sample paths of $X$ we
have $\lim_{n \to \infty} X^n = X$ and joint measurability of $X$
follows from Lemma \ref{LimitsOfMeasurableMetricSpace}.
\end{proof}

\begin{defn}A process $X$ is said to be \emph{progressively
    measurable} or simply \emph{progressive} if for every $t$, the restriction of $X$ to the time
  interval $[0,t]$, $X : \Omega \times [0,t] \to S$ is
  $\mathcal{F}_t \otimes \mathcal{B}([0,t])$ measurable.
\end{defn}

\begin{defn}The set of \emph{progressively measurable sets} is
  defined as
\begin{align*}
\mathcal{P}\mathcal{M} = \lbrace A \subset \Omega
  \times \reals_+ \mid A \cap \Omega \times [0,t] \in \mathcal{F}_t
  \otimes \mathcal{B}([0,t]) \text{ for all } t \geq 0\rbrace
\end{align*}
\end{defn}

\begin{lem}\label{ProgressivelyMeasurableProcesses}The set $\mathcal{P}\mathcal{M}$ is a sub $\sigma$-algebra
  of $\mathcal{A} \otimes \mathcal{B}(\reals_+)$.  A process $X :
  \Omega \times \reals_+ \to S$ is progressive if and only if $X$ is
  $\mathcal{P}\mathcal{M}$-measurable, in particular a progressive
  process is jointly measurable.
\end{lem}
\begin{proof}
Since for all $t\geq 0$, $\Omega \times \reals_+ \cap \Omega \times
[0,t] = \Omega \times [0,t] \in \mathcal{F}_t  \otimes
\mathcal{B}([0,t])$ we have $\Omega \times \reals_+ \in
\mathcal{P}\mathcal{M}$.  Suppose $A \in \mathcal{P}\mathcal{M}$ and
then note by the elementary set theory equality $B^c \cap C = (B \cap
C)^c \cap C$ and the fact that $\mathcal{F}_t \otimes
\mathcal{B}([0,t])$ is a $\sigma$-algebra
\begin{align*}
A^c \cap \Omega \times [0,t] &= (A \cap \Omega \times [0,t])^c \cap
\Omega \times [0,t] \in \mathcal{F}_t \otimes \mathcal{B}([0,t])
\end{align*}
thus showing $\mathcal{P}\mathcal{M}$ is closed under set complement.
Lastly if we assume that $A_1, A_2, \dots \in \mathcal{P}\mathcal{M}$,
then clearly  for every $t \geq 0$,
\begin{align*}
\left (\cap_n A_n \right )\cap \Omega \times [0,t] &= \cap_n \left (
  A_n \cap \Omega \times [0,t]\right ) \in \mathcal{F}_t \otimes \mathcal{B}([0,t])
\end{align*}
so we see that $\mathcal{P}\mathcal{M}$ is a $\sigma$-algebra.

To see that $\mathcal{P}\mathcal{M}$ is a sub $\sigma$-algebra of
$\mathcal{A} \otimes \mathcal{B}(\reals_+)$, 
if for $A \in \mathcal{P}\mathcal{M}$ we define $A_n = A \cap \Omega
\times [0,n]$ then by definition of $\mathcal{P}\mathcal{M}$ we know
$A_n \in \mathcal{F}_n \otimes \mathcal{B}([0,n]) \subset \mathcal{A}
\otimes \mathcal{B}(\reals_+)$.  But we can write $A = \cup_n A_n $
thus showing $A \in \mathcal{A}
\otimes \mathcal{B}(\reals_+)$.

To see the characterization of progressive processes, assume $X$ is a
process and that $A
\in \mathcal{S}$ and observe
\begin{align*}
\lbrace X \in A \rbrace \cap \Omega \times [0,t] &= \lbrace (\omega,
s) \in \Omega \times [0,t] \mid X_s(\omega) \in A \rbrace
\end{align*}
which shows that $X$ is progressive if and only if it is $\mathcal{P}\mathcal{M}$-measurable.
\end{proof}

\begin{examp}The following is an example of a measurable adapted process that is not
  progressively measurable.  Take $\Omega=[0,1]$ and $S = \reals$ all
  supplied with the Borel $\sigma$-algebra and Lebesgue measure.
Let $A \subset [0,1]$ be non-measurable.  Define
\begin{align*}
X_t(\omega) &= \begin{cases}
t + \omega & \text{for $t \in A$} \\
-t - \omega & \text{for $t \notin A$}
\end{cases}
\end{align*}
with filtration defined by $\mathcal{F}_t = \mathcal{B}([0,1])$. 
(Note that for every $t\geq 0$, $\sigma(X_t) =
\mathcal{B}([0,1])$ hence this is the filtration induced by $X$).   It
is easy to see that this is a process (i.e. is measurable) since for
each fixed $t$, $X_t : [0,1] \to \reals$ is continuous hence measurable.
However that $\lbrace (\omega,s) \mid X_s(\omega) \geq 0\rbrace = \Omega
\times A$ hence is not measurable thus showing that $X$ is not
progressively measurable.

There is the simpler example but the current example also provides an
example of the type of anomaly that can occur.

Define a random time 
\begin{align*}
\tau(\omega) &= \inf \lbrace t \mid 2t \geq \abs{X_t(\omega)} \rbrace = \inf \lbrace t
\mid 2t \geq t + \omega \rbrace = \inf \lbrace t
\mid t \geq \omega \rbrace = \omega
\end{align*}
which because $\lbrace \tau \leq t \rbrace = [0,t] \in
\mathcal{B}([0,1])$ is seen to be an optional time.
Because $\mathcal{F}_t = \mathcal{B}([0,1])$ we see that for every
Borel measurable $A$, $A \cap \lbrace \tau \leq t \rbrace = A \cap
[0,t] \in \mathcal{F}_t$ so we also have $\mathcal{F}_\tau =
\mathcal{B}([0,1])$.  On the other hand, the stopped process
\begin{align*}
X_\tau(\omega) &= \begin{cases}
2\omega & \text{if $\omega \in A$} \\
-2\omega & \text{if $\omega \notin A$} \\
\end{cases}
\end{align*}
and again we see that $\lbrace X_\tau > 0 \rbrace = A$  is not
$\mathcal{F}_\tau$-measurable.
\end{examp}

Note that because sections are measurable (Lemma
\ref{MeasurableSections}) a progressively measurable process is
adapted.

\begin{lem}\label{ContinuityAndProgressiveMeasurability}Let $X$ be a
  process on $\reals_+$ with values in a metric space $(S,
  \mathcal{B}(S))$ adapated to the filtration
  $\mathcal{F}$.  Suppose $X$ has left or right continuous sample
  paths, then $X$ is $\mathcal{F}$-progressively measurable.
\end{lem}
\begin{proof}
The proof is analogous to Lemma
\ref{JointMeasurabilityOfSampleContinuous}.  We give the proof for
right continuous sample paths with the case of left continuous sample
paths being very similar.

Let $t \geq 0$ be given $X^n$ be the process on $[0,t]$ be defined by $X^n_s(\omega) =
X_{\frac{k+1}{2^n} \wedge t}(\omega)$ for $\frac{k}{2^n} < s \leq \frac{k+1}{2^n} \wedge t$.
It is clear that for any $A \in \mathcal{B}(S)$ we have
\begin{align*}
&\lbrace (\omega, s) \mid 0 \leq s \leq t ; X^n_s(\omega) \in A \rbrace \\
&= \bigcup_{k=0}^{\floor{2^n t}} \lbrace (\omega, s) \mid 0 \leq s \leq t ; \frac{k}{2^n} < s \leq \frac{k+1}{2^n}
\wedge t ;X_{\frac{k+1}{2^n} \wedge t}(\omega) \in A \rbrace \\
&=\bigcup_{k=0}^{\floor{2^n t}} \lbrace X_{\frac{k+1}{2^n} \wedge t}(\omega) \in A \rbrace
\times (\frac{k}{2^n} < s \leq \frac{k+1}{2^n}] \in \mathcal{F}_t \otimes \mathcal{B}[0,t]\\
\end{align*}
which shows that $X^n$ is progressively measurable.  By right
continuity, we see that $\lim_{n \to \infty} X^n = X\mid_{\Omega
  \times [0,t]}$ and therefore by Lemma
  \ref{LimitsOfMeasurableMetricSpace} we have $X$ is progressively measurable.
\end{proof}

\begin{lem}\label{StoppedProgressivelyMeasurableProcess}Let $X$ be an $\mathcal{F}$-progressively measurable process on
  $\reals_+$ with values in a measurable space $(S, \mathcal{S})$ and
  let $\tau$ be an $\mathcal{F}$-optional time, then $X_\tau$ is
  $\mathcal{F}_\tau$-measurable.  Moreover, the stopped process $X^\tau$ is
$\mathcal{F}$-progressively measurable, in particular $X_{\tau \wedge
  t}$ is  $\mathcal{F}_t$ measurable for all $t \geq 0$.
\end{lem}
\begin{proof}
We first claim that if we can prove $X_{\tau \wedge t}$ is
$\mathcal{F}_t$-measurable for all $t \geq 0$ then it follows that
$X_\tau$ is $\mathcal{F}_\tau$-measurable.  This follows from picking
a measurable set $A \in \mathcal{S}$ and noting that
\begin{align*}
\lbrace \tau \leq t \rbrace \cap \lbrace X_\tau \in A \rbrace &=
\lbrace \tau \leq t \rbrace \cap \lbrace X_{\tau \wedge t} \in A \rbrace
\end{align*}
which is $\mathcal{F}_t$ since $\tau$ is $\mathcal{F}$-optional and we
have assumed $\lbrace X_{\tau \wedge t} \in A \rbrace \in
\mathcal{F}_t$.

To see that $X^{\tau}$ is an $\mathcal{F}$-progressively
measurable process, pick a $t \geq 0$ and consider the restriction of
$X^{\tau}$ to $\Omega \times [0,t]$.   Note that by replacing $\tau$
with $\tau \wedge t$, we can assume that $\tau \leq t$ which implies
$\tau$ is $\mathcal{F}_t$-measurable (to see this note that for $s
\leq t$, $\lbrace \tau \leq s \rbrace \in \mathcal{F}_s \subset
\mathcal{F}_t$ and for $s > t$,
$\lbrace \tau \leq s \rbrace = \Omega$). 
Now we can factor the restriction of
$X_{\tau \wedge t}$ to $\Omega \times [0,t]$ as $X^{\tau} =
X\mid_{\Omega \times [0,t]} \circ T^t$ where
$T^t : \Omega \times [0,t] \to \Omega \times [0,t]$ is defined by
$T^t(\omega, s) = (\omega, \tau(\omega) \wedge s)$.   We claim that $T^t$ is
$\mathcal{F}_t \otimes \mathcal{B}([0,t])$-measurable.  This follows
from the $\mathcal{F}_t \otimes \mathcal{B}([0,t])$-measurability of
$(\omega, s) \mapsto \tau(\omega) \wedge s$ which follows by noting
that for every $0 \leq u \leq t$,
\begin{align*}
\lbrace \tau \wedge s \leq u \rbrace &= \lbrace \tau \leq u \rbrace
\times [0,t] \cup \Omega \times [0,u] \in \mathcal{F}_t \otimes \mathcal{B}([0,t])
\end{align*}
As $X\mid_{\Omega \times [0,t]}$ is $\mathcal{F}_t \otimes
\mathcal{B}([0,t])/\mathcal{S}$-measurable by progressive
measurability of $X$, the claim follows from Lemma
\ref{CompositionOfMeasurable}.  The fact that $X_{\tau \wedge t}$ is
$\mathcal{F}_t$-measurable for all $t \geq 0$ follows from the fact
that progressive measurability implies adaptedness.
\end{proof}